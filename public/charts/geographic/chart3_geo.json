{
    "config": {
        "view": {
            "continuousWidth": 300,
            "continuousHeight": 300
        },
        "font": "Helvetica Neue"
    },
    "vconcat": [
        {
            "mark": {
                "type": "circle"
            },
            "encoding": {
                "color": {
                    "field": "organization_categorization",
                    "scale": {
                        "range": [
                            "#ECB75B",
                            "#39758D",
                            "#334EAD",
                            "#77BEFC"
                        ]
                    },
                    "title": "Organization type",
                    "type": "nominal"
                },
                "opacity": {
                    "condition": [
                        {
                            "param": "param_3",
                            "value": 0.9
                        }
                    ],
                    "value": 0.2
                },
                "shape": {
                    "field": "export_bin",
                    "sort": [
                        "Low",
                        "Medium",
                        "High"
                    ],
                    "title": "Export control level",
                    "type": "nominal"
                },
                "size": {
                    "field": "pub_bin",
                    "scale": {
                        "range": [
                            80,
                            220
                        ]
                    },
                    "sort": [
                        "Low",
                        "Medium",
                        "High"
                    ],
                    "title": "Publication intensity",
                    "type": "nominal"
                },
                "tooltip": [
                    {
                        "field": "model",
                        "title": "Model",
                        "type": "nominal"
                    },
                    {
                        "field": "country_first",
                        "title": "Country",
                        "type": "nominal"
                    },
                    {
                        "field": "organization_categorization",
                        "title": "Org type",
                        "type": "nominal"
                    },
                    {
                        "field": "year",
                        "title": "Year",
                        "type": "quantitative"
                    },
                    {
                        "field": "publication_count",
                        "title": "Publication count",
                        "type": "quantitative"
                    },
                    {
                        "field": "export_controls_sum",
                        "title": "Export control score",
                        "type": "quantitative"
                    },
                    {
                        "field": "parameters",
                        "title": "Parameters",
                        "type": "quantitative"
                    },
                    {
                        "field": "training_compute_(flop)",
                        "title": "Training compute (FLOPs)",
                        "type": "quantitative"
                    }
                ],
                "x": {
                    "field": "log_params",
                    "title": "Model size (Log parameters)",
                    "type": "quantitative"
                },
                "y": {
                    "field": "log_compute",
                    "title": "Training compute (Log FLOPs)",
                    "type": "quantitative"
                }
            },
            "height": 250,
            "name": "view_3",
            "transform": [
                {
                    "filter": "(datum.country_first === country_param)"
                }
            ],
            "width": 800
        },
        {
            "hconcat": [
                {
                    "layer": [
                        {
                            "mark": {
                                "type": "boxplot",
                                "size": 35
                            },
                            "encoding": {
                                "color": {
                                    "field": "export_bin",
                                    "scale": {
                                        "domain": [
                                            "Low",
                                            "Medium",
                                            "High"
                                        ],
                                        "range": [
                                            "#9ECAE1",
                                            "#4292C6",
                                            "#08519C"
                                        ]
                                    },
                                    "sort": [
                                        "Low",
                                        "Medium",
                                        "High"
                                    ],
                                    "title": "Export control level",
                                    "type": "nominal"
                                },
                                "x": {
                                    "axis": {
                                        "labelAngle": 0
                                    },
                                    "field": "export_bin",
                                    "scale": {
                                        "domain": [
                                            "Low",
                                            "Medium",
                                            "High"
                                        ]
                                    },
                                    "sort": [
                                        "Low",
                                        "Medium",
                                        "High"
                                    ],
                                    "title": "Export control level",
                                    "type": "nominal"
                                },
                                "y": {
                                    "field": "log_compute",
                                    "title": "Training compute (Log FLOPs)",
                                    "type": "quantitative"
                                }
                            },
                            "transform": [
                                {
                                    "filter": "(datum.country_first === country_param)"
                                }
                            ]
                        },
                        {
                            "mark": {
                                "type": "circle",
                                "opacity": 0,
                                "size": 30
                            },
                            "encoding": {
                                "tooltip": [
                                    {
                                        "field": "model",
                                        "title": "Model",
                                        "type": "nominal"
                                    },
                                    {
                                        "field": "organization_categorization",
                                        "title": "Organization",
                                        "type": "nominal"
                                    },
                                    {
                                        "field": "publication_count",
                                        "title": "Publication count",
                                        "type": "quantitative"
                                    },
                                    {
                                        "field": "export_controls_sum",
                                        "title": "Export control score",
                                        "type": "quantitative"
                                    },
                                    {
                                        "field": "training_compute_(flop)",
                                        "title": "Training compute (FLOPs)",
                                        "type": "quantitative"
                                    }
                                ],
                                "x": {
                                    "field": "export_bin",
                                    "sort": [
                                        "Low",
                                        "Medium",
                                        "High"
                                    ],
                                    "type": "nominal"
                                },
                                "y": {
                                    "field": "log_compute",
                                    "type": "quantitative"
                                }
                            },
                            "transform": [
                                {
                                    "filter": "(datum.country_first === country_param)"
                                }
                            ]
                        }
                    ],
                    "height": 260,
                    "title": {
                        "text": "Training compute by export-control level",
                        "anchor": "start"
                    },
                    "width": 400
                },
                {
                    "mark": {
                        "type": "bar"
                    },
                    "encoding": {
                        "color": {
                            "field": "export_bin",
                            "scale": {
                                "domain": [
                                    "Low",
                                    "Medium",
                                    "High"
                                ],
                                "range": [
                                    "#9ECAE1",
                                    "#4292C6",
                                    "#08519C"
                                ]
                            },
                            "type": "nominal"
                        },
                        "tooltip": [
                            {
                                "field": "export_bin",
                                "title": "Export level",
                                "type": "nominal"
                            },
                            {
                                "aggregate": "count",
                                "title": "Models in year",
                                "type": "quantitative"
                            }
                        ],
                        "x": {
                            "axis": {
                                "labelAngle": 0
                            },
                            "field": "export_bin",
                            "sort": [
                                "Low",
                                "Medium",
                                "High"
                            ],
                            "title": "Export control level",
                            "type": "nominal"
                        },
                        "y": {
                            "aggregate": "count",
                            "title": "Number of models",
                            "type": "quantitative"
                        }
                    },
                    "height": 280,
                    "title": {
                        "text": "Model counts by export level",
                        "anchor": "start"
                    },
                    "transform": [
                        {
                            "filter": "(datum.country_first === country_param)"
                        }
                    ],
                    "width": 350
                }
            ]
        }
    ],
    "data": {
        "name": "data-70b97beeb2ee88e8da0393dd2505d35a"
    },
    "params": [
        {
            "name": "country_param",
            "bind": {
                "input": "select",
                "options": [
                    "United States",
                    "China",
                    "Germany",
                    "France",
                    "United Kingdom",
                    "Spain",
                    "Japan"
                ],
                "name": "Country: "
            },
            "value": "United States"
        },
        {
            "name": "param_3",
            "select": {
                "type": "point",
                "fields": [
                    "organization_categorization"
                ]
            },
            "bind": "legend",
            "views": [
                "view_3"
            ]
        }
    ],
    "resolve": {
        "legend": {
            "size": "independent"
        },
        "scale": {
            "color": "independent"
        }
    },
    "title": {
        "text": "China produces large models with low export controls and high publication intensity",
        "anchor": "start",
        "dx": 10,
        "dy": -10,
        "fontSize": 16,
        "subtitle": "Comparatively, the United States produces large models but varies in export controls and publication intensity",
        "subtitleFontSize": 12,
        "subtitlePadding": 6
    },
    "$schema": "https://vega.github.io/schema/vega-lite/v5.20.1.json",
    "datasets": {
        "data-70b97beeb2ee88e8da0393dd2505d35a": [
            {
                "model": "Odyssey 102B",
                "domain": "Biology",
                "task": "Protein or nucleotide language model (pLM/nLM),Protein generation",
                "organization": "Anthrogen",
                "authors": "Ankit Singhal, Shyam Venkatasubramanian, Sean Moushegian, Steven Strutt, Michael Lin, Connor Lee",
                "publication_date": "2025-10-18",
                "reference": "Odyssey: reconstructing evolution through emergent consensus in the global proteome",
                "link": "https://www.biorxiv.org/content/10.1101/2025.10.15.682677v1",
                "citations": null,
                "notability_criteria": null,
                "parameters": 102000000000.0,
                "training_compute_(flop)": 1.1e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-20 17:25:36+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Reported",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Biology",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 11.008600171761918,
                "log_compute": 23.041392685158225,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "Ling-1T",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Quantitative reasoning,Code generation,Mathematical reasoning",
                "organization": "Ant Group",
                "authors": "Unknown",
                "publication_date": "2025-10-10",
                "reference": "Ling-1T",
                "link": "https://huggingface.co/inclusionAI/Ling-1T",
                "citations": null,
                "notability_criteria": "SOTA improvement",
                "parameters": 1000000000000.0,
                "training_compute_(flop)": 6.000001e+24,
                "training_dataset_size_(gradients)": 20000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 15:49:57+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 12.0,
                "log_compute": 24.77815132276605,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Granite-4.0-H-Tiny",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Text summarization,Text classification,Retrieval-augmented generation,Code generation",
                "organization": "IBM",
                "authors": "Unknown",
                "publication_date": "2025-10-02",
                "reference": "IBM Granite 4.0: hyper-efficient, high performance hybrid models for enterprise",
                "link": "https://www.ibm.com/new/announcements/ibm-granite-4-0-hyper-efficient-high-performance-hybrid-models#:~:text=We're%20launching%20Granite%204,costs%20compared%20to%20conventional%20LLMs.",
                "citations": null,
                "notability_criteria": null,
                "parameters": 7000000000.0,
                "training_compute_(flop)": 1.35e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA GB200 NVL2 (per GPU)",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-08 21:01:36+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 9.845098040014257,
                "log_compute": 23.130333768495007,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "Granite-4.0-H-Micro",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Text summarization,Text classification,Retrieval-augmented generation,Code generation",
                "organization": "IBM",
                "authors": "Unknown",
                "publication_date": "2025-10-02",
                "reference": "IBM Granite 4.0: hyper-efficient, high performance hybrid models for enterprise",
                "link": "https://www.ibm.com/new/announcements/ibm-granite-4-0-hyper-efficient-high-performance-hybrid-models#:~:text=We're%20launching%20Granite%204,costs%20compared%20to%20conventional%20LLMs.",
                "citations": null,
                "notability_criteria": null,
                "parameters": 3000000000.0,
                "training_compute_(flop)": 3.15e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA GB200 NVL2 (per GPU)",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-08 21:01:53+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 9.477121254719663,
                "log_compute": 23.4983105537896,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "Granite-4.0-H-Small",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Text summarization,Text classification,Retrieval-augmented generation,Code generation",
                "organization": "IBM",
                "authors": "Unknown",
                "publication_date": "2025-10-02",
                "reference": "IBM Granite 4.0: hyper-efficient, high performance hybrid models for enterprise",
                "link": "https://www.ibm.com/new/announcements/ibm-granite-4-0-hyper-efficient-high-performance-hybrid-models#:~:text=We're%20launching%20Granite%204,costs%20compared%20to%20conventional%20LLMs.",
                "citations": null,
                "notability_criteria": null,
                "parameters": 32000000000.0,
                "training_compute_(flop)": 1.215e+24,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA GB200 NVL2 (per GPU)",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-08 21:01:02+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 10.505149978319906,
                "log_compute": 24.08457627793433,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "GLM 4.6",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Code generation,Quantitative reasoning,System control",
                "organization": "Zhipu AI,Tsinghua University",
                "authors": "Unknown",
                "publication_date": "2025-09-30",
                "reference": "GLM-4.6: Advanced Agentic, Reasoning and Coding Capabilities",
                "link": "https://z.ai/blog/glm-4.6",
                "citations": null,
                "notability_criteria": "Discretionary",
                "parameters": 357000000000.0,
                "training_compute_(flop)": 4.42e+24,
                "training_dataset_size_(gradients)": 23000000000000.0,
                "training_time_(hours)": 2880.0,
                "training_hardware": null,
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-22 15:17:51+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 11.552668216112194,
                "log_compute": 24.64542226934909,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "DeepSeek-V3.2-Exp",
                "domain": "Language",
                "task": "Language modeling/generation,Code generation,Quantitative reasoning,Question answering,Search,System control,Instruction interpretation",
                "organization": "DeepSeek",
                "authors": "Unknown",
                "publication_date": "2025-09-29",
                "reference": "Introducing DeepSeek-V3.2-Exp",
                "link": "https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/DeepSeek_V3_2.pdf",
                "citations": null,
                "notability_criteria": null,
                "parameters": 671000000000.0,
                "training_compute_(flop)": 3.8035594e+24,
                "training_dataset_size_(gradients)": 943700000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": "DeepSeek-V3.1-Terminus",
                "finetune_compute_(flop)": 2.095014e+23,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 16:58:23+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 11.826722520168993,
                "log_compute": 24.580190202999553,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "SimpleFold",
                "domain": "Biology",
                "task": "Protein folding prediction",
                "organization": "Apple",
                "authors": "Yuyang Wang, Jiarui Lu, Navdeep Jaitly, Josh Susskind, Miguel Angel Bautista",
                "publication_date": "2025-09-23",
                "reference": "SimpleFold: Folding Proteins is Simpler than You Think",
                "link": "https://arxiv.org/abs/2509.18480v1",
                "citations": null,
                "notability_criteria": null,
                "parameters": 3000000000.0,
                "training_compute_(flop)": 2e+21,
                "training_dataset_size_(gradients)": 2227200000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": 141.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-17 18:20:45+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Reported,Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Biology",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 9.477121254719663,
                "log_compute": 21.30102999566398,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "DeepSeek-V3.1-Terminus",
                "domain": "Language",
                "task": "Language modeling/generation,Code generation,Quantitative reasoning,Question answering,Search,System control,Instruction interpretation",
                "organization": "DeepSeek",
                "authors": "Unknown",
                "publication_date": "2025-09-22",
                "reference": "The latest update builds on V3.1\u2019s strengths while addressing key user feedback.",
                "link": "https://api-docs.deepseek.com/news/news250922",
                "citations": null,
                "notability_criteria": null,
                "parameters": 671000000000.0,
                "training_compute_(flop)": 3.594058e+24,
                "training_dataset_size_(gradients)": 839000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": "DeepSeek-V3",
                "finetune_compute_(flop)": 1.86258e+23,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 16:58:23+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 11.826722520168993,
                "log_compute": 24.555585081364136,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Qwen3-Omni-Flash",
                "domain": "Multimodal,Language,Vision,Speech,Video",
                "task": "Language modeling/generation,Question answering,Visual question answering,Image captioning,Video description,Speech recognition (ASR),Speech synthesis,Speech-to-text,Text-to-speech (TTS)",
                "organization": "Alibaba",
                "authors": "Jin Xu, Zhifang Guo, Hangrui Hu, Yunfei Chu, Xiong Wang, Jinzheng He, Yuxuan Wang, Xian Shi, Ting He, Xinfa Zhu, Yuanjun Lv, Yongqi Wang, Dake Guo, He Wang, Linhan Ma, Pei Zhang, Xinyu Zhang, Hongkun Hao, Zishan Guo, Baosong Yang, Bin Zhang, Ziyang Ma, Xipin Wei, Shuai Bai, Keqin Chen, Xuejing Liu, Peng Wang, Mingkun Yang, Dayiheng Liu, Xingzhang Ren, Bo Zheng, Rui Men, Fan Zhou, Bowen Yu, Jianxin Yang, Le Yu, Jingren Zhou, Junyang Lin",
                "publication_date": "2025-09-22",
                "reference": "Qwen3-Omni Technical Report",
                "link": "https://arxiv.org/abs/2509.17765",
                "citations": null,
                "notability_criteria": null,
                "parameters": 35300000000.0,
                "training_compute_(flop)": 3.6e+22,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-03 19:31:18+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Multimodal",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 10.547774705387823,
                "log_compute": 22.556302500767288,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Qwen3-Omni-30B-A3B",
                "domain": "Multimodal,Language,Vision,Speech,Video",
                "task": "Language modeling/generation,Question answering,Visual question answering,Image captioning,Video description,Speech recognition (ASR),Speech synthesis,Speech-to-text,Text-to-speech (TTS)",
                "organization": "Alibaba",
                "authors": "Jin Xu, Zhifang Guo, Hangrui Hu, Yunfei Chu, Xiong Wang, Jinzheng He, Yuxuan Wang, Xian Shi, Ting He, Xinfa Zhu, Yuanjun Lv, Yongqi Wang, Dake Guo, He Wang, Linhan Ma, Pei Zhang, Xinyu Zhang, Hongkun Hao, Zishan Guo, Baosong Yang, Bin Zhang, Ziyang Ma, Xipin Wei, Shuai Bai, Keqin Chen, Xuejing Liu, Peng Wang, Mingkun Yang, Dayiheng Liu, Xingzhang Ren, Bo Zheng, Rui Men, Fan Zhou, Bowen Yu, Jianxin Yang, Le Yu, Jingren Zhou, Junyang Lin",
                "publication_date": "2025-09-22",
                "reference": "Qwen3-Omni Technical Report",
                "link": "https://arxiv.org/abs/2509.17765",
                "citations": null,
                "notability_criteria": "SOTA improvement",
                "parameters": 35300000000.0,
                "training_compute_(flop)": 3.6e+22,
                "training_dataset_size_(gradients)": 2000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-03 19:30:25+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "China",
                "domain_group": "Multimodal",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 10.547774705387823,
                "log_compute": 22.556302500767288,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "AgentFounder-30B",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,System control,Quantitative reasoning,Mathematical reasoning,Code generation,Search",
                "organization": "Alibaba",
                "authors": "Liangcai Su, Zhen Zhang, Guangyu Li, Zhuo Chen, Chenxi Wang, Maojia Song, Xinyu Wang, Kuan Li, Jialong Wu, Xuanzhong Chen, Zile Qiao, Zhongwang Zhang, Huifeng Yin, Shihao Cai, Runnan Fang, Zhengwei Tao, Wenbiao Yin, Chenxiong Qian, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou",
                "publication_date": "2025-09-16",
                "reference": "Scaling Agents via Continual Pre-training",
                "link": "https://arxiv.org/abs/2509.13310",
                "citations": null,
                "notability_criteria": "SOTA improvement",
                "parameters": 30000000000.0,
                "training_compute_(flop)": 6.5367e+23,
                "training_dataset_size_(gradients)": 315000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": "Qwen3-30B-A3B",
                "finetune_compute_(flop)": 5.67e+21,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 16:30:09+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 10.477121254719663,
                "log_compute": 23.81535855360072,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Qwen3-Next-80B-A3B",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,System control,Code generation",
                "organization": "Alibaba",
                "authors": "Unknown",
                "publication_date": "2025-09-10",
                "reference": "Qwen3-Next: Towards Ultimate Training & Inference Efficiency",
                "link": "https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&from=research.latest-advancements-list",
                "citations": null,
                "notability_criteria": null,
                "parameters": 80000000000.0,
                "training_compute_(flop)": 2.7e+23,
                "training_dataset_size_(gradients)": 15000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 10.903089986991944,
                "log_compute": 23.431363764158988,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Qwen3-Max",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Mathematical reasoning,Code generation,Quantitative reasoning,Retrieval-augmented generation,Translation",
                "organization": "Alibaba",
                "authors": "Unknown",
                "publication_date": "2025-09-05",
                "reference": "Introducing Qwen3-Max-Preview (Instruct) \u2014 our biggest model yet, with over 1 trillion parameters! ",
                "link": "https://modelstudio.console.alibabacloud.com/?tab=doc#/doc/?type=model&url=2840914_2&modelId=qwen3-max-preview\n\nhttps://qwen.ai/blog?id=87dc93fc8a590dc718c77e1f6e84c07b474f6c5a&from=home.latest-research-list",
                "citations": null,
                "notability_criteria": "Discretionary",
                "parameters": 1000000000000.0,
                "training_compute_(flop)": 1.512e+25,
                "training_dataset_size_(gradients)": 36000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Speculative",
                "epochs": null,
                "model_accessibility": "API access",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-08 21:58:37+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 12.0,
                "log_compute": 25.17955179116519,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "LongCat-Flash",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Chat,Code generation,Quantitative reasoning,Instruction interpretation",
                "organization": "Meituan Inc",
                "authors": "Meituan LongCat Team, Bayan, Bei Li, Bingye Lei, Bo Wang, Bolin Rong, Chao Wang, Chao Zhang, Chen Gao, Chen Zhang, Cheng Sun, Chengcheng Han, Chenguang Xi, Chi Zhang, Chong Peng, Chuan Qin, Chuyu Zhang, Cong Chen, Congkui Wang, Dan Ma, Daoru Pan, Defei Bu, Dengchang Zhao, Deyang Kong, Dishan Liu, Feiye Huo, Fengcun Li, Fubao Zhang, Gan Dong, Gang Liu, Gang Xu, Ge Li, Guoqiang Tan, Guoyuan Lin, Haihang Jing, Haomin Fu, Haonan Yan, Haoxing Wen, Haozhe Zhao, Hong Liu, Hongmei Shi, Hongyan Hao, Hongyin Tang, Huantian Lv, Hui Su, Jiacheng Li, Jiahao Liu, Jiahuan Li, Jiajun Yang, Jiaming Wang, Jian Yang, Jianchao Tan, Jiaqi Sun, Jiaqi Zhang, Jiawei Fu, Jiawei Yang, Jiaxi Hu, Jiayu Qin, Jingang Wang, Jiyuan He, Jun Kuang, Junhui Mei, Kai Liang, Ke He, Kefeng Zhang, Keheng Wang, Keqing He, Liang Gao, Liang Shi, Lianhui Ma, Lin Qiu, Lingbin Kong, Lingtong Si, Linkun Lyu, Linsen Guo, Liqi Yang, Lizhi Yan, Mai Xia, Man Gao, Manyuan Zhang, Meng Zhou, Mengxia Shen, Mingxiang Tuo, Mingyang Zhu, Peiguang Li, Peng Pei, Peng Zhao, Pengcheng Jia, Pingwei Sun, Qi Gu, Qianyun Li, Qingyuan Li, Qiong Huang, Qiyuan Duan, Ran Meng, Rongxiang Weng, Ruichen Shao, Rumei Li, Shizhe Wu, Shuai Liang et al. (82 additional authors not shown)",
                "publication_date": "2025-09-01",
                "reference": "LongCat-Flash Technical Report",
                "link": "https://arxiv.org/abs/2509.01322",
                "citations": null,
                "notability_criteria": "SOTA improvement",
                "parameters": 560000000000.0,
                "training_compute_(flop)": 3.726e+24,
                "training_dataset_size_(gradients)": 23000000000000.0,
                "training_time_(hours)": 720.0,
                "training_hardware": null,
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 11.7481880270062,
                "log_compute": 24.571242850560225,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "DeepSeek-V3.1",
                "domain": "Language",
                "task": "Language modeling/generation,Code generation,Quantitative reasoning,Question answering,Search,System control,Instruction interpretation",
                "organization": "DeepSeek",
                "authors": "Unknown",
                "publication_date": "2025-08-21",
                "reference": "Introducing DeepSeek-V3.1: our first step toward the agent era!",
                "link": "https://api-docs.deepseek.com/news/news250821\n\nhttps://huggingface.co/deepseek-ai/DeepSeek-V3.1",
                "citations": null,
                "notability_criteria": null,
                "parameters": 671000000000.0,
                "training_compute_(flop)": 3.594058e+24,
                "training_dataset_size_(gradients)": 840000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": "DeepSeek-V3",
                "finetune_compute_(flop)": 1.86258e+23,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 16:58:23+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 11.826722520168993,
                "log_compute": 24.555585081364136,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Surya",
                "domain": "Earth science",
                "task": "Heliophysics,Weather forecasting",
                "organization": "NASA,University of Alabama,IBM Research",
                "authors": "Sujit Roy, Johannes Schmude, Rohit Lal, Vishal Gaur, Marcus Freitag, Julian Kuehnert, Theodore van Kessel, Dinesha V. Hegde, Andr\u00e9s Mu\u00f1oz-Jaramillo, Johannes Jakubik, Etienne Vos, Kshitiz Mandal, Ata Akbari Asanjan, Joao Lucas de Sousa Almeida, Amy Lin, Talwinder Singh, Kang Yang, Chetraj Pandey, Jinsu Hong, Berkay Aydin, Thorsten Kurth, Ryan McGranaghan, Spiridon Kasapis, Vishal Upendran, Shah Bahauddin, Daniel da Silva, Nikolai V. Pogorelov, Anne Spalding, Campbell Watson, Manil Maskey, Madhulika Guhathakurta, Juan Bernabe-Moreno, Rahul Ramachandran",
                "publication_date": "2025-08-20",
                "reference": "Surya: Foundation Model for Heliophysics",
                "link": "https://arxiv.org/abs/2508.14112",
                "citations": null,
                "notability_criteria": null,
                "parameters": 366000000.0,
                "training_compute_(flop)": 2.9474214e+21,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 128.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry, Government",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 100113.42287839846,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Earth science",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 8.563481085394411,
                "log_compute": 21.469442232426523,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "FlowER",
                "domain": "Materials science",
                "task": null,
                "organization": "Massachusetts Institute of Technology (MIT)",
                "authors": "Joonyoung F. Joung, Mun Hong Fong, Nicholas Casetti, Jordan P. Liles, Ne S. Dassanayake, Connor W. Coley",
                "publication_date": "2025-08-20",
                "reference": "Electron flow matching for generative reaction mechanism prediction",
                "link": "https://www.nature.com/articles/s41586-025-09426-9",
                "citations": null,
                "notability_criteria": null,
                "parameters": 7000000000.0,
                "training_compute_(flop)": 5.88e+16,
                "training_dataset_size_(gradients)": 1400000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-06 19:12:29+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Materials science",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 9.845098040014257,
                "log_compute": 16.76937732607614,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "NVIDIA-Nemotron-Nano-9B-v2",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering",
                "organization": "NVIDIA",
                "authors": "Unknown",
                "publication_date": "2025-08-18",
                "reference": "NVIDIA-Nemotron-Nano-9B-v2 Overview",
                "link": "https://build.nvidia.com/nvidia/nvidia-nemotron-nano-9b-v2/modelcard",
                "citations": null,
                "notability_criteria": null,
                "parameters": 9000000000.0,
                "training_compute_(flop)": 1.53e+24,
                "training_dataset_size_(gradients)": 21100000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Reported,Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 9.954242509439325,
                "log_compute": 24.184691430817598,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "NVIDIA-Nemotron-Nano-12B-v2",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering",
                "organization": "NVIDIA",
                "authors": "Unknown",
                "publication_date": "2025-08-18",
                "reference": "NVIDIA-Nemotron-Nano-12B-v2",
                "link": "https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-12B-v2",
                "citations": null,
                "notability_criteria": null,
                "parameters": 12000000000.0,
                "training_compute_(flop)": 1.5192e+24,
                "training_dataset_size_(gradients)": 21100000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 10.079181246047625,
                "log_compute": 24.18161495172896,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "GLM-4.5V",
                "domain": "Multimodal,Vision,Language,Video",
                "task": "Language modeling/generation,Question answering,Visual question answering,Image captioning,Video description,Table tasks,Character recognition (OCR)",
                "organization": "Zhipu AI,Tsinghua University",
                "authors": "GLM-V Team: Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, Shuaiqi Duan, Weihan Wang, Yan Wang, Yean Cheng, Zehai He, Zhe Su, Zhen Yang, Ziyang Pan, Aohan Zeng, Baoxu Wang, Bin Chen, Boyan Shi, Changyu Pang, Chenhui Zhang, Da Yin, Fan Yang, Guoqing Chen, Jiazheng Xu, Jiale Zhu, Jiali Chen, Jing Chen, Jinhao Chen, Jinghao Lin, Jinjiang Wang, Junjie Chen, Leqi Lei, Letian Gong, Leyi Pan, Mingdao Liu, Mingde Xu, Mingzhi Zhang, Qinkai Zheng, Sheng Yang, Shi Zhong, Shiyu Huang, Shuyuan Zhao, Siyan Xue, Shangqin Tu, Shengbiao Meng, Tianshu Zhang, Tianwei Luo, Tianxiang Hao, Tianyu Tong, Wenkai Li, Wei Jia, Xiao Liu, Xiaohan Zhang, Xin Lyu, Xinyue Fan, Xuancheng Huang, Yanling Wang, Yadong Xue, Yanfeng Wang, Yanzi Wang, Yifan An, Yifan Du, Yiming Shi, Yiheng Huang, Yilin Niu, Yuan Wang, Yuanchang Yue, Yuchen Li, Yutao Zhang, Yuting Wang, Yu Wang, Yuxuan Zhang, Zhao Xue, Zhenyu Hou, Zhengxiao Du, Zihan Wang, Peng Zhang, Debing Liu, Bin Xu, Juanzi Li, Minlie Huang, Yuxiao Dong, Jie Tang",
                "publication_date": "2025-08-15",
                "reference": "GLM-4.5V and GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning",
                "link": "https://arxiv.org/abs/2507.01006",
                "citations": null,
                "notability_criteria": null,
                "parameters": 108000000000.0,
                "training_compute_(flop)": 1.8e+24,
                "training_dataset_size_(gradients)": 2013265900000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": "GLM-4.5-Air",
                "finetune_compute_(flop)": 1.44e+23,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Multimodal",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 11.03342375548695,
                "log_compute": 24.255272505103306,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "GLM-4.1V-Thinking",
                "domain": "Multimodal,Vision,Language",
                "task": "Language modeling/generation,Question answering,Visual question answering,Image captioning",
                "organization": "Zhipu AI,Tsinghua University",
                "authors": "GLM-V Team: Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, Shuaiqi Duan, Weihan Wang, Yan Wang, Yean Cheng, Zehai He, Zhe Su, Zhen Yang, Ziyang Pan, Aohan Zeng, Baoxu Wang, Bin Chen, Boyan Shi, Changyu Pang, Chenhui Zhang, Da Yin, Fan Yang, Guoqing Chen, Jiazheng Xu, Jiale Zhu, Jiali Chen, Jing Chen, Jinhao Chen, Jinghao Lin, Jinjiang Wang, Junjie Chen, Leqi Lei, Letian Gong, Leyi Pan, Mingdao Liu, Mingde Xu, Mingzhi Zhang, Qinkai Zheng, Sheng Yang, Shi Zhong, Shiyu Huang, Shuyuan Zhao, Siyan Xue, Shangqin Tu, Shengbiao Meng, Tianshu Zhang, Tianwei Luo, Tianxiang Hao, Tianyu Tong, Wenkai Li, Wei Jia, Xiao Liu, Xiaohan Zhang, Xin Lyu, Xinyue Fan, Xuancheng Huang, Yanling Wang, Yadong Xue, Yanfeng Wang, Yanzi Wang, Yifan An, Yifan Du, Yiming Shi, Yiheng Huang, Yilin Niu, Yuan Wang, Yuanchang Yue, Yuchen Li, Yutao Zhang, Yuting Wang, Yu Wang, Yuxuan Zhang, Zhao Xue, Zhenyu Hou, Zhengxiao Du, Zihan Wang, Peng Zhang, Debing Liu, Bin Xu, Juanzi Li, Minlie Huang, Yuxiao Dong, Jie Tang",
                "publication_date": "2025-08-15",
                "reference": "GLM-4.5V and GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning",
                "link": "https://arxiv.org/abs/2507.01006",
                "citations": null,
                "notability_criteria": null,
                "parameters": 9000000000.0,
                "training_compute_(flop)": 9.18e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": "GLM-4-9B-0414",
                "finetune_compute_(flop)": 1.08e+23,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Multimodal",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 9.954242509439325,
                "log_compute": 23.96284268120124,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Gemma 3 270M",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering",
                "organization": "Google DeepMind",
                "authors": "Unknown",
                "publication_date": "2025-08-14",
                "reference": "Introducing Gemma 3 270M: The compact model for hyper-efficient AI",
                "link": "https://developers.googleblog.com/en/introducing-gemma-3-270m/",
                "citations": null,
                "notability_criteria": null,
                "parameters": 270000000.0,
                "training_compute_(flop)": 9.72e+21,
                "training_dataset_size_(gradients)": 6000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 8.431363764158988,
                "log_compute": 21.987666264926276,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "gpt-oss-120b",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering",
                "organization": "OpenAI",
                "authors": "Unknown",
                "publication_date": "2025-08-05",
                "reference": "gpt-oss-120b & gpt-oss-20b Model Card",
                "link": "https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf",
                "citations": null,
                "notability_criteria": "Discretionary",
                "parameters": 116830000000.0,
                "training_compute_(flop)": 4.94e+24,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-03 13:19:00+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 11.067554376693504,
                "log_compute": 24.693726948923647,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "gpt-oss-20b",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering",
                "organization": "OpenAI",
                "authors": "Unknown",
                "publication_date": "2025-08-05",
                "reference": "gpt-oss-120b & gpt-oss-20b Model Card",
                "link": "https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf",
                "citations": null,
                "notability_criteria": "Discretionary",
                "parameters": 20910000000.0,
                "training_compute_(flop)": 5.49e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-01 23:38:04+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 10.320354032817672,
                "log_compute": 23.739572344450092,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "GLM 4.5",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Code generation,Quantitative reasoning",
                "organization": "Zhipu AI,Tsinghua University",
                "authors": "Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, Kedong Wang, Lucen\nZhong, Mingdao Liu, Rui Lu, Shulin Cao, Xiaohan Zhang, Xuancheng Huang, Yao Wei, Yean Cheng,\nYifan An, Yilin Niu, Yuanhao Wen, Yushi Bai, Zhengxiao Du, Zihan Wang (\u6c6a\u5b50\u6db5), Zilin Zhu",
                "publication_date": "2025-08-05",
                "reference": "GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models",
                "link": "https://arxiv.org/abs/2508.06471",
                "citations": null,
                "notability_criteria": "Discretionary",
                "parameters": 355000000000.0,
                "training_compute_(flop)": 4.42e+24,
                "training_dataset_size_(gradients)": 23100000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-22 15:05:44+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 11.550228353055093,
                "log_compute": 24.64542226934909,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "GLM-4.5-Air",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Visual question answering,Image captioning,Video description,Table tasks,Character recognition (OCR)",
                "organization": "Zhipu AI,Tsinghua University",
                "authors": "Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, Kedong Wang, Lucen\nZhong, Mingdao Liu, Rui Lu, Shulin Cao, Xiaohan Zhang, Xuancheng Huang, Yao Wei, Yean Cheng,\nYifan An, Yilin Niu, Yuanhao Wen, Yushi Bai, Zhengxiao Du, Zihan Wang (\u6c6a\u5b50\u6db5), Zilin Zhu",
                "publication_date": "2025-08-05",
                "reference": "GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models",
                "link": "https://arxiv.org/abs/2508.06471",
                "citations": null,
                "notability_criteria": null,
                "parameters": 106000000000.0,
                "training_compute_(flop)": 1.656e+24,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-22 15:05:46+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": null,
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 11.02530586526477,
                "log_compute": 24.219060332448862,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "AlphaEarth Foundations (AEF)",
                "domain": "Earth science",
                "task": "Entity embedding,Crop Mapping / Segmentation",
                "organization": "Google DeepMind,Google",
                "authors": "Christopher F. Brown, Michal R. Kazmierski, Valerie J. Pasquarella, William J. Rucklidge, Masha Samsikova, Chenhui Zhang, Evan Shelhamer, Estefania Lahera, Olivia Wiles, Simon Ilyushchenko, Noel Gorelick, Lihui Lydia Zhang, Sophia Alj, Emily Schechter, Sean Askay, Oliver Guinan, Rebecca Moore, Alexis Boukouvalas, Pushmeet Kohli",
                "publication_date": "2025-07-30",
                "reference": "AlphaEarth Foundations: An embedding field model for accurate and efficient global mapping from sparse label data",
                "link": "https://arxiv.org/abs/2507.22291",
                "citations": null,
                "notability_criteria": null,
                "parameters": 480000000.0,
                "training_compute_(flop)": 2.36544e+18,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 56.0,
                "training_hardware": "Google TPU v4",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Hosted access (no API)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 512.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 170272.4293355278,
                "training_compute_estimation_method": "Hardware",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Earth science",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 8.681241237375588,
                "log_compute": 18.373911936531957,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "Wan 2.2 14B T2V",
                "domain": "Video",
                "task": "Video generation,Text-to-video",
                "organization": "Alibaba",
                "authors": "Unknown",
                "publication_date": "2025-07-28",
                "reference": "We are excited to introduce Wan2.2, a major upgrade to our foundational video models.",
                "link": "https://huggingface.co/Wan-AI/Wan2.2-T2V-A14B",
                "citations": null,
                "notability_criteria": null,
                "parameters": 14000000000.0,
                "training_compute_(flop)": 4.2e+23,
                "training_dataset_size_(gradients)": 5000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Vision",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 10.146128035678238,
                "log_compute": 23.6232492903979,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Qwen3-Coder-480B-A35B",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Code generation,System control",
                "organization": "Alibaba",
                "authors": "Unknown",
                "publication_date": "2025-07-22",
                "reference": "Qwen3-Coder: Agentic Coding in the World",
                "link": "https://qwenlm.github.io/blog/qwen3-coder/",
                "citations": null,
                "notability_criteria": "Discretionary",
                "parameters": 480000000000.0,
                "training_compute_(flop)": 1.575e+24,
                "training_dataset_size_(gradients)": 7500000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 11.681241237375588,
                "log_compute": 24.19728055812562,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Kimi K2",
                "domain": "Language",
                "task": "Language modeling/generation,Code generation,Question answering,Quantitative reasoning,Search,Table tasks",
                "organization": "Moonshot",
                "authors": "Unknown",
                "publication_date": "2025-07-11",
                "reference": "Kimi K2: Open Agentic Intelligence",
                "link": "https://moonshotai.github.io/Kimi-K2/",
                "citations": null,
                "notability_criteria": "Training cost",
                "parameters": 1000000000000.0,
                "training_compute_(flop)": 2.976e+24,
                "training_dataset_size_(gradients)": 15500000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA H800 SXM5",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-03 13:33:09+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 12.0,
                "log_compute": 24.47363292687384,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "dots.llm1",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering",
                "organization": "Rednote",
                "authors": "Bi Huo, Bin Tu, Cheng Qin, Da Zheng, Debing Zhang, Dongjie Zhang, En Li, Fu Guo, Jian Yao, Jie Lou, Junfeng Tian, Li Hu, Ran Zhu, Shengdong Chen, Shuo Liu, Su Guang, Te Wo, Weijun Zhang, Xiaoming Shi, Xinxin Peng, Xing Wu, Yawen Liu, Yuqiu Ji, Ze Wen, Zhenhai Liu, Zichao Li, Zilong Liao",
                "publication_date": "2025-07-06",
                "reference": "dots.llm1 Technical Report",
                "link": "https://www.arxiv.org/abs/2506.05767",
                "citations": null,
                "notability_criteria": "SOTA improvement",
                "parameters": 142000000000.0,
                "training_compute_(flop)": 1.2164856e+24,
                "training_dataset_size_(gradients)": 11328000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA H800 SXM5",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-29 19:36:51+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": 1456000.0,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware,Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 11.152288344383056,
                "log_compute": 24.085106972389276,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "ERNIE-4.5-300B-A47B",
                "domain": "Language",
                "task": "Language modeling/generation,Quantitative reasoning,Code generation,Translation,Question answering",
                "organization": "Baidu",
                "authors": "Unknown",
                "publication_date": "2025-06-29",
                "reference": "ERNIE 4.5 Technical Report",
                "link": "https://yiyan.baidu.com/blog/publication/ERNIE_Technical_Report.pdf",
                "citations": null,
                "notability_criteria": null,
                "parameters": 300000000000.0,
                "training_compute_(flop)": 2.82e+24,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA H800 SXM5",
                "approach": null,
                "confidence": "Speculative",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": null,
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 11.477121254719663,
                "log_compute": 24.450249108319362,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "ERNIE-4.5-21B-A3B",
                "domain": "Language",
                "task": "Language modeling/generation,Quantitative reasoning,Code generation,Translation,Question answering",
                "organization": "Baidu",
                "authors": "Unknown",
                "publication_date": "2025-06-29",
                "reference": "ERNIE 4.5 Technical Report",
                "link": "https://yiyan.baidu.com/blog/publication/ERNIE_Technical_Report.pdf",
                "citations": null,
                "notability_criteria": null,
                "parameters": 21000000000.0,
                "training_compute_(flop)": 1.8e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Speculative",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": null,
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 10.32221929473392,
                "log_compute": 23.255272505103306,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "AlphaGenome",
                "domain": "Biology",
                "task": "Gene expression enhancement,Gene expression profile generation,Molecular property prediction,Mutation prediction,Protein-DNA binding prediction,Transcriptomic prediction,RNA structure prediction",
                "organization": "Google DeepMind",
                "authors": "\u017diga Avsec, Natasha Latysheva, Jun Cheng, Guido Novati, Kyle R. Taylor, Tom Ward, Clare Bycroft, Lauren Nicolaisen, Eirini Arvaniti, Joshua Pan, Raina Thomas, Vincent Dutordoir, Matteo Perino, Soham De, Alexander Karollus, Adam Gayoso, Toby Sargeant, Anne Mottram, Lai Hong Wong, Pavol Drot\u00e1r, Adam Kosiorek, Andrew Senior, Richard Tanburn, Taylor Applebaum, Souradeep Basu, Demis Hassabis, Pushmeet Kohli",
                "publication_date": "2025-06-25",
                "reference": "AlphaGenome: advancing regulatory variant effect prediction with a unified DNA sequence model",
                "link": "https://storage.googleapis.com/deepmind-media/papers/alphagenome.pdf",
                "citations": null,
                "notability_criteria": null,
                "parameters": 450000000.0,
                "training_compute_(flop)": 1.362969e+22,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "Google TPU v3,NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "API access",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Biology",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 8.653212513775344,
                "log_compute": 22.134485978151588,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "MiniMax-M1-80k",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Code generation,Quantitative reasoning",
                "organization": "MiniMax",
                "authors": "MiniMax: Aili Chen, Aonian Li, Bangwei Gong, Binyang Jiang, Bo Fei, Bo Yang, Boji Shan, Changqing Yu, Chao Wang, Cheng Zhu, Chengjun Xiao, Chengyu Du, Chi Zhang, Chu Qiao, Chunhao Zhang, Chunhui Du, Congchao Guo, Da Chen, Deming Ding, Dianjun Sun, Dong Li, Enwei Jiao, Haigang Zhou, Haimo Zhang, Han Ding, Haohai Sun, Haoyu Feng, Huaiguang Cai, Haichao Zhu, Jian Sun, Jiaqi Zhuang, Jiaren Cai, Jiayuan Song, Jin Zhu, Jingyang Li, Jinhao Tian, Jinli Liu, Junhao Xu, Junjie Yan, Junteng Liu, Junxian He, Kaiyi Feng, Ke Yang, Kecheng Xiao, Le Han, Leyang Wang, Lianfei Yu, Liheng Feng, Lin Li, Lin Zheng, Linge Du, Lingyu Yang, Lunbin Zeng, Minghui Yu, Mingliang Tao, Mingyuan Chi, Mozhi Zhang, Mujie Lin, Nan Hu, Nongyu Di, Peng Gao, Pengfei Li, Pengyu Zhao, Qibing Ren, Qidi Xu, Qile Li, Qin Wang, Rong Tian, Ruitao Leng, Shaoxiang Chen, Shaoyu Chen, Shengmin Shi, Shitong Weng, Shuchang Guan, Shuqi Yu, Sichen Li, Songquan Zhu, Tengfei Li, Tianchi Cai, Tianrun Liang, Weiyu Cheng, Weize Kong, Wenkai Li, Xiancai Chen, Xiangjun Song, Xiao Luo, Xiao Su, Xiaobo Li, Xiaodong Han, Xinzhu Hou, Xuan Lu, Xun Zou, Xuyang Shen, Yan Gong, Yan Ma, Yang Wang, Yiqi Shi, Yiran Zhong, Yonghong Duan et al. (27 additional authors not shown)",
                "publication_date": "2025-06-13",
                "reference": "MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention",
                "link": "https://arxiv.org/abs/2506.13585",
                "citations": null,
                "notability_criteria": null,
                "parameters": 456000000000.0,
                "training_compute_(flop)": 4.3240062e+24,
                "training_dataset_size_(gradients)": 7500000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA H800 SXM5",
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": "MiniMax-Text-01",
                "finetune_compute_(flop)": 2.3411262e+24,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": null,
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 11.658964842664435,
                "log_compute": 24.635886307997243,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "MiniMax-M1-40k",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Code generation,Quantitative reasoning",
                "organization": "MiniMax",
                "authors": "MiniMax: Aili Chen, Aonian Li, Bangwei Gong, Binyang Jiang, Bo Fei, Bo Yang, Boji Shan, Changqing Yu, Chao Wang, Cheng Zhu, Chengjun Xiao, Chengyu Du, Chi Zhang, Chu Qiao, Chunhao Zhang, Chunhui Du, Congchao Guo, Da Chen, Deming Ding, Dianjun Sun, Dong Li, Enwei Jiao, Haigang Zhou, Haimo Zhang, Han Ding, Haohai Sun, Haoyu Feng, Huaiguang Cai, Haichao Zhu, Jian Sun, Jiaqi Zhuang, Jiaren Cai, Jiayuan Song, Jin Zhu, Jingyang Li, Jinhao Tian, Jinli Liu, Junhao Xu, Junjie Yan, Junteng Liu, Junxian He, Kaiyi Feng, Ke Yang, Kecheng Xiao, Le Han, Leyang Wang, Lianfei Yu, Liheng Feng, Lin Li, Lin Zheng, Linge Du, Lingyu Yang, Lunbin Zeng, Minghui Yu, Mingliang Tao, Mingyuan Chi, Mozhi Zhang, Mujie Lin, Nan Hu, Nongyu Di, Peng Gao, Pengfei Li, Pengyu Zhao, Qibing Ren, Qidi Xu, Qile Li, Qin Wang, Rong Tian, Ruitao Leng, Shaoxiang Chen, Shaoyu Chen, Shengmin Shi, Shitong Weng, Shuchang Guan, Shuqi Yu, Sichen Li, Songquan Zhu, Tengfei Li, Tianchi Cai, Tianrun Liang, Weiyu Cheng, Weize Kong, Wenkai Li, Xiancai Chen, Xiangjun Song, Xiao Luo, Xiao Su, Xiaobo Li, Xiaodong Han, Xinzhu Hou, Xuan Lu, Xun Zou, Xuyang Shen, Yan Gong, Yan Ma, Yang Wang, Yiqi Shi, Yiran Zhong, Yonghong Duan et al. (27 additional authors not shown)",
                "publication_date": "2025-06-13",
                "reference": "MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention",
                "link": "https://arxiv.org/abs/2506.13585",
                "citations": null,
                "notability_criteria": null,
                "parameters": 456000000000.0,
                "training_compute_(flop)": 4.1861931e+24,
                "training_dataset_size_(gradients)": 7500000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA H800 SXM5",
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": "MiniMax-Text-01",
                "finetune_compute_(flop)": 2.2033131e+24,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": null,
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 11.658964842664435,
                "log_compute": 24.621819257526372,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "FGN",
                "domain": "Earth science",
                "task": "Weather forecasting",
                "organization": "Google DeepMind",
                "authors": "Ferran Alet, Ilan Price, Andrew El-Kadi, Dominic Masters, Stratis Markou, Tom R. Andersson, Jacklynn Stott, Remi Lam, Matthew Willson, Alvaro Sanchez-Gonzalez, Peter Battaglia",
                "publication_date": "2025-06-12",
                "reference": "Skillful joint probabilistic weather forecasting from marginals",
                "link": "https://arxiv.org/abs/2506.10772v1",
                "citations": null,
                "notability_criteria": "SOTA improvement",
                "parameters": 720000000.0,
                "training_compute_(flop)": 9.61895088e+21,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 72.0,
                "training_hardware": "Google TPU v5p,Google TPU v6e Trillium",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Earth science",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 8.857332496431269,
                "log_compute": 21.983127706978237,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "V-JEPA 2",
                "domain": "Vision,Video,Robotics",
                "task": "Robotic manipulation",
                "organization": "Facebook AI Research",
                "authors": "Mido Assran, Adrien Bardes, David Fan, Quentin Garrido, Russell Howes, Mojtaba, Komeili, Matthew Muckley, Ammar Rizvi, Claire Roberts, Koustuv Sinha, Artem Zholus, Sergio Arnaud, Abha Gejji, Ada Martin, Francois Robert Hogan, Daniel Dugas, Piotr Bojanowski, Vasil Khalidov, Patrick Labatut, Francisco Massa, Marc Szafraniec, Kapil Krishnakumar, Yong Li, Xiaodong Ma, Sarath Chandar, Franziska Meier, Yann LeCun, Michael Rabbat, Nicolas Ballas",
                "publication_date": "2025-06-11",
                "reference": "V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning",
                "link": "https://arxiv.org/abs/2506.09985",
                "citations": null,
                "notability_criteria": null,
                "parameters": 1000000000.0,
                "training_compute_(flop)": 9.05969664e+21,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 9.0,
                "log_compute": 21.957113655758516,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "MiMo-7B-Base",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Quantitative reasoning,Code generation",
                "organization": "Xiaomi Corp",
                "authors": "LLM-Core Xiaomi: Bingquan Xia, Bowen Shen, Cici, Dawei Zhu, Di Zhang, Gang Wang, Hailin Zhang, Huaqiu Liu, Jiebao Xiao, Jinhao Dong, Liang Zhao, Peidian Li, Peng Wang, Shihua Yu, Shimao Chen, Weikun Wang, Wenhan Ma, Xiangwei Deng, Yi Huang, Yifan Song, Zihan Jiang, Bowen Ye, Can Cai, Chenhong He, Dong Zhang, Duo Zhang, Guoan Wang, Hao Tian, Haochen Zhao, Heng Qu, Hongshen Xu, Jun Shi, Kainan Bao, Kai Fang, Kang Zhou, Kangyang Zhou, Lei Li, Menghang Zhu, Nuo Chen, Qiantong Wang, Shaohui Liu, Shicheng Li, Shuhao Gu, Shuhuai Ren, Shuo Liu, Sirui Deng, Weiji Zhuang, Weiwei Lv, Wenyu Yang, Xin Zhang, Xing Yong, Xing Zhang, Xingchen Song, Xinzhe Xu, Xu Wang, Yihan Yan, Yu Tu, Yuanyuan Tian, Yudong Wang, Yue Yu, Zhenru Lin, Zhichao Song, Zihao Yue",
                "publication_date": "2025-06-05",
                "reference": "MiMo: Unlocking the Reasoning Potential of Language Model -- From Pretraining to Posttraining",
                "link": "https://arxiv.org/abs/2505.07608",
                "citations": null,
                "notability_criteria": null,
                "parameters": 7000000000.0,
                "training_compute_(flop)": 1.05e+24,
                "training_dataset_size_(gradients)": 25000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 9.845098040014257,
                "log_compute": 24.02118929906994,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "MiMo-VL-7B-SFT",
                "domain": "Vision,Multimodal,Language,Video",
                "task": "Character recognition (OCR),Image captioning,Language modeling/generation,Question answering,Visual question answering,Video,Video description",
                "organization": "Xiaomi Corp",
                "authors": "Xiaomi LLM-Core Team: Zihao Yue, Zhenru Lin, Yifan Song, Weikun Wang, Shuhuai Ren, Shuhao Gu, Shicheng Li, Peidian Li, Liang Zhao, Lei Li, Kainan Bao, Hao Tian, Hailin Zhang, Gang Wang, Dawei Zhu, Cici, Chenhong He, Bowen Ye, Bowen Shen, Zihan Zhang, Zihan Jiang, Zhixian Zheng, Zhichao Song, Zhenbo Luo, Yue Yu, Yudong Wang, Yuanyuan Tian, Yu Tu, Yihan Yan, Yi Huang, Xu Wang, Xinzhe Xu, Xingchen Song, Xing Zhang, Xing Yong, Xin Zhang, Xiangwei Deng, Wenyu Yang, Wenhan Ma, Weiwei Lv, Weiji Zhuang, Wei Liu, Sirui Deng, Shuo Liu, Shimao Chen, Shihua Yu, Shaohui Liu, Shande Wang, Rui Ma, Qiantong Wang, Peng Wang, Nuo Chen, Menghang Zhu, Kangyang Zhou, Kang Zhou, Kai Fang, Jun Shi, Jinhao Dong, Jiebao Xiao, Jiaming Xu, Huaqiu Liu, Hongshen Xu, Heng Qu, Haochen Zhao, Hanglong Lv, Guoan Wang, Duo Zhang, Dong Zhang, Di Zhang, Chong Ma, Chang Liu, Can Cai, Bingquan Xia",
                "publication_date": "2025-06-04",
                "reference": "MiMo-VL Technical Report\n",
                "link": "https://arxiv.org/abs/2506.03569",
                "citations": null,
                "notability_criteria": null,
                "parameters": 7000000000.0,
                "training_compute_(flop)": 1.1508e+24,
                "training_dataset_size_(gradients)": 2400000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": "Supervised fine-tuning (SFT)",
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": "MiMo-7B-Base",
                "finetune_compute_(flop)": 1.008e+23,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Multimodal",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 9.845098040014257,
                "log_compute": 24.060999853218288,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "OpenAudio-S1",
                "domain": "Speech",
                "task": "Speech synthesis,Text-to-speech (TTS)",
                "organization": "Fish Audio",
                "authors": "Unknown",
                "publication_date": "2025-06-03",
                "reference": "Our cutting-edge text-to-speech model that performs like voice actors",
                "link": "https://openaudio.com/blogs/s1",
                "citations": null,
                "notability_criteria": null,
                "parameters": 4000000000.0,
                "training_compute_(flop)": 3.36e+22,
                "training_dataset_size_(gradients)": 1400000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "API access",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Audio",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 9.602059991327963,
                "log_compute": 22.526339277389845,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "Pangu Pro MoE",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Quantitative reasoning,Mathematical reasoning,Code generation",
                "organization": "Huawei",
                "authors": "Yehui Tang, Xiaosong Li, Fangcheng Liu, Wei Guo, Hang Zhou, Yaoyuan Wang, Kai Han, Xianzhi Yu, Jinpeng Li, Hui Zang, Fei Mi, Xiaojun Meng, Zhicheng Liu, Hanting Chen, Binfan Zheng, Can Chen, Youliang Yan, Ruiming Tang, Peifeng Qin, Xinghao Chen, Dacheng Tao, Yunhe Wang (and Other Contributors)",
                "publication_date": "2025-05-28",
                "reference": "Pangu Pro MoE: Mixture of Grouped Experts for Efficient Sparsity",
                "link": "https://arxiv.org/abs/2505.21411",
                "citations": null,
                "notability_criteria": null,
                "parameters": 71990000000.0,
                "training_compute_(flop)": 1.287e+24,
                "training_dataset_size_(gradients)": 13000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "Huawei Ascend 800T A2",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 4000.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 2429161.768914584,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 10.857272173564041,
                "log_compute": 24.109578546904388,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "DataRater test model (1B)",
                "domain": "Language",
                "task": "Language modeling",
                "organization": "Google DeepMind",
                "authors": "Dan A. Calian, Gregory Farquhar, Iurii Kemaev, Luisa M. Zintgraf, Matteo Hessel, Jeremy Shar, Junhyuk Oh, Andr\u00e1s Gy\u00f6rgy, Tom Schaul, Jeffrey Dean, Hado van Hasselt, David Silver",
                "publication_date": "2025-05-23",
                "reference": "DataRater: Meta-Learned Dataset Curation",
                "link": "https://arxiv.org/abs/2505.17895",
                "citations": 3.0,
                "notability_criteria": null,
                "parameters": 1000000000.0,
                "training_compute_(flop)": 1.2e+20,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "Google TPU v6e Trillium",
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Unknown",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 32.0,
                "last_modified": "2025-10-14 17:37:15+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": null,
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 9.0,
                "log_compute": 20.079181246047625,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "Gemma 3n",
                "domain": "Language,Multimodal,Speech,Vision",
                "task": "Language modeling/generation,Question answering,Chat,Speech recognition (ASR),Translation,Speech-to-text,Visual question answering,Mathematical reasoning,Code generation,Character recognition (OCR)",
                "organization": "Google",
                "authors": "Lucas Gonzalez, Rakesh Shivanna",
                "publication_date": "2025-05-20",
                "reference": "Announcing Gemma 3n preview: powerful, efficient, mobile-first AI",
                "link": "https://developers.googleblog.com/en/introducing-gemma-3n/",
                "citations": null,
                "notability_criteria": null,
                "parameters": 7850000000.0,
                "training_compute_(flop)": 5.181e+23,
                "training_dataset_size_(gradients)": 11000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 9.894869656745252,
                "log_compute": 23.714413592287123,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "Earth-2 (cBottle-SR)",
                "domain": "Earth science,Image generation",
                "task": "Image generation,Weather forecasting",
                "organization": "NVIDIA",
                "authors": "Noah D. Brenowitz, Tao Ge, Akshay Subramaniam, Aayush Gupta, David M. Hall, Morteza Mardani, Arash Vahdat, Karthik Kashinath, Michael S. Pritchard",
                "publication_date": "2025-05-10",
                "reference": "Climate in a Bottle: Towards a Generative Foundation Model for the Kilometer-Scale Global Atmosphere",
                "link": "https://arxiv.org/abs/2505.06474v1",
                "citations": null,
                "notability_criteria": null,
                "parameters": 330000000.0,
                "training_compute_(flop)": 1.6014864e+21,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 64.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": 1499.0,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 87798.45074975025,
                "training_compute_estimation_method": "Hardware",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 8.518513939877888,
                "log_compute": 21.20452325493863,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "Pangu Ultra MoE",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering",
                "organization": "Huawei",
                "authors": "Yehui Tang, Yichun Yin, Yaoyuan Wang, Hang Zhou, Yu Pan, Wei Guo, Ziyang Zhang, Miao Rang, Fangcheng Liu, Naifu Zhang, Binghan Li, Yonghan Dong, Xiaojun Meng, Yasheng Wang, Dong Li, Yin Li, Dandan Tu, Can Chen, Youliang Yan, Fisher Yu, Ruiming Tang, Yunhe Wang, Botian Huang, Bo Wang, Boxiao Liu, Changzheng Zhang, Da Kuang, Fei Liu, Gang Huang, Jiansheng Wei, Jiarui Qin, Jie Ran, Jinpeng Li, Jun Zhao, Liang Dai, Lin Li, Liqun Deng, Peifeng Qin, Pengyuan Zeng, Qiang Gu, Shaohua Tang, Shengjun Cheng, Tao Gao, Tao Yu, Tianshu Li, Tianyu Bi, Wei He, Weikai Mao, Wenyong Huang, Wulong Liu, Xiabing Li, Xianzhi Yu, Xueyu Wu, Xu He, Yangkai Du, Yan Xu, Ye Tian, Yimeng Wu, Yongbing Huang, Yong Tian, Yong Zhu, Yue Li, Yufei Wang, Yuhang Gai, Yujun Li, Yu Luo, Yunsheng Ni, Yusen Sun, Zelin Chen, Zhe Liu, Zhicheng Liu, Zhipeng Tu, Zilin Ding, Zongyuan Zhan",
                "publication_date": "2025-05-07",
                "reference": "Pangu Ultra MoE: How to Train Your Big MoE on Ascend NPUs",
                "link": "https://arxiv.org/abs/2505.04519",
                "citations": null,
                "notability_criteria": null,
                "parameters": 718000000000.0,
                "training_compute_(flop)": 3.0888e+24,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "Huawei Ascend 910B",
                "approach": null,
                "confidence": "Speculative",
                "epochs": null,
                "model_accessibility": "Hosted access (no API)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 6000.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 4703802.674709778,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 11.856124444242301,
                "log_compute": 24.489789788615994,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Apriel Nemotron 15B",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Quantitative reasoning,Chat",
                "organization": "NVIDIA,ServiceNow",
                "authors": "Unknown",
                "publication_date": "2025-05-06",
                "reference": "New Apriel Nemotron 15B reasoning model delivers lower latency, lower inference costs, and faster agentic AI\u2014purpose built for performance, cost, and scale",
                "link": "https://www.servicenow.com/company/media/press-room/nvidia-enterprise-ai-agents.html",
                "citations": null,
                "notability_criteria": null,
                "parameters": 15000000000.0,
                "training_compute_(flop)": 9.00001e+21,
                "training_dataset_size_(gradients)": 100000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 10.176091259055681,
                "log_compute": 21.95424299198848,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "Phi-4-Reasoning",
                "domain": "Language",
                "task": "Language modeling/generation,Quantitative reasoning,Question answering,Mathematical reasoning,Code generation",
                "organization": "Microsoft",
                "authors": "Marah Abdin, Sahaj Agarwal, Ahmed Awadallah, Vidhisha Balachandran, Harkirat Behl, Lingjiao Chen, Gustavo de Rosa, Suriya Gunasekar, Mojan Javaheripi, Neel Joshi, Piero Kauffmann, Yash Lara, Caio C\u00e9sar Teodoro Mendes, Arindam Mitra, Besmira Nushi, Dimitris Papailiopoulos, Olli Saarikivi, Shital Shah, Vaishnavi Shrivastava, Vibhav Vineet, Yue Wu, Safoora Yousefi, Guoqing Zheng",
                "publication_date": "2025-04-30",
                "reference": "Phi-4-reasoning Technical Report",
                "link": "https://arxiv.org/abs/2504.21318",
                "citations": null,
                "notability_criteria": null,
                "parameters": 14000000000.0,
                "training_compute_(flop)": 9.3368077e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 60.0,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.93,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": "Phi-4",
                "finetune_compute_(flop)": 1.6606191e+21,
                "hardware_quantity": 32.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 43909.002544977615,
                "training_compute_estimation_method": "Operation counting,Hardware",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 10.146128035678238,
                "log_compute": 23.97019841421128,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "Qwen3-235B-A22B",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Mathematical reasoning,Quantitative reasoning,Code generation,Translation",
                "organization": "Alibaba",
                "authors": "Unknown",
                "publication_date": "2025-04-29",
                "reference": "Qwen3: Think Deeper, Act Faster",
                "link": "https://qwenlm.github.io/blog/qwen3/",
                "citations": null,
                "notability_criteria": "Discretionary",
                "parameters": 235000000000.0,
                "training_compute_(flop)": 4.752e+24,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Likely",
                "epochs": 1.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-03 13:41:26+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 11.371067862271737,
                "log_compute": 24.676876431973138,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Qwen3-30B-A3B",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Mathematical reasoning,Quantitative reasoning,Code generation,Translation",
                "organization": "Alibaba",
                "authors": "Unknown",
                "publication_date": "2025-04-29",
                "reference": "Qwen3: Think Deeper, Act Faster",
                "link": "https://qwenlm.github.io/blog/qwen3/",
                "citations": null,
                "notability_criteria": null,
                "parameters": 30000000000.0,
                "training_compute_(flop)": 6.48e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Likely",
                "epochs": 1.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 10.477121254719663,
                "log_compute": 23.811575005870594,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Qwen3-32B",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Mathematical reasoning,Quantitative reasoning,Code generation,Translation",
                "organization": "Alibaba",
                "authors": "Unknown",
                "publication_date": "2025-04-29",
                "reference": "Qwen3: Think Deeper, Act Faster",
                "link": "https://qwenlm.github.io/blog/qwen3/",
                "citations": null,
                "notability_criteria": null,
                "parameters": 32800000000.0,
                "training_compute_(flop)": 7.0848e+24,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 10.51587384371168,
                "log_compute": 24.85032759486261,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Qwen3-14B",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Mathematical reasoning,Quantitative reasoning,Code generation,Translation",
                "organization": "Alibaba",
                "authors": "Unknown",
                "publication_date": "2025-04-29",
                "reference": "Qwen3: Think Deeper, Act Faster",
                "link": "https://qwenlm.github.io/blog/qwen3/",
                "citations": null,
                "notability_criteria": null,
                "parameters": 14800000000.0,
                "training_compute_(flop)": 3.1968e+24,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 10.170261715394957,
                "log_compute": 24.50471546654589,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Qwen3-8B",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Mathematical reasoning,Quantitative reasoning,Code generation,Translation",
                "organization": "Alibaba",
                "authors": "Unknown",
                "publication_date": "2025-04-29",
                "reference": "Qwen3: Think Deeper, Act Faster",
                "link": "https://qwenlm.github.io/blog/qwen3/",
                "citations": null,
                "notability_criteria": null,
                "parameters": 8200000000.0,
                "training_compute_(flop)": 1.7712e+24,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 9.913813852383717,
                "log_compute": 24.248267603534646,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Qwen3-4B",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Mathematical reasoning,Quantitative reasoning,Code generation,Translation",
                "organization": "Alibaba",
                "authors": "Unknown",
                "publication_date": "2025-04-29",
                "reference": "Qwen3: Think Deeper, Act Faster",
                "link": "https://qwenlm.github.io/blog/qwen3/",
                "citations": null,
                "notability_criteria": null,
                "parameters": 4000000000.0,
                "training_compute_(flop)": 8.64e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 9.602059991327963,
                "log_compute": 23.936513742478894,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Qwen3-1.7B",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Mathematical reasoning,Quantitative reasoning,Code generation,Translation",
                "organization": "Alibaba",
                "authors": "Unknown",
                "publication_date": "2025-04-29",
                "reference": "Qwen3: Think Deeper, Act Faster",
                "link": "https://qwenlm.github.io/blog/qwen3/",
                "citations": null,
                "notability_criteria": null,
                "parameters": 1700000000.0,
                "training_compute_(flop)": 3.672e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 9.230448921378274,
                "log_compute": 23.564902672529204,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Qwen3-0.6B",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Mathematical reasoning,Quantitative reasoning,Code generation,Translation",
                "organization": "Alibaba",
                "authors": "Unknown",
                "publication_date": "2025-04-29",
                "reference": "Qwen3: Think Deeper, Act Faster",
                "link": "https://qwenlm.github.io/blog/qwen3/",
                "citations": null,
                "notability_criteria": null,
                "parameters": 600000000.0,
                "training_compute_(flop)": 1.296e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 8.778151250383644,
                "log_compute": 23.112605001534575,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Foundation-sec-8b",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering",
                "organization": "Cisco",
                "authors": "Paul Kassianik, Baturay Saglam, Alexander Chen, Blaine Nelson, Anu Vellore, Massimo Aufiero, Fraser Burch, Dhruv Kedia, Avi Zohary, Sajana Weerawardhena, Aman Priyanshu, Adam Swanda, Amy Chang, Hyrum Anderson, Kojin Oshiba, Omar Santos, Yaron Singer, Amin Karbasi",
                "publication_date": "2025-04-28",
                "reference": "Llama-3.1-FoundationAI-SecurityLLM-Base-8B Technical Report",
                "link": "https://arxiv.org/abs/2504.21039",
                "citations": null,
                "notability_criteria": null,
                "parameters": 8000000000.0,
                "training_compute_(flop)": 1.4688e+24,
                "training_dataset_size_(gradients)": 5100000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": "Llama 3.1-8B",
                "finetune_compute_(flop)": 2.448e+23,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 9.903089986991944,
                "log_compute": 24.166962663857166,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "Nemotron-H 8B",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Translation,Quantitative reasoning,Code generation",
                "organization": "NVIDIA",
                "authors": "NVIDIA: Aaron Blakeman, Aarti Basant, Abhinav Khattar, Adithya Renduchintala, Akhiad Bercovich, Aleksander Ficek, Alexis Bjorlin, Ali Taghibakhshi, Amala Sanjay Deshmukh, Ameya Sunil Mahabaleshwarkar, Andrew Tao, Anna Shors, Ashwath Aithal, Ashwin Poojary, Ayush Dattagupta, Balaram Buddharaju, Bobby Chen, Boris Ginsburg, Boxin Wang, Brandon Norick, Brian Butterfield, Bryan Catanzaro, Carlo del Mundo, Chengyu Dong, Christine Harvey, Christopher Parisien, Dan Su, Daniel Korzekwa, Danny Yin, Daria Gitman, David Mosallanezhad, Deepak Narayanan, Denys Fridman, Dima Rekesh, Ding Ma, Dmytro Pykhtar, Dong Ahn, Duncan Riach, Dusan Stosic, Eileen Long, Elad Segal, Ellie Evans, Eric Chung, Erick Galinkin, Evelina Bakhturina, Ewa Dobrowolska, Fei Jia, Fuxiao Liu, Gargi Prasad, Gerald Shen, Guilin Liu, Guo Chen, Haifeng Qian, Helen Ngo, Hongbin Liu, Hui Li, Igor Gitman, Ilia Karmanov, Ivan Moshkov, Izik Golan, Jan Kautz, Jane Polak Scowcroft, Jared Casper, Jarno Seppanen, Jason Lu, Jason Sewall, Jiaqi Zeng, Jiaxuan You, Jimmy Zhang, Jing Zhang, Jining Huang, Jinze Xue, Jocelyn Huang, Joey Conway, John Kamalu, Jon Barker, Jonathan Cohen, Joseph Jennings, Jupinder Parmar, Karan Sapra, Kari Briski, Kateryna Chumachenko, Katherine Luna, Keshav Santhanam, Kezhi Kong, Kirthi Sivamani, Krzysztof Pawelec, Kumar Anik, Kunlun Li, Lawrence McAfee, Leon Derczynski, Lindsey Pavao, Luis Vega, Lukas Voegtle, Maciej Bala, Maer Rodrigues de Melo, Makesh Narsimhan Sreedhar, Marcin Chochowski, Markus Kliegl et al. (100 additional authors not shown)",
                "publication_date": "2025-04-14",
                "reference": "Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models",
                "link": "https://arxiv.org/abs/2504.03624",
                "citations": null,
                "notability_criteria": null,
                "parameters": 8000000000.0,
                "training_compute_(flop)": 7.2e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 6291456.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 9.903089986991944,
                "log_compute": 23.85733249643127,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "Nemotron-H 56B",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Translation,Quantitative reasoning,Code generation",
                "organization": "NVIDIA",
                "authors": "NVIDIA: Aaron Blakeman, Aarti Basant, Abhinav Khattar, Adithya Renduchintala, Akhiad Bercovich, Aleksander Ficek, Alexis Bjorlin, Ali Taghibakhshi, Amala Sanjay Deshmukh, Ameya Sunil Mahabaleshwarkar, Andrew Tao, Anna Shors, Ashwath Aithal, Ashwin Poojary, Ayush Dattagupta, Balaram Buddharaju, Bobby Chen, Boris Ginsburg, Boxin Wang, Brandon Norick, Brian Butterfield, Bryan Catanzaro, Carlo del Mundo, Chengyu Dong, Christine Harvey, Christopher Parisien, Dan Su, Daniel Korzekwa, Danny Yin, Daria Gitman, David Mosallanezhad, Deepak Narayanan, Denys Fridman, Dima Rekesh, Ding Ma, Dmytro Pykhtar, Dong Ahn, Duncan Riach, Dusan Stosic, Eileen Long, Elad Segal, Ellie Evans, Eric Chung, Erick Galinkin, Evelina Bakhturina, Ewa Dobrowolska, Fei Jia, Fuxiao Liu, Gargi Prasad, Gerald Shen, Guilin Liu, Guo Chen, Haifeng Qian, Helen Ngo, Hongbin Liu, Hui Li, Igor Gitman, Ilia Karmanov, Ivan Moshkov, Izik Golan, Jan Kautz, Jane Polak Scowcroft, Jared Casper, Jarno Seppanen, Jason Lu, Jason Sewall, Jiaqi Zeng, Jiaxuan You, Jimmy Zhang, Jing Zhang, Jining Huang, Jinze Xue, Jocelyn Huang, Joey Conway, John Kamalu, Jon Barker, Jonathan Cohen, Joseph Jennings, Jupinder Parmar, Karan Sapra, Kari Briski, Kateryna Chumachenko, Katherine Luna, Keshav Santhanam, Kezhi Kong, Kirthi Sivamani, Krzysztof Pawelec, Kumar Anik, Kunlun Li, Lawrence McAfee, Leon Derczynski, Lindsey Pavao, Luis Vega, Lukas Voegtle, Maciej Bala, Maer Rodrigues de Melo, Makesh Narsimhan Sreedhar, Marcin Chochowski, Markus Kliegl et al. (100 additional authors not shown)",
                "publication_date": "2025-04-14",
                "reference": "Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models",
                "link": "https://arxiv.org/abs/2504.03624",
                "citations": null,
                "notability_criteria": null,
                "parameters": 56000000000.0,
                "training_compute_(flop)": 6.72e+24,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 6144.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 6291456.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 8433532.904958995,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 10.7481880270062,
                "log_compute": 24.827369273053826,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "GLM-Z1-Rumination-32B-0414",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Quantitative reasoning,Code generation",
                "organization": "Tsinghua University",
                "authors": "Unknown",
                "publication_date": "2025-04-14",
                "reference": "GLM-4-Z1-Rumination-32B-0414",
                "link": "https://huggingface.co/THUDM/GLM-Z1-Rumination-32B-0414",
                "citations": null,
                "notability_criteria": null,
                "parameters": 32000000000.0,
                "training_compute_(flop)": 2.88e+24,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 10.505149978319906,
                "log_compute": 24.45939248775923,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "GLM-4-9B-0414",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Quantitative reasoning,Code generation",
                "organization": "Tsinghua University",
                "authors": "Unknown",
                "publication_date": "2025-04-14",
                "reference": "GLM-4-9B-0414",
                "link": "https://huggingface.co/THUDM/GLM-4-9B-0414",
                "citations": null,
                "notability_criteria": null,
                "parameters": 9000000000.0,
                "training_compute_(flop)": 8.1e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 9.954242509439325,
                "log_compute": 23.90848501887865,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Seaweed-7B",
                "domain": "Video",
                "task": "Video generation,Text-to-video,Image-to-video,Audio generation",
                "organization": "ByteDance",
                "authors": "Team Seawead, Ceyuan Yang, Zhijie Lin, Yang Zhao, Shanchuan Lin, Zhibei Ma, Haoyuan Guo, Hao Chen, Lu Qi, Sen Wang, Feng Cheng, Feilong Zuo Xuejiao Zeng, Ziyan Yang, Fangyuan Kong, Zhiwu Qing, Fei Xiao, Meng Wei, Tuyen Hoang, Siyu Zhang, Peihao Zhu, Qi Zhao, Jiangqiao Yan, Liangke Gui, Sheng Bi, Jiashi Li, Yuxi Ren, Rui Wang, Huixia Li, Xuefeng Xiao, Shu Liu, Feng Ling, Heng Zhang, Houmin Wei, Huafeng Kuang, Jerry Duncan, Junda Zhang, Junru Zheng, Li Sun, Manlin Zhang, Renfei Sun, Xiaobin Zhuang, Xiaojie Li, Xin Xia, Xuyan Chi, Yanghua Peng, Yuping Wang, Yuxuan Wang, Zhongkai Zhao, Zhuo Chen, Zuquan Song, Zhenheng Yang, Jiashi Feng, Jianchao Yang, Lu Jiang",
                "publication_date": "2025-04-11",
                "reference": "Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model\n",
                "link": "https://arxiv.org/abs/2504.08685",
                "citations": 45.0,
                "notability_criteria": null,
                "parameters": 7000000000.0,
                "training_compute_(flop)": 9.0007697e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:08:20+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": 665000.0,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Vision",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 9.845098040014257,
                "log_compute": 23.95427964968038,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Pangu Ultra",
                "domain": "Language",
                "task": "Code generation,Language modeling/generation",
                "organization": "Huawei",
                "authors": "Yichun Yin, Wenyong Huang, Kaikai Song, Yehui Tang, Xueyu Wu, Wei Guo, Peng Guo, Yaoyuan Wang, Xiaojun Meng, Yasheng Wang, Dong Li, Can Chen, Dandan Tu, Yin Li, Fisher Yu, Ruiming Tang, Yunhe Wang, Baojun Wang, Bin Wang, Bo Wang, Boxiao Liu, Changzheng Zhang, Duyu Tang, Fei Mi, Hui Jin, Jiansheng Wei, Jiarui Qin, Jinpeng Li, Jun Zhao, Liqun Deng, Lin Li, Minghui Xu, Naifu Zhang, Nianzu Zheng, Qiang Li, Rongju Ruan, Shengjun Cheng, Tianyu Guo, Wei He, Wei Li, Weiwen Liu, Wulong Liu, Xinyi Dai, Yonghan Dong, Yu Pan, Yue Li, Yufei Wang, Yujun Li, Yunsheng Ni, Zhe Liu, Zhenhe Zhang, Zhicheng Liu",
                "publication_date": "2025-04-10",
                "reference": "Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs",
                "link": "https://arxiv.org/abs/2504.07866",
                "citations": 11.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 135000000000.0,
                "training_compute_(flop)": 1.0692e+25,
                "training_dataset_size_(gradients)": 13200000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "Huawei Ascend 910B",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Hosted access (no API)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 8192.0,
                "last_modified": "2025-10-14 18:02:45+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 6426121.277196856,
                "training_compute_estimation_method": "Operation counting,Comparison with other models",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 11.130333768495007,
                "log_compute": 25.0290589500845,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "AMIE (Articulate Medical Intelligence Explorer)",
                "domain": "Medicine,Language",
                "task": "Medical diagnosis,Language modeling/generation,Question answering,Chat",
                "organization": "Google DeepMind,Google Research",
                "authors": "Tao Tu, Mike Schaekermann, Anil Palepu, Khaled Saab, Jan Freyberg, Ryutaro Tanno, Amy Wang, Brenna Li, Mohamed Amin, Yong Cheng, Elahe Vedadi, Nenad Tomasev, Shekoofeh Azizi, Karan Singhal, Le Hou, Albert Webson, Kavita Kulkarni, S. Sara Mahdavi, Christopher Semturs, Juraj Gottweis, Joelle Barral, Katherine Chou, Greg S. Corrado, Yossi Matias, Alan Karthikesalingam, Vivek Natarajan",
                "publication_date": "2025-04-09",
                "reference": "Towards conversational diagnostic artificial intelligence",
                "link": "https://www.nature.com/articles/s41586-025-08866-7?linkId=13898052\nhttps://www.nature.com/articles/s41586-025-08869-4?linkId=13898054",
                "citations": null,
                "notability_criteria": null,
                "parameters": 340000000000.0,
                "training_compute_(flop)": 7.34e+24,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": "PaLM 2",
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Comparison with other models",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 11.531478917042255,
                "log_compute": 24.865696059916072,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "TxGemma 27B",
                "domain": "Language,Biology",
                "task": "Protein or nucleotide language model (pLM/nLM),Protein property prediction,Small molecule property prediction,Chat,Question answering,Protein question answering,Protein function prediction,Language modeling/generation",
                "organization": "Google DeepMind,Google Research",
                "authors": "Eric Wang, Samuel Schmidgall, Paul F. Jaeger, Fan Zhang, Rory Pilgrim, Yossi Matias, Joelle Barral, David Fleet, Shekoofeh Azizi",
                "publication_date": "2025-04-08",
                "reference": "TxGemma: Efficient and Agentic LLMs for Therapeutics",
                "link": "https://arxiv.org/abs/2504.06196",
                "citations": null,
                "notability_criteria": null,
                "parameters": 27000000000.0,
                "training_compute_(flop)": 2.116854e+24,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "Google TPU v4",
                "approach": null,
                "confidence": "Confident",
                "epochs": 12.0,
                "model_accessibility": "Open weights (restricted use)",
                "country": "Multinational",
                "base_model": "Gemma 2 27B",
                "finetune_compute_(flop)": 1.0854e+22,
                "hardware_quantity": 256.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 85350.72454206382,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 10.431363764158988,
                "log_compute": 24.32569090564298,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "TxGemma 9B",
                "domain": "Language,Biology",
                "task": "Protein or nucleotide language model (pLM/nLM),Protein property prediction,Small molecule property prediction,Chat,Question answering,Protein question answering,Protein function prediction,Language modeling/generation",
                "organization": "Google DeepMind,Google Research",
                "authors": "Eric Wang, Samuel Schmidgall, Paul F. Jaeger, Fan Zhang, Rory Pilgrim, Yossi Matias, Joelle Barral, David Fleet, Shekoofeh Azizi",
                "publication_date": "2025-04-08",
                "reference": "TxGemma: Efficient and Agentic LLMs for Therapeutics",
                "link": "https://arxiv.org/abs/2504.06196",
                "citations": null,
                "notability_criteria": null,
                "parameters": 9000000000.0,
                "training_compute_(flop)": 4.35618e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "Google TPU v4",
                "approach": null,
                "confidence": "Confident",
                "epochs": 12.0,
                "model_accessibility": "Open weights (restricted use)",
                "country": "Multinational",
                "base_model": "Gemma 2 9B",
                "finetune_compute_(flop)": 3.618e+21,
                "hardware_quantity": 256.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 85350.72454206382,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 9.954242509439325,
                "log_compute": 23.639105816765934,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "TxGemma 2B",
                "domain": "Language,Biology",
                "task": "Protein or nucleotide language model (pLM/nLM),Protein property prediction,Small molecule property prediction,Question answering,Protein question answering,Protein function prediction,Language modeling/generation",
                "organization": "Google DeepMind,Google Research",
                "authors": "Eric Wang, Samuel Schmidgall, Paul F. Jaeger, Fan Zhang, Rory Pilgrim, Yossi Matias, Joelle Barral, David Fleet, Shekoofeh Azizi",
                "publication_date": "2025-04-08",
                "reference": "TxGemma: Efficient and Agentic LLMs for Therapeutics",
                "link": "https://arxiv.org/abs/2504.06196",
                "citations": null,
                "notability_criteria": null,
                "parameters": 2600000000.0,
                "training_compute_(flop)": 3.22452e+22,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "Google TPU v4",
                "approach": null,
                "confidence": "Confident",
                "epochs": 12.0,
                "model_accessibility": "Open weights (restricted use)",
                "country": "Multinational",
                "base_model": "Gemma 2 2B",
                "finetune_compute_(flop)": 1.0452e+21,
                "hardware_quantity": 256.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 85350.72454206382,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 9.414973347970818,
                "log_compute": 22.50846507498175,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "Llama 4 Scout",
                "domain": "Multimodal,Language,Vision",
                "task": "Chat,Code generation,Visual question answering,Language modeling/generation,Question answering",
                "organization": "Meta AI",
                "authors": "Unknown",
                "publication_date": "2025-04-05",
                "reference": "The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation",
                "link": "https://ai.meta.com/blog/llama-4-multimodal-intelligence/",
                "citations": null,
                "notability_criteria": "Discretionary",
                "parameters": 109000000000.0,
                "training_compute_(flop)": 4.08e+24,
                "training_dataset_size_(gradients)": 30000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": null,
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 11.037426497940624,
                "log_compute": 24.61066016308988,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "Llama 4 Maverick",
                "domain": "Multimodal,Language,Vision",
                "task": "Chat,Code generation,Visual question answering,Language modeling/generation,Question answering",
                "organization": "Meta AI",
                "authors": "Unknown",
                "publication_date": "2025-04-05",
                "reference": "The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation",
                "link": "https://ai.meta.com/blog/llama-4-multimodal-intelligence/",
                "citations": null,
                "notability_criteria": "Discretionary",
                "parameters": 400000000000.0,
                "training_compute_(flop)": 2.244000000001e+24,
                "training_dataset_size_(gradients)": 30000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": null,
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 11.602059991327963,
                "log_compute": 24.351022852584318,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "Llama 4 Behemoth (preview)",
                "domain": "Multimodal,Language,Vision",
                "task": "Chat,Code generation,Visual question answering,Translation,Language modeling/generation,Quantitative reasoning,Question answering",
                "organization": "Meta AI",
                "authors": "Unknown",
                "publication_date": "2025-04-05",
                "reference": "The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation",
                "link": "https://ai.meta.com/blog/llama-4-multimodal-intelligence/",
                "citations": null,
                "notability_criteria": "Training cost",
                "parameters": 2000000000000.0,
                "training_compute_(flop)": 5.18400000000001e+25,
                "training_dataset_size_(gradients)": 30000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 32000.0,
                "last_modified": "2025-10-16 15:12:26+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": 44588963.624007165,
                "frontier_model": true,
                "training_power_draw_(w)": 43933454.99810563,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 12.301029995663981,
                "log_compute": 25.714664992862538,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "Lumina-Image-2.0",
                "domain": "Image generation",
                "task": "Image generation,Text-to-image",
                "organization": "Shanghai AI Lab,University of Sydney,Chinese University of Hong Kong (CUHK),Shanghai Jiao Tong University,Krea AI",
                "authors": "Qi Qin, Le Zhuo, Yi Xin, Ruoyi Du, Zhen Li, Bin Fu, Yiting Lu, Jiakang Yuan, Xinyue Li, Dongyang Liu, Xiangyang Zhu, Manyuan Zhang, Will Beddow, Erwann Millon, Victor Perez, Wenhai Wang, Conghui He, Bo Zhang, Xiaohong Liu, Hongsheng Li, Yu Qiao, Chang Xu, Peng Gao",
                "publication_date": "2025-03-27",
                "reference": "Lumina-Image 2.0: A Unified and Efficient Image Generative Framework",
                "link": "https://arxiv.org/abs/2503.21758",
                "citations": null,
                "notability_criteria": null,
                "parameters": 2600000000.0,
                "training_compute_(flop)": 4.7794406e+21,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": 14184.0,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Vision",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 9.414973347970818,
                "log_compute": 21.679377068463566,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Llama Nemotron Ultra 253B",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Quantitative reasoning,Code generation,Neural Architecture Search - NAS",
                "organization": "NVIDIA",
                "authors": "Akhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido Galil, Zach Moshe, Tomer Ronen, Najeeb Nabwani, Ido Shahaf, Oren Tropp, Ehud Karpas, Ran Zilberstein, Jiaqi Zeng, Soumye Singhal, Alexander Bukharin, Yian Zhang, Tugrul Konuk, Gerald Shen, Ameya Sunil Mahabaleshwarkar, Bilal Kartal, Yoshi Suhara, Olivier Delalleau, Zijia Chen, Zhilin Wang, David Mosallanezhad, Adi Renduchintala, Haifeng Qian, Dima Rekesh, Fei Jia, Somshubra Majumdar, Vahid Noroozi, Wasi Uddin Ahmad, Sean Narenthiran, Aleksander Ficek, Mehrzad Samadi, Jocelyn Huang, Siddhartha Jain, Igor Gitman, Ivan Moshkov, Wei Du, Shubham Toshniwal, George Armstrong, Branislav Kisacanin, Matvei Novikov, Daria Gitman, Evelina Bakhturina, Jane Polak Scowcroft, John Kamalu, Dan Su, Kezhi Kong, Markus Kliegl, Rabeeh Karimi, Ying Lin, Sanjeev Satheesh, Jupinder Parmar, Pritam Gundecha, Brandon Norick, Joseph Jennings, Shrimai Prabhumoye, Syeda Nahida Akter, Mostofa Patwary, Abhinav Khattar, Deepak Narayanan, Roger Waleffe, Jimmy Zhang, Bor-Yiing Su, Guyue Huang, Terry Kong, Parth Chadha, Sahil Jain, Christine Harvey, Elad Segal, Jining Huang, Sergey Kashirsky, Robert McQueen, Izzy Putterman, George Lam, Arun Venkatesan, Sherry Wu, Vinh Nguyen, Manoj Kilaru, Andrew Wang, Anna Warno, Abhilash Somasamudramath, Sandip Bhaskar, Maka Dong, Nave Assaf, Shahar Mor, Omer Ullman Argov, Scot Junkin, Oleksandr Romanenko, Pedro Larroy, Monika Katariya, Marco Rovinelli, Viji Balas, Nicholas Edelman, Anahita Bhiwandiwalla, Muthu Subramaniam et al. (32 additional authors not shown)",
                "publication_date": "2025-03-18",
                "reference": "Ultra is 253B distilled from Llama 3.1 405B for maximum agentic accuracy on multi-GPU data center servers.",
                "link": "https://arxiv.org/abs/2505.00949",
                "citations": null,
                "notability_criteria": null,
                "parameters": 253000000000.0,
                "training_compute_(flop)": 3.911001e+25,
                "training_dataset_size_(gradients)": 603000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "Multinational",
                "base_model": "Llama 3.1-405B",
                "finetune_compute_(flop)": 1.114817000000001e+24,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": true,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 11.403120521175818,
                "log_compute": 25.592287926996473,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "GR00T N1 2B",
                "domain": "Robotics,Vision,Language",
                "task": "Robotic manipulation,Animal (human/non-human) imitation",
                "organization": "NVIDIA",
                "authors": "Johan Bjorck, Fernando Casta\u00f1eda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi \"Jim\" Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, Joel Jang, Zhenyu Jiang, Jan Kautz, Kaushil Kundalia, Lawrence Lao, Zhiqi Li, Zongyu Lin, Kevin Lin, Guilin Liu, Edith Llontop, Loic Magne, Ajay Mandlekar, Avnish Narayan, Soroush Nasiriany, Scott Reed, You Liang Tan, Guanzhi Wang, Zu Wang, Jing Wang, Qi Wang, Jiannan Xiang, Yuqi Xie, Yinzhen Xu, Zhenjia Xu, Seonghyeon Ye, Zhiding Yu, Ao Zhang, Hao Zhang, Yizhou Zhao, Ruijie Zheng, Yuke Zhu",
                "publication_date": "2025-03-18",
                "reference": "GR00T N1: An Open Foundation Model for Generalist Humanoid Robots",
                "link": "https://arxiv.org/abs/2503.14734v2\nhttps://arxiv.org/abs/2503.14734v1\nhttps://github.com/NVIDIA/Isaac-GR00T\nhttps://nvidianews.nvidia.com/news/nvidia-isaac-gr00t-n1-open-humanoid-robot-foundation-model-simulation-frameworks\nhttps://developer.nvidia.com/blog/accelerate-generalist-humanoid-robot-development-with-nvidia-isaac-gr00t-n1/\nhttps://huggingface.co/nvidia/GR00T-N1-2B\nhttps://huggingface.co/datasets/nvidia/PhysicalAI-Robotics-GR00T-X-Embodiment-Sim\n\n",
                "citations": 224.0,
                "notability_criteria": null,
                "parameters": 2190000000.0,
                "training_compute_(flop)": 7.12368e+22,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 48.828125,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "Multinational",
                "base_model": "Eagle 2",
                "finetune_compute_(flop)": null,
                "hardware_quantity": 1024.0,
                "last_modified": "2025-10-16 13:09:06+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 16384.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 1406434.214264239,
                "training_compute_estimation_method": "Hardware",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 9.340444114840118,
                "log_compute": 22.85270440245943,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "Cosmos-Transfer1-7B",
                "domain": "Video,Vision,Robotics",
                "task": "Robotic manipulation,System control,Video generation",
                "organization": "NVIDIA",
                "authors": "NVIDIA: Hassan Abu Alhaija, Jose Alvarez, Maciej Bala, Tiffany Cai, Tianshi Cao, Liz Cha, Joshua Chen, Mike Chen, Francesco Ferroni, Sanja Fidler, Dieter Fox, Yunhao Ge, Jinwei Gu, Ali Hassani, Michael Isaev, Pooya Jannaty, Shiyi Lan, Tobias Lasser, Huan Ling, Ming-Yu Liu, Xian Liu, Yifan Lu, Alice Luo, Qianli Ma, Hanzi Mao, Fabio Ramos, Xuanchi Ren, Tianchang Shen, Xinglong Sun, Shitao Tang, Ting-Chun Wang, Jay Wu, Jiashu Xu, Stella Xu, Kevin Xie, Yuchong Ye, Xiaodong Yang, Xiaohui Zeng, Yu Zeng",
                "publication_date": "2025-03-18",
                "reference": "Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal Control",
                "link": "https://arxiv.org/abs/2503.14492",
                "citations": null,
                "notability_criteria": null,
                "parameters": 7000000000.0,
                "training_compute_(flop)": 3.6059017e+24,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 2016.0,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "Multinational",
                "base_model": "Cosmos-Predict1-7b-Video2World",
                "finetune_compute_(flop)": 2.205901651968e+24,
                "hardware_quantity": 1024.0,
                "last_modified": "2025-10-01 15:11:33+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 1406434.214264239,
                "training_compute_estimation_method": "Hardware",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 9.845098040014257,
                "log_compute": 24.557013883304446,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "Gemma 3 27B",
                "domain": "Language,Vision,Multimodal",
                "task": "Language modeling/generation,Question answering,Translation,Chat,Quantitative reasoning,Visual question answering,Code generation",
                "organization": "Google DeepMind",
                "authors": "Core contributors: Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ram\u00e9, Morgane Rivi\u00e8re, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean-bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, Ga\u00ebl Liu, Francesco Visin, Kathleen Kenealy, Lucas Beyer, Xiaohai Zhai, Anton Tsitsulin, Robert Busa-Fekete, Alex Feng, Noveen Sachdeva, Benjamin Coleman, Yi Gao, Basil Mustafa, Iain Barr, Emilio Parisotto, David Tian, Matan Eyal, Colin Cherry, Jan-Thorsten Peter, Danila Sinopalnikov, Surya Bhupatiraju, Rishabh Agarwal, Mehran Kazemi, Dan Malkin, David Vilar, Idan Brusilovsky, Jiaming Luo, Andreas Steiner",
                "publication_date": "2025-03-12",
                "reference": "Gemma 3 Technical Report\n",
                "link": "https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf",
                "citations": null,
                "notability_criteria": null,
                "parameters": 27000000000.0,
                "training_compute_(flop)": 2.268e+24,
                "training_dataset_size_(gradients)": 14000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "Google TPU v5p",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "Multinational",
                "base_model": "SigLIP 400M",
                "finetune_compute_(flop)": null,
                "hardware_quantity": 6144.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 10.431363764158988,
                "log_compute": 24.35564305022087,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "Gemma 3 12B",
                "domain": "Language,Vision,Multimodal",
                "task": "Language modeling/generation,Question answering,Translation,Chat,Quantitative reasoning,Visual question answering,Code generation",
                "organization": "Google DeepMind",
                "authors": "Core contributors: Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ram\u00e9, Morgane Rivi\u00e8re, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean-bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, Ga\u00ebl Liu, Francesco Visin, Kathleen Kenealy, Lucas Beyer, Xiaohai Zhai, Anton Tsitsulin, Robert Busa-Fekete, Alex Feng, Noveen Sachdeva, Benjamin Coleman, Yi Gao, Basil Mustafa, Iain Barr, Emilio Parisotto, David Tian, Matan Eyal, Colin Cherry, Jan-Thorsten Peter, Danila Sinopalnikov, Surya Bhupatiraju, Rishabh Agarwal, Mehran Kazemi, Dan Malkin, David Vilar, Idan Brusilovsky, Jiaming Luo, Andreas Steiner",
                "publication_date": "2025-03-12",
                "reference": "Gemma 3 Technical Report\n",
                "link": "https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf",
                "citations": null,
                "notability_criteria": null,
                "parameters": 12000000000.0,
                "training_compute_(flop)": 8.64e+23,
                "training_dataset_size_(gradients)": 12000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "Google TPU v4",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "Multinational",
                "base_model": "SigLIP 400M",
                "finetune_compute_(flop)": null,
                "hardware_quantity": 6144.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 2049649.4174839524,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 10.079181246047625,
                "log_compute": 23.936513742478894,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "Gemma 3 4B",
                "domain": "Language,Vision,Multimodal",
                "task": "Language modeling/generation,Question answering,Translation,Chat,Quantitative reasoning,Visual question answering,Code generation",
                "organization": "Google DeepMind",
                "authors": "Core contributors: Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ram\u00e9, Morgane Rivi\u00e8re, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean-bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, Ga\u00ebl Liu, Francesco Visin, Kathleen Kenealy, Lucas Beyer, Xiaohai Zhai, Anton Tsitsulin, Robert Busa-Fekete, Alex Feng, Noveen Sachdeva, Benjamin Coleman, Yi Gao, Basil Mustafa, Iain Barr, Emilio Parisotto, David Tian, Matan Eyal, Colin Cherry, Jan-Thorsten Peter, Danila Sinopalnikov, Surya Bhupatiraju, Rishabh Agarwal, Mehran Kazemi, Dan Malkin, David Vilar, Idan Brusilovsky, Jiaming Luo, Andreas Steiner",
                "publication_date": "2025-03-12",
                "reference": "Gemma 3 Technical Report\n",
                "link": "https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf",
                "citations": null,
                "notability_criteria": null,
                "parameters": 4000000000.0,
                "training_compute_(flop)": 9.6e+22,
                "training_dataset_size_(gradients)": 4000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "Google TPU v5e",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "Multinational",
                "base_model": "SigLIP 400M",
                "finetune_compute_(flop)": null,
                "hardware_quantity": 2048.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 562648.8597014771,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 9.602059991327963,
                "log_compute": 22.98227123303957,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "Gemma 3 1B",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Translation,Chat,Quantitative reasoning,Code generation,Mathematical reasoning",
                "organization": "Google DeepMind",
                "authors": "Core contributors: Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ram\u00e9, Morgane Rivi\u00e8re, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean-bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, Ga\u00ebl Liu, Francesco Visin, Kathleen Kenealy, Lucas Beyer, Xiaohai Zhai, Anton Tsitsulin, Robert Busa-Fekete, Alex Feng, Noveen Sachdeva, Benjamin Coleman, Yi Gao, Basil Mustafa, Iain Barr, Emilio Parisotto, David Tian, Matan Eyal, Colin Cherry, Jan-Thorsten Peter, Danila Sinopalnikov, Surya Bhupatiraju, Rishabh Agarwal, Mehran Kazemi, Dan Malkin, David Vilar, Idan Brusilovsky, Jiaming Luo, Andreas Steiner",
                "publication_date": "2025-03-12",
                "reference": "Gemma 3 Technical Report\n",
                "link": "https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf",
                "citations": null,
                "notability_criteria": null,
                "parameters": 1000000000.0,
                "training_compute_(flop)": 1.2e+22,
                "training_dataset_size_(gradients)": 2000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "Google TPU v5e",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "Multinational",
                "base_model": "SigLIP 400M",
                "finetune_compute_(flop)": null,
                "hardware_quantity": 512.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 140662.21492536928,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 9.0,
                "log_compute": 22.079181246047625,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "Ling-lite-1.5 (\"Bailing\")",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Code generation",
                "organization": "Ant Group",
                "authors": "Ling Team, Binwei Zeng, Chao Huang, Chao Zhang, Changxin Tian, Cong Chen, Dingnan Jin, Feng Yu, Feng Zhu, Feng Yuan, Fakang Wang, Gangshan Wang, Guangyao Zhai, Haitao Zhang, Huizhong Li, Jun Zhou, Jia Liu, Junpeng Fang, Junjie Ou, Jun Hu, Ji Luo, Ji Zhang, Jian Liu, Jian Sha, Jianxue Qian, Jiewei Wu, Junping Zhao, Jianguo Li, Jubao Feng, Jingchao Di, Junming Xu, Jinghua Yao, Kuan Xu, Kewei Du, Longfei Li, Lei Liang, Lu Yu, Li Tang, Lin Ju, Peng Xu, Qing Cui, Song Liu, Shicheng Li, Shun Song, Song Yan, Tengwei Cai, Tianyi Chen, Ting Guo, Ting Huang, Tao Feng, Tao Wu, Wei Wu, Xiaolu Zhang, Xueming Yang, Xin Zhao, Xiaobo Hu, Xin Lin, Yao Zhao, Yilong Wang, Yongzhen Guo, Yuanyuan Wang, Yue Yang, Yang Cao, Yuhao Fu, Yi Xiong, Yanzhe Li, Zhe Li, Zhiqiang Zhang, Ziqi Liu, Zhaoxin Huan, Zujie Wen, Zhenhang Sun, Zhuoxuan Du, Zhengyu He",
                "publication_date": "2025-03-10",
                "reference": "Every FLOP Counts: Scaling a 300B Mixture-of-Experts LING LLM without Premium GPUs",
                "link": "https://arxiv.org/abs/2503.05139",
                "citations": null,
                "notability_criteria": null,
                "parameters": 16800000000.0,
                "training_compute_(flop)": 1.485e+23,
                "training_dataset_size_(gradients)": 9000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 10.225309281725863,
                "log_compute": 23.171726453653232,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Ling-Plus (\"Bailing\")",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Code generation",
                "organization": "Ant Group",
                "authors": "Ling Team, Binwei Zeng, Chao Huang, Chao Zhang, Changxin Tian, Cong Chen, Dingnan Jin, Feng Yu, Feng Zhu, Feng Yuan, Fakang Wang, Gangshan Wang, Guangyao Zhai, Haitao Zhang, Huizhong Li, Jun Zhou, Jia Liu, Junpeng Fang, Junjie Ou, Jun Hu, Ji Luo, Ji Zhang, Jian Liu, Jian Sha, Jianxue Qian, Jiewei Wu, Junping Zhao, Jianguo Li, Jubao Feng, Jingchao Di, Junming Xu, Jinghua Yao, Kuan Xu, Kewei Du, Longfei Li, Lei Liang, Lu Yu, Li Tang, Lin Ju, Peng Xu, Qing Cui, Song Liu, Shicheng Li, Shun Song, Song Yan, Tengwei Cai, Tianyi Chen, Ting Guo, Ting Huang, Tao Feng, Tao Wu, Wei Wu, Xiaolu Zhang, Xueming Yang, Xin Zhao, Xiaobo Hu, Xin Lin, Yao Zhao, Yilong Wang, Yongzhen Guo, Yuanyuan Wang, Yue Yang, Yang Cao, Yuhao Fu, Yi Xiong, Yanzhe Li, Zhe Li, Zhiqiang Zhang, Ziqi Liu, Zhaoxin Huan, Zujie Wen, Zhenhang Sun, Zhuoxuan Du, Zhengyu He",
                "publication_date": "2025-03-10",
                "reference": "Every FLOP Counts: Scaling a 300B Mixture-of-Experts LING LLM without Premium GPUs",
                "link": "https://arxiv.org/abs/2503.05139",
                "citations": null,
                "notability_criteria": null,
                "parameters": 290000000000.0,
                "training_compute_(flop)": 1.5552e+24,
                "training_dataset_size_(gradients)": 9000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 11.462397997898956,
                "log_compute": 24.1917862475822,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "QwQ-32B",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Quantitative reasoning,Code generation",
                "organization": "Alibaba",
                "authors": "Qwen Team",
                "publication_date": "2025-03-06",
                "reference": "QwQ-32B: Embracing the Power of Reinforcement Learning",
                "link": "https://qwenlm.github.io/blog/qwq-32b/",
                "citations": null,
                "notability_criteria": "SOTA improvement",
                "parameters": 32500000000.0,
                "training_compute_(flop)": 3.51e+24,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Speculative",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": "Qwen2.5-Coder (32B)",
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": null,
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 10.511883360978874,
                "log_compute": 24.545307116465825,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Phi-4 Mini",
                "domain": "Language",
                "task": "Language modeling/generation,Visual question answering,Code generation,Quantitative reasoning,Translation",
                "organization": "Microsoft",
                "authors": "Abdelrahman Abouelenin, Atabak Ashfaq, Adam Atkinson, Hany Awadalla, Nguyen Bach, Jianmin Bao, Alon Benhaim, Martin Cai, Vishrav Chaudhary, Congcong Chen, Dong Chen, Dongdong Chen, Junkun Chen, Weizhu Chen, Yen-Chun Chen, Yi-ling Chen, Qi Dai, Xiyang Dai, Ruchao Fan, Mei Gao, Min Gao, Amit Garg, Abhishek Goswami, Junheng Hao, Amr Hendy, Yuxuan Hu, Xin Jin, Mahmoud Khademi, Dongwoo Kim, Young Jin Kim, Gina Lee, Jinyu Li, Yunsheng Li, Chen Liang, Xihui Lin, Zeqi Lin, Mengchen Liu, Yang Liu, Gilsinia Lopez, Chong Luo, Piyush Madan, Vadim Mazalov, Ali Mousavi, Anh Nguyen, Jing Pan, Daniel Perez-Becker, Jacob Platin, Thomas Portet, Kai Qiu, Bo Ren, Liliang Ren, Sambuddha Roy, Ning Shang, Yelong Shen, Saksham Singhal, Subhojit Som, Xia Song, Tetyana Sych, Praneetha Vaddamanu, Shuohang Wang, Yiming Wang, Zhenghao Wang, Haibin Wu, Haoran Xu, Weijian Xu, Yifan Yang, Ziyi Yang, Donghan Yu, Ishmam Zabir, Jianwen Zhang, Li Lyna Zhang, Yunan Zhang, Xiren Zhou",
                "publication_date": "2025-03-03",
                "reference": "Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs",
                "link": "https://arxiv.org/abs/2503.01743",
                "citations": null,
                "notability_criteria": null,
                "parameters": 3800000000.0,
                "training_compute_(flop)": 9.9561596e+22,
                "training_dataset_size_(gradients)": 5000000000000.0,
                "training_time_(hours)": 504.0,
                "training_hardware": "NVIDIA A100 SXM4 80 GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 512.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 401972.5996644288,
                "training_compute_estimation_method": "Operation counting,Hardware",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 9.57978359661681,
                "log_compute": 22.998091849853363,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "Phi-4-Multimodal",
                "domain": "Multimodal,Language,Vision,Speech",
                "task": "Language modeling/generation,Question answering,Visual question answering,Speech recognition (ASR),Translation,Audio question answering,Character recognition (OCR)",
                "organization": "Microsoft",
                "authors": "Abdelrahman Abouelenin, Atabak Ashfaq, Adam Atkinson, Hany Awadalla, Nguyen Bach, Jianmin Bao, Alon Benhaim, Martin Cai, Vishrav Chaudhary, Congcong Chen, Dong Chen, Dongdong Chen, Junkun Chen, Weizhu Chen, Yen-Chun Chen, Yi-ling Chen, Qi Dai, Xiyang Dai, Ruchao Fan, Mei Gao, Min Gao, Amit Garg, Abhishek Goswami, Junheng Hao, Amr Hendy, Yuxuan Hu, Xin Jin, Mahmoud Khademi, Dongwoo Kim, Young Jin Kim, Gina Lee, Jinyu Li, Yunsheng Li, Chen Liang, Xihui Lin, Zeqi Lin, Mengchen Liu, Yang Liu, Gilsinia Lopez, Chong Luo, Piyush Madan, Vadim Mazalov, Ali Mousavi, Anh Nguyen, Jing Pan, Daniel Perez-Becker, Jacob Platin, Thomas Portet, Kai Qiu, Bo Ren, Liliang Ren, Sambuddha Roy, Ning Shang, Yelong Shen, Saksham Singhal, Subhojit Som, Xia Song, Tetyana Sych, Praneetha Vaddamanu, Shuohang Wang, Yiming Wang, Zhenghao Wang, Haibin Wu, Haoran Xu, Weijian Xu, Yifan Yang, Ziyi Yang, Donghan Yu, Ishmam Zabir, Jianwen Zhang, Li Lyna Zhang, Yunan Zhang, Xiren Zhou",
                "publication_date": "2025-03-03",
                "reference": "Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs",
                "link": "https://arxiv.org/abs/2503.01743",
                "citations": null,
                "notability_criteria": null,
                "parameters": 5600000000.0,
                "training_compute_(flop)": 1.1852519e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 672.0,
                "training_hardware": "NVIDIA A100 SXM4 80 GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": "Phi-4 Mini",
                "finetune_compute_(flop)": 7.1724e+21,
                "hardware_quantity": 512.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 401972.5996644288,
                "training_compute_estimation_method": "Operation counting,Hardware",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 9.7481880270062,
                "log_compute": 23.073810660180705,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "Wan 2.1 14B I2V",
                "domain": "Video,Vision",
                "task": "Video generation,Image-to-video,Text-to-video",
                "organization": "Alibaba",
                "authors": "Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, Ziyu Liu",
                "publication_date": "2025-02-25",
                "reference": "Wan 2.1 by Wan AI :best cost efficient video generation model Now Available",
                "link": "https://arxiv.org/abs/2503.20314\n",
                "citations": null,
                "notability_criteria": null,
                "parameters": 14000000000.0,
                "training_compute_(flop)": 2.5e+23,
                "training_dataset_size_(gradients)": 3000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Vision",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 10.146128035678238,
                "log_compute": 23.397940008672037,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Step-Video-T2V",
                "domain": "Video",
                "task": "Video generation,Text-to-video",
                "organization": "StepFun",
                "authors": "Guoqing Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan, Shengming Yin, Changyi Wan, Ranchen Ming, Xiaoniu Song, Xing Chen, Yu Zhou, Deshan Sun, Deyu Zhou, Jian Zhou, Kaijun Tan, Kang An, Mei Chen, Wei Ji, Qiling Wu, Wen Sun, Xin Han, Yanan Wei, Zheng Ge, Aojie Li, Bin Wang, Bizhu Huang, Bo Wang, Brian Li, Changxing Miao, Chen Xu, Chenfei Wu, Chenguang Yu, Dapeng Shi, Dingyuan Hu, Enle Liu, Gang Yu, Ge Yang, Guanzhe Huang, Gulin Yan, Haiyang Feng, Hao Nie, Haonan Jia, Hanpeng Hu, Hanqi Chen, Haolong Yan, Heng Wang, Hongcheng Guo, Huilin Xiong, Huixin Xiong, Jiahao Gong, Jianchang Wu, Jiaoren Wu, Jie Wu, Jie Yang, Jiashuai Liu, Jiashuo Li, Jingyang Zhang, Junjing Guo, Junzhe Lin, Kaixiang Li, Lei Liu, Lei Xia, Liang Zhao, Liguo Tan, Liwen Huang, Liying Shi, Ming Li, Mingliang Li, Muhua Cheng, Na Wang, Qiaohui Chen, Qinglin He, Qiuyan Liang, Quan Sun, Ran Sun, Rui Wang, Shaoliang Pang, Shiliang Yang, Sitong Liu, Siqi Liu, Shuli Gao, Tiancheng Cao, Tianyu Wang, Weipeng Ming, Wenqing He, Xu Zhao, Xuelin Zhang, Xianfang Zeng, Xiaojia Liu, Xuan Yang, Yaqi Dai, Yanbo Yu, Yang Li, Yineng Deng, Yingming Wang, Yilei Wang, Yuanwei Lu, Yu Chen, Yu Luo, Yuchu Luo et al. (15 additional authors not shown)",
                "publication_date": "2025-02-24",
                "reference": "Step-Video-T2V Technical Report: The Practice, Challenges, and Future of Video Foundation Model",
                "link": "https://arxiv.org/abs/2502.10248",
                "citations": null,
                "notability_criteria": null,
                "parameters": 30000000000.0,
                "training_compute_(flop)": 4.1015808e+24,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 720.0,
                "training_hardware": "NVIDIA H800 SXM5",
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 5000.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 6870719.882854385,
                "training_compute_estimation_method": "Hardware",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Vision",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 10.477121254719663,
                "log_compute": 24.61295127145166,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "SigLIP 2",
                "domain": "Vision",
                "task": "Image classification,Image embedding",
                "organization": "Google DeepMind",
                "authors": "Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, Olivier H\u00e9naff, Jeremiah Harmsen, Andreas Steiner, Xiaohua Zhai",
                "publication_date": "2025-02-20",
                "reference": "SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features",
                "link": "https://arxiv.org/abs/2502.14786",
                "citations": null,
                "notability_criteria": null,
                "parameters": 1140000000.0,
                "training_compute_(flop)": 8.208e+22,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "Google TPU v5e",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 2048.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 562899.512243166,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Vision",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 9.056904851336473,
                "log_compute": 22.91423734776774,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "Evo 2 40B",
                "domain": "Biology",
                "task": "Protein or nucleotide language model (pLM/nLM)",
                "organization": "Arc Institute,Stanford University,NVIDIA,Liquid,University of California (UC) Berkeley,Goodfire,Columbia University,University of California San Francisco",
                "authors": "Garyk Brixi, Matthew G. Durrant, Jerome Ku, Michael Poli, Greg Brockman, Daniel Chang, Gabriel A. Gonzalez, Samuel H. King, David B. Li, Aditi T. Merchant, Mohsen Naghipourfar, Eric Nguyen, Chiara Ricci-Tam, David W. Romero, Gwanggyu Sun, Ali Taghibakshi, Anton Vorontsov, Brandon Yang, Myra Deng, Liv Gorton, Nam Nguyen, Nicholas K. Wang, Etowah Adams, Stephen A. Baccus, Steven Dillmann, Stefano Ermon, Daniel Guo, Rajesh Ilango, Ken Janik, Amy X. Lu, Reshma Mehta, Mohammad R.K. Mofrad, Madelena Y. Ng, Jaspreet Pannu, Christopher R\u00e9, Jonathan C. Schmok, John St. John, Jeremy Sullivan, Kevin Zhu, Greg Zynda, Daniel Balsam, Patrick Collison, Anthony B. Costa, Tina Hernandez-Boussard, Eric Ho, Ming-Yu Liu, Thomas McGrath, Kimberly Powell, Dave P. Burke, Hani Goodarzi, Patrick D. Hsu, Brian L. Hie",
                "publication_date": "2025-02-19",
                "reference": "Genome modeling and design across all domains of life with Evo 2",
                "link": "https://arcinstitute.org/manuscripts/Evo2",
                "citations": null,
                "notability_criteria": null,
                "parameters": 40300000000.0,
                "training_compute_(flop)": 2.25e+24,
                "training_dataset_size_(gradients)": 9300000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting,Reported",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Biology",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 10.60530504614111,
                "log_compute": 24.352182518111363,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "Evo 2 7B",
                "domain": "Biology",
                "task": "Protein or nucleotide language model (pLM/nLM)",
                "organization": "Arc Institute,Stanford University,NVIDIA,Liquid,University of California (UC) Berkeley,Goodfire,Columbia University,University of California San Francisco",
                "authors": "Garyk Brixi, Matthew G. Durrant, Jerome Ku, Michael Poli, Greg Brockman, Daniel Chang, Gabriel A. Gonzalez, Samuel H. King, David B. Li, Aditi T. Merchant, Mohsen Naghipourfar, Eric Nguyen, Chiara Ricci-Tam, David W. Romero, Gwanggyu Sun, Ali Taghibakshi, Anton Vorontsov, Brandon Yang, Myra Deng, Liv Gorton, Nam Nguyen, Nicholas K. Wang, Etowah Adams, Stephen A. Baccus, Steven Dillmann, Stefano Ermon, Daniel Guo, Rajesh Ilango, Ken Janik, Amy X. Lu, Reshma Mehta, Mohammad R.K. Mofrad, Madelena Y. Ng, Jaspreet Pannu, Christopher R\u00e9, Jonathan C. Schmok, John St. John, Jeremy Sullivan, Kevin Zhu, Greg Zynda, Daniel Balsam, Patrick Collison, Anthony B. Costa, Tina Hernandez-Boussard, Eric Ho, Ming-Yu Liu, Thomas McGrath, Kimberly Powell, Dave P. Burke, Hani Goodarzi, Patrick D. Hsu, Brian L. Hie",
                "publication_date": "2025-02-19",
                "reference": "Genome modeling and design across all domains of life with Evo 2",
                "link": "https://arcinstitute.org/manuscripts/Evo2",
                "citations": null,
                "notability_criteria": null,
                "parameters": 7000000000.0,
                "training_compute_(flop)": 1.008e+23,
                "training_dataset_size_(gradients)": 2400000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Biology",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 9.845098040014257,
                "log_compute": 23.003460532109507,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "PaliGemma 2 3B Mix 224",
                "domain": "Vision,Multimodal,Language",
                "task": "Image captioning,Visual question answering,Object detection,Image segmentation,Object recognition,Character recognition (OCR)",
                "organization": "Google",
                "authors": "Omar Sanseviero, Andreas Steiner",
                "publication_date": "2025-02-19",
                "reference": "Introducing PaliGemma 2 mix: A vision-language model for multiple tasks",
                "link": "https://developers.googleblog.com/en/introducing-paligemma-2-mix/\n\nhttps://arxiv.org/abs/2412.03555",
                "citations": null,
                "notability_criteria": null,
                "parameters": 2920000000.0,
                "training_compute_(flop)": 4.0397694e+22,
                "training_dataset_size_(gradients)": 256000000000.0,
                "training_time_(hours)": 72.0,
                "training_hardware": "Google TPU v5e",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "Multinational",
                "base_model": "Gemma 2 2B,SigLIP 400M",
                "finetune_compute_(flop)": 4.2509643e+21,
                "hardware_quantity": 256.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 70364.00597512173,
                "training_compute_estimation_method": "Hardware,Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 9.465382851448418,
                "log_compute": 22.606356575218072,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "Qwen2.5-VL-72B",
                "domain": "Language,Vision,Multimodal",
                "task": "Visual question answering,Video description,Language modeling/generation,Translation,Question answering,Character recognition (OCR),Quantitative reasoning",
                "organization": "Alibaba",
                "authors": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, Junyang Lin",
                "publication_date": "2025-02-19",
                "reference": "Qwen2.5-VL Technical Report",
                "link": "https://arxiv.org/abs/2502.13923",
                "citations": null,
                "notability_criteria": null,
                "parameters": 72000000000.0,
                "training_compute_(flop)": 9.5712e+24,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "China",
                "base_model": "Qwen2.5-72B",
                "finetune_compute_(flop)": 1.7712e+24,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Multimodal",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 10.857332496431269,
                "log_compute": 24.980966391351224,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Qwen2.5-VL-7B ",
                "domain": "Language,Vision,Multimodal",
                "task": "Visual question answering,Video description,Language modeling/generation,Translation,Question answering,Character recognition (OCR),Quantitative reasoning",
                "organization": "Alibaba",
                "authors": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, Junyang Lin",
                "publication_date": "2025-02-19",
                "reference": "Qwen2.5-VL Technical Report",
                "link": "https://arxiv.org/abs/2502.13923",
                "citations": null,
                "notability_criteria": null,
                "parameters": 7000000000.0,
                "training_compute_(flop)": 9.9408e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": "Qwen2.5-7B",
                "finetune_compute_(flop)": 1.722e+23,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Multimodal",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 9.845098040014257,
                "log_compute": 23.99742133626904,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Qwen2.5-VL-3B",
                "domain": "Language,Vision,Multimodal",
                "task": "Visual question answering,Video description,Language modeling/generation,Translation,Question answering,Character recognition (OCR),Quantitative reasoning",
                "organization": "Alibaba",
                "authors": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, Junyang Lin",
                "publication_date": "2025-02-19",
                "reference": "Qwen2.5-VL Technical Report",
                "link": "https://arxiv.org/abs/2502.13923",
                "citations": null,
                "notability_criteria": null,
                "parameters": 3000000000.0,
                "training_compute_(flop)": 4.0752e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "China",
                "base_model": "Qwen2.5-3B",
                "finetune_compute_(flop)": 7.38e+22,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Multimodal",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 9.477121254719663,
                "log_compute": 23.61014892761954,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Brain2Qwerty",
                "domain": "Language",
                "task": "Language generation",
                "organization": "Meta AI,Universite de Technologie de Compi\u00e8gne \u2013 CNRS,Basque Center on Cognition",
                "authors": "Jarod L\u00e9vy,, Mingfang (Lucy) Zhang, Svetlana Pinet, J\u00e9r\u00e9my Rapin, Hubert Banville, St\u00e9phane d\u2019Ascoli, Jean-R\u00e9mi King",
                "publication_date": "2025-02-18",
                "reference": "Brain-to-Text Decoding: A Non-invasive Approach via Typing",
                "link": "https://ai.meta.com/research/publications/brain-to-text-decoding-a-non-invasive-approach-via-typing/",
                "citations": null,
                "notability_criteria": null,
                "parameters": 400000000.0,
                "training_compute_(flop)": 1.62e+18,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 12.0,
                "training_hardware": "NVIDIA V100",
                "approach": null,
                "confidence": "Confident",
                "epochs": 100.0,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 1.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 323.6250227878313,
                "training_compute_estimation_method": "Hardware",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 8.602059991327963,
                "log_compute": 18.20951501454263,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "Step-Audio-Chat 130B",
                "domain": "Speech",
                "task": "Speech synthesis,Speech recognition (ASR),Speech-to-text,Text-to-speech (TTS),Audio question answering,Audio generation,Speech-to-speech",
                "organization": "StepFun",
                "authors": "Ailin Huang, Boyong Wu, Bruce Wang, Chao Yan, Chen Hu, Chengli Feng, Fei Tian, Feiyu Shen, Jingbei Li, Mingrui Chen, Peng Liu, Ruihang Miao, Wang You, Xi Chen, Xuerui Yang, Yechang Huang, Yuxiang Zhang, Zheng Gong, Zixin Zhang, Hongyu Zhou, Jianjian Sun, Brian Li, Chengting Feng, Changyi Wan, Hanpeng Hu, Jianchang Wu, Jiangjie Zhen, Ranchen Ming, Song Yuan, Xuelin Zhang, Yu Zhou, Bingxin Li, Buyun Ma, Hongyuan Wang, Kang An, Wei Ji, Wen Li, Xuan Wen, Xiangwen Kong, Yuankai Ma, Yuanwei Liang, Yun Mou, Bahtiyar Ahmidi, Bin Wang, Bo Li, Changxin Miao, Chen Xu, Chenrun Wang, Dapeng Shi, Deshan Sun, Dingyuan Hu, Dula Sai, Enle Liu, Guanzhe Huang, Gulin Yan, Heng Wang, Haonan Jia, Haoyang Zhang, Jiahao Gong, Junjing Guo, Jiashuai Liu, Jiahong Liu, Jie Feng, Jie Wu, Jiaoren Wu, Jie Yang, Jinguo Wang, Jingyang Zhang, Junzhe Lin, Kaixiang Li, Lei Xia, Li Zhou, Liang Zhao, Longlong Gu, Mei Chen, Menglin Wu, Ming Li, Mingxiao Li, Mingliang Li, Mingyao Liang, Na Wang, Nie Hao, Qiling Wu, Qinyuan Tan, Ran Sun, Shuai Shuai, Shaoliang Pang, Shiliang Yang, Shuli Gao, Shanshan Yuan, Siqi Liu, Shihong Deng, Shilei Jiang, Sitong Liu, Tiancheng Cao, Tianyu Wang, Wenjin Deng, Wuxun Xie, Weipeng Ming, Wenqing He et al. (45 additional authors not shown)",
                "publication_date": "2025-02-18",
                "reference": "Step-Audio: Unified Understanding and Generation in Intelligent Speech Interaction",
                "link": "https://arxiv.org/abs/2502.11946",
                "citations": null,
                "notability_criteria": null,
                "parameters": 130000000000.0,
                "training_compute_(flop)": 1.92504e+24,
                "training_dataset_size_(gradients)": 1668000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": "Step-1",
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Audio",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 11.113943352306837,
                "log_compute": 24.284439758051686,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Step-1",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering",
                "organization": "StepFun",
                "authors": "Ailin Huang, Boyong Wu, Bruce Wang, Chao Yan, Chen Hu, Chengli Feng, Fei Tian, Feiyu Shen, Jingbei Li, Mingrui Chen, Peng Liu, Ruihang Miao, Wang You, Xi Chen, Xuerui Yang, Yechang Huang, Yuxiang Zhang, Zheng Gong, Zixin Zhang, Hongyu Zhou, Jianjian Sun, Brian Li, Chengting Feng, Changyi Wan, Hanpeng Hu, Jianchang Wu, Jiangjie Zhen, Ranchen Ming, Song Yuan, Xuelin Zhang, Yu Zhou, Bingxin Li, Buyun Ma, Hongyuan Wang, Kang An, Wei Ji, Wen Li, Xuan Wen, Xiangwen Kong, Yuankai Ma, Yuanwei Liang, Yun Mou, Bahtiyar Ahmidi, Bin Wang, Bo Li, Changxin Miao, Chen Xu, Chenrun Wang, Dapeng Shi, Deshan Sun, Dingyuan Hu, Dula Sai, Enle Liu, Guanzhe Huang, Gulin Yan, Heng Wang, Haonan Jia, Haoyang Zhang, Jiahao Gong, Junjing Guo, Jiashuai Liu, Jiahong Liu, Jie Feng, Jie Wu, Jiaoren Wu, Jie Yang, Jinguo Wang, Jingyang Zhang, Junzhe Lin, Kaixiang Li, Lei Xia, Li Zhou, Liang Zhao, Longlong Gu, Mei Chen, Menglin Wu, Ming Li, Mingxiao Li, Mingliang Li, Mingyao Liang, Na Wang, Nie Hao, Qiling Wu, Qinyuan Tan, Ran Sun, Shuai Shuai, Shaoliang Pang, Shiliang Yang, Shuli Gao, Shanshan Yuan, Siqi Liu, Shihong Deng, Shilei Jiang, Sitong Liu, Tiancheng Cao, Tianyu Wang, Wenjin Deng, Wuxun Xie, Weipeng Ming, Wenqing He et al. (45 additional authors not shown)",
                "publication_date": "2025-02-18",
                "reference": "Step-Audio: Unified Understanding and Generation in Intelligent Speech Interaction",
                "link": "https://arxiv.org/abs/2502.11946",
                "citations": null,
                "notability_criteria": null,
                "parameters": 130000000000.0,
                "training_compute_(flop)": 6.24e+23,
                "training_dataset_size_(gradients)": 800000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 11.113943352306837,
                "log_compute": 23.795184589682425,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Step-Omni",
                "domain": "Speech,Language,Multimodal,Vision",
                "task": "Speech synthesis,Speech recognition (ASR),Speech-to-text,Text-to-speech (TTS),Audio question answering,Audio generation,Image captioning,Visual question answering,Speech-to-speech",
                "organization": "StepFun",
                "authors": "Ailin Huang, Boyong Wu, Bruce Wang, Chao Yan, Chen Hu, Chengli Feng, Fei Tian, Feiyu Shen, Jingbei Li, Mingrui Chen, Peng Liu, Ruihang Miao, Wang You, Xi Chen, Xuerui Yang, Yechang Huang, Yuxiang Zhang, Zheng Gong, Zixin Zhang, Hongyu Zhou, Jianjian Sun, Brian Li, Chengting Feng, Changyi Wan, Hanpeng Hu, Jianchang Wu, Jiangjie Zhen, Ranchen Ming, Song Yuan, Xuelin Zhang, Yu Zhou, Bingxin Li, Buyun Ma, Hongyuan Wang, Kang An, Wei Ji, Wen Li, Xuan Wen, Xiangwen Kong, Yuankai Ma, Yuanwei Liang, Yun Mou, Bahtiyar Ahmidi, Bin Wang, Bo Li, Changxin Miao, Chen Xu, Chenrun Wang, Dapeng Shi, Deshan Sun, Dingyuan Hu, Dula Sai, Enle Liu, Guanzhe Huang, Gulin Yan, Heng Wang, Haonan Jia, Haoyang Zhang, Jiahao Gong, Junjing Guo, Jiashuai Liu, Jiahong Liu, Jie Feng, Jie Wu, Jiaoren Wu, Jie Yang, Jinguo Wang, Jingyang Zhang, Junzhe Lin, Kaixiang Li, Lei Xia, Li Zhou, Liang Zhao, Longlong Gu, Mei Chen, Menglin Wu, Ming Li, Mingxiao Li, Mingliang Li, Mingyao Liang, Na Wang, Nie Hao, Qiling Wu, Qinyuan Tan, Ran Sun, Shuai Shuai, Shaoliang Pang, Shiliang Yang, Shuli Gao, Shanshan Yuan, Siqi Liu, Shihong Deng, Shilei Jiang, Sitong Liu, Tiancheng Cao, Tianyu Wang, Wenjin Deng, Wuxun Xie, Weipeng Ming, Wenqing He et al. (45 additional authors not shown)",
                "publication_date": "2025-02-18",
                "reference": "Step-Audio: Unified Understanding and Generation in Intelligent Speech Interaction",
                "link": "https://arxiv.org/abs/2502.11946",
                "citations": null,
                "notability_criteria": null,
                "parameters": 130000000000.0,
                "training_compute_(flop)": 2.54904e+24,
                "training_dataset_size_(gradients)": 2468000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA H800 SXM5",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "China",
                "base_model": "Step-1",
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Multimodal",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 11.113943352306837,
                "log_compute": 24.406376650550857,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "HAMSTER VLM",
                "domain": "Robotics",
                "task": "Robotic manipulation",
                "organization": "NVIDIA,University of Washington,University of Southern California",
                "authors": "Yi Li, Yuquan Deng, Jesse Zhang, Joel Jang, Marius Memmel, Raymond Yu, Caelan Reed Garrett, Fabio Ramos, Dieter Fox, Anqi Li, Abhishek Gupta, Ankit Goyal",
                "publication_date": "2025-02-08",
                "reference": "HAMSTER: Hierarchical Action Models For Open-World Robot Manipulation",
                "link": "https://arxiv.org/abs/2502.05485\nhttps://hamster-robot.github.io/\nhttps://github.com/liyi14/HAMSTER_beta\nhttps://huggingface.co/yili18/Hamster_dev",
                "citations": 48.0,
                "notability_criteria": null,
                "parameters": 13493916736.0,
                "training_compute_(flop)": 2.4081408e+21,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 30.0,
                "training_hardware": "NVIDIA A100 SXM4 80 GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "Multinational",
                "base_model": "VILA1.5-13B",
                "finetune_compute_(flop)": 1.078272e+20,
                "hardware_quantity": 8.0,
                "last_modified": "2025-10-16 13:10:07+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 256.0,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 6284.039702711232,
                "training_compute_estimation_method": "Hardware",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Robotics",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 10.130138026020305,
                "log_compute": 21.381681875806464,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "Prithvi-EO-2.0 600M",
                "domain": "Earth science",
                "task": null,
                "organization": "IBM Research,NASA,University of Alabama,University of Iceland,Forschungszentrum Julich,Virginia Tech (Virginia Polytechnic Institute and State University),Arizona State University,Oregon State University,Boston University,University of California (UC) Berkeley,Julich Supercomputing Center",
                "authors": "Daniela Szwarcman, Sujit Roy, Paolo Fraccaro, \u00deorsteinn El\u00ed G\u00edslason, Benedikt Blumenstiel, Rinki Ghosal, Pedro Henrique de Oliveira, Joao Lucas de Sousa Almeida, Rocco Sedona, Yanghui Kang, Srija Chakraborty, Sizhe Wang, Carlos Gomes, Ankur Kumar, Myscon Truong, Denys Godwin, Hyunho Lee, Chia-Yu Hsu, Ata Akbari Asanjan, Besart Mujeci, Disha Shidham, Trevor Keenan, Paulo Arevalo, Wenwen Li, Hamed Alemohammad, Pontus Olofsson, Christopher Hain, Robert Kennedy, Bianca Zadrozny, David Bell, Gabriele Cavallaro, Campbell Watson, Manil Maskey, Rahul Ramachandran, Juan Bernabe Moreno",
                "publication_date": "2025-02-03",
                "reference": "Prithvi-EO-2.0: A Versatile Multi-Temporal Foundation Model for Earth Observation Applications",
                "link": "https://arxiv.org/abs/2412.02732",
                "citations": null,
                "notability_criteria": null,
                "parameters": 600000000.0,
                "training_compute_(flop)": 1.954368e+22,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100",
                "approach": null,
                "confidence": "Confident",
                "epochs": 400.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 240.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry, Government",
                "foundation_model": false,
                "training_chip-hours": 58000.0,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 188542.18349202207,
                "training_compute_estimation_method": "Hardware",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Earth science",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 8.778151250383644,
                "log_compute": 22.29100634306833,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "Prithvi-EO-2.0 300M",
                "domain": "Earth science",
                "task": null,
                "organization": "IBM Research,NASA,University of Alabama,University of Iceland,Forschungszentrum Julich,Virginia Tech (Virginia Polytechnic Institute and State University),Arizona State University,Oregon State University,Boston University,University of California (UC) Berkeley,Julich Supercomputing Center",
                "authors": "Daniela Szwarcman, Sujit Roy, Paolo Fraccaro, \u00deorsteinn El\u00ed G\u00edslason, Benedikt Blumenstiel, Rinki Ghosal, Pedro Henrique de Oliveira, Joao Lucas de Sousa Almeida, Rocco Sedona, Yanghui Kang, Srija Chakraborty, Sizhe Wang, Carlos Gomes, Ankur Kumar, Myscon Truong, Denys Godwin, Hyunho Lee, Chia-Yu Hsu, Ata Akbari Asanjan, Besart Mujeci, Disha Shidham, Trevor Keenan, Paulo Arevalo, Wenwen Li, Hamed Alemohammad, Pontus Olofsson, Christopher Hain, Robert Kennedy, Bianca Zadrozny, David Bell, Gabriele Cavallaro, Campbell Watson, Manil Maskey, Rahul Ramachandran, Juan Bernabe Moreno",
                "publication_date": "2025-02-03",
                "reference": "Prithvi-EO-2.0: A Versatile Multi-Temporal Foundation Model for Earth Observation Applications",
                "link": "https://arxiv.org/abs/2412.02732",
                "citations": null,
                "notability_criteria": null,
                "parameters": 300000000.0,
                "training_compute_(flop)": 7.07616e+21,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100",
                "approach": null,
                "confidence": "Confident",
                "epochs": 400.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 80.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry, Government",
                "foundation_model": false,
                "training_chip-hours": 21000.0,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 62847.3944973407,
                "training_compute_estimation_method": "Hardware",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Earth science",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 8.477121254719663,
                "log_compute": 21.849797644239313,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "Baichuan-Omni-1.5",
                "domain": "Multimodal,Language,Speech,Vision,Video,Audio",
                "task": "Language modeling/generation,Question answering,Audio question answering,Speech recognition (ASR),Speech-to-text,Visual question answering,Image captioning,Speech synthesis,Text-to-speech (TTS),Video,Video classification",
                "organization": "Baichuan",
                "authors": "Yadong Li, Jun Liu, Tao Zhang, Tao Zhang, Song Chen, Tianpeng Li, Zehuan Li, Lijun Liu, Lingfeng Ming, Guosheng Dong, Da Pan, Chong Li, Yuanbo Fang, Dongdong Kuang, Mingrui Wang, Chenglin Zhu, Youwei Zhang, Hongyu Guo, Fengyu Zhang, Yuran Wang, Bowen Ding, Wei Song, Xu Li, Yuqi Huo, Zheng Liang, Shusen Zhang, Xin Wu, Shuai Zhao, Linchu Xiong, Yozhen Wu, Jiahui Ye, Wenhao Lu, Bowen Li, Yan Zhang, Yaqi Zhou, Xin Chen, Lei Su, Hongda Zhang, Fuzhong Chen, Xuezhen Dong, Na Nie, Zhiying Wu, Bin Xiao, Ting Li, Shunya Dang, Ping Zhang, Yijia Sun, Jincheng Wu, Jinjie Yang, Xionghai Lin, Zhi Ma, Kegeng Wu, Jia li, Aiyuan Yang, Hui Liu, Jianqiang Zhang, Xiaoxi Chen, Guangwei Ai, Wentao Zhang, Yicong Chen, Xiaoqin Huang, Kun Li, Wenjing Luo, Yifei Duan, Lingling Zhu, Ran Xiao, Zhe Su, Jiani Pu, Dian Wang, Xu Jia, Tianyu Zhang, Mengyu Ai, Mang Wang, Yujing Qiao, Lei Zhang, Yanjun Shen, Fan Yang, Miao Zhen, Yijie Zhou, Mingyang Chen, Fei Li, Chenzheng Zhu, Keer Lu, Yaqi Zhao, Hao Liang, Youquan Li, Yanzhao Qin, Linzhuang Sun, Jianhua Xu, Haoze Sun, Mingan Lin, Zenan Zhou, Weipeng Chen",
                "publication_date": "2025-01-26",
                "reference": "Baichuan-Omni-1.5 Technical Report",
                "link": "https://arxiv.org/abs/2501.15368",
                "citations": null,
                "notability_criteria": null,
                "parameters": 11000000000.0,
                "training_compute_(flop)": 3.3e+22,
                "training_dataset_size_(gradients)": 500000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Multimodal",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 10.041392685158225,
                "log_compute": 22.518513939877888,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "gte-modernbert",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering",
                "organization": "Alibaba",
                "authors": "Unknown",
                "publication_date": "2025-01-22",
                "reference": "gte-modernbert-base",
                "link": "https://huggingface.co/Alibaba-NLP/gte-modernbert-base",
                "citations": null,
                "notability_criteria": null,
                "parameters": 149000000.0,
                "training_compute_(flop)": 3.676128e+21,
                "training_dataset_size_(gradients)": 1028000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Likely",
                "epochs": 4.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 8.173186268412275,
                "log_compute": 21.565390624783138,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "DeepSeek-R1",
                "domain": "Language",
                "task": "Language modeling/generation,Code generation,Quantitative reasoning,Question answering",
                "organization": "DeepSeek",
                "authors": "Unknown",
                "publication_date": "2025-01-20",
                "reference": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
                "link": "https://api-docs.deepseek.com/news/news250120",
                "citations": null,
                "notability_criteria": "Training cost,SOTA improvement",
                "parameters": 671000000000.0,
                "training_compute_(flop)": 4.020010000000001e+24,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": "DeepSeek-V3",
                "finetune_compute_(flop)": 6.1e+23,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": 6770000.0,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 11.826722520168993,
                "log_compute": 24.60422713341766,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Eagle 2",
                "domain": "Vision,Robotics,Language",
                "task": null,
                "organization": "NVIDIA,Nanjing University,Tsinghua University,Hong Kong Polytechnic University,Johns Hopkins University,New York University (NYU)",
                "authors": "Zhiqi Li, Guo Chen, Shilong Liu, Shihao Wang, Vibashan VS, Yishen Ji, Shiyi Lan, Hao Zhang, Yilin Zhao, Subhashree Radhakrishnan, Nadine Chang, Karan Sapra, Amala Sanjay Deshmukh, Tuomas Rintamaki, Matthieu Le, Ilia Karmanov, Lukas Voegtle, Philipp Fischer, De-An Huang, Timo Roman, Tong Lu, Jose M. Alvarez, Bryan Catanzaro, Jan Kautz, Andrew Tao, Guilin Liu, Zhiding Yu",
                "publication_date": "2025-01-20",
                "reference": "Eagle 2: Building Post-Training Data Strategies from Scratch for Frontier Vision-Language Models",
                "link": "arxiv.org/abs/2501.14818",
                "citations": 25.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 8930000000.0,
                "training_compute_(flop)": 4.7156e+22,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 130.5,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "Multinational",
                "base_model": "Qwen2.5-7B",
                "finetune_compute_(flop)": null,
                "hardware_quantity": 256.0,
                "last_modified": "2025-10-14 18:16:28+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 352055.152733459,
                "training_compute_estimation_method": null,
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 9.950851458888547,
                "log_compute": 22.67353695902688,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "MatterGen",
                "domain": "Materials science",
                "task": "Materials design,Crystal discovery",
                "organization": "Microsoft Research AI for Science",
                "authors": "Claudio Zeni, Robert Pinsler, Daniel Z\u00fcgner, Andrew Fowler, Matthew Horton, Xiang Fu, Zilong Wang, Aliaksandra Shysheya, Jonathan Crabb\u00e9, Shoko Ueda, Roberto Sordillo, Lixin Sun, Jake Smith, Bichlien Nguyen, Hannes Schulz, Sarah Lewis, Chin-Wei Huang, Ziheng Lu, Yichi Zhou, Han Yang, Hongxia Hao, Jielan Li, Chunlei Yang, Wenjie Li, Ryota Tomioka, Tian Xie",
                "publication_date": "2025-01-16",
                "reference": "A generative model for inorganic materials design",
                "link": "https://www.nature.com/articles/s41586-025-08628-5",
                "citations": null,
                "notability_criteria": null,
                "parameters": 46800000.0,
                "training_compute_(flop)": 2.69568e+19,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 10.0,
                "training_hardware": "NVIDIA A100",
                "approach": null,
                "confidence": "Confident",
                "epochs": 100.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 8.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 6287.259184247614,
                "training_compute_estimation_method": "Hardware",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Materials science",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 7.670245853074124,
                "log_compute": 19.430668336497337,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "MiniMax-Text-01",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering",
                "organization": "MiniMax",
                "authors": "MiniMax, Aonian Li, Bangwei Gong, Bo Yang, Boji Shan, Chang Liu, Cheng Zhu, Chunhao Zhang, Congchao Guo, Da Chen, Dong Li, Enwei Jiao, Gengxin Li, Guojun Zhang, Haohai Sun, Houze Dong, Jiadai Zhu, Jiaqi Zhuang, Jiayuan Song, Jin Zhu, Jingtao Han, Jingyang Li, Junbin Xie, Junhao Xu, Junjie Yan, Kaishun Zhang, Kecheng Xiao, Kexi Kang, Le Han, Leyang Wang, Lianfei Yu, Liheng Feng, Lin Zheng, Linbo Chai, Long Xing, Meizhi Ju, Mingyuan Chi, Mozhi Zhang, Peikai Huang, Pengcheng Niu, Pengfei Li, Pengyu Zhao, Qi Yang, Qidi Xu, Qiexiang Wang, Qin Wang, Qiuhui Li, Ruitao Leng, Shengmin Shi, Shuqi Yu, Sichen Li, Songquan Zhu, Tao Huang, Tianrun Liang, Weigao Sun, Weixuan Sun, Weiyu Cheng, Wenkai Li, Xiangjun Song, Xiao Su, Xiaodong Han, Xinjie Zhang, Xinzhu Hou, Xu Min, Xun Zou, Xuyang Shen, Yan Gong, Yingjie Zhu, Yipeng Zhou, Yiran Zhong, Yongyi Hu, Yuanxiang Fan, Yue Yu, Yufeng Yang, Yuhao Li, Yunan Huang, Yunji Li, Yunpeng Huang, Yunzhi Xu, Yuxin Mao, Zehan Li, Zekang Li, Zewei Tao, Zewen Ying, Zhaoyang Cong, Zhen Qin, Zhenhua Fan, Zhihang Yu, Zhuo Jiang, Zijia Wu",
                "publication_date": "2025-01-14",
                "reference": "MiniMax-01: Scaling Foundation Models with Lightning Attention",
                "link": "https://arxiv.org/abs/2501.08313",
                "citations": null,
                "notability_criteria": null,
                "parameters": 456000000000.0,
                "training_compute_(flop)": 1.98288e+24,
                "training_dataset_size_(gradients)": 7200000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA H800 SXM5",
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 34.84465752823738,
                "log_params": 11.658964842664435,
                "log_compute": 24.297296432352173,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Cosmos-1.0-\nDiffusion-14B Video2World",
                "domain": "Robotics,Vision,Video",
                "task": "Robotic manipulation,Self-driving car,Video generation",
                "organization": "NVIDIA",
                "authors": "NVIDIA: Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, Daniel Dworakowski, Jiaojiao Fan, Michele Fenzi, Francesco Ferroni, Sanja Fidler, Dieter Fox, Songwei Ge, Yunhao Ge, Jinwei Gu, Siddharth Gururani, Ethan He, Jiahui Huang, Jacob Huffman, Pooya Jannaty, Jingyi Jin, Seung Wook Kim, Gergely Kl\u00e1r, Grace Lam, Shiyi Lan, Laura Leal-Taixe, Anqi Li, Zhaoshuo Li, Chen-Hsuan Lin, Tsung-Yi Lin, Huan Ling, Ming-Yu Liu, Xian Liu, Alice Luo, Qianli Ma, Hanzi Mao, Kaichun Mo, Arsalan Mousavian, Seungjun Nah, Sriharsha Niverty, David Page, Despoina Paschalidou, Zeeshan Patel, Lindsey Pavao, Morteza Ramezanali, Fitsum Reda, Xiaowei Ren, Vasanth Rao Naik Sabavat, Ed Schmerling, Stella Shi, Bartosz Stefaniak, Shitao Tang, Lyne Tchapmi, Przemek Tredak, Wei-Cheng Tseng, Jibin Varghese, Hao Wang, Haoxiang Wang, Heng Wang, Ting-Chun Wang, Fangyin Wei, Xinyue Wei, Jay Zhangjie Wu, Jiashu Xu, Wei Yang, Lin Yen-Chen, Xiaohui Zeng, Yu Zeng, Jing Zhang, Qinsheng Zhang, Yuxuan Zhang, Qingqing Zhao, Artur Zolkowski",
                "publication_date": "2025-01-07",
                "reference": "Cosmos World Foundation Model Platform for Physical AI",
                "link": "https://arxiv.org/abs/2501.03575",
                "citations": null,
                "notability_criteria": null,
                "parameters": 14000000000.0,
                "training_compute_(flop)": 2.7999999999999996e+24,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 10000.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 13756136.253818648,
                "training_compute_estimation_method": "Hardware",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 10.146128035678238,
                "log_compute": 24.44715803134222,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "Cosmos-Predict1-7b-Video2World",
                "domain": "Video,Vision,Robotics",
                "task": "Robotic manipulation,System control,Video generation",
                "organization": "NVIDIA",
                "authors": "NVIDIA: Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, Daniel Dworakowski, Jiaojiao Fan, Michele Fenzi, Francesco Ferroni, Sanja Fidler, Dieter Fox, Songwei Ge, Yunhao Ge, Jinwei Gu, Siddharth Gururani, Ethan He, Jiahui Huang, Jacob Huffman, Pooya Jannaty, Jingyi Jin, Seung Wook Kim, Gergely Kl\u00e1r, Grace Lam, Shiyi Lan, Laura Leal-Taixe, Anqi Li, Zhaoshuo Li, Chen-Hsuan Lin, Tsung-Yi Lin, Huan Ling, Ming-Yu Liu, Xian Liu, Alice Luo, Qianli Ma, Hanzi Mao, Kaichun Mo, Arsalan Mousavian, Seungjun Nah, Sriharsha Niverty, David Page, Despoina Paschalidou, Zeeshan Patel, Lindsey Pavao, Morteza Ramezanali, Fitsum Reda, Xiaowei Ren, Vasanth Rao Naik Sabavat, Ed Schmerling, Stella Shi, Bartosz Stefaniak, Shitao Tang, Lyne Tchapmi, Przemek Tredak, Wei-Cheng Tseng, Jibin Varghese, Hao Wang, Haoxiang Wang, Heng Wang, Ting-Chun Wang, Fangyin Wei, Xinyue Wei, Jay Zhangjie Wu, Jiashu Xu, Wei Yang, Lin Yen-Chen, Xiaohui Zeng, Yu Zeng, Jing Zhang, Qinsheng Zhang, Yuxuan Zhang, Qingqing Zhao, Artur Zolkowski",
                "publication_date": "2025-01-07",
                "reference": "Cosmos World Foundation Model Platform for Physical AI",
                "link": "https://arxiv.org/abs/2501.03575",
                "citations": null,
                "notability_criteria": null,
                "parameters": 7000000000.0,
                "training_compute_(flop)": 1.3999999999999998e+24,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 10000.0,
                "last_modified": "2025-10-01 15:08:40+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 13756136.253818648,
                "training_compute_estimation_method": "Hardware",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 9.845098040014257,
                "log_compute": 24.146128035678238,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "Cosmos-Predict1-14b-Video2World",
                "domain": "Video,Vision,Robotics",
                "task": "Robotic manipulation,System control,Video generation",
                "organization": "NVIDIA",
                "authors": "NVIDIA: Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, Daniel Dworakowski, Jiaojiao Fan, Michele Fenzi, Francesco Ferroni, Sanja Fidler, Dieter Fox, Songwei Ge, Yunhao Ge, Jinwei Gu, Siddharth Gururani, Ethan He, Jiahui Huang, Jacob Huffman, Pooya Jannaty, Jingyi Jin, Seung Wook Kim, Gergely Kl\u00e1r, Grace Lam, Shiyi Lan, Laura Leal-Taixe, Anqi Li, Zhaoshuo Li, Chen-Hsuan Lin, Tsung-Yi Lin, Huan Ling, Ming-Yu Liu, Xian Liu, Alice Luo, Qianli Ma, Hanzi Mao, Kaichun Mo, Arsalan Mousavian, Seungjun Nah, Sriharsha Niverty, David Page, Despoina Paschalidou, Zeeshan Patel, Lindsey Pavao, Morteza Ramezanali, Fitsum Reda, Xiaowei Ren, Vasanth Rao Naik Sabavat, Ed Schmerling, Stella Shi, Bartosz Stefaniak, Shitao Tang, Lyne Tchapmi, Przemek Tredak, Wei-Cheng Tseng, Jibin Varghese, Hao Wang, Haoxiang Wang, Heng Wang, Ting-Chun Wang, Fangyin Wei, Xinyue Wei, Jay Zhangjie Wu, Jiashu Xu, Wei Yang, Lin Yen-Chen, Xiaohui Zeng, Yu Zeng, Jing Zhang, Qinsheng Zhang, Yuxuan Zhang, Qingqing Zhao, Artur Zolkowski",
                "publication_date": "2025-01-07",
                "reference": "Cosmos World Foundation Model Platform for Physical AI",
                "link": "https://arxiv.org/abs/2501.03575",
                "citations": null,
                "notability_criteria": null,
                "parameters": 14000000000.0,
                "training_compute_(flop)": 2.7999999999999996e+24,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 10000.0,
                "last_modified": "2025-10-01 15:08:40+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 13756136.253818648,
                "training_compute_estimation_method": "Hardware",
                "year": 2025,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 31.0,
                "publication_count": 12.493526880374354,
                "log_params": 10.146128035678238,
                "log_compute": 24.44715803134222,
                "pub_bin": "Medium",
                "export_bin": "High"
            },
            {
                "model": "OLMo 2 Furious 7B",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering",
                "organization": "Allen Institute for AI,University of Washington,New York University (NYU)",
                "authors": "Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, Nathan Lambert, Dustin Schwenk, Oyvind Tafjord, Taira Anderson, David Atkinson, Faeze Brahman, Christopher Clark, Pradeep Dasigi, Nouha Dziri, Michal Guerquin, Hamish Ivison, Pang Wei Koh, Jiacheng Liu, Saumya Malik, William Merrill, Lester James V. Miranda, Jacob Morrison, Tyler Murray, Crystal Nam, Valentina Pyatkin, Aman Rangapur, Michael Schmitz, Sam Skjonsberg, David Wadden, Christopher Wilhelm, Michael Wilson, Luke Zettlemoyer, Ali Farhadi, Noah A. Smith, Hannaneh Hajishirzi",
                "publication_date": "2024-12-31",
                "reference": "2 OLMo 2 Furious",
                "link": "https://arxiv.org/abs/2501.00656",
                "citations": null,
                "notability_criteria": null,
                "parameters": 7000000000.0,
                "training_compute_(flop)": 1.8e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Reported,Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.845098040014257,
                "log_compute": 23.255272505103306,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "OLMo 2 Furious 13B",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering",
                "organization": "Allen Institute for AI,University of Washington,New York University (NYU)",
                "authors": "Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, Nathan Lambert, Dustin Schwenk, Oyvind Tafjord, Taira Anderson, David Atkinson, Faeze Brahman, Christopher Clark, Pradeep Dasigi, Nouha Dziri, Michal Guerquin, Hamish Ivison, Pang Wei Koh, Jiacheng Liu, Saumya Malik, William Merrill, Lester James V. Miranda, Jacob Morrison, Tyler Murray, Crystal Nam, Valentina Pyatkin, Aman Rangapur, Michael Schmitz, Sam Skjonsberg, David Wadden, Christopher Wilhelm, Michael Wilson, Luke Zettlemoyer, Ali Farhadi, Noah A. Smith, Hannaneh Hajishirzi",
                "publication_date": "2024-12-31",
                "reference": "2 OLMo 2 Furious",
                "link": "https://arxiv.org/abs/2501.00656",
                "citations": null,
                "notability_criteria": null,
                "parameters": 13000000000.0,
                "training_compute_(flop)": 4.6e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.2,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": 1280.0,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Reported,Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 10.113943352306837,
                "log_compute": 23.662757831681574,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "DeepSeek-V3",
                "domain": "Language",
                "task": "Language modeling/generation,Code generation,Quantitative reasoning,Question answering",
                "organization": "DeepSeek",
                "authors": "DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J.L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R.J. Chen, R.L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S.S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W.L. Xiao, Wangding Zeng et al. (100 additional authors not shown)",
                "publication_date": "2024-12-24",
                "reference": "DeepSeek-V3 Technical Report",
                "link": "https://arxiv.org/abs/2412.19437",
                "citations": null,
                "notability_criteria": "Training cost",
                "parameters": 671000000000.0,
                "training_compute_(flop)": 3.4078e+24,
                "training_dataset_size_(gradients)": 14800000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA H800 SXM5",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 2048.0,
                "last_modified": "2025-10-03 17:07:31+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": 2788000.0,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": 5390000.0,
                "frontier_model": false,
                "training_power_draw_(w)": 2818135.1812142837,
                "training_compute_estimation_method": "Operation counting,Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 11.826722520168993,
                "log_compute": 24.532474098581414,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Granite 3.1 2B",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Translation",
                "organization": "IBM",
                "authors": "Unknown",
                "publication_date": "2024-12-18",
                "reference": null,
                "link": "https://huggingface.co/ibm-granite/granite-3.1-2b-base",
                "citations": null,
                "notability_criteria": null,
                "parameters": 2500000000.0,
                "training_compute_(flop)": 1.8e+23,
                "training_dataset_size_(gradients)": 12000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-08-01 16:23:39+00:00",
                "training_cloud_compute_vendor": "IBM",
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.397940008672037,
                "log_compute": 23.255272505103306,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "Granite 3.1 8B",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Translation",
                "organization": "IBM",
                "authors": "Unknown",
                "publication_date": "2024-12-18",
                "reference": null,
                "link": "https://huggingface.co/ibm-granite/granite-3.1-8b-base",
                "citations": null,
                "notability_criteria": null,
                "parameters": 8100000000.0,
                "training_compute_(flop)": 5.832e+23,
                "training_dataset_size_(gradients)": 12000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-08-05 19:36:28+00:00",
                "training_cloud_compute_vendor": "IBM",
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.90848501887865,
                "log_compute": 23.76581751530992,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "F5-TTS",
                "domain": "Speech",
                "task": "Speech synthesis,Translation",
                "organization": "Shanghai Jiao Tong University,University of Cambridge,Geely Automobile Research Institute (Ningbo) Company",
                "authors": "Yushen Chen, Zhikang Niu, Ziyang Ma, Keqi Deng, Chunhui Wang, Jian Zhao, Kai Yu, Xie Chen",
                "publication_date": "2024-12-15",
                "reference": "F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching",
                "link": "https://arxiv.org/abs/2410.06885",
                "citations": null,
                "notability_criteria": null,
                "parameters": 335800000.0,
                "training_compute_(flop)": 4.5287424e+20,
                "training_dataset_size_(gradients)": 27325800000.0,
                "training_time_(hours)": 168.0,
                "training_hardware": "NVIDIA A100 SXM4 80 GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 8.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 307200.0,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 6291.741206937689,
                "training_compute_estimation_method": "Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Audio",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 8.52608069180203,
                "log_compute": 20.6559776182232,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Phi-4",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Code generation,Quantitative reasoning",
                "organization": "Microsoft Research",
                "authors": "Marah Abdin, Jyoti Aneja, Harkirat Behl, S\u00e9bastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell J. Hewett, Mojan Javaheripi, Piero Kauffmann, James R. Lee, Yin Tat Lee, Yuanzhi Li, Weishung Liu, Caio C. T. Mendes, Anh Nguyen, Eric Price, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Xin Wang, Rachel Ward, Yue Wu, Dingli Yu, Cyril Zhang, Yi Zhang",
                "publication_date": "2024-12-12",
                "reference": "Phi-4 Technical Report",
                "link": "https://arxiv.org/abs/2412.08905",
                "citations": null,
                "notability_criteria": null,
                "parameters": 14000000000.0,
                "training_compute_(flop)": 9.3202015e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 504.0,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 1920.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 2642707.855343539,
                "training_compute_estimation_method": "Operation counting,Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 10.146128035678238,
                "log_compute": 23.969425301773736,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "Llama 3.3 70B",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Translation,Code generation",
                "organization": "Meta AI",
                "authors": "Unknown",
                "publication_date": "2024-12-06",
                "reference": "Meta Llama 3.3 multilingual large language model (LLM) ",
                "link": "https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_3/",
                "citations": null,
                "notability_criteria": "Training cost",
                "parameters": 70000000000.0,
                "training_compute_(flop)": 6.8649768e+24,
                "training_dataset_size_(gradients)": 15000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": 7000000.0,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting,Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 10.845098040014257,
                "log_compute": 24.836639073889014,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "NVILA 8B",
                "domain": "Vision,Language,Multimodal,Video",
                "task": "Visual question answering,Video description",
                "organization": "NVIDIA,Massachusetts Institute of Technology (MIT),University of California (UC) Berkeley,University of California San Diego,University of Washington,Tsinghua University",
                "authors": "Zhijian Liu, Ligeng Zhu, Baifeng Shi, Zhuoyang Zhang, Yuming Lou, Shang Yang, Haocheng Xi, Shiyi Cao, Yuxian Gu, Dacheng Li, Xiuyu Li, Yunhao Fang, Yukang Chen, Cheng-Yu Hsieh, De-An Huang, An-Chieh Cheng, Vishwesh Nath, Jinyi Hu, Sifei Liu, Ranjay Krishna, Daguang Xu, Xiaolong Wang, Pavlo Molchanov, Jan Kautz, Hongxu Yin, Song Han, Yao Lu",
                "publication_date": "2024-12-05",
                "reference": "NVILA: Efficient Frontier Visual Language Models",
                "link": "https://arxiv.org/abs/2412.04468",
                "citations": null,
                "notability_criteria": null,
                "parameters": 8000000000.0,
                "training_compute_(flop)": 2.2794518e+21,
                "training_dataset_size_(gradients)": 47488579166.0,
                "training_time_(hours)": 16.7,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 128.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": 2133.0,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 176207.9898367568,
                "training_compute_estimation_method": "Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.903089986991944,
                "log_compute": 21.35783041328931,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "Pleias 1.0 350m",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Translation",
                "organization": "PleIAs",
                "authors": "Unknown",
                "publication_date": "2024-12-05",
                "reference": "Pleias-pico-350m-Preview",
                "link": "https://huggingface.co/PleIAs/Pleias-350m-Preview",
                "citations": null,
                "notability_criteria": null,
                "parameters": 350000000.0,
                "training_compute_(flop)": 2.6788982e+21,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 46.0,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "France",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 64.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 88103.9949183784,
                "training_compute_estimation_method": "Hardware,Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "France",
                "domain_group": "Language",
                "export_controls_sum": 10.0,
                "publication_count": 2.4529600742986126,
                "log_params": 8.544068044350276,
                "log_compute": 21.42795621042856,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "Pleias 1.0 1.2B",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Translation",
                "organization": "PleIAs",
                "authors": "Unknown",
                "publication_date": "2024-12-05",
                "reference": "Pleias-nano-1.2b-Preview ",
                "link": "https://huggingface.co/PleIAs/Pleias-1.2b-Preview",
                "citations": null,
                "notability_criteria": null,
                "parameters": 1200000000.0,
                "training_compute_(flop)": 2.9770787e+22,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 120.0,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": 3.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "France",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 192.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": "Nebius AI",
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 264311.9847551352,
                "training_compute_estimation_method": "Hardware,Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "France",
                "domain_group": "Language",
                "export_controls_sum": 10.0,
                "publication_count": 2.4529600742986126,
                "log_params": 9.079181246047625,
                "log_compute": 22.47379031550798,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "Hunyuan Video",
                "domain": "Video",
                "task": "Video generation,Text-to-video",
                "organization": "Tencent",
                "authors": "Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, Kathrina Wu, Qin Lin, Junkun Yuan, Yanxin Long, Aladdin Wang, Andong Wang, Changlin Li, Duojun Huang, Fang Yang, Hao Tan, Hongmei Wang, Jacob Song, Jiawang Bai, Jianbing Wu, Jinbao Xue, Joey Wang, Kai Wang, Mengyang Liu, Pengyu Li, Shuai Li, Weiyan Wang, Wenqing Yu, Xinchi Deng, Yang Li, Yi Chen, Yutao Cui, Yuanbo Peng, Zhentao Yu, Zhiyu He, Zhiyong Xu, Zixiang Zhou, Zunnan Xu, Yangyu Tao, Qinglin Lu, Songtao Liu, Daquan Zhou, Hongfa Wang, Yong Yang, Di Wang, Yuhong Liu, Jie Jiang, Caesar Zhong",
                "publication_date": "2024-12-03",
                "reference": "HunyuanVideo: A Systematic Framework For Large Video Generative Models",
                "link": "https://www.arxiv.org/abs/2412.03603",
                "citations": null,
                "notability_criteria": null,
                "parameters": 13000000000.0,
                "training_compute_(flop)": 1.4814815e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Reported,Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Vision",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 10.113943352306837,
                "log_compute": 23.170696232597656,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "INTELLECT-1",
                "domain": "Language",
                "task": null,
                "organization": "Prime Intellect,Hugging Face,Arcee AI",
                "authors": "Sami Jaghouar, Jack Min Ong, Manveer Basra, Fares Obeid, Jannik Straube, Michael Keiblinger, Elie Bakouch, Lucas Atkins, Maziyar Panahi, Charles Goddard, Max Ryabinin, Johannes Hagemann",
                "publication_date": "2024-11-29",
                "reference": "INTELLECT-1 Technical Report",
                "link": "https://github.com/PrimeIntellect-ai/prime/blob/main/INTELLECT_1_Technical_Report.pdf",
                "citations": null,
                "notability_criteria": null,
                "parameters": 10000000000.0,
                "training_compute_(flop)": 6.000001e+22,
                "training_dataset_size_(gradients)": 1000000000000.0,
                "training_time_(hours)": 1008.0,
                "training_hardware": null,
                "approach": "Self-supervised learning",
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unknown",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 10.0,
                "log_compute": 22.77815132276605,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "Hymba",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering",
                "organization": "NVIDIA",
                "authors": "Xin Dong, Yonggan Fu, Shizhe Diao, Wonmin Byeon, Zijia Chen, Ameya Sunil Mahabaleshwarkar, Shih-Yang Liu, Matthijs Van Keirsbilck, Min-Hung Chen, Yoshi Suhara, Yingyan Lin, Jan Kautz, Pavlo Molchanov",
                "publication_date": "2024-11-22",
                "reference": "Hymba: A Hybrid-head Architecture for Small Language Models",
                "link": "https://arxiv.org/abs/2411.13676",
                "citations": null,
                "notability_criteria": null,
                "parameters": 1500000000.0,
                "training_compute_(flop)": 1.35e+22,
                "training_dataset_size_(gradients)": 1500000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 128.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 2000000.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 100719.43414656924,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.176091259055681,
                "log_compute": 22.130333768495007,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "360Zhinao2-7B",
                "domain": "Language",
                "task": "Question answering,Language modeling/generation,Code generation,Quantitative reasoning",
                "organization": "360 Security Technology",
                "authors": "Unknown",
                "publication_date": "2024-11-18",
                "reference": "360Zhinao2 (360\u667a\u8111)",
                "link": "https://github.com/Qihoo360/360zhinao2",
                "citations": null,
                "notability_criteria": null,
                "parameters": 7000000000.0,
                "training_compute_(flop)": 4.242e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": "360Zhinao-7B",
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 9.845098040014257,
                "log_compute": 23.627570664180542,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Qwen2.5-Coder (32B)",
                "domain": "Language",
                "task": "Language modeling/generation,Code generation",
                "organization": "Alibaba",
                "authors": "Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, Kai Dang, Yang Fan, Yichang Zhang, An Yang, Rui Men, Fei Huang, Bo Zheng, Yibo Miao, Shanghaoran Quan, Yunlong Feng, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, Junyang Lin",
                "publication_date": "2024-11-12",
                "reference": "Qwen2.5-Coder Technical Report",
                "link": "https://arxiv.org/abs/2409.12186",
                "citations": null,
                "notability_criteria": null,
                "parameters": 32500000000.0,
                "training_compute_(flop)": 1.0725e+24,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 10.511883360978874,
                "log_compute": 24.030397300856762,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Hunyuan-Large",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Code generation,Translation",
                "organization": "Tencent",
                "authors": "Xingwu Sun, Yanfeng Chen, Yiqing Huang, Ruobing Xie, Jiaqi Zhu, Kai Zhang, Shuaipeng Li, Zhen Yang, Jonny Han, Xiaobo Shu, Jiahao Bu, Zhongzhi Chen, Xuemeng Huang, Fengzong Lian,\nSaiyong Yang, Jianfeng Yan, Yuyuan Zeng, Xiaoqin Ren, Chao Yu, Lulu Wu, Yue Mao, Jun Xia, Tao Yang, Suncong Zheng, Kan Wu, Dian Jiao, Jinbao Xue, Xipeng Zhang, Decheng Wu, Kai Liu, Dengpeng Wu, Guanghui Xu, Shaohua Chen, Shuang Chen, Xiao Feng, Yigeng Hong, Junqiang Zheng, Chengcheng Xu, Zongwei Li, Xiong Kuang, Jianglu Hu, Yiqi Chen, Yuchi Deng, Guiyang Li, Ao Liu, Chenchen Zhang, Shihui Hu, Zilong Zhao, Zifan Wu, Yao Ding, Weichao Wang, Han Liu, Roberts Wang, Hao Fei, Peijie Yu, Ze Zhao, Xun Cao, Hai Wang, Fusheng Xiang, Mengyuan Huang, Zhiyuan Xiong, Bin Hu, Xuebin Hou, Lei Jiang, Jianqiang Ma, Jiajia Wu, Yaping Deng, Yi Shen, Qian Wang, Weijie Liu, Jie Liu, Meng Chen, Liang Dong, Weiwen Jia, Hu Chen, Feifei Liu, Rui Yuan, Huilin Xu, Zhenxiang Yan, Tengfei Cao, Zhichao Hu, Xinhua Feng, Dong Du, Tinghao Yu, Yangyu Tao, Feng Zhang, Jianchen Zhu, Chengzhong Xu, Xirui Li, Chong Zha, Wen Ouyang, Yinben Xia, Xiang Li, Zekun He, Rongpeng Chen, Jiawei Song, Ruibin Chen, Fan Jiang, Chongqing Zhao, Bo Wang, Hao Gong, Rong Gan, Winston Hu, Zhanhui Kang, Yong Yang, Yuhong Liu, Di Wang, and Jie Jiang.\n",
                "publication_date": "2024-11-06",
                "reference": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent",
                "link": "https://arxiv.org/abs/2411.02265",
                "citations": null,
                "notability_criteria": "Training cost",
                "parameters": 389000000000.0,
                "training_compute_(flop)": 3.49237e+24,
                "training_dataset_size_(gradients)": 7000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open (restricted use)",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 11.589949601325708,
                "log_compute": 24.543120248906924,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "OpenPhenom-S/16",
                "domain": "Biology,Vision",
                "task": "Image embedding",
                "organization": "Recursion Pharmaceuticals",
                "authors": "OpenPhenom",
                "publication_date": "2024-11-05",
                "reference": "Generative deep computer vision models",
                "link": "https://www.rxrx.ai/phenom#:~:text=We%20call%20this%20model%20Phenom-Beta.%20It%20flexibly%20processes,create%20a%20meaningful%20representation%20of%20the%20input%20image.",
                "citations": null,
                "notability_criteria": null,
                "parameters": 178045568.0,
                "training_compute_(flop)": 3.18e+19,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 400.0,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": null,
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 8.250531167467727,
                "log_compute": 19.502427119984432,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "Uni-Med",
                "domain": null,
                "task": null,
                "organization": "Tsinghua University,Beijing University of Posts and Telecommunications",
                "authors": "Xun Zhu, Ying Hu, Fanbin Mo, Miao Li, Ji Wu",
                "publication_date": "2024-11-01",
                "reference": "Uni-Med: A Unified Medical Generalist Foundation\nModel For Multi-Task Learning Via Connector-MoE",
                "link": "https://arxiv.org/pdf/2409.17508",
                "citations": 11.0,
                "notability_criteria": null,
                "parameters": 8800000000.0,
                "training_compute_(flop)": 1.425e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 10.0,
                "training_hardware": "NVIDIA A800 SXM",
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "China",
                "base_model": "Llama 2-7B,ViT-G/14",
                "finetune_compute_(flop)": 5.616e+18,
                "hardware_quantity": 1.0,
                "last_modified": "2025-10-14 18:16:28+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open (restricted use)",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 432.54870917774826,
                "training_compute_estimation_method": null,
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": null,
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 9.94448267215017,
                "log_compute": 23.15381486434453,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "VASA-1",
                "domain": "Video,Audio",
                "task": "Video generation",
                "organization": "Microsoft Research Asia",
                "authors": "Sicheng Xu, Guojun Chen, Yu-Xiao Guo, Jiaolong Yang, Chong Li, Zhenyu Zang, Yizhong Zhang, Xin Tong, Baining Guo",
                "publication_date": "2024-10-31",
                "reference": "VASA-1: Lifelike Audio-Driven Talking Faces Generated in Real Time",
                "link": "https://arxiv.org/abs/2404.10667",
                "citations": null,
                "notability_criteria": null,
                "parameters": 229000000.0,
                "training_compute_(flop)": 4.012416e+19,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 240.0,
                "training_hardware": "NVIDIA RTX A6000",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 4.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 2361.768546625511,
                "training_compute_estimation_method": "Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Multimodal",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 8.359835482339887,
                "log_compute": 19.60340595354543,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Pro-PRIME",
                "domain": "Biology",
                "task": "Protein design",
                "organization": "Shanghai Jiao Tong University,Shanghai AI Lab,East China University of Science and Technology,Shanghai Tech University,Guangzhou Inernational Bio Island,Chinese Academy of Sciences,Shanghai Academy of Experimental Medicine",
                "authors": "Fan Jiang, Mingchen Li, Jiajun Dong, Yuanxi Yu, Xinyu Sun, Banghao Wu, Jin Huang, Liqi Kang, Yufeng Pei, Liang Zhang, Shaojie Wang, Wenxue Xu, Jingyao Xin, Wanli Ouyang, Guisheng Fan, Lirong Zheng, Yang Tan, Zhiqiang Hu, Yi Xiong, Yan Feng, Guangyu Yang, Qian Liu, Jie Song, Jia Liu, Liang Hong, Pan Tan",
                "publication_date": "2024-10-28",
                "reference": "Pro-PRIME: A general Temperature-Guided Language model to engineer enhanced Stability and Activity in Proteins",
                "link": "https://arxiv.org/abs/2307.12682",
                "citations": 1.0,
                "notability_criteria": null,
                "parameters": 650000000.0,
                "training_compute_(flop)": 8.18e+20,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": 7.0,
                "model_accessibility": "Unknown",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Biology",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 8.812913356642856,
                "log_compute": 20.912753303671323,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Doubao-pro",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Text summarization,Text classification",
                "organization": "ByteDance",
                "authors": "Unknown",
                "publication_date": "2024-10-28",
                "reference": "Doubao General Model Pro (Doubao-pro)",
                "link": "https://www.volcengine.com/docs/6360/1264663",
                "citations": null,
                "notability_criteria": "Training cost",
                "parameters": 500000000000.0,
                "training_compute_(flop)": 2.505e+25,
                "training_dataset_size_(gradients)": 8350000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "API access",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 11.698970004336019,
                "log_compute": 25.398807730203263,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "NVLM-D 72B",
                "domain": "Vision,Language",
                "task": "Language modeling/generation,Vision-language generation,Question answering,Code generation,Translation,Quantitative reasoning",
                "organization": "NVIDIA",
                "authors": "Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuolin Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping",
                "publication_date": "2024-10-22",
                "reference": "NVLM: Open Frontier-Class Multimodal LLMs",
                "link": "https://arxiv.org/abs/2409.11402",
                "citations": null,
                "notability_criteria": "SOTA improvement",
                "parameters": 72000000000.0,
                "training_compute_(flop)": 3.02e+24,
                "training_dataset_size_(gradients)": 57016320000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "Multinational",
                "base_model": "Qwen2-72B,InternViT-6B",
                "finetune_compute_(flop)": 2.463e+22,
                "hardware_quantity": 128.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 176380.73226445544,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 10.857332496431269,
                "log_compute": 24.48000694295715,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "NVLM-H 72B",
                "domain": "Vision,Language",
                "task": "Language modeling/generation,Vision-language generation,Question answering,Code generation,Translation,Quantitative reasoning",
                "organization": "NVIDIA",
                "authors": "Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuolin Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping",
                "publication_date": "2024-10-22",
                "reference": "NVLM: Open Frontier-Class Multimodal LLMs",
                "link": "https://arxiv.org/abs/2409.11402",
                "citations": null,
                "notability_criteria": "Training cost",
                "parameters": 72000000000.0,
                "training_compute_(flop)": 3.02e+24,
                "training_dataset_size_(gradients)": 125829120000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Likely",
                "epochs": 1.0,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": "Qwen2-72B,InternViT-6B",
                "finetune_compute_(flop)": 5.436e+22,
                "hardware_quantity": 128.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 176380.73226445544,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 10.857332496431269,
                "log_compute": 24.48000694295715,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "NVLM-X 72B",
                "domain": "Vision,Language",
                "task": "Language modeling/generation,Vision-language generation,Question answering,Code generation,Translation,Quantitative reasoning",
                "organization": "NVIDIA",
                "authors": "Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuolin Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping",
                "publication_date": "2024-10-22",
                "reference": "NVLM: Open Frontier-Class Multimodal LLMs",
                "link": "https://arxiv.org/abs/2409.11402",
                "citations": null,
                "notability_criteria": "Training cost",
                "parameters": 72000000000.0,
                "training_compute_(flop)": 3.0398181e+24,
                "training_dataset_size_(gradients)": 45875200000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Likely",
                "epochs": 1.0,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": "Qwen2-72B,InternViT-6B",
                "finetune_compute_(flop)": 1.9818086e+22,
                "hardware_quantity": 128.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 176380.73226445544,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 10.857332496431269,
                "log_compute": 24.48284759659237,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "Granite 3.0 8B",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Translation,Text summarization,Text classification,Code generation",
                "organization": "IBM",
                "authors": "Granite Team IBM",
                "publication_date": "2024-10-21",
                "reference": "Granite 3.0 Language Models",
                "link": "https://github.com/ibm-granite/granite-3.0-language-models/tree/main",
                "citations": null,
                "notability_criteria": null,
                "parameters": 8100000000.0,
                "training_compute_(flop)": 5.832e+23,
                "training_dataset_size_(gradients)": 12000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 256.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": 832102.0,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 352769.3203924083,
                "training_compute_estimation_method": "Operation counting,Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.90848501887865,
                "log_compute": 23.76581751530992,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "Granite 3.0 2B",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Translation,Text summarization,Text classification,Code generation",
                "organization": "IBM",
                "authors": "Granite Team IBM",
                "publication_date": "2024-10-21",
                "reference": "Granite 3.0 Language Models",
                "link": "https://github.com/ibm-granite/granite-3.0-language-models/tree/main",
                "citations": null,
                "notability_criteria": null,
                "parameters": 2500000000.0,
                "training_compute_(flop)": 1.8e+23,
                "training_dataset_size_(gradients)": 12000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 768.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": 192030.0,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 1058307.961177225,
                "training_compute_estimation_method": "Operation counting,Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.397940008672037,
                "log_compute": 23.255272505103306,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "Allegro",
                "domain": "Video",
                "task": "Video generation,Text-to-video",
                "organization": "Rhymes AI",
                "authors": "Yuan Zhou, Qiuyue Wang, Yuxuan Cai, Huan Yang",
                "publication_date": "2024-10-20",
                "reference": "Allegro: Open the Black Box of Commercial-Level Video Generation Model",
                "link": "https://arxiv.org/abs/2410.15458",
                "citations": null,
                "notability_criteria": null,
                "parameters": 2975000000.0,
                "training_compute_(flop)": 6.565847e+22,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Speculative",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 256.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 352777.1764308529,
                "training_compute_estimation_method": "Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Vision",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.473486970064569,
                "log_compute": 22.81729075840704,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "RDT-1B",
                "domain": "Robotics",
                "task": "Robotic manipulation",
                "organization": "Tsinghua University",
                "authors": "Songming Liu, Lingxuan Wu, Bangguo Li, Hengkai Tan, Huayu Chen, Zhengyi Wang, Ke Xu, Hang Su, Jun Zhu",
                "publication_date": "2024-10-10",
                "reference": "RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation",
                "link": "https://arxiv.org/abs/2410.07864",
                "citations": 248.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 1200000000.0,
                "training_compute_(flop)": 4.06e+22,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 720.0,
                "training_hardware": "NVIDIA H100 PCIe",
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": 3.69e+21,
                "hardware_quantity": 48.0,
                "last_modified": "2025-10-16 13:10:31+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 1536.0,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 33080.22622858553,
                "training_compute_estimation_method": null,
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "China",
                "domain_group": "Robotics",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 9.079181246047625,
                "log_compute": 22.608526033577196,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Pyramid Flow",
                "domain": "Video",
                "task": "Video generation,Text-to-video",
                "organization": "Peking University,Kuaishou Technology,Beijing University of Posts and Telecommunications",
                "authors": "Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, Zhouchen Lin",
                "publication_date": "2024-10-08",
                "reference": "Pyramidal Flow Matching for Efficient Video Generative Modeling",
                "link": "https://arxiv.org/abs/2410.05954",
                "citations": 137.0,
                "notability_criteria": null,
                "parameters": 2000000000.0,
                "training_compute_(flop)": 7.67e+21,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 162.0,
                "training_hardware": "NVIDIA A100",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 128.0,
                "last_modified": "2025-10-16 13:10:31+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": 20700.0,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 100820.41786842256,
                "training_compute_estimation_method": null,
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Vision",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 9.301029995663981,
                "log_compute": 21.88479536394898,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Aria",
                "domain": "Multimodal,Language,Video,Vision",
                "task": "Language modeling/generation,Visual question answering,Image captioning,Question answering,Code generation,Video description,Character recognition (OCR)",
                "organization": "Rhymes AI",
                "authors": "Dongxu Li, Yudong Liu, Haoning Wu, Yue Wang, Zhiqi Shen, Bowen Qu, Xinyao Niu, Fan Zhou, Chengen Huang, Yanpeng Li, Chongyan Zhu, Xiaoyi Ren, Chao Li, Yifan Ye, Peng Liu, Lihuan Zhang, Hanshu Yan, Guoyin Wang, Bei Chen, Junnan Li",
                "publication_date": "2024-10-08",
                "reference": "ARIA : An Open Multimodal Native\nMixture-of-Experts Model",
                "link": "https://arxiv.org/abs/2410.05993",
                "citations": 100.0,
                "notability_criteria": null,
                "parameters": 24900000000.0,
                "training_compute_(flop)": 1.428e+23,
                "training_dataset_size_(gradients)": 6800000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:10:31+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 10.396199347095736,
                "log_compute": 23.154728207440154,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "Movie Gen Video",
                "domain": "Video,Vision",
                "task": "Video generation,Text-to-video,Image-to-video",
                "organization": "Meta AI",
                "authors": "Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen\nShi, Chih-Yao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary, Dingkang Wang, Geet Sethi, Guan\nPang, Haoyu Ma, Ishan Misra, Ji Hou, Jialiang Wang, Kiran Jagadeesh, Kunpeng Li, Luxin Zhang, Mannat\nSingh, Mary Williamson, Matt Le, Mitesh Kumar Singh, Peizhao Zhang, Peter Vajda, Quentin Duval, Rohit\nGirdhar, Roshan Sumbaly, Sai Saketh Rambhatla, Sam Tsai, Samaneh Azadi, Samyak Datta, Sanyuan Chen,\nSean Bell, Sharadh Ramaswamy, Shelly Sheynin, Siddharth Bhattacharya, Tao Xu, Tingbo Hou, Wei-Ning\nHsu, Xi Yin, Xiaoliang Dai, Yaniv Taigman, Yaqiao Luo, Yen-Cheng Liu, Yi-Chiao Wu, Yue Zhao, Yuval\nKirstain, Zecheng He, Zijian He",
                "publication_date": "2024-10-04",
                "reference": "Movie Gen: A Cast of Media Foundation Models",
                "link": "https://ai.meta.com/static-resource/movie-gen-research-paper",
                "citations": null,
                "notability_criteria": "Training cost",
                "parameters": 30000000000.0,
                "training_compute_(flop)": 1.65e+24,
                "training_dataset_size_(gradients)": 3400000000.0,
                "training_time_(hours)": 331.0,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 6144.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 8469669.524206791,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Vision",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 10.477121254719663,
                "log_compute": 24.217483944213907,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "Movie Gen Audio",
                "domain": "Audio",
                "task": "Audio generation",
                "organization": "Meta AI",
                "authors": "Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen\nShi, Chih-Yao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary, Dingkang Wang, Geet Sethi, Guan\nPang, Haoyu Ma, Ishan Misra, Ji Hou, Jialiang Wang, Kiran Jagadeesh, Kunpeng Li, Luxin Zhang, Mannat\nSingh, Mary Williamson, Matt Le, Mitesh Kumar Singh, Peizhao Zhang, Peter Vajda, Quentin Duval, Rohit\nGirdhar, Roshan Sumbaly, Sai Saketh Rambhatla, Sam Tsai, Samaneh Azadi, Samyak Datta, Sanyuan Chen,\nSean Bell, Sharadh Ramaswamy, Shelly Sheynin, Siddharth Bhattacharya, Tao Xu, Tingbo Hou, Wei-Ning\nHsu, Xi Yin, Xiaoliang Dai, Yaniv Taigman, Yaqiao Luo, Yen-Cheng Liu, Yi-Chiao Wu, Yue Zhao, Yuval\nKirstain, Zecheng He, Zijian He",
                "publication_date": "2024-10-04",
                "reference": "Movie Gen: A Cast of Media Foundation Models",
                "link": "https://ai.meta.com/static-resource/movie-gen-research-paper",
                "citations": null,
                "notability_criteria": null,
                "parameters": 13000000000.0,
                "training_compute_(flop)": 1.4e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 360.0,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 384.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 529354.3452629244,
                "training_compute_estimation_method": "Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Audio",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 10.113943352306837,
                "log_compute": 23.146128035678238,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "Molmo 72B",
                "domain": "Language,Vision,Multimodal",
                "task": "Language modeling/generation,Visual question answering,Question answering",
                "organization": "Allen Institute for AI,University of Washington",
                "authors": "Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bransom, Kiana Ehsani, Huong Ngo, YenSung Chen, Ajay Patel, Mark Yatskar, Chris Callison-Burch, Andrew Head, Rose Hendrix, Favyen Bastani, Eli VanderBilt, Nathan Lambert, Yvonne Chou, Arnavi Chheda, Jenna Sparks, Sam Skjonsberg, Michael Schmitz, Aaron Sarnat, Byron Bischoff, Pete Walsh, Chris Newell, Piper Wolters, Tanmay Gupta, Kuo-Hao Zeng, Jon Borchardt, Dirk Groeneveld, Jen Dumas, Crystal Nam, Sophie Lebrecht, Caitlin Wittlif, Carissa Schoenick, Oscar Michel, Ranjay Krishna, Luca Weihs, Noah A. Smith, Hannaneh Hajishirzi, Ross Girshick, Ali Farhadi, Aniruddha Kembhavi",
                "publication_date": "2024-09-25",
                "reference": "Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models",
                "link": "https://arxiv.org/abs/2409.17146",
                "citations": null,
                "notability_criteria": null,
                "parameters": 72000000000.0,
                "training_compute_(flop)": 1.33583e+22,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": "Qwen2-72B,CLIP (ViT L/14@336px)",
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware,Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 10.857332496431269,
                "log_compute": 22.12575119260091,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "ProtBFN",
                "domain": "Biology",
                "task": "Protein or nucleotide language model (pLM/nLM)",
                "organization": "InstaDeep",
                "authors": "Timothy Atkinson, Thomas D. Barrett, Scott Cameron, Bora Guloglu, Matthew Greenig, Louis Robinson, Alex Graves, Liviu Copoiu, Alexandre Laterre",
                "publication_date": "2024-09-24",
                "reference": "Protein Sequence Modelling with Bayesian Flow Networks",
                "link": "https://www.biorxiv.org/content/10.1101/2024.09.24.614734v1.abstract",
                "citations": 8.0,
                "notability_criteria": null,
                "parameters": 650000000.0,
                "training_compute_(flop)": 3.900000000000003e+22,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 432.0,
                "training_hardware": "Google TPU v4",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unknown",
                "country": "United Kingdom",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 256.0,
                "last_modified": "2025-10-14 21:57:19+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 85724.0773206184,
                "training_compute_estimation_method": "Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United Kingdom",
                "domain_group": "Biology",
                "export_controls_sum": 1.0,
                "publication_count": 2.8041950763331074,
                "log_params": 8.812913356642856,
                "log_compute": 22.5910646070265,
                "pub_bin": "Low",
                "export_bin": "Low"
            },
            {
                "model": "Llama 3.2 11B",
                "domain": "Multimodal,Vision,Language",
                "task": "Visual question answering,Image captioning,Object detection",
                "organization": "Meta AI",
                "authors": "Meta AI",
                "publication_date": "2024-09-24",
                "reference": "Llama 3.2: Revolutionizing edge AI and vision with open, customizable models",
                "link": "https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/",
                "citations": null,
                "notability_criteria": "Significant use",
                "parameters": 10600000000.0,
                "training_compute_(flop)": 5.79e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "United States",
                "base_model": "Llama 3.1-8B",
                "finetune_compute_(flop)": 3.50010000000001e+23,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": 246120.0,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 10.02530586526477,
                "log_compute": 23.762678563727437,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "Llama 3.2 1B",
                "domain": "Language",
                "task": "Language modeling/generation,Text summarization,Question answering,Quantitative reasoning,Translation",
                "organization": "Meta AI",
                "authors": "Unknown",
                "publication_date": "2024-09-24",
                "reference": "Llama 3.2: Revolutionizing edge AI and vision with open, customizable models",
                "link": "https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/",
                "citations": null,
                "notability_criteria": null,
                "parameters": 1230000000.0,
                "training_compute_(flop)": 6.642e+22,
                "training_dataset_size_(gradients)": 9000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": 370000.0,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting,Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.089905111439398,
                "log_compute": 22.822298871262365,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "Llama 3.2 3B",
                "domain": "Language",
                "task": "Language modeling/generation,Text summarization,Question answering,Quantitative reasoning,Translation",
                "organization": "Meta AI",
                "authors": "Unknown",
                "publication_date": "2024-09-24",
                "reference": "Llama 3.2: Revolutionizing edge AI and vision with open, customizable models",
                "link": "https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/",
                "citations": null,
                "notability_criteria": null,
                "parameters": 3210000000.0,
                "training_compute_(flop)": 1.7334e+23,
                "training_dataset_size_(gradients)": 9000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": 460000.0,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting,Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.506505032404872,
                "log_compute": 23.23889879222784,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "PocketGen",
                "domain": "Biology",
                "task": "Protein-ligand contact prediction",
                "organization": "University of Science and Technology of China (USTC),Hefei Comprehensive National Science Center,Harvard University,Broad Institute,Harvard Data Science Initiative",
                "authors": "Zaixi Zhang, Wan Xiang Shen, Qi Liu, Marinka Zitnik",
                "publication_date": "2024-09-23",
                "reference": "Efficient Generation of Protein Pockets with PocketGen",
                "link": "https://www.biorxiv.org/content/10.1101/2024.02.25.581968v4.abstract",
                "citations": 13.0,
                "notability_criteria": null,
                "parameters": 7900000.0,
                "training_compute_(flop)": 2.1e+19,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 48.0,
                "training_hardware": "NVIDIA A100",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unknown",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 1.0,
                "last_modified": "2025-10-14 21:57:19+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 432.9245432852901,
                "training_compute_estimation_method": "Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Biology",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 6.897627091290442,
                "log_compute": 19.32221929473392,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Telechat2-115B",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Quantitative reasoning,Chat,Text summarization,Code generation",
                "organization": "China Telecom",
                "authors": "Zihan Wang and Xinzhang Liu and Shixuan Liu and Yitong Yao and Yuyao Huang and Zhongjiang He and Xuelong Li and Yongxiang Li and Zhonghao Che and Zhaoxi Zhang and Yan Wang and Xin Wang and Luwen Pu and Huihan Xu and Ruiyu Fang and Yu Zhao and Jie Zhang and Xiaomeng Huang and Zhilong Lu and Jiaxin Peng and Wenjun Zheng and Shiquan Wang and Bingkai Yang and Xuewei he and Zhuoru Jiang and Qiyi Xie and Yanhan Zhang and Zhongqiu Li and Lingling Shi and Weiwei Fu and Yin Zhang and Zilu Huang and Sishi Xiong and Yuxiang Zhang and Chao Wang and Shuangyong Song",
                "publication_date": "2024-09-20",
                "reference": "TeleChat Technical Report",
                "link": "https://huggingface.co/Tele-AI/TeleChat2-115B",
                "citations": null,
                "notability_criteria": "Training cost",
                "parameters": 115000000000.0,
                "training_compute_(flop)": 6.899999999999999e+24,
                "training_dataset_size_(gradients)": 10000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": "Supervised",
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open (restricted use)",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 11.060697840353612,
                "log_compute": 24.838849090737256,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Prithvi WxC",
                "domain": "Earth science",
                "task": "Weather forecasting",
                "organization": "IBM Research,University of Alabama,Stanford University,Colorado State University,Oak Ridge National Laboratory,NASA",
                "authors": "Johannes Schmude, Sujit Roy, Will Trojak, Johannes Jakubik, Daniel Salles Civitarese, Shraddha Singh, Julian Kuehnert, Kumar Ankur, Aman Gupta, Christopher E Phillips, Romeo Kienzler, Daniela Szwarcman, Vishal Gaur, Rajat Shinde, Rohit Lal, Arlindo Da Silva, Jorge Luis Guevara Diaz, Anne Jones, Simon Pfreundschuh, Amy Lin, Aditi Sheshadri, Udaysankar Nair, Valentine Anantharaj, Hendrik Hamann, Campbell Watson, Manil Maskey, Tsengdar J Lee, Juan Bernabe Moreno, Rahul Ramachandran",
                "publication_date": "2024-09-20",
                "reference": "Prithvi WxC: Foundation Model for Weather and Climate",
                "link": "https://arxiv.org/abs/2409.13598",
                "citations": 16.0,
                "notability_criteria": null,
                "parameters": 2300000000.0,
                "training_compute_(flop)": 7.15e+19,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 64.0,
                "last_modified": "2025-10-14 18:18:49+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry, Government",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 50430.41985055912,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Earth science",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.361727836017593,
                "log_compute": 19.85430604180108,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "Qwen2.5-72B",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Quantitative reasoning",
                "organization": "Alibaba",
                "authors": "Qwen Team",
                "publication_date": "2024-09-19",
                "reference": "Qwen2.5: A Party of Foundation Models!",
                "link": "https://qwenlm.github.io/blog/qwen2.5/",
                "citations": null,
                "notability_criteria": "Training cost",
                "parameters": 72700000000.0,
                "training_compute_(flop)": 7.8e+24,
                "training_dataset_size_(gradients)": 18000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 10.861534410859038,
                "log_compute": 24.89209460269048,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Qwen2.5 Instruct (72B)",
                "domain": "Language",
                "task": "Code generation,Code autocompletion,Quantitative reasoning,Question answering,Language modeling/generation",
                "organization": "Alibaba",
                "authors": "Qwen Team",
                "publication_date": "2024-09-19",
                "reference": "Qwen2.5: A Party of Foundation Models!",
                "link": "https://qwenlm.github.io/blog/qwen2.5/",
                "citations": null,
                "notability_criteria": "Training cost",
                "parameters": 72700000000.0,
                "training_compute_(flop)": 7.8516e+24,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "China",
                "base_model": "Qwen2.5-72B",
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": null,
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 10.861534410859038,
                "log_compute": 24.894958166345987,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Qwen2.5-3B",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Quantitative reasoning",
                "organization": "Alibaba",
                "authors": "Qwen Team",
                "publication_date": "2024-09-19",
                "reference": "Qwen2.5: A Party of Foundation Models!",
                "link": "https://qwenlm.github.io/blog/qwen2.5-llm/",
                "citations": null,
                "notability_criteria": null,
                "parameters": 3090000000.0,
                "training_compute_(flop)": 3.3372e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-03 19:14:51+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 9.489958479424836,
                "log_compute": 23.523382234911786,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Qwen2.5-7B",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Quantitative reasoning",
                "organization": "Alibaba",
                "authors": "Qwen Team",
                "publication_date": "2024-09-19",
                "reference": "Qwen2.5: A Party of Foundation Models!",
                "link": "https://qwenlm.github.io/blog/qwen2.5/",
                "citations": null,
                "notability_criteria": null,
                "parameters": 7610000000.0,
                "training_compute_(flop)": 8.2188e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 9.881384656770573,
                "log_compute": 23.91480841225752,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Qwen2.5-1.5B",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Quantitative reasoning",
                "organization": "Alibaba",
                "authors": "Qwen Team",
                "publication_date": "2024-09-19",
                "reference": "Qwen2.5-LLM: Extending the boundary of LLMs",
                "link": "https://qwenlm.github.io/blog/qwen2.5-llm/",
                "citations": null,
                "notability_criteria": null,
                "parameters": 1540000000.0,
                "training_compute_(flop)": 1.6632e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 9.187520720836464,
                "log_compute": 23.220944476323414,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Qwen2.5-14B",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Quantitative reasoning",
                "organization": "Alibaba",
                "authors": "Qwen Team",
                "publication_date": "2024-09-19",
                "reference": "Qwen2.5: A Party of Foundation Models!",
                "link": "https://qwenlm.github.io/blog/qwen2.5/",
                "citations": null,
                "notability_criteria": null,
                "parameters": 14700000000.0,
                "training_compute_(flop)": 1.58760000000001e+24,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 10.167317334748176,
                "log_compute": 24.200741090235127,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Qwen2.5-Math-7B-Base",
                "domain": "Language",
                "task": "Language modeling/generation,Quantitative reasoning,Question answering",
                "organization": "Alibaba",
                "authors": "Unknown",
                "publication_date": "2024-09-19",
                "reference": "Qwen2.5-Math: The world's leading open-sourced mathematical LLMs",
                "link": "https://qwenlm.github.io/blog/qwen2.5-math/",
                "citations": null,
                "notability_criteria": null,
                "parameters": 7000000000.0,
                "training_compute_(flop)": 8.6388e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": "Qwen2.5-7B",
                "finetune_compute_(flop)": 4.2e+22,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 9.845098040014257,
                "log_compute": 23.936453419611667,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Qwen2.5-Math-1.5B",
                "domain": "Language",
                "task": "Language modeling/generation,Quantitative reasoning,Mathematical reasoning",
                "organization": "Alibaba",
                "authors": "Unknown",
                "publication_date": "2024-09-19",
                "reference": "Qwen2.5-Math: The world's leading open-sourced mathematical LLMs",
                "link": "https://qwenlm.github.io/blog/qwen2.5-math/",
                "citations": null,
                "notability_criteria": null,
                "parameters": 1500000000.0,
                "training_compute_(flop)": 1.7532e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": "Qwen2.5-1.5B",
                "finetune_compute_(flop)": 9e+21,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 9.176091259055681,
                "log_compute": 23.243831461981923,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Qwen2-VL-72B",
                "domain": "Language,Vision,Multimodal",
                "task": "Visual question answering,Video description,Language modeling/generation,Translation,Question answering,Character recognition (OCR),Quantitative reasoning",
                "organization": "Alibaba",
                "authors": "Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, Junyang Lin",
                "publication_date": "2024-09-18",
                "reference": "Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution",
                "link": "https://arxiv.org/abs/2409.12191",
                "citations": null,
                "notability_criteria": null,
                "parameters": 72000000000.0,
                "training_compute_(flop)": 6.048e+23,
                "training_dataset_size_(gradients)": 1400000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Multimodal",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 10.857332496431269,
                "log_compute": 23.78161178249315,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Qwen2-VL-2B",
                "domain": "Language,Vision,Multimodal",
                "task": "Visual question answering,Video description,Language modeling/generation,Translation,Question answering,Character recognition (OCR),Quantitative reasoning",
                "organization": "Alibaba",
                "authors": "Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, Junyang Lin",
                "publication_date": "2024-09-18",
                "reference": "Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution",
                "link": "https://arxiv.org/abs/2409.12191",
                "citations": null,
                "notability_criteria": null,
                "parameters": 2000000000.0,
                "training_compute_(flop)": 1.68e+22,
                "training_dataset_size_(gradients)": 1400000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Multimodal",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 9.301029995663981,
                "log_compute": 22.225309281725863,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Qwen2-VL-7B",
                "domain": "Language,Vision,Multimodal",
                "task": "Visual question answering,Video description,Language modeling/generation,Translation,Question answering,Character recognition (OCR),Quantitative reasoning",
                "organization": "Alibaba",
                "authors": "Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, Junyang Lin",
                "publication_date": "2024-09-18",
                "reference": "Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution",
                "link": "https://arxiv.org/abs/2409.12191",
                "citations": null,
                "notability_criteria": null,
                "parameters": 8000000000.0,
                "training_compute_(flop)": 6.72e+22,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Multimodal",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 9.903089986991944,
                "log_compute": 22.827369273053826,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Qwen2.5-Coder (7B)",
                "domain": "Language",
                "task": "Code generation,Code autocompletion,Quantitative reasoning,Question answering,Language modeling/generation",
                "organization": "Alibaba",
                "authors": "Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, An Yang, Rui Men, Fei Huang, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, Junyang Lin",
                "publication_date": "2024-09-18",
                "reference": "Qwen2.5-Coder Technical Report",
                "link": "https://arxiv.org/abs/2409.12186",
                "citations": null,
                "notability_criteria": null,
                "parameters": 7610000000.0,
                "training_compute_(flop)": 2.5113e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 9.881384656770573,
                "log_compute": 23.39989859664846,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Qwen2.5-Coder (1.5B)",
                "domain": "Language",
                "task": "Code generation,Code autocompletion,Quantitative reasoning,Question answering,Language modeling/generation",
                "organization": "Alibaba",
                "authors": "Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, An Yang, Rui Men, Fei Huang, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, Junyang Lin",
                "publication_date": "2024-09-18",
                "reference": "Qwen2.5-Coder Technical Report",
                "link": "https://arxiv.org/abs/2409.12186",
                "citations": null,
                "notability_criteria": null,
                "parameters": 1540000000.0,
                "training_compute_(flop)": 5.082e+22,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 9.187520720836464,
                "log_compute": 22.70603466071435,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Qwen2.5-32B",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Quantitative reasoning",
                "organization": "Alibaba",
                "authors": "Qwen Team",
                "publication_date": "2024-09-17",
                "reference": "Qwen2.5: A Party of Foundation Models!",
                "link": "https://qwenlm.github.io/blog/qwen2.5/ ",
                "citations": null,
                "notability_criteria": "Training cost",
                "parameters": 32500000000.0,
                "training_compute_(flop)": 3.51e+24,
                "training_dataset_size_(gradients)": 18000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 10.511883360978874,
                "log_compute": 24.545307116465825,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "IDPFold",
                "domain": "Biology",
                "task": "Protein folding prediction",
                "organization": "Shandong University,BioMap Research,Fuzhou University,Shanghai Jiao Tong University",
                "authors": "Junjie Zhu, Zhengxin Li, Zhuoqi Zheng, Bo Zhang, Bozitao Zhong, Jie Bai, Xiaokun Hong, Taifeng Wang, Ting Wei, Jianyi Yang, Hai-Feng Chen",
                "publication_date": "2024-09-13",
                "reference": "Precise Generation of Conformational Ensembles for Intrinsically Disordered Proteins via Fine-tuned Diffusion Models",
                "link": "https://www.biorxiv.org/content/10.1101/2024.05.05.592611.abstract",
                "citations": null,
                "notability_criteria": null,
                "parameters": 17800000.0,
                "training_compute_(flop)": 2.59999999999998e+20,
                "training_dataset_size_(gradients)": 30928500.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100",
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Unknown",
                "country": "China",
                "base_model": "ESM2-650M",
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Biology",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 7.250420002308894,
                "log_compute": 20.414973347970815,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Novae",
                "domain": "Biology",
                "task": "Spatial Transcriptomics",
                "organization": "CentraleSupelec,Gustave Roussy,Universit\u00e9 Paris Cit\u00e9",
                "authors": "Quentin Blampey, Hakim Benkirane, Nad\u00e8ge Bercovici, Fabrice Andr\u00e9, Paul-Henry Courn\u00e8de",
                "publication_date": "2024-09-13",
                "reference": "Novae: a graph-based foundation model for spatial transcriptomics data",
                "link": "https://www.biorxiv.org/content/10.1101/2024.09.09.612009v1.abstract",
                "citations": null,
                "notability_criteria": null,
                "parameters": 32000000.0,
                "training_compute_(flop)": 1.1e+19,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 24.0,
                "training_hardware": "NVIDIA A100 SXM4 40 GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "France",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 1.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 433.0209635948764,
                "training_compute_estimation_method": "Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "France",
                "domain_group": "Biology",
                "export_controls_sum": 10.0,
                "publication_count": 2.4529600742986126,
                "log_params": 7.505149978319906,
                "log_compute": 19.041392685158225,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "E2 TTS",
                "domain": "Speech",
                "task": "Text-to-speech (TTS),Speech synthesis",
                "organization": "Microsoft",
                "authors": "Sefik Emre Eskimez, Xiaofei Wang, Manthan Thakker, Canrun Li, Chung-Hsien Tsai, Zhen Xiao, Hemin Yang, Zirun Zhu, Min Tang, Xu Tan, Yanqing Liu, Sheng Zhao, Naoyuki Kanda",
                "publication_date": "2024-09-12",
                "reference": "E2 TTS: Embarrassingly Easy Fully Non-Autoregressive Zero-Shot TTS",
                "link": "https://arxiv.org/abs/2406.18009",
                "citations": null,
                "notability_criteria": null,
                "parameters": 335000000.0,
                "training_compute_(flop)": 4.939776e+20,
                "training_dataset_size_(gradients)": 245760000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Audio",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 8.525044807036846,
                "log_compute": 20.69370725577191,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "AbGPT",
                "domain": "Biology",
                "task": "Protein design",
                "organization": "Carnegie Mellon University (CMU)",
                "authors": "Desmond Kuan, Amir Barati Farimani",
                "publication_date": "2024-09-09",
                "reference": "AbGPT: De Novo Antibody Design via Generative Language Modeling",
                "link": "https://arxiv.org/abs/2409.06090",
                "citations": null,
                "notability_criteria": null,
                "parameters": 734000000.0,
                "training_compute_(flop)": 4.2506168e+21,
                "training_dataset_size_(gradients)": 6840000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA RTX A6000",
                "approach": null,
                "confidence": "Confident",
                "epochs": 5.0,
                "model_accessibility": "Unreleased",
                "country": "United States",
                "base_model": "ProtGPT2",
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": null,
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Biology",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 8.86569605991607,
                "log_compute": 21.62845195437977,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "ALOHA Unleashed",
                "domain": "Robotics",
                "task": "Robotic manipulation",
                "organization": "Google DeepMind",
                "authors": "Tony Z. Zhao, Jonathan Tompson, Danny Driess, Pete Florence, Kamyar Ghasemipour, Chelsea Finn, Ayzaan Wahid\n",
                "publication_date": "2024-09-08",
                "reference": "ALOHA Unleashed: A Simple Recipe for Robot Dexterity",
                "link": "https://aloha-unleashed.github.io/assets/aloha_unleashed.pdf\nhttps://arxiv.org/abs/2410.13126",
                "citations": null,
                "notability_criteria": null,
                "parameters": 217000000.0,
                "training_compute_(flop)": 3.6084096e+21,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 265.0,
                "training_hardware": "Google TPU v5e",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 64.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 17655.364403717842,
                "training_compute_estimation_method": "Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Robotics",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 8.33645973384853,
                "log_compute": 21.557315829569237,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "DeepSeek-V2.5",
                "domain": "Language",
                "task": "Language modeling/generation,Chat,Code generation",
                "organization": "DeepSeek",
                "authors": "DeepSeek-AI, Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Hanwei Xu, Hao Yang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J.L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jin Chen, Jingyang Yuan, Junjie Qiu, Junxiao Song, Kai Dong, Kaige Gao, Kang Guan, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qihao Zhu, Qinyu Chen, Qiushi Du, R.J. Chen, R.L. Jin, Ruiqi Ge, Ruizhe Pan, Runxin Xu, Ruyi Chen, S.S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Size Zheng, T. Wang, Tian Pei, Tian Yuan, Tianyu Sun, W.L. Xiao, Wangding Zeng, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wentao Zhang, X.Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun",
                "publication_date": "2024-09-06",
                "reference": "DeepSeek-V2.5",
                "link": "https://huggingface.co/deepseek-ai/DeepSeek-V2.5",
                "citations": null,
                "notability_criteria": "Training cost",
                "parameters": 236000000000.0,
                "training_compute_(flop)": 1.7892e+24,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": "Self-supervised learning",
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-29 19:33:41+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 36864000.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 11.372912002970107,
                "log_compute": 24.25265888950062,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "OLMoE",
                "domain": "Language",
                "task": "Language modeling/generation,Chat",
                "organization": "Allen Institute for AI,Contextual AI,University of Washington,Princeton University",
                "authors": "Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi, Pete Walsh, Oyvind Tafjord, Nathan Lambert, Yuling Gu, Shane Arora, Akshita Bhagia, Dustin Schwenk, David Wadden, Alexander Wettig, Binyuan Hui, Tim Dettmers, Douwe Kiela, Ali Farhadi, Noah A. Smith, Pang Wei Koh, Amanpreet Singh, Hannaneh Hajishirzi",
                "publication_date": "2024-09-03",
                "reference": "OLMoE: Open Mixture-of-Experts Language Models",
                "link": "https://arxiv.org/abs/2409.02060v1",
                "citations": null,
                "notability_criteria": null,
                "parameters": 7000000000.0,
                "training_compute_(flop)": 5.1741608015e+22,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 287.0,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.2642857143,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 256.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 4194304.0,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 353146.6076498626,
                "training_compute_estimation_method": "Operation counting,Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.845098040014257,
                "log_compute": 22.713839921504405,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "GLA Transformer 1.3B",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering",
                "organization": "MIT-IBM Watson AI Lab,Massachusetts Institute of Technology (MIT)",
                "authors": "Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim",
                "publication_date": "2024-08-27",
                "reference": "Gated Linear Attention Transformers with Hardware-Efficient Training",
                "link": "https://arxiv.org/abs/2312.06635",
                "citations": null,
                "notability_criteria": null,
                "parameters": 1300000000.0,
                "training_compute_(flop)": 7.8e+20,
                "training_dataset_size_(gradients)": 100000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 2000000.0,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.113943352306837,
                "log_compute": 20.89209460269048,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "GLA Transformer 340M",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering",
                "organization": "MIT-IBM Watson AI Lab,Massachusetts Institute of Technology (MIT)",
                "authors": "Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim",
                "publication_date": "2024-08-27",
                "reference": "Gated Linear Attention Transformers with Hardware-Efficient Training",
                "link": "https://arxiv.org/abs/2312.06635",
                "citations": null,
                "notability_criteria": null,
                "parameters": 340000000.0,
                "training_compute_(flop)": 3.06e+19,
                "training_dataset_size_(gradients)": 15000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 500000.0,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 8.531478917042255,
                "log_compute": 19.48572142648158,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "Pharia-1-LLM-7B",
                "domain": "Language",
                "task": "Language modeling/generation,Translation,Question answering",
                "organization": "Aleph Alpha",
                "authors": "Unknown",
                "publication_date": "2024-08-26",
                "reference": "Introducing Pharia-1-LLM: transparent and compliant",
                "link": "https://huggingface.co/Aleph-Alpha/Pharia-1-LLM-7B-control",
                "citations": null,
                "notability_criteria": null,
                "parameters": 7041544704.0,
                "training_compute_(flop)": 4.43e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100 SXM4 80 GB,NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "Germany",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 256.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open (non-commercial)",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Reported",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "Germany",
                "domain_group": "Language",
                "export_controls_sum": 10.0,
                "publication_count": 2.7830121675064055,
                "log_params": 9.847667940794825,
                "log_compute": 23.64640372622307,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "DISTRO",
                "domain": "Language",
                "task": "Language modeling/generation,Chat,Question answering",
                "organization": "Nous Research",
                "authors": "Bowen Peng, Jeffrey Quesnelle, Dillon Rolnick, Ari Lotter, Umer H. Adil, Esteban La Rocca",
                "publication_date": "2024-08-26",
                "reference": "A PRELIMINARY REPORT ON DISTRO",
                "link": "https://github.com/NousResearch/DisTrO/blob/main/A_Preliminary_Report_on_DisTrO.pdf",
                "citations": null,
                "notability_criteria": null,
                "parameters": 1200000000.0,
                "training_compute_(flop)": 7.1497946e+20,
                "training_dataset_size_(gradients)": 100000000000.0,
                "training_time_(hours)": 19.8,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unknown",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 32.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 44151.1910097316,
                "training_compute_estimation_method": "Operation counting,Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.079181246047625,
                "log_compute": 20.85429356552585,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "Kosmos-2.5",
                "domain": "Multimodal,Language,Vision",
                "task": "Character recognition (OCR),Document classification,Language modeling/generation,Visual question answering,Document representation",
                "organization": "Microsoft",
                "authors": "Tengchao Lv, Yupan Huang, Jingye Chen, Yuzhong Zhao, Yilin Jia, Lei Cui, Shuming Ma, Yaoyao Chang, Shaohan Huang, Wenhui Wang, Li Dong, Weiyao Luo, Shaoxiang Wu, Guoxin Wang, Cha Zhang, Furu Wei",
                "publication_date": "2024-08-21",
                "reference": "KOSMOS-2.5: A Multimodal Literate Model",
                "link": "https://arxiv.org/abs/2309.11419",
                "citations": null,
                "notability_criteria": null,
                "parameters": 1300000000.0,
                "training_compute_(flop)": 2.2018015e+21,
                "training_dataset_size_(gradients)": 260000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": "Pix2Struct-Large",
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.113943352306837,
                "log_compute": 21.34277816325495,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "AntiFormer",
                "domain": "Biology",
                "task": "Protein protein binding affinity prediction",
                "organization": "University of Florida,Sichuan University,Shihezi University,University of Macau,University of Texas Health Science Center",
                "authors": "Qing Wang, Yuzhou Feng, Yanfei Wang, Bo Li, Jianguo Wen, Xiaobo Zhou, Qianqian Song",
                "publication_date": "2024-08-20",
                "reference": "AntiFormer: graph enhanced large language model for binding affinity prediction ",
                "link": "https://academic.oup.com/bib/article/25/5/bbae403/7736247",
                "citations": 0.0,
                "notability_criteria": null,
                "parameters": 24670596.0,
                "training_compute_(flop)": 1.7100000000000148e+18,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100 SXM4 40 GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 1.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 64.0,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 433.2524599633082,
                "training_compute_estimation_method": "Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Biology",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 7.392179641438849,
                "log_compute": 18.232996110392158,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "Hybrid-Phi-Mamba-1.5B",
                "domain": "Language",
                "task": "Language modeling/generation",
                "organization": "Carnegie Mellon University (CMU),Mohamed bin Zayed University of Artificial Intelligence (MBZUAI),Cartesia",
                "authors": "Aviv Bick, Kevin Y. Li, Eric P. Xing, J. Zico Kolter, Albert Gu",
                "publication_date": "2024-08-19",
                "reference": "Transformers to SSMs: Distilling Quadratic Knowledge to Subquadratic Models",
                "link": "https://arxiv.org/abs/2408.10189",
                "citations": null,
                "notability_criteria": null,
                "parameters": 1500000000.0,
                "training_compute_(flop)": 1.215e+21,
                "training_dataset_size_(gradients)": 5000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": "Phi-1.5",
                "finetune_compute_(flop)": 4.5e+19,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.176091259055681,
                "log_compute": 21.08457627793433,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "xGen-MM (BLIP-3)",
                "domain": "Multimodal,Vision,Language",
                "task": "Image captioning,Character recognition (OCR),Visual question answering,Chat",
                "organization": "Salesforce Research,University of Washington",
                "authors": "Le Xue, Manli Shu, Anas Awadalla, Jun Wang, An Yan, Senthil Purushwalkam, Honglu Zhou, Viraj Prabhu, Yutong Dai, Michael S Ryoo, Shrikant Kendre, Jieyu Zhang, Can Qin, Shu Zhang, Chia-Chih Chen, Ning Yu, Juntao Tan, Tulika Manoj Awalgaonkar, Shelby Heinecke, Huan Wang, Yejin Choi, Ludwig Schmidt, Zeyuan Chen, Silvio Savarese, Juan Carlos Niebles, Caiming Xiong, Ran Xu",
                "publication_date": "2024-08-16",
                "reference": "xGen-MM (BLIP-3): A Family of Open Large Multimodal\nModels",
                "link": "https://arxiv.org/pdf/2408.08872v1",
                "citations": 123.0,
                "notability_criteria": null,
                "parameters": 4000000000.0,
                "training_compute_(flop)": 2.4e+21,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unknown",
                "country": "United States",
                "base_model": "phi-3-mini 3.8B",
                "finetune_compute_(flop)": 0.0,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:10:08+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": null,
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.602059991327963,
                "log_compute": 21.380211241711606,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "LLaVA-OV-72B",
                "domain": "Multimodal,Vision,Language,Video",
                "task": "Image captioning,Visual question answering,Video description,Object recognition,Action recognition,Language modeling/generation",
                "organization": "ByteDance,Nanyang Technological University,Chinese University of Hong Kong (CUHK),Hong Kong University of Science and Technology (HKUST)",
                "authors": "Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, Chunyuan Li",
                "publication_date": "2024-08-06",
                "reference": "LLaVA-OneVision: Easy Visual Task Transfer\n",
                "link": "https://arxiv.org/abs/2408.03326",
                "citations": 1331.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 72000000000.0,
                "training_compute_(flop)": 3.036551985824e+24,
                "training_dataset_size_(gradients)": 38314782000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": "Qwen2-72B",
                "finetune_compute_(flop)": 1.6551985824e+22,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:10:07+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 256.0,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "China",
                "domain_group": "Multimodal",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 10.857332496431269,
                "log_compute": 24.48238072065148,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "AFM-on-device",
                "domain": "Language",
                "task": "Language modeling/generation",
                "organization": "Apple",
                "authors": "Andy Narayanan, Aonan Zhang, Bowen Zhang, Chen Chen, Chong Wang, Chung-Cheng Chiu, David Qiu, Deepak Gopinath, Dian Ang Yap, Dong Yin, Feng Nan, Floris Weers, Guoli Yin, Haoshuo Huang, Jianyu Wang, Jiarui Lu, John Peebles, Ke Ye, Mark Lee, Nan Du, Qibin Chen, Quentin Keunebroek, Ruoming Pang, Sam Wiseman, Syd Evans, Tao Lei, Tom Gunter, Vivek Rathod, Xiang Kong, Xianzhi Du, Yanghao Li, Yongqiang Wang, Yuan Gao, Zaid Ahmed, Zhaoyang Xu, Zhiyun Lu, Zirui Wang, Al Rashid, Albin Madappally Jose, Alec Doane, Alfredo Bencomo, Allison Vanderby, Andrew Hansen, Ankur Jain, Anupama Mann Anupama, Areeba\nKamal, Bugu Wu, Carolina Brum, Charlie Maalouf, Chinguun Erdenebileg,\nChris Dulhanty, Dominik Moritz, Doug Kang, Eduardo Jimenez, Evan Ladd,\nFangping Shi, Felix Bai, Frank Chu, Fred Hohman, Hadas Kotek, Hannah\nGillis Coleman, Jane Li, Jeffrey Bigham, Jeffery Cao, Jeff Lai, Jessica Cheung, Jiulong Shan, Joe Zhou, John Li, Jun Qin, Karanjeet Singh, Karla Vega, Ke Ye, Kelvin Zou, Laura Heckman, Lauren Gardiner, Margit Bowler, Mark Lee, Maria Cordell, Meng Cao, Nicole Hay, Nilesh Shahdadpuri, Otto Godwin, Pranay Dighe, Pushyami Rachapudi, Ramsey Tantawi, Roman Frigg, Sam Davarnia, Sanskruti Shah, Saptarshi Guha, Sasha Sirovica, Shen Ma, Shuang Ma, Simon Wang, Sulgi Kim, Suma Jayaram, Vaishaal Shankar, Varsha Paidi, Vivek Kumar, Xiang Kong, Xin Wang, Xin Zheng, Walker Cheng, Yael Shrager, Yang Ye, Yasu Tanaka, Yihao Guo, Yunsong Meng, Zhao Tang Luo, Zhi Ouyang, Zhiyun Lu, Alp Aygar, Alvin Wan, Andrew Walkingshaw, Andy Narayanan, Antonie Lin, Arsalan Farooq, Brent Ramerth, Chong Wang, Colorado Reed, Chris Bartels, Chris Chaney, David Riazati, Eric Liang Yang, Erin Feldman, Gabriel Hochstrasser, Guillaume Seguin, Guoli Yin, Irina Belousova, Jianyu Wang, Joris Pelemans, Karen Yang, Keivan Alizadeh Vahid, Liangliang Cao, Mahyar Najibi , Marco Zuliani, Max Horton, Minsik Cho, Nikhil Bhendawade, Patrick Dong, Piotr Maj, Pulkit Agrawal, Qi Shan, Qibin Chen, Qichen Fu, Regan Poston, Sam Xu, Shuangning Liu, Sushma Rao, Tashweena Heeramun, Thomas Merth, Uday Rayala, Victor Cui, Vivek Rangarajan Sridhar, Vivek Rathod, Wencong Zhang, Wenqi Zhang, Wentao Wu, Xiang Kong, Xingyu Zhou, Xinwen Liu, Yang Zhao, Yin Xia, Zhile Ren, Zhongzheng Ren",
                "publication_date": "2024-07-29",
                "reference": "Apple Intelligence Foundation Language Models",
                "link": "https://machinelearning.apple.com/research/apple-intelligence-foundation-language-models",
                "citations": null,
                "notability_criteria": "Significant use",
                "parameters": 2730000000.0,
                "training_compute_(flop)": 4.5126e+23,
                "training_dataset_size_(gradients)": 7588000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "Google TPU v5p",
                "approach": "Self-supervised learning",
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Hosted access (no API)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 2048.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": "Google Cloud",
                "batch_size": 18949752.5758905,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.436162647040756,
                "log_compute": 23.65442683906152,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "Segment Anything Model 2",
                "domain": "Vision,Video",
                "task": "Image segmentation",
                "organization": "Meta AI",
                "authors": "Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali,  Tengyu Ma, Haitham Khedr, Roman R\u00e4dle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Doll\u00e1r, Christoph Feichtenhofer",
                "publication_date": "2024-07-29",
                "reference": "SAM 2: Segment Anything in Images and Videos",
                "link": "https://ai.meta.com/research/publications/sam-2-segment-anything-in-images-and-videos/",
                "citations": null,
                "notability_criteria": null,
                "parameters": 224400000.0,
                "training_compute_(flop)": 1.24e+22,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": null,
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Vision",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 8.351022852584125,
                "log_compute": 22.093421685162234,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "Florence-2-B (base)",
                "domain": "Vision",
                "task": "Image captioning,Visual question answering,Image classification,Object detection",
                "organization": "Microsoft",
                "authors": "Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael Zeng, Ce Liu, Lu Yuan",
                "publication_date": "2024-07-29",
                "reference": "Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks",
                "link": "https://arxiv.org/abs/2311.06242",
                "citations": null,
                "notability_criteria": null,
                "parameters": 232000000.0,
                "training_compute_(flop)": 5.4310727e+21,
                "training_dataset_size_(gradients)": 22879290000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Vision",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 8.3654879848909,
                "log_compute": 21.73488561627035,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "Florence-2-L (large)",
                "domain": "Vision",
                "task": "Image captioning,Visual question answering,Image classification,Object detection",
                "organization": "Microsoft",
                "authors": "Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael Zeng, Ce Liu, Lu Yuan",
                "publication_date": "2024-07-29",
                "reference": "Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks",
                "link": "https://arxiv.org/abs/2311.06242",
                "citations": null,
                "notability_criteria": null,
                "parameters": 771000000.0,
                "training_compute_(flop)": 1.2406517e+22,
                "training_dataset_size_(gradients)": 22879290000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Vision",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 8.887054378050957,
                "log_compute": 22.093649874972492,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "Mistral Large 2",
                "domain": "Language",
                "task": "Language modeling/generation,Translation,Code generation",
                "organization": "Mistral AI",
                "authors": "Albert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Alok Kothari, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Augustin Garreau, Austin Birky, Bam4d, Baptiste Bout, Baudouin de Monicault, Blanche Savary, Carole Rambaud, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Diogo Costa, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gaspard Blanchet, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Henri Roussez, Hichem Sattouf, Ian Mack, Jean-Malo Delignon, Jessica Chudnovsky, Justus Murke, Kartik Khandelwal, Lawrence Stewart, Louis Martin, Louis Ternon, Lucile Saulnier, L\u00e9lio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Marjorie Janiewicz, Micka\u00ebl Seznec, Nicolas Schuhl, Niklas Muhs, Olivier de Garrigues, Patrick von Platen, Paul Jacob, Pauline Buche, Pavan Kumar Reddy, Perry Savas, Pierre Stock, Romain Sauvestre, Sagar Vaze, Sandeep Subramanian, Saurabh Garg, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibault Schueller, Thibaut Lavril, Thomas Wang, Th\u00e9ophile Gervet, Timoth\u00e9e Lacroix, Valera Nemychnikova, Wendy Shang, William El Sayed, William Marshall",
                "publication_date": "2024-07-24",
                "reference": "Top-tier reasoning for high-complexity tasks, for your most sophisticated needs.",
                "link": "https://mistral.ai/news/mistral-large-2407/",
                "citations": null,
                "notability_criteria": "Training cost",
                "parameters": 123000000000.0,
                "training_compute_(flop)": 2.13e+25,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "France",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware,Cost,Benchmarks",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "France",
                "domain_group": "Language",
                "export_controls_sum": 10.0,
                "publication_count": 2.4529600742986126,
                "log_params": 11.089905111439398,
                "log_compute": 25.328379603438737,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "Llama 3.1-405B",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Code generation,Mathematical reasoning",
                "organization": "Meta AI",
                "authors": "Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAlan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie\nSravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen\nGregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux,\nChaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang\nWu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle\nPintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino,\nDieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip\nRadenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire\nMialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov,\nImanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert,\nJana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong,\nJenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe\nSpisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala,\nKartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer,\nKshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence\nChen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat,\nLuke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas,\nMathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar\nSingh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji,\nOlivier Duchenne, Onur \u00c7elebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng,\nPrajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong,\nRagavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta\nRaileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor,\nRuan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun\nSonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan,\nShruti Bhosale, Shun Zhang, Simon Vandenhende, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin\nGururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas\nScialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta,\nVignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei\nChu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan,\nXinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen\nSong, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, and Zoe\nPapakipos.\n(core contributors)",
                "publication_date": "2024-07-23",
                "reference": "The Llama 3 Herd of Models",
                "link": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/",
                "citations": null,
                "notability_criteria": "SOTA improvement,Training cost",
                "parameters": 405000000000.0,
                "training_compute_(flop)": 3.8e+25,
                "training_dataset_size_(gradients)": 15600000000000.0,
                "training_time_(hours)": 2142.0,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Open weights (restricted use)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 16384.0,
                "last_modified": "2025-10-16 15:12:24+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 16000000.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open (restricted use)",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": 52885433.95402246,
                "frontier_model": true,
                "training_power_draw_(w)": 22622532.159299143,
                "training_compute_estimation_method": "Reported,Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 11.60745502321467,
                "log_compute": 25.57978359661681,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "Llama 3.1-70B",
                "domain": "Language",
                "task": "Language modeling/generation",
                "organization": "Meta AI",
                "authors": "Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAlan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie\nSravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen\nGregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux,\nChaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang\nWu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle\nPintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino,\nDieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip\nRadenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire\nMialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov,\nImanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert,\nJana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong,\nJenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe\nSpisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala,\nKartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer,\nKshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence\nChen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat,\nLuke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas,\nMathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar\nSingh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji,\nOlivier Duchenne, Onur \u00c7elebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng,\nPrajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong,\nRagavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta\nRaileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor,\nRuan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun\nSonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan,\nShruti Bhosale, Shun Zhang, Simon Vandenhende, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin\nGururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas\nScialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta,\nVignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei\nChu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan,\nXinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen\nSong, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, and Zoe\nPapakipos.\n(core contributors)",
                "publication_date": "2024-07-23",
                "reference": "The Llama 3 Herd of Models",
                "link": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/",
                "citations": null,
                "notability_criteria": "Training cost",
                "parameters": 70000000000.0,
                "training_compute_(flop)": 7.929e+24,
                "training_dataset_size_(gradients)": 15000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open (restricted use)",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware,Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 10.845098040014257,
                "log_compute": 24.899218417851372,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "Llama 3.1-8B",
                "domain": "Language",
                "task": "Language modeling/generation",
                "organization": "Meta AI",
                "authors": "Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux,\nChaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat,\nLuke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar\nSingh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Olivier Duchenne, Onur \u00c7elebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng,\nPrajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta\nRaileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun\nSonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin\nGururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta,\nVignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, and Zoe Papakipos.\n(core contributors)",
                "publication_date": "2024-07-23",
                "reference": "The Llama 3 Herd of Models",
                "link": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/",
                "citations": null,
                "notability_criteria": null,
                "parameters": 8000000000.0,
                "training_compute_(flop)": 1.224e+24,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open (restricted use)",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware,Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.903089986991944,
                "log_compute": 24.08778141780954,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "DCLM 7B",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Quantitative reasoning",
                "organization": "Apple",
                "authors": "Unknown",
                "publication_date": "2024-07-20",
                "reference": "Model Card for DCLM-Baseline-7B",
                "link": "https://huggingface.co/apple/DCLM-7B",
                "citations": null,
                "notability_criteria": null,
                "parameters": 7000000000.0,
                "training_compute_(flop)": 1.05e+23,
                "training_dataset_size_(gradients)": 2500000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.845098040014257,
                "log_compute": 23.02118929906994,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "OmniGenome",
                "domain": "Biology",
                "task": "Protein or nucleotide language model (pLM/nLM)",
                "organization": "University of Exeter",
                "authors": "Heng Yang, Ke Li",
                "publication_date": "2024-07-15",
                "reference": "OmniGenome: Aligning RNA Sequences with Secondary Structures in Genomic Foundation Models",
                "link": "https://arxiv.org/abs/2407.11242",
                "citations": 1.0,
                "notability_criteria": null,
                "parameters": 186000000.0,
                "training_compute_(flop)": 3.3900690258459335e+20,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA GeForce RTX 4090",
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Unknown",
                "country": "United Kingdom",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 8.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 7102.366961281501,
                "training_compute_estimation_method": "Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United Kingdom",
                "domain_group": "Biology",
                "export_controls_sum": 1.0,
                "publication_count": 2.8041950763331074,
                "log_params": 8.269512944217917,
                "log_compute": 20.530208541046388,
                "pub_bin": "Low",
                "export_bin": "Low"
            },
            {
                "model": "Deep learning linking mechanistic models to single-cell transcriptomics data reveals transcriptional bursting in response to DNA damage",
                "domain": "Biology",
                "task": "Transcriptomic prediction",
                "organization": "Sun Yat-sen University,University of California Irvine,Guangdong Provincial People's Hospital,Guangdong Academy of Medical Sciences",
                "authors": "Zhiwei Huang, Songhao Luo, Zihao Wang, Zhenquan Zhang, Benyuan Jiang, Qing Nie, Jiajun Zhang",
                "publication_date": "2024-07-12",
                "reference": "Deep learning linking mechanistic models to single-cell transcriptomics data reveals transcriptional bursting in response to DNA damage",
                "link": "https://www.biorxiv.org/content/10.1101/2024.07.10.602845v1",
                "citations": null,
                "notability_criteria": null,
                "parameters": 2176.0,
                "training_compute_(flop)": 39168000000.0,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": 200.0,
                "model_accessibility": "Unknown",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": null,
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Biology",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 3.3376588910261424,
                "log_compute": 10.592931396129448,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "PaliGemma",
                "domain": "Vision",
                "task": "Visual question answering",
                "organization": "Google DeepMind",
                "authors": "Lucas Beyer, Andreas Steiner, Andr\u00e9 Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, Thomas Unterthiner, Daniel Keysers, Skanda Koppula, Fangyu Liu, Adam Grycner, Alexey Gritsenko, Neil Houlsby, Manoj Kumar, Keran Rong, Julian Eisenschlos, Rishabh Kabra, Matthias Bauer, Matko Bo\u0161njak, Xi Chen, Matthias Minderer, Paul Voigtlaender, Ioana Bica, Ivana Balazevic, Joan Puigcerver, Pinelopi Papalampidi, Olivier Henaff, Xi Xiong, Radu Soricut, Jeremiah Harmsen, Xiaohua Zhai",
                "publication_date": "2024-07-10",
                "reference": "PaliGemma: A versatile 3B VLM for transfer",
                "link": "https://arxiv.org/abs/2407.07726v1",
                "citations": null,
                "notability_criteria": null,
                "parameters": 3000000000.0,
                "training_compute_(flop)": 1.0652844e+22,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 162.0,
                "training_hardware": "Google TPU v5e,Google TPU v3",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 256.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Vision",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.477121254719663,
                "log_compute": 22.02746556726954,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "OpenDiLoCo 150M",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering",
                "organization": "Prime Intellect",
                "authors": "Sami Jaghouar, Jack Min Ong, Johannes Hagemann",
                "publication_date": "2024-07-10",
                "reference": "OpenDiLoCo: An Open-Source Framework for Globally Distributed\nLow-Communication Training",
                "link": "https://arxiv.org/abs/2407.07852",
                "citations": null,
                "notability_criteria": null,
                "parameters": 150000000.0,
                "training_compute_(flop)": 4.152361e+19,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 8.176091259055681,
                "log_compute": 19.61829510338873,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "OpenDiLoCo 1.1B",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering",
                "organization": "Prime Intellect",
                "authors": "Sami Jaghouar, Jack Min Ong, Johannes Hagemann",
                "publication_date": "2024-07-10",
                "reference": "OpenDiLoCo: An Open-Source Framework for Globally Distributed\nLow-Communication Training",
                "link": "https://arxiv.org/abs/2407.07852",
                "citations": null,
                "notability_criteria": null,
                "parameters": 1100000000.0,
                "training_compute_(flop)": 6.0901294e+20,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.041392685158225,
                "log_compute": 20.784626520401172,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "ESM3 (98B)",
                "domain": "Biology",
                "task": "Protein generation",
                "organization": "EvolutionaryScale,University of California (UC) Berkeley",
                "authors": "Thomas Hayes, Roshan Rao, Halil Akin, Nicholas James Sofroniew, Deniz Oktay, Zeming Lin, Robert Verkuil, Vincent Quy Tran, Jonathan Deaton, Marius Wiggert, Rohil Badkundri, Irhum Shafkat, Jun Gong, Alexander Derry, Raul Santiago Molina, Neil Thomas, Yousuf Khan, Chetan Mishra, Carolyn Kim, Liam J Bartie, Patrick D Hsu, Tom Sercu, Salvatore Candido, Alexander Rives",
                "publication_date": "2024-06-25",
                "reference": "ESM3: Simulating 500 million years of evolution with a language model",
                "link": "https://www.evolutionaryscale.ai/blog/esm3-release ",
                "citations": null,
                "notability_criteria": "Historical significance",
                "parameters": 98500000000.0,
                "training_compute_(flop)": 1.07e+24,
                "training_dataset_size_(gradients)": 771000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": 2.3,
                "model_accessibility": "Unreleased",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 4194304.0,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Reported",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Biology",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 10.993436230497611,
                "log_compute": 24.029383777685208,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "ESM3-open-small",
                "domain": "Biology",
                "task": "Protein generation",
                "organization": "EvolutionaryScale,University of California (UC) Berkeley",
                "authors": "Thomas Hayes, Roshan Rao, Halil Akin, Nicholas James Sofroniew, Deniz Oktay, Zeming Lin, Robert Verkuil, Vincent Quy Tran, Jonathan Deaton, Marius Wiggert, Rohil Badkundri, Irhum Shafkat, Jun Gong, Alexander Derry, Raul Santiago Molina, Neil Thomas, Yousuf Khan, Chetan Mishra, Carolyn Kim, Liam J Bartie, Patrick D Hsu, Tom Sercu, Salvatore Candido, Alexander Rives",
                "publication_date": "2024-06-25",
                "reference": "ESM3: Simulating 500 million years of evolution with a language model",
                "link": "https://www.evolutionaryscale.ai/blog/esm3-release ",
                "citations": null,
                "notability_criteria": null,
                "parameters": 1400000000.0,
                "training_compute_(flop)": 2.7e+21,
                "training_dataset_size_(gradients)": 48000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Reported",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Biology",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.146128035678238,
                "log_compute": 21.431363764158988,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "Gemma 2 9B",
                "domain": "Language",
                "task": "Language modeling/generation,Chat,Code generation,Question answering,Quantitative reasoning",
                "organization": "Google DeepMind",
                "authors": "Gemma Team, Google DeepMind",
                "publication_date": "2024-06-24",
                "reference": "Gemma 2 offers best-in-class performance, runs at incredible speed across different hardware and easily integrates with other AI tools.",
                "link": "https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf",
                "citations": null,
                "notability_criteria": null,
                "parameters": 9000000000.0,
                "training_compute_(flop)": 4.32e+23,
                "training_dataset_size_(gradients)": 8000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "Google TPU v4",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 4096.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 1374398.1970781134,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.954242509439325,
                "log_compute": 23.635483746814913,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "Gemma 2 27B",
                "domain": "Language",
                "task": "Language modeling/generation,Chat,Code generation,Question answering,Quantitative reasoning",
                "organization": "Google DeepMind",
                "authors": "Gemma Team, Google DeepMind",
                "publication_date": "2024-06-24",
                "reference": "Gemma 2 offers best-in-class performance, runs at incredible speed across different hardware and easily integrates with other AI tools.",
                "link": "https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf",
                "citations": null,
                "notability_criteria": null,
                "parameters": 27000000000.0,
                "training_compute_(flop)": 2.106e+24,
                "training_dataset_size_(gradients)": 13000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "Google TPU v5p",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 6144.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 10.431363764158988,
                "log_compute": 24.32345836684947,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "Gemma 2 2B",
                "domain": "Language",
                "task": "Language modeling/generation,Chat,Code generation,Question answering",
                "organization": "Google DeepMind",
                "authors": "Gemma Team, Google DeepMind",
                "publication_date": "2024-06-24",
                "reference": "Gemma 2 offers best-in-class performance, runs at incredible speed across different hardware and easily integrates with other AI tools.",
                "link": "https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf",
                "citations": null,
                "notability_criteria": null,
                "parameters": 2600000000.0,
                "training_compute_(flop)": 3.12e+22,
                "training_dataset_size_(gradients)": 2000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "Google TPU v5e",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 512.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 141482.16734627637,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.414973347970818,
                "log_compute": 22.494154594018443,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "DeepSeek-Coder-V2 236B",
                "domain": "Language",
                "task": "Code generation,Code autocompletion",
                "organization": "DeepSeek",
                "authors": "Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Y. Wu, Yukun Li, Huazuo Gao, Shirong Ma, Wangding Zeng, Xiao Bi, Zihui Gu, Hanwei Xu, Damai Dai, Kai Dong, Liyue Zhang, Yishi Piao, Zhibin Gou, Zhenda Xie, Zhewen Hao, Bingxuan Wang, Junxiao Song, Deli Chen, Xin Xie, Kang Guan, Yuxiang You, Aixin Liu, Qiushi Du, Wenjun Gao, Xuan Lu, Qinyu Chen, Yaohui Wang, Chengqi Deng, Jiashi Li, Chenggang Zhao, Chong Ruan, Fuli Luo, Wenfeng Liang",
                "publication_date": "2024-06-17",
                "reference": "DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence",
                "link": "https://github.com/deepseek-ai/DeepSeek-Coder-V2",
                "citations": null,
                "notability_criteria": "SOTA improvement",
                "parameters": 236000000000.0,
                "training_compute_(flop)": 1.2852e+24,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": "Self-supervised learning",
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "China",
                "base_model": "DeepSeek-V2 (MoE-236B)",
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 36864000.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 11.372912002970107,
                "log_compute": 24.10897071687948,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Ovis-7B",
                "domain": "Multimodal,Language,Vision",
                "task": "Visual question answering,Language modeling/generation,Question answering",
                "organization": "Alibaba,Nanjing University",
                "authors": "Shiyin Lu, Yang Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, Han-Jia Ye",
                "publication_date": "2024-06-17",
                "reference": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model\n",
                "link": "https://arxiv.org/abs/2405.20797",
                "citations": 97.0,
                "notability_criteria": null,
                "parameters": 7000000000.0,
                "training_compute_(flop)": 1.7e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 15.0,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": "Qwen1.5-7B",
                "finetune_compute_(flop)": null,
                "hardware_quantity": 128.0,
                "last_modified": "2025-10-16 13:09:29+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 176880.2801222388,
                "training_compute_estimation_method": null,
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Multimodal",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 9.845098040014257,
                "log_compute": 23.230448921378272,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "JIUTIAN-139MoE",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Code generation",
                "organization": "China Mobile",
                "authors": "Unknown",
                "publication_date": "2024-06-15",
                "reference": "JIUTIAN-139MOE: TECHNICAL REPORT",
                "link": "https://gitee.com/CMCC-jiutian/JIUTIAN-139MoE?skip_mobile=true",
                "citations": null,
                "notability_criteria": null,
                "parameters": 38800000000.0,
                "training_compute_(flop)": 4.3596131e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 1464.0,
                "training_hardware": "NVIDIA A800 PCIe,Ascend (\u6607\u817e) 910B",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 896.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting,Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 10.588831725594208,
                "log_compute": 23.639447948903726,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Nemotron-4 340B",
                "domain": "Language",
                "task": "Language modeling/generation,Chat,Question answering",
                "organization": "NVIDIA",
                "authors": "Bo Adler, Niket Agarwal, Ashwath Aithal, Dong H. Anh, Pallab Bhattacharya, Annika Brundyn, Jared Casper, Bryan Catanzaro, Sharon Clay, Jonathan Cohen, Sirshak Das, Ayush Dattagupta, Olivier Delalleau, Leon Derczynski, Yi Dong, Daniel Egert, Ellie Evans, Aleksander Ficek, Denys Fridman, Shaona Ghosh, Boris Ginsburg, Igor Gitman, Tomasz Grzegorzek, Robert Hero, Jining Huang, Vibhu Jawa, Joseph Jennings, Aastha Jhunjhunwala, John Kamalu, Sadaf Khan, Oleksii Kuchaiev, Patrick LeGresley, Hui Li, Jiwei Liu, Zihan Liu, Eileen Long, Ameya Sunil Mahabaleshwarkar, Somshubra Majumdar, James Maki, Miguel Martinez, Maer Rodrigues de Melo, Ivan Moshkov, Deepak Narayanan, Sean Narenthiran, Jesus Navarro, Phong Nguyen, Osvald Nitski, Vahid Noroozi, Guruprasad Nutheti, Christopher Parisien, Jupinder Parmar, Mostofa Patwary, Krzysztof Pawelec, Wei Ping, Shrimai Prabhumoye, Rajarshi Roy, Trisha Saar, Vasanth Rao Naik Sabavat, Sanjeev Satheesh, Jane Polak Scowcroft, Jason Sewall, Pavel Shamis, Gerald Shen, Mohammad Shoeybi, Dave Sizer, Misha Smelyanskiy, Felipe Soares, Makesh Narsimhan Sreedhar, Dan Su, Sandeep Subramanian, Shengyang Sun, Shubham Toshniwal, Hao Wang, Zhilin Wang, Jiaxuan You, Jiaqi Zeng, Jimmy Zhang, Jing Zhang, Vivienne Zhang, Yian Zhang, Chen Zhu",
                "publication_date": "2024-06-14",
                "reference": "NVIDIA Releases Open Synthetic Data Generation Pipeline for Training Large Language Models",
                "link": "https://blogs.nvidia.com/blog/nemotron-4-synthetic-data-generation-llm-training/ ",
                "citations": null,
                "notability_criteria": "Training cost",
                "parameters": 340000000000.0,
                "training_compute_(flop)": 1.7999999999999999e+25,
                "training_dataset_size_(gradients)": 9000000000000.0,
                "training_time_(hours)": 2200.0,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 6144.0,
                "last_modified": "2025-10-16 15:12:24+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": 21271017.96222655,
                "frontier_model": true,
                "training_power_draw_(w)": 8490820.682633882,
                "training_compute_estimation_method": "Operation counting,Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 11.531478917042255,
                "log_compute": 25.255272505103306,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "OpenVLA",
                "domain": "Robotics,Vision,Language",
                "task": "Robotic manipulation",
                "organization": "Stanford University,University of California (UC) Berkeley,Toyota Research Institute,Google DeepMind,Massachusetts Institute of Technology (MIT),Physical Intelligence",
                "authors": "Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, Chelsea Finn",
                "publication_date": "2024-06-13",
                "reference": "OpenVLA: An Open-Source Vision-Language-Action Mode",
                "link": "https://openvla.github.io/\nhttps://arxiv.org/abs/2406.09246",
                "citations": 970.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 7188100000.0,
                "training_compute_(flop)": 1.1e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 336.0,
                "training_hardware": "NVIDIA A100",
                "approach": "Supervised",
                "confidence": "Confident",
                "epochs": 27.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": "Llama 2-7B",
                "finetune_compute_(flop)": 9.66e+21,
                "hardware_quantity": 64.0,
                "last_modified": "2025-10-16 13:09:29+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 2048.0,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 50541.724821294745,
                "training_compute_estimation_method": "Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.85661411033261,
                "log_compute": 23.041392685158225,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "Mamba2-Hybrid",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering",
                "organization": "NVIDIA",
                "authors": "Roger Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, Vijay Korthikanti, Tri Dao, Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, Garvit Kulshreshtha, Vartika Singh, Jared Casper, Jan Kautz, Mohammad Shoeybi, Bryan Catanzaro",
                "publication_date": "2024-06-12",
                "reference": "An Empirical Study of Mamba-based Language Models",
                "link": "https://arxiv.org/abs/2406.07887",
                "citations": null,
                "notability_criteria": null,
                "parameters": 8660000000.0,
                "training_compute_(flop)": 1.8186e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 1024.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 4194304.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 1415199.8102553426,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.937517892017347,
                "log_compute": 23.259737186751266,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "Megrez-3B-Omni",
                "domain": "Multimodal,Language,Vision,Speech",
                "task": "Text summarization,Image captioning,Character recognition (OCR),Visual question answering,Language modeling/generation,Question answering,Speech recognition (ASR)",
                "organization": "Infinigence AI,Tsinghua University,Shanghai Jiao Tong University",
                "authors": "Boxun Li, Yadong Li, Zhiyuan Li, Congyi Liu, Weilin Liu, Guowei Niu,\nZheyue Tan, Haiyang Xu, Zhuyu Yao, Tao Yuan, Dong Zhou, Yueqing Zhuang, Shengen Yan, Guohao Dai, Yu Wang",
                "publication_date": "2024-06-12",
                "reference": "Megrez-3B-Omni: The First Open-Source End-Side Full Modality Understanding Model",
                "link": "https://github.com/infinigence/Infini-Megrez-Omni/blob/main/assets/Megrez_Omni_Technical_Report.pdf",
                "citations": null,
                "notability_criteria": null,
                "parameters": 3000000000.0,
                "training_compute_(flop)": 3.6e+22,
                "training_dataset_size_(gradients)": 2000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": "SigLIP 400M",
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-01 17:07:47+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Multimodal",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 9.477121254719663,
                "log_compute": 22.556302500767288,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Samba 3.8B",
                "domain": "Language",
                "task": "Language modeling/generation",
                "organization": "Microsoft,University of Illinois Urbana-Champaign (UIUC)",
                "authors": "Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, Weizhu Chen",
                "publication_date": "2024-06-11",
                "reference": "Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling",
                "link": "https://arxiv.org/abs/2406.07522",
                "citations": null,
                "notability_criteria": null,
                "parameters": 3800000000.0,
                "training_compute_(flop)": 7.3e+22,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.57978359661681,
                "log_compute": 22.863322860120455,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "TiTok-L",
                "domain": "Image generation",
                "task": "Image generation",
                "organization": "ByteDance,Technical University of Munich",
                "authors": "Qihang Yu, Mark Weber, Xueqing Deng, Xiaohui Shen, Daniel Cremers, Liang-Chieh Chen",
                "publication_date": "2024-06-11",
                "reference": "An Image is Worth 32 Tokens for Reconstruction and Generation",
                "link": "https://arxiv.org/abs/2406.07550",
                "citations": null,
                "notability_criteria": null,
                "parameters": 307000000.0,
                "training_compute_(flop)": 1.7252352e+21,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100 SXM4 80 GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": 5120.0,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Vision",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 8.487138375477187,
                "log_compute": 21.236848310481225,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Qwen2-72B",
                "domain": "Language",
                "task": "Chat,Language modeling/generation,Question answering",
                "organization": "Alibaba",
                "authors": "An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, Zhihao Fan",
                "publication_date": "2024-06-07",
                "reference": "Hello Qwen2",
                "link": "https://qwenlm.github.io/blog/qwen2/ \nhttps://arxiv.org/abs/2407.10671 ",
                "citations": null,
                "notability_criteria": "Training cost",
                "parameters": 72710000000.0,
                "training_compute_(flop)": 3.02e+24,
                "training_dataset_size_(gradients)": 7000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": "Self-supervised learning",
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 18:39:42+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 10.861594144643865,
                "log_compute": 24.48000694295715,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Qwen2-7B",
                "domain": "Language",
                "task": "Chat,Language modeling/generation,Question answering",
                "organization": "Alibaba",
                "authors": "An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, Zhihao Fan",
                "publication_date": "2024-06-07",
                "reference": "Hello Qwen2",
                "link": "https://qwenlm.github.io/blog/qwen2/ \nhttps://arxiv.org/abs/2407.10671 ",
                "citations": null,
                "notability_criteria": null,
                "parameters": 7000000000.0,
                "training_compute_(flop)": 2.9400000000001e+23,
                "training_dataset_size_(gradients)": 7000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": "Self-supervised learning",
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 18:39:40+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 9.845098040014257,
                "log_compute": 23.468347330412172,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Qwen2-57B-A14B",
                "domain": "Language",
                "task": "Chat,Language modeling/generation,Question answering",
                "organization": "Alibaba",
                "authors": "An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, Zhihao Fan",
                "publication_date": "2024-06-07",
                "reference": "Hello Qwen2",
                "link": "https://qwenlm.github.io/blog/qwen2/ \nhttps://arxiv.org/abs/2407.10671 ",
                "citations": null,
                "notability_criteria": null,
                "parameters": 57000000000.0,
                "training_compute_(flop)": 3.7800000000001e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": "Self-supervised learning",
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 18:39:40+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 10.755874855672491,
                "log_compute": 23.577491799837237,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Qwen2-1.5B",
                "domain": "Language",
                "task": "Chat,Language modeling/generation,Question answering",
                "organization": "Alibaba",
                "authors": "An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, Zhihao Fan",
                "publication_date": "2024-06-07",
                "reference": "Hello Qwen2",
                "link": "https://qwenlm.github.io/blog/qwen2/ \nhttps://arxiv.org/abs/2407.10671 ",
                "citations": null,
                "notability_criteria": null,
                "parameters": 1500000000.0,
                "training_compute_(flop)": 6.3e+22,
                "training_dataset_size_(gradients)": 7000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 18:39:38+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 9.176091259055681,
                "log_compute": 22.799340549453582,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Qwen2-0.5B",
                "domain": "Language",
                "task": "Chat,Language modeling/generation,Question answering",
                "organization": "Alibaba",
                "authors": "An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, Zhihao Fan",
                "publication_date": "2024-06-07",
                "reference": "Hello Qwen2",
                "link": "https://qwenlm.github.io/blog/qwen2/ \nhttps://arxiv.org/abs/2407.10671 ",
                "citations": null,
                "notability_criteria": null,
                "parameters": 500000000.0,
                "training_compute_(flop)": 3.6e+22,
                "training_dataset_size_(gradients)": 12000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 18:39:36+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 8.698970004336019,
                "log_compute": 22.556302500767288,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "MiniCPM-2.4B",
                "domain": "Language",
                "task": "Language modeling/generation,Code generation,Translation",
                "organization": "Tsinghua University,ModelBest",
                "authors": "Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, Xinrong Zhang, Zheng Leng Thai, Kaihuo Zhang, Chongyi Wang, Yuan Yao, Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chao Jia, Guoyang Zeng, Dahai Li, Zhiyuan Liu, Maosong Sun",
                "publication_date": "2024-06-03",
                "reference": "MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies",
                "link": "https://arxiv.org/abs/2404.06395",
                "citations": null,
                "notability_criteria": null,
                "parameters": 2442057984.0,
                "training_compute_(flop)": 1.584e+22,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 4000000.0,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 9.387755971579493,
                "log_compute": 22.199755177253476,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "MiniCPM-1.2B",
                "domain": "Language",
                "task": "Language modeling/generation,Code generation,Translation",
                "organization": "Tsinghua University,ModelBest",
                "authors": "Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, Xinrong Zhang, Zheng Leng Thai, Kaihuo Zhang, Chongyi Wang, Yuan Yao, Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chao Jia, Guoyang Zeng, Dahai Li, Zhiyuan Liu, Maosong Sun",
                "publication_date": "2024-06-03",
                "reference": "MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies",
                "link": "https://arxiv.org/abs/2404.06395",
                "citations": null,
                "notability_criteria": null,
                "parameters": 1247442432.0,
                "training_compute_(flop)": 7.92e+21,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 4000000.0,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 9.096020512578699,
                "log_compute": 21.898725181589494,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Mamba 2, 2.7B",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering",
                "organization": "Princeton University,Carnegie Mellon University (CMU)",
                "authors": "Tri Dao, Albert Gu",
                "publication_date": "2024-05-31",
                "reference": "Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality",
                "link": "https://arxiv.org/abs/2405.21060",
                "citations": null,
                "notability_criteria": null,
                "parameters": 2700000000.0,
                "training_compute_(flop)": 4.86e+21,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.431363764158988,
                "log_compute": 21.686636269262294,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "Granite 20B",
                "domain": "Language",
                "task": "Language modeling/generation",
                "organization": "IBM Research",
                "authors": "IBM Research",
                "publication_date": "2024-05-31",
                "reference": "Granite Foundation Models",
                "link": "https://www.ibm.com/downloads/documents/us-en/10a99803c92fdb35",
                "citations": null,
                "notability_criteria": null,
                "parameters": 20000000000.0,
                "training_compute_(flop)": 3.0000000000001e+23,
                "training_dataset_size_(gradients)": 2500000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 10.301029995663981,
                "log_compute": 23.477121254719677,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "CLAY",
                "domain": "3D modeling,Vision",
                "task": "3D reconstruction",
                "organization": "Shanghai Tech University,Deemos Technology,Huazhong University of Science and Technology",
                "authors": "Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, Jingyi Yu",
                "publication_date": "2024-05-30",
                "reference": "CLAY: A Controllable Large-scale Generative Model for Creating High-quality 3D Assets",
                "link": "https://arxiv.org/abs/2406.13897v1",
                "citations": null,
                "notability_criteria": null,
                "parameters": 1500000000.0,
                "training_compute_(flop)": 3.1054234e+22,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 360.0,
                "training_hardware": "NVIDIA A800 PCIe",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 256.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Multimodal",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 9.176091259055681,
                "log_compute": 22.492120821178542,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Aurora",
                "domain": "Earth science",
                "task": "Weather forecasting",
                "organization": "Microsoft Research",
                "authors": "Cristian Bodnar, Wessel P. Bruinsma, Ana Lucic, Megan Stanley, Johannes Brandstetter, Patrick Garvan, Maik Riechert, Jonathan Weyn, Haiyu Dong, Anna Vaughan, Jayesh K. Gupta, Kit Tambiratnam, Alex Archibald, Elizabeth Heider, Max Welling, Richard E. Turner, Paris Perdikaris",
                "publication_date": "2024-05-28",
                "reference": "Aurora: A Foundation Model of the Atmosphere",
                "link": "https://arxiv.org/abs/2405.13063",
                "citations": null,
                "notability_criteria": null,
                "parameters": 1300000000.0,
                "training_compute_(flop)": 4.5287424e+21,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 420.0,
                "training_hardware": "NVIDIA A100",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 32.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 25279.86827447104,
                "training_compute_estimation_method": "Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Earth science",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.113943352306837,
                "log_compute": 21.6559776182232,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "Nanbeige2-16B-Chat",
                "domain": "Language",
                "task": "Chat,Question answering",
                "organization": "Nanbeige LLM Lab",
                "authors": "Nanbeige Lab",
                "publication_date": "2024-05-28",
                "reference": "Nanbeige2-16B-Chat",
                "link": "https://huggingface.co/Nanbeige/Nanbeige2-16B-Chat",
                "citations": null,
                "notability_criteria": null,
                "parameters": 15800000000.0,
                "training_compute_(flop)": 4.05e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": null,
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 10.198657086954423,
                "log_compute": 23.60745502321467,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Zamba2-7B",
                "domain": "Language",
                "task": "Language modeling/generation,Chat,Text summarization,Code generation,Question answering",
                "organization": "Zyphra",
                "authors": "Paolo Glorioso, Quentin Anthony, Yury Tokpanov, James Whittington, Jonathan Pilault, Adam Ibrahim, Beren Millidge",
                "publication_date": "2024-05-26",
                "reference": "Zamba: A Compact 7B SSM Hybrid Model",
                "link": "https://arxiv.org/abs/2405.16712",
                "citations": 63.0,
                "notability_criteria": null,
                "parameters": 7000000000.0,
                "training_compute_(flop)": 8.82e+22,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": "Mamba 2, 2.7B",
                "finetune_compute_(flop)": null,
                "hardware_quantity": 128.0,
                "last_modified": "2025-10-14 18:13:46+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 176966.95963163028,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.845098040014257,
                "log_compute": 22.94546858513182,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "Genie 2 (bio)",
                "domain": "Biology",
                "task": "Protein generation",
                "organization": "Columbia University,Rutgers University",
                "authors": "Yeqing Lin, Minji Lee, Zhao Zhang, Mohammed AlQuraishi",
                "publication_date": "2024-05-24",
                "reference": "Out of Many, One: Designing and Scaffolding Proteins at the Scale of the Structural Universe with Genie 2",
                "link": "https://arxiv.org/abs/2405.15489",
                "citations": null,
                "notability_criteria": null,
                "parameters": 15700000.0,
                "training_compute_(flop)": 3.234816e+20,
                "training_dataset_size_(gradients)": 150673920.0,
                "training_time_(hours)": 120.0,
                "training_hardware": "NVIDIA A100",
                "approach": null,
                "confidence": "Confident",
                "epochs": 40.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 8.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 48.0,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": 960.0,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 6320.530060464735,
                "training_compute_estimation_method": "Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Biology",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 7.195899652409234,
                "log_compute": 20.509849582544962,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "YOLOv10-X",
                "domain": "Vision",
                "task": "Object detection",
                "organization": "Tsinghua University",
                "authors": "Ao Wang, Hui Chen, Lihao Liu, Kai Chen, Zijia Lin, Jungong Han, Guiguang Ding",
                "publication_date": "2024-05-23",
                "reference": "YOLOv10: Real-Time End-to-End Object Detection",
                "link": "https://arxiv.org/abs/2405.14458",
                "citations": null,
                "notability_criteria": null,
                "parameters": 29500000.0,
                "training_compute_(flop)": 1.478888e+17,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA GeForce RTX 3090 Ti",
                "approach": null,
                "confidence": "Likely",
                "epochs": 500.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 8.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 7110.754668295396,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Vision",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 7.469822015978163,
                "log_compute": 17.169935285001774,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "360Zhinao-7B",
                "domain": "Language",
                "task": "Question answering,Language modeling/generation",
                "organization": "360 Security Technology",
                "authors": "360zhinao",
                "publication_date": "2024-05-22",
                "reference": "360Zhinao Technical Report",
                "link": "https://arxiv.org/abs/2405.13386",
                "citations": 0.0,
                "notability_criteria": null,
                "parameters": 7000000000.0,
                "training_compute_(flop)": 1.428e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": null,
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 9.845098040014257,
                "log_compute": 23.154728207440154,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Octo-Base",
                "domain": "Robotics",
                "task": "Robotic manipulation",
                "organization": "University of California (UC) Berkeley,Stanford University,Carnegie Mellon University (CMU),DeepMind",
                "authors": "Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, Jianlan Luo, You Liang Tan, Lawrence Yunliang Chen, Pannag Sanketi, Quan Vuong, Ted Xiao, Dorsa Sadigh, Chelsea Finn, Sergey Levine",
                "publication_date": "2024-05-20",
                "reference": "Octo: An Open-Source Generalist Robot Policy",
                "link": "https://arxiv.org/abs/2405.12213 ",
                "citations": 687.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 93000000.0,
                "training_compute_(flop)": 5.85e+20,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 14.0,
                "training_hardware": "Google TPU v4",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 128.0,
                "last_modified": "2025-10-16 13:09:29+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 42983.43309675412,
                "training_compute_estimation_method": "Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Robotics",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 7.968482948553935,
                "log_compute": 20.76715586608218,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "ProSST",
                "domain": "Biology",
                "task": "Protein or nucleotide language model (pLM/nLM)",
                "organization": "Shanghai Jiao Tong University,Shanghai AI Lab,East China University of Science and Technology",
                "authors": "Mingchen Li, Pan Tan, Xinzhu Ma, Bozitao Zhong, Huiqun Yu, Ziyi Zhou, Wanli Ouyang, Bingxin Zhou, Liang Hong, Yang Tan",
                "publication_date": "2024-05-17",
                "reference": "ProSST: Protein Language Modeling with Quantized Structure and Disentangled Attention",
                "link": "https://www.biorxiv.org/content/10.1101/2024.04.15.589672v3.abstract",
                "citations": 38.0,
                "notability_criteria": null,
                "parameters": 110000000.0,
                "training_compute_(flop)": 6.46714368e+20,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 720.0,
                "training_hardware": "NVIDIA A800 PCIe 40 GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unknown",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 8.0,
                "last_modified": "2025-10-14 21:54:47+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 3950.9471355488704,
                "training_compute_estimation_method": "Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Biology",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 8.041392685158225,
                "log_compute": 20.810712509739137,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Chameleon-34B",
                "domain": "Multimodal,Image generation,Language,Vision",
                "task": "Language modeling/generation,Vision-language generation,Visual question answering,Text-to-image",
                "organization": "Facebook AI Research",
                "authors": "Srinivasan Iyer, Bernie Huang, Lili Yu, Arun Babu, Chunting Zhou, Kushal Tirumala, Xi Victoria Lin, Hu Xu, Xian Li, Akshat Shrivastava, Omer Levy, Armen Aghajanyan, Ram Pasunuru, Andrew Cohen, Aram H. Markosyan, Koustuv Sinha, Xiaoqing Ellen Tan, Ivan Evtimov, Ping Yu, Tianlu Wang, Olga Golovneva, Asli Celikyilmaz, Pedro Rodriguez, Leonid Shamis, Vasu Sharma, Christine Jou, Karthik Padthe, Ching-Feng Yeh, Mingda Chen, Bapi Akula, Jacob Kahn, Daniel Li, Scott Yih, Barlas Oguz, Morteza Behrooz, Benjamin Muller, Carleigh Wood, Mary Williamson, Ramya Raghavendra, Barbara Usher, William Ngan, Nikolay Bashlykov, Lukas Blecher, Sony Theakanath, Ammar Rizvi, Gargi Ghosh, Luke Zettlemoyer",
                "publication_date": "2024-05-16",
                "reference": "Chameleon: Mixed-Modal Early-Fusion Foundation Models",
                "link": "https://arxiv.org/abs/2405.09818v1",
                "citations": null,
                "notability_criteria": null,
                "parameters": 34000000000.0,
                "training_compute_(flop)": 1.6453571041e+24,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 1394.0,
                "training_hardware": "NVIDIA A100 SXM4 80 GB",
                "approach": "Self-supervised learning",
                "confidence": "Confident",
                "epochs": 2.1,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 3072.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 2427515.9787339047,
                "training_compute_estimation_method": "Hardware,Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 10.531478917042255,
                "log_compute": 24.216260170680453,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "FragLlama: Next-fragment prediction for molecular design",
                "domain": "Multimodal,Image generation,Vision,Language",
                "task": "Language modeling/generation,Vision-language generation,Visual question answering,Text-to-image",
                "organization": "Facebook AI Research",
                "authors": "Srinivasan Iyer, Bernie Huang, Lili Yu, Arun Babu, Chunting Zhou, Kushal Tirumala, Xi Victoria Lin, Hu Xu, Xian Li, Akshat Shrivastava, Omer Levy, Armen Aghajanyan, Ram Pasunuru, Andrew Cohen, Aram H. Markosyan, Koustuv Sinha, Xiaoqing Ellen Tan, Ivan Evtimov, Ping Yu, Tianlu Wang, Olga Golovneva, Asli Celikyilmaz, Pedro Rodriguez, Leonid Shamis, Vasu Sharma, Christine Jou, Karthik Padthe, Ching-Feng Yeh, Mingda Chen, Bapi Akula, Jacob Kahn, Daniel Li, Scott Yih, Barlas Oguz, Morteza Behrooz, Benjamin Muller, Carleigh Wood, Mary Williamson, Ramya Raghavendra, Barbara Usher, William Ngan, Nikolay Bashlykov, Lukas Blecher, Sony Theakanath, Ammar Rizvi, Gargi Ghosh, Luke Zettlemoyer",
                "publication_date": "2024-05-16",
                "reference": "Chameleon: Mixed-Modal Early-Fusion Foundation Models",
                "link": "https://arxiv.org/abs/2405.09818v1",
                "citations": null,
                "notability_criteria": null,
                "parameters": 7000000000.0,
                "training_compute_(flop)": 3.3399700602e+23,
                "training_dataset_size_(gradients)": 4400000000000.0,
                "training_time_(hours)": 836.4,
                "training_hardware": "NVIDIA A100 SXM4 80 GB",
                "approach": "Self-supervised learning",
                "confidence": "Confident",
                "epochs": 2.1,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware,Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.845098040014257,
                "log_compute": 23.52374257377318,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "LBSTER",
                "domain": "Biology",
                "task": "Protein or nucleotide language model (pLM/nLM)",
                "organization": "Prescient Design,Genentech",
                "authors": "Nathan C. Frey, Taylor Joren, Aya Abdelsalam Ismail, Allen Goodman, Richard Bonneau, Kyunghyun Cho, Vladimir Gligorijevi\u0107",
                "publication_date": "2024-05-15",
                "reference": "Cramming Protein Language Model Training in 24 GPU Hours",
                "link": "https://www.biorxiv.org/content/10.1101/2024.05.14.594108v1.abstract",
                "citations": 4.0,
                "notability_criteria": null,
                "parameters": 67000000.0,
                "training_compute_(flop)": 1.078272e+19,
                "training_dataset_size_(gradients)": 3375000000.0,
                "training_time_(hours)": 24.0,
                "training_hardware": "NVIDIA A100 SXM4 80 GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": 4.06,
                "model_accessibility": "Unknown",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 1.0,
                "last_modified": "2025-10-14 21:54:47+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 434.1893527126348,
                "training_compute_estimation_method": "Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Biology",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 7.826074802700826,
                "log_compute": 19.0327283278253,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "Yi-Large",
                "domain": "Language",
                "task": "Chat,Language modeling/generation",
                "organization": "01.AI",
                "authors": "Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, Zonghong Dai",
                "publication_date": "2024-05-13",
                "reference": null,
                "link": null,
                "citations": null,
                "notability_criteria": "Training cost",
                "parameters": 100000000000.0,
                "training_compute_(flop)": 1.8e+24,
                "training_dataset_size_(gradients)": 3000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Speculative",
                "epochs": null,
                "model_accessibility": "API access",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-08-01 16:23:25+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 11.0,
                "log_compute": 24.255272505103306,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Yi-1.5-34B",
                "domain": "Language",
                "task": "Chat,Language modeling/generation,Translation,Code generation",
                "organization": "01.AI",
                "authors": "Unknown",
                "publication_date": "2024-05-13",
                "reference": "Yi-1.5 is an upgraded version of Yi, delivering stronger performance in coding, math, reasoning, and instruction-following capability.",
                "link": "https://huggingface.co/01-ai/Yi-1.5-34B",
                "citations": null,
                "notability_criteria": null,
                "parameters": 34000000000.0,
                "training_compute_(flop)": 7.344e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 10.531478917042255,
                "log_compute": 23.865932668193185,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Yi-1.5-9B",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Chat",
                "organization": "01.AI",
                "authors": "Unknown",
                "publication_date": "2024-05-13",
                "reference": "Yi-1.5 is an upgraded version of Yi.",
                "link": "https://huggingface.co/01-ai/Yi-1.5-9B",
                "citations": null,
                "notability_criteria": null,
                "parameters": 8830000000.0,
                "training_compute_(flop)": 1.90728e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 9.945960703577569,
                "log_compute": 23.2804144547285,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "MatterSim (M3GNet - MatterSim-v1.0.0-5M)",
                "domain": "Materials science",
                "task": "Atomistic simulations,Molecular simulation",
                "organization": "Microsoft Research AI for Science",
                "authors": "Han Yang, Chenxi Hu, Yichi Zhou, Xixian Liu, Yu Shi, Jielan Li, Guanzhi Li, Zekun Chen, Shuizhou Chen, Claudio Zeni, Matthew Horton, Robert Pinsler, Andrew Fowler, Daniel Z\u00fcgner, Tian Xie, Jake Smith, Lixin Sun, Qian Wang, Lingyu Kong, Chang Liu, Hongxia Hao, Ziheng Lu",
                "publication_date": "2024-05-10",
                "reference": "MatterSim: A Deep Learning Atomistic Model Across Elements, Temperatures and Pressures",
                "link": "https://arxiv.org/abs/2405.04967",
                "citations": null,
                "notability_criteria": null,
                "parameters": 4500000.0,
                "training_compute_(flop)": 1.62e+16,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100",
                "approach": null,
                "confidence": "Speculative",
                "epochs": 200.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 8.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 6322.500926906498,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Materials science",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 6.653212513775344,
                "log_compute": 16.20951501454263,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "MatterSim (Grpaphomer)",
                "domain": "Materials science",
                "task": "Atomistic simulations,Molecular simulation",
                "organization": "Microsoft Research AI for Science",
                "authors": "Han Yang, Chenxi Hu, Yichi Zhou, Xixian Liu, Yu Shi, Jielan Li, Guanzhi Li, Zekun Chen, Shuizhou Chen, Claudio Zeni, Matthew Horton, Robert Pinsler, Andrew Fowler, Daniel Z\u00fcgner, Tian Xie, Jake Smith, Lixin Sun, Qian Wang, Lingyu Kong, Chang Liu, Hongxia Hao, Ziheng Lu",
                "publication_date": "2024-05-10",
                "reference": "MatterSim: A Deep Learning Atomistic Model Across Elements, Temperatures and Pressures",
                "link": "https://arxiv.org/abs/2405.04967",
                "citations": null,
                "notability_criteria": null,
                "parameters": 182000000.0,
                "training_compute_(flop)": 1.118208e+20,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100",
                "approach": null,
                "confidence": "Speculative",
                "epochs": null,
                "model_accessibility": "Unknown",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 64.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 50580.00741525198,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Materials science",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 8.260071387985075,
                "log_compute": 20.04852259500853,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "DeepSeek-V2 (MoE-236B)",
                "domain": "Language",
                "task": "Language modeling/generation,Chat,Code generation",
                "organization": "DeepSeek",
                "authors": "DeepSeek-AI, Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Hanwei Xu, Hao Yang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J.L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jin Chen, Jingyang Yuan, Junjie Qiu, Junxiao Song, Kai Dong, Kaige Gao, Kang Guan, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qihao Zhu, Qinyu Chen, Qiushi Du, R.J. Chen, R.L. Jin, Ruiqi Ge, Ruizhe Pan, Runxin Xu, Ruyi Chen, S.S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Size Zheng, T. Wang, Tian Pei, Tian Yuan, Tianyu Sun, W.L. Xiao, Wangding Zeng, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wentao Zhang, X.Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun",
                "publication_date": "2024-05-07",
                "reference": "DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model",
                "link": "https://arxiv.org/abs/2405.04434 \nhttps://github.com/deepseek-ai/DeepSeek-V2 ",
                "citations": null,
                "notability_criteria": null,
                "parameters": 236000000000.0,
                "training_compute_(flop)": 1.02e+24,
                "training_dataset_size_(gradients)": 8100000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA H800 SXM5",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 18432000.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": 172800.0,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 11.372912002970107,
                "log_compute": 24.008600171761916,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Microsoft MAI-1",
                "domain": "Language",
                "task": "Language modeling,Language modeling/generation",
                "organization": "Microsoft",
                "authors": "Unknown",
                "publication_date": "2024-05-06",
                "reference": "Meet MAI-1: Microsoft Readies New AI Model to Compete With Google, OpenAI",
                "link": "https://www.theinformation.com/articles/meet-mai-1-microsoft-readies-new-ai-model-to-compete-with-google-openai\n\nhttps://em360tech.com/tech-articles/what-mai-1-deep-dive-microsofts-gpt-4-rival",
                "citations": null,
                "notability_criteria": null,
                "parameters": 500000000000.0,
                "training_compute_(flop)": 1.602828e+22,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": "Supervised",
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Hosted access (no API)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": 15000.0,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": null,
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 11.698970004336019,
                "log_compute": 22.204886920570793,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "VILA1.5-13B",
                "domain": "Multimodal,Language,Vision,Video",
                "task": "Chat,Visual question answering,Image captioning,Language modeling/generation,Question answering",
                "organization": "NVIDIA,Massachusetts Institute of Technology (MIT)",
                "authors": "Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, Song Han",
                "publication_date": "2024-05-03",
                "reference": "VILA: On Pre-training for Visual Language Models",
                "link": "https://huggingface.co/Efficient-Large-Model/VILA1.5-13b\nhttps://github.com/NVlabs/VILA/tree/bbc609baf326b1b49b93450b48edc516db3737fc/scripts/v1_5/release/13b\nhttps://developer.nvidia.com/blog/visual-language-models-on-nvidia-hardware-with-vila/\n",
                "citations": 567.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 13493916736.0,
                "training_compute_(flop)": 2.3003136e+21,
                "training_dataset_size_(gradients)": 32430000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "Multinational",
                "base_model": "SigLIP 400M,Vicuna-13B-v1.5",
                "finetune_compute_(flop)": null,
                "hardware_quantity": 128.0,
                "last_modified": "2025-10-16 13:09:06+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 101175.78544917757,
                "training_compute_estimation_method": null,
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 10.130138026020305,
                "log_compute": 21.361787047089525,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "OpenELM-1.1B",
                "domain": "Language",
                "task": "Language modeling/generation,Code generation,Question answering",
                "organization": "Apple",
                "authors": "Sachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, Mohammad Rastegari",
                "publication_date": "2024-05-02",
                "reference": "OpenELM: An Efficient Language Model Family with Open Training and Inference Framework",
                "link": "https://arxiv.org/abs/2404.14619",
                "citations": null,
                "notability_criteria": null,
                "parameters": 1080000000.0,
                "training_compute_(flop)": 1.0520327e+22,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 240.0,
                "training_hardware": "NVIDIA A100 SXM4 80 GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 128.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 4000000.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 101178.0385953958,
                "training_compute_estimation_method": "Operation counting,Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.03342375548695,
                "log_compute": 22.022029239065674,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "OpenELM-3B",
                "domain": "Language",
                "task": "Language modeling/generation,Code generation,Question answering",
                "organization": "Apple",
                "authors": "Sachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, Mohammad Rastegari",
                "publication_date": "2024-05-02",
                "reference": "OpenELM: An Efficient Language Model Family with Open Training and Inference Framework",
                "link": "https://arxiv.org/abs/2404.14619",
                "citations": null,
                "notability_criteria": null,
                "parameters": 3040000000.0,
                "training_compute_(flop)": 3.417119e+22,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 288.0,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 128.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 4000000.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 177061.56754194264,
                "training_compute_estimation_method": "Operation counting,Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.482873583608754,
                "log_compute": 22.533660103218853,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "OpenELM-450M",
                "domain": "Language",
                "task": "Language modeling/generation,Code generation,Question answering",
                "organization": "Apple",
                "authors": "Sachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, Mohammad Rastegari",
                "publication_date": "2024-05-02",
                "reference": "OpenELM: An Efficient Language Model Family with Open Training and Inference Framework",
                "link": "https://arxiv.org/abs/2404.14619",
                "citations": null,
                "notability_criteria": null,
                "parameters": 450000000.0,
                "training_compute_(flop)": 6.3156568e+21,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 72.0,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 128.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 4000000.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 177061.56754194264,
                "training_compute_estimation_method": "Operation counting,Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 8.653212513775344,
                "log_compute": 21.800418521919557,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "OpenELM-270M",
                "domain": "Language",
                "task": "Language modeling/generation,Code generation,Question answering",
                "organization": "Apple",
                "authors": "Sachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, Mohammad Rastegari",
                "publication_date": "2024-05-02",
                "reference": "OpenELM: An Efficient Language Model Family with Open Training and Inference Framework",
                "link": "https://arxiv.org/abs/2404.14619",
                "citations": null,
                "notability_criteria": null,
                "parameters": 270000000.0,
                "training_compute_(flop)": 2.7470309e+21,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 72.0,
                "training_hardware": "NVIDIA A100 SXM4 80 GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 128.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 4000000.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 101178.0385953958,
                "training_compute_estimation_method": "Operation counting,Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 8.431363764158988,
                "log_compute": 21.43886354461314,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "Multi-Token Prediction 7B",
                "domain": "Language",
                "task": "Code generation",
                "organization": "Facebook AI Research",
                "authors": "Fabian Gloeckle, Badr Youbi Idrissi, Baptiste Rozi\u00e8re, David Lopez-Paz, Gabriel Synnaeve",
                "publication_date": "2024-04-30",
                "reference": "Better & Faster Large Language Models via Multi-token Prediction",
                "link": "https://arxiv.org/abs/2404.19737",
                "citations": null,
                "notability_criteria": null,
                "parameters": 6700000000.0,
                "training_compute_(flop)": 3.841092e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100 SXM4 80 GB,NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Likely",
                "epochs": 4.0,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.826074802700827,
                "log_compute": 23.58445470930364,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "Multi-Token Prediction 13B",
                "domain": "Language",
                "task": "Code generation",
                "organization": "Facebook AI Research",
                "authors": "Fabian Gloeckle, Badr Youbi Idrissi, Baptiste Rozi\u00e8re, David Lopez-Paz, Gabriel Synnaeve",
                "publication_date": "2024-04-30",
                "reference": "Better & Faster Large Language Models via Multi-token Prediction",
                "link": "https://arxiv.org/abs/2404.19737",
                "citations": null,
                "notability_criteria": null,
                "parameters": 13000000000.0,
                "training_compute_(flop)": 1.5364368e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100 SXM4 80 GB,NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 10.113943352306837,
                "log_compute": 23.1865147006316,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "DiffPepBuilder",
                "domain": "Biology",
                "task": "Protein generation",
                "organization": "Peking University",
                "authors": "Fanhao Wang, Yuzhe Wang, Laiyi Feng, Changsheng Zhang, Luhua Lai",
                "publication_date": "2024-04-30",
                "reference": "Target-Specific De Novo Peptide Binder Design with DiffPepBuilder",
                "link": "https://arxiv.org/abs/2405.00128",
                "citations": 11.0,
                "notability_criteria": null,
                "parameters": 104000000.0,
                "training_compute_(flop)": 7.667785728001e+21,
                "training_dataset_size_(gradients)": 1489700.0,
                "training_time_(hours)": 120.0,
                "training_hardware": "NVIDIA A800 PCIe 40 GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unknown",
                "country": "China",
                "base_model": "ESM2-650M",
                "finetune_compute_(flop)": null,
                "hardware_quantity": 8.0,
                "last_modified": "2025-10-14 18:13:47+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 3952.4431655610742,
                "training_compute_estimation_method": "Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Biology",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 8.01703333929878,
                "log_compute": 21.884669968258322,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Arctic",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Code generation,Quantitative reasoning",
                "organization": "Snowflake",
                "authors": "Snowflake AI Research",
                "publication_date": "2024-04-24",
                "reference": "Snowflake Arctic: The Best LLM for Enterprise AI \u2014 Efficiently Intelligent, Truly Open",
                "link": "https://www.snowflake.com/en/blog/arctic-open-efficient-foundation-language-models-snowflake/",
                "citations": null,
                "notability_criteria": null,
                "parameters": 480000000000.0,
                "training_compute_(flop)": 3.8347175e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": "AWS",
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": 504000.0,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": 2000000.0,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Other,Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 11.681241237375588,
                "log_compute": 23.58373337540099,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "phi-3-mini 3.8B",
                "domain": "Language",
                "task": "Chat,Language modeling/generation",
                "organization": "Microsoft",
                "authors": "Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, S\u00e9bastien Bubeck, Martin Cai, Caio C\u00e9sar Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, Xiren Zhou",
                "publication_date": "2024-04-23",
                "reference": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone",
                "link": "https://arxiv.org/abs/2404.14219",
                "citations": null,
                "notability_criteria": "Significant use",
                "parameters": 3800000000.0,
                "training_compute_(flop)": 7.524e+22,
                "training_dataset_size_(gradients)": 3300000000000.0,
                "training_time_(hours)": 168.0,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 512.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 708388.2341861207,
                "training_compute_estimation_method": "Hardware,Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.57978359661681,
                "log_compute": 22.876448786878342,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "phi-3-medium 14B",
                "domain": "Language",
                "task": "Chat,Language modeling/generation",
                "organization": "Microsoft",
                "authors": "Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, S\u00e9bastien Bubeck, Martin Cai, Caio C\u00e9sar Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, Xiren Zhou",
                "publication_date": "2024-04-23",
                "reference": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone",
                "link": "https://arxiv.org/abs/2404.14219",
                "citations": null,
                "notability_criteria": null,
                "parameters": 14000000000.0,
                "training_compute_(flop)": 4.032e+23,
                "training_dataset_size_(gradients)": 4800000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 10.146128035678238,
                "log_compute": 23.60552052343747,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "phi-3-small 7.4B",
                "domain": "Language",
                "task": "Chat,Language modeling/generation",
                "organization": "Microsoft",
                "authors": "Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, S\u00e9bastien Bubeck, Martin Cai, Caio C\u00e9sar Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, Xiren Zhou",
                "publication_date": "2024-04-23",
                "reference": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone",
                "link": "https://arxiv.org/abs/2404.14219",
                "citations": null,
                "notability_criteria": null,
                "parameters": 7400000000.0,
                "training_compute_(flop)": 2.1312e+23,
                "training_dataset_size_(gradients)": 4800000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.869231719730976,
                "log_compute": 23.328624207490208,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "phi-3.5-mini",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Translation",
                "organization": "Microsoft",
                "authors": "Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, S\u00e9bastien Bubeck, Martin Cai, Caio C\u00e9sar Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, Xiren Zhou",
                "publication_date": "2024-04-23",
                "reference": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone",
                "link": "https://arxiv.org/abs/2404.14219",
                "citations": null,
                "notability_criteria": null,
                "parameters": 3800000000.0,
                "training_compute_(flop)": 3.7101154e+22,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 240.0,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 512.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 708388.2341861207,
                "training_compute_estimation_method": "Hardware,Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.57978359661681,
                "log_compute": 22.569387418187493,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "phi-3.5-Vision",
                "domain": "Vision",
                "task": "Visual question answering",
                "organization": "Microsoft",
                "authors": "Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, S\u00e9bastien Bubeck, Martin Cai, Caio C\u00e9sar Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, Xiren Zhou",
                "publication_date": "2024-04-23",
                "reference": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone",
                "link": "https://arxiv.org/abs/2404.14219",
                "citations": null,
                "notability_criteria": null,
                "parameters": 4200000000.0,
                "training_compute_(flop)": 8.784e+22,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 144.0,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": "phi-3-mini 3.8B",
                "finetune_compute_(flop)": 8.1926884e+21,
                "hardware_quantity": 256.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 354194.11709306034,
                "training_compute_estimation_method": "Hardware,Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Vision",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.6232492903979,
                "log_compute": 22.943692327106017,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "Phi-3.5-MoE",
                "domain": "Language",
                "task": "Language modeling/generation,Translation,Question answering,Code generation,Quantitative reasoning",
                "organization": "Microsoft",
                "authors": "Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, S\u00e9bastien Bubeck, Martin Cai, Caio C\u00e9sar Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, Xiren Zhou",
                "publication_date": "2024-04-23",
                "reference": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone",
                "link": "https://arxiv.org/abs/2404.14219",
                "citations": null,
                "notability_criteria": null,
                "parameters": 60800000000.0,
                "training_compute_(flop)": 3.0202896e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 552.0,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 512.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 708388.2341861207,
                "training_compute_estimation_method": "Hardware,Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 10.783903579272735,
                "log_compute": 23.480048587212767,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "SaProt",
                "domain": "Biology",
                "task": "Protein or nucleotide language model (pLM/nLM)",
                "organization": "Zhejiang University (ZJU),Westlake University",
                "authors": "Jin Su, Chenchen Han, Yuyang Zhou, Junjie Shan, Xibin Zhou, Fajie Yuan",
                "publication_date": "2024-04-19",
                "reference": "SaProt: Protein Language Modeling with Structure-aware Vocabulary",
                "link": "https://www.biorxiv.org/content/10.1101/2023.10.01.560349v5.abstract",
                "citations": 183.0,
                "notability_criteria": null,
                "parameters": 650000000.0,
                "training_compute_(flop)": 6.21084672e+22,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 2160.0,
                "training_hardware": "NVIDIA A100",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unknown",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 64.0,
                "last_modified": "2025-10-16 13:09:29+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 50603.66703135999,
                "training_compute_estimation_method": "Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Biology",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 8.812913356642856,
                "log_compute": 22.79315081124851,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Llama 3-70B",
                "domain": "Language",
                "task": "Chat,Language modeling/generation,Code generation",
                "organization": "Meta AI",
                "authors": "Aaditya Singh; Aaron Grattafiori; Abhimanyu Dubey; Abhinav Jauhri; Abhinav Pandey; Abhishek Kadian; Adam Kelsey; Adi Gangidi; Ahmad Al-Dahle; Amit Sangani; Ahuva Goldstand; Aiesha Letman; Ajay Menon; Akhil Mathur; Alan Schelten; Alex Vaughan; Amy Yang; Andrei Lupu; Andres Alvarado; Andrew Gallagher; Andrew Gu; Andrew Ho; Andrew Poulton; Andrew Ryan; Angela Fan; Ankit Ramchandani; Anthony Hartshorn; Archi Mitra; Archie Sravankumar; Artem Korenev; Arun Rao; Ashley Gabriel; Ashwin Bharambe; Assaf Eisenman; Aston Zhang; Ash JJhaveri; Aurelien Rodriguez; Austen Gregerson; Ava Spataru; Baptiste Roziere; Ben Maurer; Benjamin Leonhardi; Bernie Huang; Bhargavi Paranjape; Bing Liu; Binh Tang; Bobbie Chern; Brani Stojkovic; Brian Fuller; Catalina Mejia Arenas; Chao Zhou; Charlotte Caucheteux; Chaya Nayak; Ching-Hsiang Chu; Chloe Bi; Chris Cai; Chris Cox; Chris Marra; Chris McConnell; Christian Keller; Christoph Feichtenhofer; Christophe Touret; Chunyang Wu; Corinne Wong; Cristian Canton Ferrer; Damien Allonsius; Daniel Kreymer; Daniel Haziza; Daniel Li; Danielle Pintz; Danny Livshits; Danny Wyatt; David Adkins; David Esiobu; David Xu; Davide Testuggine; Delia David; Devi Parikh; Dhruv Choudhary; Dhruv Mahajan; Diana Liskovich; Diego Garcia-Olano; Diego Perino; Dieuwke Hupkes; Dingkang Wang; Dustin Holland; Egor Lakomkin; Elina Lobanova; Xiaoqing Ellen Tan; Emily Dinan; Eric Smith; Erik Brinkman; Esteban Arcaute; Filip Radenovic; Firat Ozgenel; Francesco Caggioni; Frank Seide; Frank Zhang; Gabriel Synnaeve; Gabriella Schwarz; Gabrielle Lee; Gada Badeer; Georgia Anderson; Graeme Nail; Gregoire Mialon; Guan Pang; Guillem Cucurell; Hailey Nguyen; Hamid Shojanazeri; Hannah Korevaar; Hannah Wang; Haroun Habeeb; Harrison Rudolph; Henry Aspegren; Hu Xu; Hugo Touvron; Iga Kozlowska; Igor Molybog; Igor Tufanov; Iliyan Zarov; Imanol Arrieta Ibarra; Irina-Elena Veliche; Isabel Kloumann; Ishan Misra; Ivan Evtimov; Jacob Xu; Jade Copet; Jake Weissman; Jan Geffert; Jana Vranes; Japhet Asher; Jason Park; Jay Mahadeokar; Jean-Baptiste Gaya; Jeet Shah; Jelmer van der Linde; Jennifer Chan; Jenny Hong; Jenya Lee; Jeremy Fu; Jeremy Teboul; Jianfeng Chi; Jianyu Huang; Jie Wang; Jiecao Yu; Joanna Bitton; Joe Spisak; Joelle Pineau; Jon Carvill; Jongsoo Park; Joseph Rocca; Joshua Johnstun; Junteng Jia; Kalyan Vasuden Alwala; Kam Hou U; Kate Plawiak; Kartikeya Upasani; Kaushik Veeraraghavan; Ke Li; Kenneth Heafield; Kevin Stone; Khalid El-Arini; Krithika Iyer; Kshitiz Malik; Kuenley Chiu; Kunal Bhalla; Kyle Huang; Lakshya Garg; Lauren Rantala-Yeary; Laurens van der Maaten; Lawrence Chen; Leandro Silva; Lee Bell; Lei Zhang; Liang Tan; Louis Martin; Lovish Madaan; Luca Wehrstedt; Lukas Blecher; Luke de Oliveira; Madeline Muzzi; Madian Khabsa; Manav Avlani; Mannat Singh; Manohar Paluri; Mark Zuckerberg; Marcin Kardas; Martynas Mankus; Mathew Oldham; Mathieu Rita; Matthew Lennie; Maya Pavlova; Meghan Keneally; Melanie Kambadur; Mihir Patel; Mikayel Samvelyan; Mike Clark; Mike Lewis; Min Si; Mitesh Kumar Singh; Mo Metanat; Mona Hassan; Naman Goyal; Narjes Torabi; Nicolas Usunier; Nikolay Bashlykov; Nikolay Bogoychev; Niladri Chatterji; Ning Dong; Oliver Aobo Yang; Olivier Duchenne; Onur Celebi; Parth Parekh; Patrick Alrassy; Paul Saab; Pavan Balaji; Pedro Rittner; Pengchuan Zhang; Pengwei Li; Petar Vasic; Peter Weng; Polina Zvyagina; Prajjwal Bhargava; Pratik Dubal; Praveen Krishnan; Punit Singh Koura; Qing He; Rachel Rodriguez; Ragavan Srinivasan; Rahul Mitra; Ramon Calderer; Raymond Li; Robert Stojnic; Roberta Raileanu; Robin Battey; Rocky Wang; Rohit Girdhar; Rohit Patel; Romain Sauvestre; Ronnie Polidoro; Roshan Sumbaly; Ross Taylor; Ruan Silva; Rui Hou; Rui Wang; Russ Howes; Ruty Rinott; Saghar Hosseini; Sai Jayesh Bondu; Samyak Datta; Sanjay Singh; Sara Chugh; Sargun Dhillon; Satadru Pan; Sean Bell; Sergey Edunov; Shaoliang Nie; Sharan Narang; Sharath Raparthy; Shaun Lindsay; Sheng Feng; Sheng Shen; Shenghao Lin; Shiva Shankar; Shruti Bhosale; Shun Zhang; Simon Vandenhende; Sinong Wang; Seohyun Sonia Kim; Soumya Batra; Sten Sootla; Steve Kehoe; Suchin Gururangan; Sumit Gupta; Sunny Virk; Sydney Borodinsky; Tamar Glaser; Tamar Herman; Tamara Best; Tara Fowler; Thomas Georgiou; Thomas Scialom; Tianhe Li; Todor Mihaylov; Tong Xiao; Ujjwal Karn; Vedanuj Goswami; Vibhor Gupta; Vignesh Ramanathan; Viktor Kerkez; Vinay Satish Kumar; Vincent Gonguet; Vish Vogeti; Vlad Poenaru; Vlad Tiberiu Mihailescu; Vladan Petrovic; Vladimir Ivanov; Wei Li; Weiwei Chu; Wenhan Xiong; Wenyin Fu; Wes Bouaziz; Whitney Meers; Will Constable; Xavier Martinet; Xiaojian Wu; Xinbo Gao; Xinfeng Xie; Xuchao Jia; Yaelle Goldschlag; Yann LeCun; Yashesh Gaur; Yasmine Babaei; Ye Qi; Yenda Li; Yi Wen; Yiwen Song; Youngjin Nam; Yuchen Hao; Yuchen Zhang; Yun Wang; Yuning Mao; Yuzi He; Zacharie Delpierre Coudert; Zachary DeVito; Zahra Hankir; Zhaoduo Wen; Zheng Yan; Zhengxing Chen; Zhenyu Yang; Zoe Papakipos",
                "publication_date": "2024-04-18",
                "reference": "Introducing Meta Llama 3: The most capable openly available LLM to date",
                "link": "https://ai.meta.com/blog/meta-llama-3/",
                "citations": null,
                "notability_criteria": "Significant use",
                "parameters": 70000000000.0,
                "training_compute_(flop)": 7.861e+24,
                "training_dataset_size_(gradients)": 15000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": 6400000.0,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting,Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 10.845098040014257,
                "log_compute": 24.895477796275713,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "Llama 3-8B",
                "domain": "Language",
                "task": "Chat,Language modeling/generation,Code generation,Question answering",
                "organization": "Meta AI",
                "authors": "Aaditya Singh; Aaron Grattafiori; Abhimanyu Dubey; Abhinav Jauhri; Abhinav Pandey; Abhishek Kadian; Adam Kelsey; Adi Gangidi; Ahmad Al-Dahle; Amit Sangani; Ahuva Goldstand; Aiesha Letman; Ajay Menon; Akhil Mathur; Alan Schelten; Alex Vaughan; Amy Yang; Andrei Lupu; Andres Alvarado; Andrew Gallagher; Andrew Gu; Andrew Ho; Andrew Poulton; Andrew Ryan; Angela Fan; Ankit Ramchandani; Anthony Hartshorn; Archi Mitra; Archie Sravankumar; Artem Korenev; Arun Rao; Ashley Gabriel; Ashwin Bharambe; Assaf Eisenman; Aston Zhang; Ash JJhaveri; Aurelien Rodriguez; Austen Gregerson; Ava Spataru; Baptiste Roziere; Ben Maurer; Benjamin Leonhardi; Bernie Huang; Bhargavi Paranjape; Bing Liu; Binh Tang; Bobbie Chern; Brani Stojkovic; Brian Fuller; Catalina Mejia Arenas; Chao Zhou; Charlotte Caucheteux; Chaya Nayak; Ching-Hsiang Chu; Chloe Bi; Chris Cai; Chris Cox; Chris Marra; Chris McConnell; Christian Keller; Christoph Feichtenhofer; Christophe Touret; Chunyang Wu; Corinne Wong; Cristian Canton Ferrer; Damien Allonsius; Daniel Kreymer; Daniel Haziza; Daniel Li; Danielle Pintz; Danny Livshits; Danny Wyatt; David Adkins; David Esiobu; David Xu; Davide Testuggine; Delia David; Devi Parikh; Dhruv Choudhary; Dhruv Mahajan; Diana Liskovich; Diego Garcia-Olano; Diego Perino; Dieuwke Hupkes; Dingkang Wang; Dustin Holland; Egor Lakomkin; Elina Lobanova; Xiaoqing Ellen Tan; Emily Dinan; Eric Smith; Erik Brinkman; Esteban Arcaute; Filip Radenovic; Firat Ozgenel; Francesco Caggioni; Frank Seide; Frank Zhang; Gabriel Synnaeve; Gabriella Schwarz; Gabrielle Lee; Gada Badeer; Georgia Anderson; Graeme Nail; Gregoire Mialon; Guan Pang; Guillem Cucurell; Hailey Nguyen; Hamid Shojanazeri; Hannah Korevaar; Hannah Wang; Haroun Habeeb; Harrison Rudolph; Henry Aspegren; Hu Xu; Hugo Touvron; Iga Kozlowska; Igor Molybog; Igor Tufanov; Iliyan Zarov; Imanol Arrieta Ibarra; Irina-Elena Veliche; Isabel Kloumann; Ishan Misra; Ivan Evtimov; Jacob Xu; Jade Copet; Jake Weissman; Jan Geffert; Jana Vranes; Japhet Asher; Jason Park; Jay Mahadeokar; Jean-Baptiste Gaya; Jeet Shah; Jelmer van der Linde; Jennifer Chan; Jenny Hong; Jenya Lee; Jeremy Fu; Jeremy Teboul; Jianfeng Chi; Jianyu Huang; Jie Wang; Jiecao Yu; Joanna Bitton; Joe Spisak; Joelle Pineau; Jon Carvill; Jongsoo Park; Joseph Rocca; Joshua Johnstun; Junteng Jia; Kalyan Vasuden Alwala; Kam Hou U; Kate Plawiak; Kartikeya Upasani; Kaushik Veeraraghavan; Ke Li; Kenneth Heafield; Kevin Stone; Khalid El-Arini; Krithika Iyer; Kshitiz Malik; Kuenley Chiu; Kunal Bhalla; Kyle Huang; Lakshya Garg; Lauren Rantala-Yeary; Laurens van der Maaten; Lawrence Chen; Leandro Silva; Lee Bell; Lei Zhang; Liang Tan; Louis Martin; Lovish Madaan; Luca Wehrstedt; Lukas Blecher; Luke de Oliveira; Madeline Muzzi; Madian Khabsa; Manav Avlani; Mannat Singh; Manohar Paluri; Mark Zuckerberg; Marcin Kardas; Martynas Mankus; Mathew Oldham; Mathieu Rita; Matthew Lennie; Maya Pavlova; Meghan Keneally; Melanie Kambadur; Mihir Patel; Mikayel Samvelyan; Mike Clark; Mike Lewis; Min Si; Mitesh Kumar Singh; Mo Metanat; Mona Hassan; Naman Goyal; Narjes Torabi; Nicolas Usunier; Nikolay Bashlykov; Nikolay Bogoychev; Niladri Chatterji; Ning Dong; Oliver Aobo Yang; Olivier Duchenne; Onur Celebi; Parth Parekh; Patrick Alrassy; Paul Saab; Pavan Balaji; Pedro Rittner; Pengchuan Zhang; Pengwei Li; Petar Vasic; Peter Weng; Polina Zvyagina; Prajjwal Bhargava; Pratik Dubal; Praveen Krishnan; Punit Singh Koura; Qing He; Rachel Rodriguez; Ragavan Srinivasan; Rahul Mitra; Ramon Calderer; Raymond Li; Robert Stojnic; Roberta Raileanu; Robin Battey; Rocky Wang; Rohit Girdhar; Rohit Patel; Romain Sauvestre; Ronnie Polidoro; Roshan Sumbaly; Ross Taylor; Ruan Silva; Rui Hou; Rui Wang; Russ Howes; Ruty Rinott; Saghar Hosseini; Sai Jayesh Bondu; Samyak Datta; Sanjay Singh; Sara Chugh; Sargun Dhillon; Satadru Pan; Sean Bell; Sergey Edunov; Shaoliang Nie; Sharan Narang; Sharath Raparthy; Shaun Lindsay; Sheng Feng; Sheng Shen; Shenghao Lin; Shiva Shankar; Shruti Bhosale; Shun Zhang; Simon Vandenhende; Sinong Wang; Seohyun Sonia Kim; Soumya Batra; Sten Sootla; Steve Kehoe; Suchin Gururangan; Sumit Gupta; Sunny Virk; Sydney Borodinsky; Tamar Glaser; Tamar Herman; Tamara Best; Tara Fowler; Thomas Georgiou; Thomas Scialom; Tianhe Li; Todor Mihaylov; Tong Xiao; Ujjwal Karn; Vedanuj Goswami; Vibhor Gupta; Vignesh Ramanathan; Viktor Kerkez; Vinay Satish Kumar; Vincent Gonguet; Vish Vogeti; Vlad Poenaru; Vlad Tiberiu Mihailescu; Vladan Petrovic; Vladimir Ivanov; Wei Li; Weiwei Chu; Wenhan Xiong; Wenyin Fu; Wes Bouaziz; Whitney Meers; Will Constable; Xavier Martinet; Xiaojian Wu; Xinbo Gao; Xinfeng Xie; Xuchao Jia; Yaelle Goldschlag; Yann LeCun; Yashesh Gaur; Yasmine Babaei; Ye Qi; Yenda Li; Yi Wen; Yiwen Song; Youngjin Nam; Yuchen Hao; Yuchen Zhang; Yun Wang; Yuning Mao; Yuzi He; Zacharie Delpierre Coudert; Zachary DeVito; Zahra Hankir; Zhaoduo Wen; Zheng Yan; Zhengxing Chen; Zhenyu Yang; Zoe Papakipos",
                "publication_date": "2024-04-18",
                "reference": "Introducing Meta Llama 3: The most capable openly available LLM to date",
                "link": "https://ai.meta.com/blog/meta-llama-3/",
                "citations": null,
                "notability_criteria": "Significant use",
                "parameters": 8000000000.0,
                "training_compute_(flop)": 7.2e+23,
                "training_dataset_size_(gradients)": 15000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": 1300000.0,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.903089986991944,
                "log_compute": 23.85733249643127,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "Reka Edge",
                "domain": "Multimodal,Language,Vision,Video,Speech",
                "task": "Chat,Language modeling/generation,Image captioning,Code generation,Code autocompletion,Question answering,Visual question answering,Video description,Speech recognition (ASR),Speech-to-text",
                "organization": "Reka AI",
                "authors": "Aitor Ormazabal, Che Zheng, Cyprien de Masson d'Autume, Dani Yogatama, Deyu Fu, Donovan Ong, Eric Chen, Eugenie Lamprecht, Hai Pham, Isaac Ong, Kaloyan Aleksiev, Lei Li, Matthew Henderson, Max Bain, Mikel Artetxe, Nishant Relan, Piotr Padlewski, Qi Liu, Ren Chen, Samuel Phua, Yazheng Yang, Yi Tay, Yuqi Wang, Zhongkai Zhu, Zhihui Xie",
                "publication_date": "2024-04-18",
                "reference": "Reka Core, Flash, and Edge: A Series of Powerful Multimodal Language Models",
                "link": "https://arxiv.org/abs/2404.12387",
                "citations": null,
                "notability_criteria": null,
                "parameters": 7000000000.0,
                "training_compute_(flop)": 1.89e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100,NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "API access",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": null,
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.845098040014257,
                "log_compute": 23.276461804173245,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "Mixtral 8x22B",
                "domain": "Language",
                "task": "Language modeling/generation,Code generation,Translation,Quantitative reasoning,Question answering",
                "organization": "Mistral AI",
                "authors": "Albert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Baptiste Bout, Baudouin de Monicault,Blanche Savary, Bam4d, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Jean-Malo Delignon, Jia Li, Justus Murke, Louis Martin, Louis Ternon, Lucile Saulnier, L\u00e9lio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Nicolas Schuhl, Patrick von Platen, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibaut Lavril, Timoth\u00e9e Lacroix, Th\u00e9ophile Gervet, Thomas Wang, Valera Nemychnikova, William El Sayed, William Marshall",
                "publication_date": "2024-04-17",
                "reference": "Mixtral 8x22B",
                "link": "https://mistral.ai/news/mixtral-8x22b/",
                "citations": null,
                "notability_criteria": null,
                "parameters": 141000000000.0,
                "training_compute_(flop)": 2.34e+24,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": "Self-supervised learning",
                "confidence": "Speculative",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "France",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "France",
                "domain_group": "Language",
                "export_controls_sum": 10.0,
                "publication_count": 2.4529600742986126,
                "log_params": 11.14921911265538,
                "log_compute": 24.369215857410143,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "Reka Core",
                "domain": "Multimodal,Language,Vision,Video,Speech",
                "task": "Chat,Language modeling/generation,Image captioning,Code generation,Code autocompletion,Question answering,Visual question answering,Video description,Speech recognition (ASR),Speech-to-text,Quantitative reasoning",
                "organization": "Reka AI",
                "authors": "Aitor Ormazabal, Che Zheng, Cyprien de Masson d'Autume, Dani Yogatama, Deyu Fu, Donovan Ong, Eric Chen, Eugenie Lamprecht, Hai Pham, Isaac Ong, Kaloyan Aleksiev, Lei Li, Matthew Henderson, Max Bain, Mikel Artetxe, Nishant Relan, Piotr Padlewski, Qi Liu, Ren Chen, Samuel Phua, Yazheng Yang, Yi Tay, Yuqi Wang, Zhongkai Zhu, Zhihui Xie",
                "publication_date": "2024-04-15",
                "reference": "Reka Core, Flash, and Edge: A Series of Powerful\nMultimodal Language Models",
                "link": "https://arxiv.org/abs/2404.12387",
                "citations": null,
                "notability_criteria": "Training cost",
                "parameters": 67000000000.0,
                "training_compute_(flop)": 8.400010000000001e+24,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100,NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Speculative",
                "epochs": null,
                "model_accessibility": "API access",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 10.826074802700827,
                "log_compute": 24.924279803078814,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "Reka Flash",
                "domain": "Multimodal,Language,Vision,Video,Speech",
                "task": "Chat,Language modeling/generation,Image captioning,Code generation,Code autocompletion,Question answering,Visual question answering,Video description,Speech recognition (ASR),Speech-to-text",
                "organization": "Reka AI",
                "authors": "Aitor Ormazabal, Che Zheng, Cyprien de Masson d'Autume, Dani Yogatama, Deyu Fu, Donovan Ong, Eric Chen, Eugenie Lamprecht, Hai Pham, Isaac Ong, Kaloyan Aleksiev, Lei Li, Matthew Henderson, Max Bain, Mikel Artetxe, Nishant Relan, Piotr Padlewski, Qi Liu, Ren Chen, Samuel Phua, Yazheng Yang, Yi Tay, Yuqi Wang, Zhongkai Zhu, Zhihui Xie",
                "publication_date": "2024-04-15",
                "reference": "Reka Core, Flash, and Edge: A Series of Powerful\nMultimodal Language Models",
                "link": "https://arxiv.org/abs/2404.12387",
                "citations": null,
                "notability_criteria": null,
                "parameters": 21000000000.0,
                "training_compute_(flop)": 6.3e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100,NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "API access",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware,Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 10.32221929473392,
                "log_compute": 23.799340549453582,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "HGRN2 3B",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering",
                "organization": "Shanghai AI Lab,Massachusetts Institute of Technology (MIT),Taptap",
                "authors": "Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, Yiran Zhong",
                "publication_date": "2024-04-11",
                "reference": "HGRN2: Gated Linear RNNs with State Expansion",
                "link": "https://arxiv.org/abs/2404.07904v1",
                "citations": null,
                "notability_criteria": null,
                "parameters": 2900000000.0,
                "training_compute_(flop)": 1.74e+21,
                "training_dataset_size_(gradients)": 100000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unknown",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 9.462397997898956,
                "log_compute": 21.2405492482826,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "HGRN2 1B",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering",
                "organization": "Shanghai AI Lab,Massachusetts Institute of Technology (MIT),Taptap",
                "authors": "Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, Yiran Zhong",
                "publication_date": "2024-04-11",
                "reference": "HGRN2: Gated Linear RNNs with State Expansion",
                "link": "https://arxiv.org/abs/2404.07904v1",
                "citations": null,
                "notability_criteria": null,
                "parameters": 1000000000.0,
                "training_compute_(flop)": 6.000000001e+21,
                "training_dataset_size_(gradients)": 100000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unknown",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 9.0,
                "log_compute": 21.778151250456027,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Stable LM 2 12B",
                "domain": "Language",
                "task": "Language modeling/generation,Translation",
                "organization": "Stability AI",
                "authors": "Unknown",
                "publication_date": "2024-04-08",
                "reference": "Introducing Stable LM 2 12B",
                "link": "https://stability.ai/news/introducing-stable-lm-2-12b\nhttps://huggingface.co/stabilityai/stablelm-2-12b",
                "citations": null,
                "notability_criteria": null,
                "parameters": 12143605760.0,
                "training_compute_(flop)": 2.91e+23,
                "training_dataset_size_(gradients)": 2000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": "Self-supervised learning",
                "confidence": "Confident",
                "epochs": 2.0,
                "model_accessibility": "Open weights (restricted use)",
                "country": "United Kingdom",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United Kingdom",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 2.8041950763331074,
                "log_params": 10.084347659487195,
                "log_compute": 23.463892988985908,
                "pub_bin": "Low",
                "export_bin": "Low"
            },
            {
                "model": "SambaLingo-Thai-Chat (7B)",
                "domain": "Language",
                "task": "Chat",
                "organization": "SambaNova Systems, Inc",
                "authors": "Zoltan Csaki, Bo Li, Jonathan Li, Qiantong Xu, Pian Pawakapan, Leon Zhang, Yun Du, Hengyu Zhao, Changran Hu, Urmish Thakker",
                "publication_date": "2024-04-08",
                "reference": "SambaLingo: Teaching Large Language Models New Languages",
                "link": "https://arxiv.org/abs/2404.05829",
                "citations": null,
                "notability_criteria": null,
                "parameters": 6950000000.0,
                "training_compute_(flop)": 8.569999999999999e+22,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": 4.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": "Llama 2-7B",
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 1024.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": null,
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.841984804590114,
                "log_compute": 22.9329808219232,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "SambaLingo-Thai-Chat-70B",
                "domain": "Language",
                "task": "Chat",
                "organization": "SambaNova Systems, Inc",
                "authors": "Zoltan Csaki, Bo Li, Jonathan Li, Qiantong Xu, Pian Pawakapan, Leon Zhang, Yun Du, Hengyu Zhao, Changran Hu, Urmish Thakker",
                "publication_date": "2024-04-08",
                "reference": "SambaLingo: Teaching Large Language Models New Languages",
                "link": "https://arxiv.org/abs/2404.05829",
                "citations": null,
                "notability_criteria": null,
                "parameters": 70000000000.0,
                "training_compute_(flop)": 8.1168e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": 0.4,
                "model_accessibility": "Open weights (restricted use)",
                "country": "United States",
                "base_model": "Llama 2-70B",
                "finetune_compute_(flop)": 1.68e+21,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 32.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": null,
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 10.845098040014257,
                "log_compute": 23.909384844973328,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "ESM-AA",
                "domain": "Biology",
                "task": "Protein folding prediction,Protein or nucleotide language model (pLM/nLM),Proteins",
                "organization": "Peking University,Nanjing University,Tsinghua University,PharMolix",
                "authors": "Kangjie Zheng, Siyu Long, Tianyu Lu, Junwei Yang, Xinyu Dai, Ming Zhang, Zaiqing Nie, Wei-Ying Ma, Hao Zhou",
                "publication_date": "2024-04-05",
                "reference": "ESM All-Atom: Multi-scale Protein Language Model for Unified Molecular Modeling",
                "link": "https://arxiv.org/abs/2403.12995",
                "citations": 12.0,
                "notability_criteria": null,
                "parameters": 35000000.0,
                "training_compute_(flop)": 7.28e+20,
                "training_dataset_size_(gradients)": 1143750000.0,
                "training_time_(hours)": 72.0,
                "training_hardware": "NVIDIA A100",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unknown",
                "country": "China",
                "base_model": "ESM2-35M",
                "finetune_compute_(flop)": null,
                "hardware_quantity": 16.0,
                "last_modified": "2025-10-14 18:13:47+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 12654.861564218108,
                "training_compute_estimation_method": "Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Biology",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 7.544068044350276,
                "log_compute": 20.862131379313038,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Mixture-of-Depths",
                "domain": "Language",
                "task": "Language modeling/generation",
                "organization": "Google DeepMind,McGill University,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms)",
                "authors": "David Raposo, Sam Ritter, Blake Richards, Timothy Lillicrap, Peter Conway Humphreys, Adam Santoro",
                "publication_date": "2024-04-02",
                "reference": "Mixture-of-Depths: Dynamically allocating compute in transformer-based language models",
                "link": "https://arxiv.org/abs/2404.02258",
                "citations": 112.0,
                "notability_criteria": null,
                "parameters": 3000000000.0,
                "training_compute_(flop)": 1e+20,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:09:29+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Reported",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.477121254719663,
                "log_compute": 20.0,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "MobileCLIP-B (LT)",
                "domain": "Vision",
                "task": "Image captioning,Image classification",
                "organization": "Apple",
                "authors": "Pavan Kumar Anasosalu Vasu, Hadi Pouransari, Fartash Faghri, Raviteja Vemulapalli, Oncel Tuzel",
                "publication_date": "2024-04-01",
                "reference": "MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training",
                "link": "https://arxiv.org/abs/2311.17049",
                "citations": null,
                "notability_criteria": null,
                "parameters": 149700000.0,
                "training_compute_(flop)": 1.3423599e+22,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100 SXM4 80 GB",
                "approach": null,
                "confidence": "Likely",
                "epochs": 30.5,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 256.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 202495.82204270296,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Vision",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 8.175221800343053,
                "log_compute": 22.127868970101996,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "TeleChat-7B",
                "domain": "Language",
                "task": "Language modeling/generation,Chat,Question answering,Text summarization,Code generation,Translation",
                "organization": "China Telecom",
                "authors": "Zhongjiang He, Zihan Wang, Xinzhang Liu, Shixuan Liu, Yitong Yao, Yuyao Huang, Xuelong Li, Yongxiang Li, Zhonghao Che, Zhaoxi Zhang, Yan Wang, Xin Wang, Luwen Pu, Huinan Xu, Ruiyu Fang, Yu Zhao, Jie Zhang, Xiaomeng Huang, Zhilong Lu, Jiaxin Peng, Wenjun Zheng, Shiquan Wang, Bingkai Yang, Xuewei he, Zhuoru Jiang, Qiyi Xie, Yanhan Zhang, Zhongqiu Li, Lingling Shi, Weiwei Fu, Yin Zhang, Zilu Huang, Sishi Xiong, Yuxiang Zhang, Chao Wang, Shuangyong Song",
                "publication_date": "2024-04-01",
                "reference": "TELECHAT TECHNICAL REPORT",
                "link": "https://arxiv.org/abs/2401.03804",
                "citations": null,
                "notability_criteria": null,
                "parameters": 7000000000.0,
                "training_compute_(flop)": 4.2e+22,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100 SXM4 40 GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 640.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open (restricted use)",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 506239.5551067574,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 9.845098040014257,
                "log_compute": 22.6232492903979,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "TeleChat-3B",
                "domain": "Language",
                "task": "Language modeling/generation,Chat,Question answering,Text summarization,Code generation,Translation",
                "organization": "China Telecom",
                "authors": "Zhongjiang He, Zihan Wang, Xinzhang Liu, Shixuan Liu, Yitong Yao, Yuyao Huang, Xuelong Li, Yongxiang Li, Zhonghao Che, Zhaoxi Zhang, Yan Wang, Xin Wang, Luwen Pu, Huinan Xu, Ruiyu Fang, Yu Zhao, Jie Zhang, Xiaomeng Huang, Zhilong Lu, Jiaxin Peng, Wenjun Zheng, Shiquan Wang, Bingkai Yang, Xuewei he, Zhuoru Jiang, Qiyi Xie, Yanhan Zhang, Zhongqiu Li, Lingling Shi, Weiwei Fu, Yin Zhang, Zilu Huang, Sishi Xiong, Yuxiang Zhang, Chao Wang, Shuangyong Song",
                "publication_date": "2024-04-01",
                "reference": "TELECHAT TECHNICAL REPORT",
                "link": "https://arxiv.org/abs/2401.03804",
                "citations": null,
                "notability_criteria": null,
                "parameters": 3000000000.0,
                "training_compute_(flop)": 1.44e+22,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100 SXM4 40 GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 640.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 506239.5551067574,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 9.477121254719663,
                "log_compute": 22.15836249209525,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "TeleChat-12B",
                "domain": "Language",
                "task": "Language modeling/generation,Chat,Question answering,Text summarization,Code generation,Translation",
                "organization": "China Telecom",
                "authors": "Zhongjiang He, Zihan Wang, Xinzhang Liu, Shixuan Liu, Yitong Yao, Yuyao Huang, Xuelong Li, Yongxiang Li, Zhonghao Che, Zhaoxi Zhang, Yan Wang, Xin Wang, Luwen Pu, Huinan Xu, Ruiyu Fang, Yu Zhao, Jie Zhang, Xiaomeng Huang, Zhilong Lu, Jiaxin Peng, Wenjun Zheng, Shiquan Wang, Bingkai Yang, Xuewei he, Zhuoru Jiang, Qiyi Xie, Yanhan Zhang, Zhongqiu Li, Lingling Shi, Weiwei Fu, Yin Zhang, Zilu Huang, Sishi Xiong, Yuxiang Zhang, Chao Wang, Shuangyong Song",
                "publication_date": "2024-04-01",
                "reference": "TELECHAT TECHNICAL REPORT",
                "link": "https://arxiv.org/abs/2401.03804",
                "citations": 10.0,
                "notability_criteria": null,
                "parameters": 12000000000.0,
                "training_compute_(flop)": 8.64e+22,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100 SXM4 40 GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 640.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open (restricted use)",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 506239.5551067574,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 10.079181246047625,
                "log_compute": 22.936513742478894,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "DBRX",
                "domain": "Language",
                "task": "Chat,Code generation",
                "organization": "Databricks",
                "authors": "Mosaic Research Team",
                "publication_date": "2024-03-27",
                "reference": "Introducing DBRX: A New State-of-the-Art Open LLM",
                "link": "https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm",
                "citations": null,
                "notability_criteria": "Training cost",
                "parameters": 132000000000.0,
                "training_compute_(flop)": 2.6e+24,
                "training_dataset_size_(gradients)": 12000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Open weights (restricted use)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 11.12057393120585,
                "log_compute": 24.414973347970818,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "ProstT5",
                "domain": "Biology",
                "task": "Protein or nucleotide language model (pLM/nLM)",
                "organization": "Technical University of Munich,Seoul National University,Institute for Advanced Study,TUM School of Life Sciences Weihenstephan",
                "authors": "Michael Heinzinger, Konstantin Weissenow, Joaquin Gomez Sanchez, Adrian Henkel, Milot Mirdita, Martin Steinegger, Burkhard Rost",
                "publication_date": "2024-03-24",
                "reference": "Bilingual Language Model for Protein Sequence and Structure",
                "link": "https://www.biorxiv.org/content/10.1101/2023.07.23.550085v2.abstract",
                "citations": 165.0,
                "notability_criteria": null,
                "parameters": 3000000000.0,
                "training_compute_(flop)": 3.09999999999999e+21,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 864.0,
                "training_hardware": "NVIDIA A100",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unknown",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 8.0,
                "last_modified": "2025-10-16 13:09:29+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 6329.121902923846,
                "training_compute_estimation_method": "Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "Germany",
                "domain_group": "Biology",
                "export_controls_sum": 10.0,
                "publication_count": 2.7830121675064055,
                "log_params": 9.477121254719663,
                "log_compute": 21.49136169383427,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "ERNIE-RNA",
                "domain": "Biology",
                "task": "Protein or nucleotide language model (pLM/nLM)",
                "organization": "Microsoft Research,Syngentech,Tsinghua University",
                "authors": "Weijie Yin, Zhaoyu Zhang, Liang He, Rui Jiang, Shuo Zhang, Gan Liu, Xuegong Zhang, Tao Qin, Zhen Xie",
                "publication_date": "2024-03-17",
                "reference": "ERNIE-RNA: An RNA Language Model with Structure-enhanced Representations",
                "link": "https://www.biorxiv.org/content/10.1101/2024.03.17.585376v1.abstract",
                "citations": 9.0,
                "notability_criteria": null,
                "parameters": 86000000.0,
                "training_compute_(flop)": 2.1000000000000013e+21,
                "training_dataset_size_(gradients)": 918000000.0,
                "training_time_(hours)": 480.0,
                "training_hardware": "NVIDIA Tesla V100 DGXS 32 GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unknown",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 24.0,
                "last_modified": "2025-10-14 21:54:47+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 11868.95362272414,
                "training_compute_estimation_method": "Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Biology",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 7.9344984512435675,
                "log_compute": 21.32221929473392,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "MM1-30B",
                "domain": "Multimodal,Language,Vision",
                "task": "Chat,Image captioning,Visual question answering",
                "organization": "Apple",
                "authors": "Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, Anton Belyi, Haotian Zhang, Karanjeet Singh, Doug Kang, Ankur Jain, Hongyu H\u00e8, Max Schwarzer, Tom Gunter, Xiang Kong, Aonan Zhang, Jianyu Wang, Chong Wang, Nan Du, Tao Lei, Sam Wiseman, Guoli Yin, Mark Lee, Zirui Wang, Ruoming Pang, Peter Grasch, Alexander Toshev, Yinfei Yang",
                "publication_date": "2024-03-14",
                "reference": "MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training",
                "link": "https://arxiv.org/abs/2403.09611",
                "citations": 230.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 30000000000.0,
                "training_compute_(flop)": 4.86e+23,
                "training_dataset_size_(gradients)": 186800000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:09:29+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 10.477121254719663,
                "log_compute": 23.686636269262294,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "RFM-1",
                "domain": "Robotics,Vision,Video",
                "task": "Robotic manipulation,Image captioning,Video description",
                "organization": "Covariant",
                "authors": "Andrew Sohn, Anusha Nagabandi, Carlos Florensa, Daniel Adelberg, Di Wu, Hassan Farooq, Ignasi Clavera, Jeremy Welborn, Juyue Chen, Nikhil Mishra, Peter Chen, Peter Qian, Pieter Abbeel, Rocky Duan, Varun Vijay, Yang Liu",
                "publication_date": "2024-03-11",
                "reference": "Introducing RFM-1: Giving robots human-like reasoning capabilities",
                "link": "https://covariant.ai/insights/introducing-rfm-1-giving-robots-human-like-reasoning-capabilities/",
                "citations": null,
                "notability_criteria": null,
                "parameters": 8000000000.0,
                "training_compute_(flop)": 2.4e+20,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.903089986991944,
                "log_compute": 20.380211241711606,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "DeepSeek-VL-7B",
                "domain": "Multimodal,Vision,Language",
                "task": "Character recognition (OCR),Language modeling/generation,Visual question answering,Question answering",
                "organization": "DeepSeek",
                "authors": "Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, Yaofeng Sun, Chengqi Deng, Hanwei Xu, Zhenda Xie, Chong Ruan",
                "publication_date": "2024-03-08",
                "reference": "DeepSeek-VL: Towards Real-World Vision-Language Understanding",
                "link": "https://arxiv.org/abs/2403.05525",
                "citations": 512.0,
                "notability_criteria": null,
                "parameters": 7000000000.0,
                "training_compute_(flop)": 1.0264959e+23,
                "training_dataset_size_(gradients)": 400000000.0,
                "training_time_(hours)": 120.0,
                "training_hardware": "NVIDIA A100",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "China",
                "base_model": "DeepSeek LLM 7B",
                "finetune_compute_(flop)": 1.8649595e+22,
                "hardware_quantity": 512.0,
                "last_modified": "2025-10-16 13:09:29+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 405208.1557620524,
                "training_compute_estimation_method": "Operation counting,Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Multimodal",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 9.845098040014257,
                "log_compute": 23.011357219063715,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "DeepSeek-VL-1.3B",
                "domain": "Multimodal,Vision,Language",
                "task": "Character recognition (OCR),Language modeling/generation,Visual question answering,Question answering",
                "organization": "DeepSeek",
                "authors": "Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, Yaofeng Sun, Chengqi Deng, Hanwei Xu, Zhenda Xie, Chong Ruan",
                "publication_date": "2024-03-08",
                "reference": "DeepSeek-VL: Towards Real-World Vision-Language Understanding",
                "link": "https://arxiv.org/abs/2403.05525",
                "citations": 512.0,
                "notability_criteria": null,
                "parameters": 1300000000.0,
                "training_compute_(flop)": 8.6547326e+21,
                "training_dataset_size_(gradients)": 400000000.0,
                "training_time_(hours)": 168.0,
                "training_hardware": "NVIDIA A100",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "China",
                "base_model": "DeepSeek-LLM-1.3b-base",
                "finetune_compute_(flop)": 4.7547326e+21,
                "hardware_quantity": 128.0,
                "last_modified": "2025-10-16 13:09:29+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 101302.0389405131,
                "training_compute_estimation_method": "Operation counting,Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Multimodal",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 9.113943352306837,
                "log_compute": 21.937253654286692,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Griffin",
                "domain": "Language",
                "task": "Language modeling/generation,Chat",
                "organization": "Google DeepMind",
                "authors": "Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, Caglar Gulcehre",
                "publication_date": "2024-02-29",
                "reference": "Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models",
                "link": "https://arxiv.org/abs/2402.19427",
                "citations": null,
                "notability_criteria": null,
                "parameters": 14000000000.0,
                "training_compute_(flop)": 1.5848931924611137e+22,
                "training_dataset_size_(gradients)": 300000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "Google TPU v3",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Reported",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 10.146128035678238,
                "log_compute": 22.2,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "Hawk",
                "domain": "Language",
                "task": "Language modeling/generation,Chat",
                "organization": "Google DeepMind",
                "authors": "Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, Caglar Gulcehre",
                "publication_date": "2024-02-29",
                "reference": "Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models",
                "link": "https://arxiv.org/abs/2402.19427",
                "citations": null,
                "notability_criteria": null,
                "parameters": 7000000000.0,
                "training_compute_(flop)": 3.95e+21,
                "training_dataset_size_(gradients)": 300000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "Google TPU v3",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Reported",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.845098040014257,
                "log_compute": 21.59659709562646,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "Evo",
                "domain": "Biology",
                "task": "Protein or nucleotide language model (pLM/nLM)",
                "organization": "Stanford University,University of California (UC) Berkeley,Together",
                "authors": "Eric Nguyen, Michael Poli, Matthew G. Durrant, Armin W. Thomas, Brian Kang, Jeremy Sullivan, Madelena Y. Ng, Ashley Lewis, Aman Patel, Aaron Lou, Stefano Ermon, Stephen A. Baccus, Tina Hernandez-Boussard, Christopher R\u00e9, Patrick D. Hsu, Brian L. Hie",
                "publication_date": "2024-02-27",
                "reference": "Sequence modeling and design from molecular to genome scale with Evo",
                "link": "https://arcinstitute.org/news/blog/evo\nhttps://www.biorxiv.org/content/10.1101/2024.02.27.582234v1",
                "citations": 172.0,
                "notability_criteria": null,
                "parameters": 7000000000.0,
                "training_compute_(flop)": 2.0000000001e+22,
                "training_dataset_size_(gradients)": 300000000000.0,
                "training_time_(hours)": 2968.0,
                "training_hardware": "NVIDIA A100,NVIDIA H100 SXM5 80GB",
                "approach": "Self-supervised learning",
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 21:54:47+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Reported",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Biology",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.845098040014257,
                "log_compute": 22.301029995685695,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "BitNet b1.58",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering",
                "organization": "University of Chinese Academy of Sciences,Microsoft Research",
                "authors": "Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Li Dong, Ruiping Wang, Jilong Xue, Furu Wei",
                "publication_date": "2024-02-27",
                "reference": "The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits",
                "link": "https://arxiv.org/abs/2402.17764",
                "citations": null,
                "notability_criteria": null,
                "parameters": 70000000000.0,
                "training_compute_(flop)": 2.8735486e+22,
                "training_dataset_size_(gradients)": 100000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unknown",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting,Comparison with other models",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 10.845098040014257,
                "log_compute": 22.458418546704912,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Nemotron-4 15B",
                "domain": "Language",
                "task": "Language modeling/generation,Code generation,Question answering,Translation,Quantitative reasoning",
                "organization": "NVIDIA",
                "authors": "Jupinder Parmar, Shrimai Prabhumoye, Joseph Jennings, Mostofa Patwary, Sandeep Subramanian, Dan Su, Chen Zhu, Deepak Narayanan, Aastha Jhunjhunwala, Ayush Dattagupta, Vibhu Jawa, Jiwei Liu, Ameya Mahabaleshwarkar, Osvald Nitski, Annika Brundyn, James Maki, Miguel Martinez, Jiaxuan You, John Kamalu, Patrick LeGresley, Denys Fridman, Jared Casper, Ashwath Aithal, Oleksii Kuchaiev, Mohammad Shoeybi, Jonathan Cohen, Bryan Catanzaro",
                "publication_date": "2024-02-27",
                "reference": "Nemotron-4 15B Technical Report",
                "link": "https://arxiv.org/abs/2402.16819",
                "citations": null,
                "notability_criteria": null,
                "parameters": 15000000000.0,
                "training_compute_(flop)": 7.5005116e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 312.0,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 3072.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 4255633.2327503795,
                "training_compute_estimation_method": "Operation counting,Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 10.176091259055681,
                "log_compute": 23.875090887055606,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "ProLLaMA",
                "domain": "Biology",
                "task": "Protein question answering",
                "organization": "Peking University,Peng Cheng Laboratory",
                "authors": "Liuzhenghao Lv, Zongying Lin, Hao Li, Yuyang Liu, Jiaxi Cui, Calvin Yu-Chian Chen, Li Yuan, Yonghong Tian",
                "publication_date": "2024-02-26",
                "reference": "ProLLaMA: A Protein Language Model for Multi-Task Protein Language Processing",
                "link": "https://arxiv.org/abs/2402.16445",
                "citations": 57.0,
                "notability_criteria": null,
                "parameters": 7000000000.0,
                "training_compute_(flop)": 8.412e+22,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 264.0,
                "training_hardware": "NVIDIA RTX A6000",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unknown",
                "country": "China",
                "base_model": "Llama 2-7B",
                "finetune_compute_(flop)": null,
                "hardware_quantity": 8.0,
                "last_modified": "2025-10-14 18:07:57+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 4749.696433127339,
                "training_compute_estimation_method": "Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Biology",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 9.845098040014257,
                "log_compute": 22.924899264014282,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Gemma 1.1 7B Instruct",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering",
                "organization": "Google",
                "authors": "Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Laurent Sifre, Morgane Rivi\u00e8re, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, L\u00e9onard Hussenot and et al.",
                "publication_date": "2024-02-24",
                "reference": null,
                "link": "https://huggingface.co/google/gemma-1.1-7b-it",
                "citations": null,
                "notability_criteria": null,
                "parameters": 8540000000.0,
                "training_compute_(flop)": 3.0744e+23,
                "training_dataset_size_(gradients)": 6000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "Google TPU v5e",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-08-05 19:36:12+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.931457870689005,
                "log_compute": 23.487760371456293,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "MegaScale (175B)",
                "domain": "Language",
                "task": "Language modeling/generation",
                "organization": "ByteDance,Peking University",
                "authors": "Ziheng Jiang, Haibin Lin, Yinmin Zhong, Qi Huang, Yangrui Chen, Zhi Zhang, Yanghua Peng, Xiang Li, Cong Xie, Shibiao Nong, Yulu Jia, Sun He, Hongmin Chen, Zhihao Bai, Qi Hou, Shipeng Yan, Ding Zhou, Yiyao Sheng, Zhuo Jiang, Haohan Xu, Haoran Wei, Zhang Zhang, Pengfei Nie, Leqi Zou, Sida Zhao, Liang Xiang, Zherui Liu, Zhe Li, Xiaoying Jia, Jianxi Ye, Xin Jin, Xin Liu",
                "publication_date": "2024-02-23",
                "reference": "MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs",
                "link": "https://arxiv.org/abs/2402.15627",
                "citations": 189.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 175000000000.0,
                "training_compute_(flop)": 2.7385671436e+23,
                "training_dataset_size_(gradients)": 300000000000.0,
                "training_time_(hours)": 42.0,
                "training_hardware": "NVIDIA A100",
                "approach": "Self-supervised learning",
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Unreleased",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 12288.0,
                "last_modified": "2025-10-16 13:09:06+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 9728028.18454986,
                "training_compute_estimation_method": "Operation counting,Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 11.243038048686294,
                "log_compute": 23.437523393334004,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "MegaScale (530B)",
                "domain": "Language",
                "task": "Language modeling/generation",
                "organization": "ByteDance,Peking University",
                "authors": "Ziheng Jiang, Haibin Lin, Yinmin Zhong, Qi Huang, Yangrui Chen, Zhi Zhang, Yanghua Peng, Xiang Li, Cong Xie, Shibiao Nong, Yulu Jia, Sun He, Hongmin Chen, Zhihao Bai, Qi Hou, Shipeng Yan, Ding Zhou, Yiyao Sheng, Zhuo Jiang, Haohan Xu, Haoran Wei, Zhang Zhang, Pengfei Nie, Leqi Zou, Sida Zhao, Liang Xiang, Zherui Liu, Zhe Li, Xiaoying Jia, Jianxi Ye, Xin Jin, Xin Liu",
                "publication_date": "2024-02-23",
                "reference": "MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs",
                "link": "https://arxiv.org/abs/2402.15627",
                "citations": 189.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 530000000000.0,
                "training_compute_(flop)": 9.6910000000001e+23,
                "training_dataset_size_(gradients)": 300000000000.0,
                "training_time_(hours)": 117.9,
                "training_hardware": "NVIDIA A100",
                "approach": "Self-supervised learning",
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Unreleased",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 11200.0,
                "last_modified": "2025-10-16 13:09:07+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 6144.0,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 8866692.355709508,
                "training_compute_estimation_method": "Comparison with other models",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 11.724275869600788,
                "log_compute": 23.986368593570276,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "MegaScale (Production)",
                "domain": "Language",
                "task": "Language modeling/generation",
                "organization": "ByteDance,Peking University",
                "authors": "Ziheng Jiang, Haibin Lin, Yinmin Zhong, Qi Huang, Yangrui Chen, Zhi Zhang, Yanghua Peng, Xiang Li, Cong Xie, Shibiao Nong, Yulu Jia, Sun He, Hongmin Chen, Zhihao Bai, Qi Hou, Shipeng Yan, Ding Zhou, Yiyao Sheng, Zhuo Jiang, Haohan Xu, Haoran Wei, Zhang Zhang, Pengfei Nie, Leqi Zou, Sida Zhao, Liang Xiang, Zherui Liu, Zhe Li, Xiaoying Jia, Jianxi Ye, Xin Jin, Xin Liu",
                "publication_date": "2024-02-23",
                "reference": "MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs",
                "link": "https://arxiv.org/abs/2402.15627",
                "citations": 189.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 530000000000.0,
                "training_compute_(flop)": 3.9e+24,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 504.0,
                "training_hardware": "NVIDIA A100",
                "approach": "Self-supervised learning",
                "confidence": "Speculative",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 12288.0,
                "last_modified": "2025-10-16 13:09:07+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": 2614019.245,
                "frontier_model": false,
                "training_power_draw_(w)": 9728028.18454986,
                "training_compute_estimation_method": "Other",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 11.724275869600788,
                "log_compute": 24.5910646070265,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Genie",
                "domain": "Video,Games",
                "task": "Video generation,Image-to-video,Text-to-video",
                "organization": "Google DeepMind",
                "authors": "Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, Yusuf Aytar, Sarah Bechtle, Feryal Behbahani, Stephanie Chan, Nicolas Heess, Lucy Gonzalez, Simon Osindero, Sherjil Ozair, Scott Reed, Jingwei Zhang, Konrad Zolna, Jeff Clune, Nando de Freitas, Satinder Singh, Tim Rockt\u00e4schel",
                "publication_date": "2024-02-23",
                "reference": "Genie: Generative Interactive Environments",
                "link": "https://arxiv.org/abs/2402.15391",
                "citations": null,
                "notability_criteria": null,
                "parameters": 10700000000.0,
                "training_compute_(flop)": 6.6e+22,
                "training_dataset_size_(gradients)": 942000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "Google TPU v5p",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 256.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 512.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Reported",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 10.02938377768521,
                "log_compute": 22.81954393554187,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "Stable Diffusion 3",
                "domain": "Image generation",
                "task": "Image generation,Text-to-image",
                "organization": "Stability AI",
                "authors": "Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M\u00fcller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, Robin Rombach",
                "publication_date": "2024-02-22",
                "reference": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
                "link": "https://arxiv.org/abs/2403.03206\nhttps://stability.ai/news/stable-diffusion-3",
                "citations": null,
                "notability_criteria": "SOTA improvement",
                "parameters": 8000000000.0,
                "training_compute_(flop)": 5e+22,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "API access",
                "country": "United Kingdom",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Reported",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United Kingdom",
                "domain_group": "Vision",
                "export_controls_sum": 1.0,
                "publication_count": 2.8041950763331074,
                "log_params": 9.903089986991944,
                "log_compute": 22.69897000433602,
                "pub_bin": "Low",
                "export_bin": "Low"
            },
            {
                "model": "Gemma 7B",
                "domain": "Language",
                "task": "Language modeling/generation,Chat,Code generation,Question answering,Quantitative reasoning",
                "organization": "Google DeepMind",
                "authors": "Gemma Team, Google DeepMind",
                "publication_date": "2024-02-21",
                "reference": "Gemma: Open Models Based on Gemini Research and Technology",
                "link": "https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf",
                "citations": null,
                "notability_criteria": null,
                "parameters": 8538074112.0,
                "training_compute_(flop)": 3.07e+23,
                "training_dataset_size_(gradients)": 6000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "Google TPU v5e",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 4096.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 1134987.171306518,
                "training_compute_estimation_method": "Operation counting,Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.93135992023754,
                "log_compute": 23.487138375477187,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "Gemma 2B",
                "domain": "Language",
                "task": "Language modeling/generation,Chat,Code generation,Question answering,Quantitative reasoning",
                "organization": "Google DeepMind",
                "authors": "Gemma Team, Google DeepMind",
                "publication_date": "2024-02-21",
                "reference": "Gemma: Open Models Based on Gemini Research and Technology",
                "link": "https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf",
                "citations": null,
                "notability_criteria": null,
                "parameters": 2506434560.0,
                "training_compute_(flop)": 4.5115822e+22,
                "training_dataset_size_(gradients)": 3000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "Google TPU v5e",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 512.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 141873.39641331477,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.399056370189088,
                "log_compute": 22.654328874522296,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "V-JEPA",
                "domain": "Vision",
                "task": "Representation learning",
                "organization": "Meta AI",
                "authors": "Adrien Bardes, Quentin Garrido, Jean Ponce, Xinlei Chen, Michael Rabbat, Yann LeCun, Mahmoud Assran, Nicolas Ballas",
                "publication_date": "2024-02-15",
                "reference": "Revisiting Feature Prediction for Learning Visual Representations from Video",
                "link": "https://ai.meta.com/research/publications/revisiting-feature-prediction-for-learning-visual-representations-from-video/",
                "citations": null,
                "notability_criteria": null,
                "parameters": 630000000.0,
                "training_compute_(flop)": 1.6387080192e+21,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 50.0,
                "training_hardware": "NVIDIA A100",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unknown",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": null,
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Vision",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 8.799340549453582,
                "log_compute": 21.214501578984443,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "PLAPT",
                "domain": "Biology",
                "task": "Protein-ligand binding affinity prediction",
                "organization": "Wolfram Research,ASC27,Newport High School,Sanskriti School",
                "authors": "Tyler Rose, Nicol\u00f2 Monti, Navvye Anand, Tianyu Shen",
                "publication_date": "2024-02-12",
                "reference": "PLAPT: Protein-Ligand Binding Affinity Prediction Using Pretrained Transformers",
                "link": "https://www.biorxiv.org/content/10.1101/2024.02.08.575577v3.abstract",
                "citations": 6.0,
                "notability_criteria": null,
                "parameters": 1474624.0,
                "training_compute_(flop)": 3.900846548437782e+22,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA GeForce RTX 4060 Ti",
                "approach": null,
                "confidence": "Confident",
                "epochs": 60.0,
                "model_accessibility": "Unknown",
                "country": "Multinational",
                "base_model": "ChemBERTa,ProtBERT-BFD",
                "finetune_compute_(flop)": null,
                "hardware_quantity": 1.0,
                "last_modified": "2025-10-14 21:50:39+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 174.03580522109968,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Biology",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 6.168681297912899,
                "log_compute": 22.591158866364726,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "Distilled Grandmaster",
                "domain": "Games",
                "task": "Chess",
                "organization": "DeepMind",
                "authors": "Anian Ruoss, Gr\u00e9goire Del\u00e9tang, Sourabh Medapati, Jordi Grau-Moya, Li Kevin Wenliang, Elliot Catt, John Reid, Tim Genewein",
                "publication_date": "2024-02-07",
                "reference": "Grandmaster-Level Chess Without Search",
                "link": "https://arxiv.org/abs/2402.04494",
                "citations": 24.0,
                "notability_criteria": null,
                "parameters": 270000000.0,
                "training_compute_(flop)": 1.035671832e+22,
                "training_dataset_size_(gradients)": 1194960000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": "Supervised",
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United Kingdom",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 18:07:59+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United Kingdom",
                "domain_group": "Games",
                "export_controls_sum": 1.0,
                "publication_count": 2.8041950763331074,
                "log_params": 8.431363764158988,
                "log_compute": 22.015222164550924,
                "pub_bin": "Low",
                "export_bin": "Low"
            },
            {
                "model": "CARP",
                "domain": "Biology",
                "task": "Protein or nucleotide language model (pLM/nLM)",
                "organization": "Microsoft Research",
                "authors": "Kevin K. Yang, Nicolo Fusi, Alex X. Lu",
                "publication_date": "2024-02-06",
                "reference": "Convolutions are competitive with transformers for protein sequence pretraining",
                "link": "https://www.cell.com/cell-systems/abstract/S2405-4712(24)00029-2",
                "citations": 126.0,
                "notability_criteria": null,
                "parameters": 643000000.0,
                "training_compute_(flop)": 1.0193977e+22,
                "training_dataset_size_(gradients)": 1867500000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA V100",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 128.0,
                "last_modified": "2025-10-14 21:50:39+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 76028.99774979397,
                "training_compute_estimation_method": "Hardware,Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Biology",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 8.808210972924222,
                "log_compute": 22.00834364938371,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "DeepSeekMath 7B",
                "domain": "Language",
                "task": "Quantitative reasoning",
                "organization": "DeepSeek,Tsinghua University,Peking University",
                "authors": "Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y.K. Li, Y. Wu, Daya Guo",
                "publication_date": "2024-02-05",
                "reference": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models",
                "link": "https://arxiv.org/abs/2402.03300",
                "citations": null,
                "notability_criteria": null,
                "parameters": 7000000000.0,
                "training_compute_(flop)": 1.014e+23,
                "training_dataset_size_(gradients)": 500000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "China",
                "base_model": "DeepSeek Coder 6.7B",
                "finetune_compute_(flop)": 2.1e+22,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 9.845098040014257,
                "log_compute": 23.006037954997318,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Qwen1.5-72B",
                "domain": "Language",
                "task": "Chat,Language modeling/generation,Quantitative reasoning,Code generation,Translation",
                "organization": "Alibaba",
                "authors": "Qwen Team",
                "publication_date": "2024-02-04",
                "reference": "Introducing Qwen1.5",
                "link": "https://qwenlm.github.io/blog/qwen1.5/",
                "citations": null,
                "notability_criteria": "SOTA improvement",
                "parameters": 72000000000.0,
                "training_compute_(flop)": 1.3e+24,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 10.857332496431269,
                "log_compute": 24.113943352306837,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Qwen1.5-7B",
                "domain": "Language",
                "task": "Chat,Language modeling/generation,Quantitative reasoning,Code generation,Translation",
                "organization": "Alibaba",
                "authors": "Qwen Team",
                "publication_date": "2024-02-04",
                "reference": "Introducing Qwen1.5",
                "link": "https://huggingface.co/Qwen/Qwen1.5-7B",
                "citations": null,
                "notability_criteria": null,
                "parameters": 7000000000.0,
                "training_compute_(flop)": 1.68e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 9.845098040014257,
                "log_compute": 23.225309281725863,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Qwen1.5-14B",
                "domain": "Language",
                "task": "Chat,Language modeling/generation,Quantitative reasoning,Code generation,Translation",
                "organization": "Alibaba",
                "authors": "Qwen Team",
                "publication_date": "2024-02-04",
                "reference": "Introducing Qwen1.5",
                "link": "https://huggingface.co/Qwen/Qwen1.5-14B",
                "citations": null,
                "notability_criteria": null,
                "parameters": 14000000000.0,
                "training_compute_(flop)": 3.36e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 10.146128035678238,
                "log_compute": 23.526339277389845,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "OLMo-7B",
                "domain": "Language",
                "task": "Language modeling/generation,Chat",
                "organization": "Allen Institute for AI,University of Washington",
                "authors": "Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, Hannaneh Hajishirzi",
                "publication_date": "2024-02-01",
                "reference": "OLMo: Accelerating the Science of Language Models",
                "link": "https://arxiv.org/abs/2402.00838v1",
                "citations": null,
                "notability_criteria": null,
                "parameters": 7000000000.0,
                "training_compute_(flop)": 1.0332e+23,
                "training_dataset_size_(gradients)": 2000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "AMD Radeon Instinct MI250X,NVIDIA A100",
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.23,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 4000000.0,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.845098040014257,
                "log_compute": 23.01418439750128,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "OLMo-1B",
                "domain": "Language",
                "task": "Language modeling/generation,Chat",
                "organization": "Allen Institute for AI,University of Washington",
                "authors": "Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, Hannaneh Hajishirzi",
                "publication_date": "2024-02-01",
                "reference": "OLMo: Accelerating the Science of Language Models",
                "link": "https://arxiv.org/abs/2402.00838v1",
                "citations": null,
                "notability_criteria": null,
                "parameters": 1000000000.0,
                "training_compute_(flop)": 1.2e+22,
                "training_dataset_size_(gradients)": 2000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "AMD Radeon Instinct MI250X,NVIDIA A100",
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 4000000.0,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 9.0,
                "log_compute": 22.079181246047625,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "LLaVA-NeXT-34B (LLaVA-1.6)",
                "domain": "Multimodal,Language,Vision",
                "task": "Visual question answering,Chat,Question answering",
                "organization": "University of Wisconsin Madison,ByteDance,Nanyang Technological University,University of California (UC) Berkeley",
                "authors": "Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, Yong Jae Lee",
                "publication_date": "2024-01-30",
                "reference": "LLaVA-NeXT: Improved reasoning, OCR, and world knowledge",
                "link": "https://llava-vl.github.io/blog/2024-01-30-llava-next/, https://huggingface.co/liuhaotian/llava-v1.6-34b",
                "citations": null,
                "notability_criteria": null,
                "parameters": 34750000000.0,
                "training_compute_(flop)": 2.5878528e+20,
                "training_dataset_size_(gradients)": 89338000.0,
                "training_time_(hours)": 24.0,
                "training_hardware": "NVIDIA A100",
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 32.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": 768.0,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 25346.95016648552,
                "training_compute_estimation_method": "Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 10.540954808926132,
                "log_compute": 20.412939569536906,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "Code Llama-70B",
                "domain": "Language",
                "task": "Code generation",
                "organization": "Meta AI",
                "authors": "Baptiste Rozi\u00e8re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Ellen Tan, Yossef (Yossi) Adi, Jingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Defossez, Jade Copet, Faisal Azhar, Hugo Touvron, Gabriel Synnaeve, Louis Martin, Nicolas Usunier, Thomas Scialom",
                "publication_date": "2024-01-29",
                "reference": "Code Llama: Open Foundation Models for Code",
                "link": "https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/\nhttps://arxiv.org/abs/2308.12950",
                "citations": 2432.0,
                "notability_criteria": null,
                "parameters": 70000000000.0,
                "training_compute_(flop)": 1.26e+24,
                "training_dataset_size_(gradients)": 3000000000000.0,
                "training_time_(hours)": 6480.0,
                "training_hardware": "NVIDIA A100 SXM4 80 GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Open weights (restricted use)",
                "country": "United States",
                "base_model": "Llama 2-70B",
                "finetune_compute_(flop)": 4.2e+23,
                "hardware_quantity": 400.0,
                "last_modified": "2025-10-16 13:08:38+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 4000000.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": 2705320.0,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 316843.9329176827,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 10.845098040014257,
                "log_compute": 24.100370545117563,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "ProteinStructureTransformer",
                "domain": "Biology",
                "task": "Protein or nucleotide language model (pLM/nLM)",
                "organization": "Max Planck Institute of Biochemistry",
                "authors": "Dexiong Chen, Philip Hartout, Paolo Pellizzoni, Carlos Oliver, Karsten Borgwardt",
                "publication_date": "2024-01-26",
                "reference": "ENDOWING PROTEIN LANGUAGE MODELS WITH STRUCTURAL KNOWLEDGE",
                "link": "https://arxiv.org/abs/2401.14819",
                "citations": 16.0,
                "notability_criteria": null,
                "parameters": 1137000000.0,
                "training_compute_(flop)": 7.616995200001001e+21,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 10.0,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": 100.0,
                "model_accessibility": "Unknown",
                "country": "Germany",
                "base_model": "ESM2-650M",
                "finetune_compute_(flop)": 5.69952e+19,
                "hardware_quantity": 4.0,
                "last_modified": "2025-10-14 18:07:57+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 5545.139273980643,
                "training_compute_estimation_method": "Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "Germany",
                "domain_group": "Biology",
                "export_controls_sum": 10.0,
                "publication_count": 2.7830121675064055,
                "log_params": 9.055760464687735,
                "log_compute": 21.881783681914282,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "DeepSeek Coder 33B",
                "domain": "Language",
                "task": "Code generation",
                "organization": "DeepSeek,Peking University",
                "authors": "Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y.K. Li, Fuli Luo, Yingfei Xiong, Wenfeng Liang",
                "publication_date": "2024-01-25",
                "reference": "DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence",
                "link": "https://arxiv.org/abs/2401.14196",
                "citations": null,
                "notability_criteria": null,
                "parameters": 33000000000.0,
                "training_compute_(flop)": 3.96e+23,
                "training_dataset_size_(gradients)": 2000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 10.518513939877888,
                "log_compute": 23.597695185925513,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "DeepSeek Coder 1.3B",
                "domain": "Language",
                "task": "Code generation",
                "organization": "DeepSeek,Peking University",
                "authors": "Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y.K. Li, Fuli Luo, Yingfei Xiong, Wenfeng Liang",
                "publication_date": "2024-01-25",
                "reference": "DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence",
                "link": "https://arxiv.org/abs/2401.14196",
                "citations": null,
                "notability_criteria": null,
                "parameters": 1300000000.0,
                "training_compute_(flop)": 1.56e+22,
                "training_dataset_size_(gradients)": 2000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 9.113943352306837,
                "log_compute": 22.193124598354462,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "DeepSeek Coder 6.7B",
                "domain": "Language",
                "task": "Code generation",
                "organization": "DeepSeek,Peking University",
                "authors": "Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y.K. Li, Fuli Luo, Yingfei Xiong, Wenfeng Liang",
                "publication_date": "2024-01-25",
                "reference": "DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence",
                "link": "https://arxiv.org/abs/2401.14196",
                "citations": null,
                "notability_criteria": null,
                "parameters": 6700000000.0,
                "training_compute_(flop)": 8.04e+22,
                "training_dataset_size_(gradients)": 2000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 9.826074802700827,
                "log_compute": 22.905256048748452,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Yi-VL-34B",
                "domain": "Vision,Language,Multimodal",
                "task": "Visual question answering,Language modeling/generation",
                "organization": "01.AI",
                "authors": "Unknown",
                "publication_date": "2024-01-23",
                "reference": "Yi Vision Language Model\nBetter Bilingual Multimodal Model",
                "link": "https://huggingface.co/01-ai/Yi-VL-34B",
                "citations": null,
                "notability_criteria": null,
                "parameters": 34000000000.0,
                "training_compute_(flop)": 1.85174e+22,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 240.0,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Open weights (restricted use)",
                "country": "China",
                "base_model": "Yi-34B,CLIP ViT-H/14 - LAION-2B",
                "finetune_compute_(flop)": null,
                "hardware_quantity": 128.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 177456.311892851,
                "training_compute_estimation_method": "Hardware,Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Multimodal",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 10.531478917042255,
                "log_compute": 22.267580007998106,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "StableLM-2-1.6B",
                "domain": "Language",
                "task": "Language modeling/generation",
                "organization": "Stability AI",
                "authors": "Stability AI Language Team",
                "publication_date": "2024-01-18",
                "reference": "Stable LM 2 1.6B",
                "link": "https://huggingface.co/stabilityai/stablelm-2-1_6b",
                "citations": null,
                "notability_criteria": null,
                "parameters": 1644417024.0,
                "training_compute_(flop)": 1.92e+22,
                "training_dataset_size_(gradients)": 2000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100 SXM4 40 GB",
                "approach": "Self-supervised learning",
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "United Kingdom",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 512.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 405659.59358954936,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United Kingdom",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 2.8041950763331074,
                "log_params": 9.216011964210836,
                "log_compute": 22.28330122870355,
                "pub_bin": "Low",
                "export_bin": "Low"
            },
            {
                "model": "OmniNA",
                "domain": "Biology",
                "task": "Protein or nucleotide language model (pLM/nLM)",
                "organization": "Tianjin Medical University",
                "authors": "Xilin Shen, Xiangchun Li",
                "publication_date": "2024-01-15",
                "reference": "OmniNA: A foundation model for nucleotide sequences",
                "link": "https://www.biorxiv.org/content/10.1101/2024.01.14.575543v1.abstract",
                "citations": 3.0,
                "notability_criteria": null,
                "parameters": 1700000000.0,
                "training_compute_(flop)": 2.51092992e+21,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100 SXM4 80 GB",
                "approach": null,
                "confidence": "Likely",
                "epochs": 0.22,
                "model_accessibility": "Unknown",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 8.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 6338.854622611998,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Biology",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 9.230448921378274,
                "log_compute": 21.399834591732432,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": " InternVL",
                "domain": "Vision,Language",
                "task": "Visual question answering,Image classification,Image captioning",
                "organization": "Shanghai AI Lab,Nanjing University,The University of Hong Kong,Tsinghua University,SenseTime,University of Science and Technology of China (USTC)",
                "authors": "Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, Jifeng Dai",
                "publication_date": "2024-01-15",
                "reference": "InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks",
                "link": "https://arxiv.org/abs/2312.14238",
                "citations": null,
                "notability_criteria": null,
                "parameters": 14000000000.0,
                "training_compute_(flop)": 1.744956e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 800.0,
                "training_hardware": "NVIDIA A100 SXM4 80 GB",
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": "InternViT-6B,LLaMA-7B",
                "finetune_compute_(flop)": null,
                "hardware_quantity": 640.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 164000.0,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 507108.36980895983,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Multimodal",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 10.146128035678238,
                "log_compute": 23.241784480465327,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "InternLM2-20B",
                "domain": "Language",
                "task": "Chat,Language modeling/generation,Question answering",
                "organization": "Shanghai AI Lab,SenseTime,Chinese University of Hong Kong (CUHK),Fudan University",
                "authors": "Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, Xiaoyi Dong, Haodong Duan, Qi Fan, Zhaoye Fei, Yang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao Gui, Aijia Guo, Qipeng Guo, Conghui He, Yingfan Hu, Ting Huang, Tao Jiang, Penglong Jiao, Zhenjiang Jin, Zhikai Lei, Jiaxing Li, Jingwen Li, Linyang Li, Shuaibin Li, Wei Li, Yining Li, Hongwei Liu, Jiangning Liu, Jiawei Hong, Kaiwen Liu, Kuikun Liu, Xiaoran Liu, Chengqi Lv, Haijun Lv, Kai Lv, Li Ma, Runyuan Ma, Zerun Ma, Wenchang Ning, Linke Ouyang, Jiantao Qiu, Yuan Qu, Fukai Shang, Yunfan Shao, Demin Song, Zifan Song, Zhihao Sui, Peng Sun, Yu Sun, Huanze Tang, Bin Wang, Guoteng Wang, Jiaqi Wang, Jiayu Wang, Rui Wang, Yudong Wang, Ziyi Wang, Xingjian Wei, Qizhen Weng, Fan Wu, Yingtong Xiong, Chao Xu, Ruiliang Xu, Hang Yan, Yirong Yan, Xiaogui Yang, Haochen Ye, Huaiyuan Ying, Jia Yu, Jing Yu, Yuhang Zang, Chuyu Zhang, Li Zhang, Pan Zhang, Peng Zhang, Ruijie Zhang, Shuo Zhang, Songyang Zhang, Wenjian Zhang, Wenwei Zhang, Xingcheng Zhang, Xinyue Zhang, Hui Zhao, Qian Zhao, Xiaomeng Zhao, Fengzhe Zhou, Zaida Zhou, Jingming Zhuo, Yicheng Zou, Xipeng Qiu, Yu Qiao, Dahua Lin",
                "publication_date": "2024-01-12",
                "reference": "InternLM2 Technical Report",
                "link": "https://arxiv.org/abs/2403.17297",
                "citations": null,
                "notability_criteria": null,
                "parameters": 20000000000.0,
                "training_compute_(flop)": 3.12e+23,
                "training_dataset_size_(gradients)": 2600000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Open weights (restricted use)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 5000000.0,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 10.301029995663981,
                "log_compute": 23.494154594018443,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "DeepSeekMoE-16B",
                "domain": "Language",
                "task": "Chat",
                "organization": "DeepSeek",
                "authors": "Damai Dai, Chengqi Deng, Chenggang Zhao, R.X. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y. Wu, Zhenda Xie, Y.K. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui, Wenfeng Liang",
                "publication_date": "2024-01-11",
                "reference": "DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models",
                "link": "https://arxiv.org/abs/2401.06066",
                "citations": 473.0,
                "notability_criteria": null,
                "parameters": 16000000000.0,
                "training_compute_(flop)": 3.4e+22,
                "training_dataset_size_(gradients)": 2000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100,NVIDIA H800 SXM5",
                "approach": null,
                "confidence": "Likely",
                "epochs": 1.0,
                "model_accessibility": "Open weights (restricted use)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:09:06+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 10.204119982655925,
                "log_compute": 22.531478917042254,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Stable Code 3B",
                "domain": "Language",
                "task": "Language modeling/generation,Code generation",
                "organization": "Stability AI",
                "authors": "Pinnaparaju, Nikhil and Adithyan, Reshinth and Phung, Duy and Tow, Jonathan and Baicoianu, James and  and Cooper, Nathan",
                "publication_date": "2024-01-09",
                "reference": "Stable Code 3B",
                "link": "https://huggingface.co/stabilityai/stable-code-3b",
                "citations": null,
                "notability_criteria": null,
                "parameters": 2796431360.0,
                "training_compute_(flop)": 2.106e+22,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100 SXM4 40 GB",
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "United Kingdom",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 256.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 202870.4528973297,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United Kingdom",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 2.8041950763331074,
                "log_params": 9.44660416378975,
                "log_compute": 22.32345836684947,
                "pub_bin": "Low",
                "export_bin": "Low"
            },
            {
                "model": "Improved motif-scaffolding with SE(3) flow matching",
                "domain": "Biology",
                "task": "Protein design",
                "organization": "University of Oxford,Massachusetts Institute of Technology (MIT),Microsoft Research AI for Science",
                "authors": "Jason Yim, Andrew Campbell, Emile Mathieu, Andrew Y. K. Foong, Michael Gastegger, Jos\u00e9 Jim\u00e9nez-Luna, Sarah Lewis, Victor Garcia Satorras, Bastiaan S. Veeling, Frank No\u00e9, Regina Barzilay, Tommi S. Jaakkola",
                "publication_date": "2024-01-08",
                "reference": "Improved motif-scaffolding with SE(3) flow matching",
                "link": "https://arxiv.org/abs/2401.04082",
                "citations": null,
                "notability_criteria": null,
                "parameters": 16800000.0,
                "training_compute_(flop)": 1.6000000000000008e+19,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 144.0,
                "training_hardware": "NVIDIA RTX A6000",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unknown",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 2.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 1188.7205317093983,
                "training_compute_estimation_method": "Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United Kingdom",
                "domain_group": "Biology",
                "export_controls_sum": 1.0,
                "publication_count": 2.8041950763331074,
                "log_params": 7.225309281725863,
                "log_compute": 19.204119982655925,
                "pub_bin": "Low",
                "export_bin": "Low"
            },
            {
                "model": "DeepSeek LLM 67B",
                "domain": "Language",
                "task": "Chat,Language modeling/generation,Question answering",
                "organization": "DeepSeek",
                "authors": "Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He, Wenjie Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y.K. Li, Wenfeng Liang, Fangyun Lin, A.X. Liu, Bo Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Minghui Tang, Bingxuan Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Y. Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yiliang Xiong, Hanwei Xu, R.X. Xu, Yanhong Xu, Dejian Yang, Yuxiang You, Shuiping Yu, Xingkai Yu, B. Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang, Minghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou, Qihao Zhu, Yuheng Zou",
                "publication_date": "2024-01-05",
                "reference": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
                "link": "https://arxiv.org/abs/2401.02954, https://github.com/deepseek-ai/DeepSeek-LLM",
                "citations": null,
                "notability_criteria": null,
                "parameters": 67000000000.0,
                "training_compute_(flop)": 8.04e+23,
                "training_dataset_size_(gradients)": 2000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Open weights (restricted use)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 10.826074802700827,
                "log_compute": 23.905256048748452,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "DeepSeek-LLM-1.3b-base",
                "domain": "Language",
                "task": "Language modeling/generation",
                "organization": "DeepSeek",
                "authors": "Unknown",
                "publication_date": "2024-01-05",
                "reference": null,
                "link": "it is only mentioned in Janus 1.3B release https://huggingface.co/deepseek-ai/Janus-1.3B\n\nupd + supposedly in Janus-Pro-1B paper \"In our experiments, we utilize DeepSeek-LLM (1.5B and 7B) [3]\" while there is no 1.5B model mentioned in the linked paper",
                "citations": null,
                "notability_criteria": null,
                "parameters": 1300000000.0,
                "training_compute_(flop)": 3.9e+21,
                "training_dataset_size_(gradients)": 500000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-08-05 19:36:09+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 9.113943352306837,
                "log_compute": 21.5910646070265,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "DeepSeek LLM 7B",
                "domain": "Language",
                "task": "Chat,Language modeling/generation,Question answering",
                "organization": "DeepSeek",
                "authors": "Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He, Wenjie Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y.K. Li, Wenfeng Liang, Fangyun Lin, A.X. Liu, Bo Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Minghui Tang, Bingxuan Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Y. Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yiliang Xiong, Hanwei Xu, R.X. Xu, Yanhong Xu, Dejian Yang, Yuxiang You, Shuiping Yu, Xingkai Yu, B. Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang, Minghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou, Qihao Zhu, Yuheng Zou",
                "publication_date": "2024-01-05",
                "reference": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
                "link": "https://arxiv.org/abs/2401.02954, https://github.com/deepseek-ai/DeepSeek-LLM",
                "citations": null,
                "notability_criteria": null,
                "parameters": 7000000000.0,
                "training_compute_(flop)": 8.4e+22,
                "training_dataset_size_(gradients)": 2000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Open weights (restricted use)",
                "country": "China",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "China",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 32.96410593554805,
                "log_params": 9.845098040014257,
                "log_compute": 22.924279286061882,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "PLLaMa",
                "domain": "Biology",
                "task": "Language modeling/generation",
                "organization": "University of California Santa Barbara (UCSB),University of Lincoln,Chinese Academy of Agricultural Sciences,Swedish University of Agricultural Sciences",
                "authors": "Xianjun Yang, Junfeng Gao, Wenxin Xue, Erik Alexandersson",
                "publication_date": "2024-01-03",
                "reference": "PLLaMa: An Open-source Large Language Model for Plant Science",
                "link": "https://arxiv.org/abs/2401.01600",
                "citations": 27.0,
                "notability_criteria": null,
                "parameters": 13000000000.0,
                "training_compute_(flop)": 1.60209723904e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 59.7,
                "training_hardware": "NVIDIA A100",
                "approach": null,
                "confidence": "Confident",
                "epochs": 16.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": "Llama 2-13B",
                "finetune_compute_(flop)": null,
                "hardware_quantity": 8.0,
                "last_modified": "2025-10-14 18:07:58+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 6340.548796655679,
                "training_compute_estimation_method": "Hardware",
                "year": 2024,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Biology",
                "export_controls_sum": 25.0,
                "publication_count": 11.2300474377788,
                "log_params": 10.113943352306837,
                "log_compute": 23.204688871983482,
                "pub_bin": "Low",
                "export_bin": "High"
            },
            {
                "model": "FunSearch",
                "domain": "Language,Search",
                "task": "Code generation",
                "organization": "Google DeepMind",
                "authors": "Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M. Pawan Kumar, Emilien Dupont, Francisco J. R. Ruiz, Jordan S. Ellenberg, Pengming Wang, Omar Fawzi, Pushmeet Kohli, Alhussein Fawzi ",
                "publication_date": "2023-12-14",
                "reference": "Mathematical discoveries from program search with large language models",
                "link": "https://www.nature.com/articles/s41586-023-06924-6\nhttps://deepmind.google/discover/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/",
                "citations": 488.0,
                "notability_criteria": "SOTA improvement,Historical significance",
                "parameters": 15000000000.0,
                "training_compute_(flop)": 3.87e+23,
                "training_dataset_size_(gradients)": 0.0,
                "training_time_(hours)": 48.0,
                "training_hardware": null,
                "approach": null,
                "confidence": "Speculative",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": "PaLM 2",
                "finetune_compute_(flop)": 0.0,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 21:50:39+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 10.176091259055681,
                "log_compute": 23.58771096501891,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "Phi-2",
                "domain": "Language",
                "task": "Language generation,Code generation",
                "organization": "Microsoft",
                "authors": "Marah Abdin, Jyoti Aneja, Sebastien Bubeck, Caio C\u00e9sar Teodoro Mendes, Weizhu Chen, Allie Del Giorno, Ronen Eldan, Sivakanth Gopi, Suriya Gunasekar, Mojan Javaheripi, Piero Kauffmann, Yin Tat Lee, Yuanzhi Li, Anh Nguyen, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Michael Santacroce, Harkirat Singh Behl, Adam Taumann Kalai, Xin Wang, Rachel Ward, Philipp Witte, Cyril Zhang, Yi Zhang",
                "publication_date": "2023-12-12",
                "reference": "Phi-2: The surprising power of small language models",
                "link": "https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/\nhttps://huggingface.co/microsoft/phi-2 ",
                "citations": null,
                "notability_criteria": null,
                "parameters": 2700000000.0,
                "training_compute_(flop)": 2.27e+22,
                "training_dataset_size_(gradients)": 250000000000.0,
                "training_time_(hours)": 336.0,
                "training_hardware": "NVIDIA A100",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting,Hardware",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 9.431363764158988,
                "log_compute": 22.356025857193124,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "VILA-13B",
                "domain": "Multimodal,Language,Vision",
                "task": "Chat,Visual question answering,Image captioning,Language modeling/generation,Question answering",
                "organization": "NVIDIA,Massachusetts Institute of Technology (MIT)",
                "authors": "Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, Song Han",
                "publication_date": "2023-12-12",
                "reference": "VILA: On Pre-training for Visual Language Models",
                "link": "https://arxiv.org/abs/2312.07533\nhttps://huggingface.co/Efficient-Large-Model/VILA-13b",
                "citations": 567.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 13350839296.0,
                "training_compute_(flop)": 2.3003136e+21,
                "training_dataset_size_(gradients)": 32430000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100 SXM4 80 GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "Multinational",
                "base_model": "Llama 2-13B,CLIP (ViT L/14@336px)",
                "finetune_compute_(flop)": null,
                "hardware_quantity": 128.0,
                "last_modified": "2025-10-16 13:09:07+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 101498.495336141,
                "training_compute_estimation_method": null,
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 10.12550856833461,
                "log_compute": 21.361787047089525,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "Mixtral 8x7B",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Quantitative reasoning,Translation",
                "organization": "Mistral AI",
                "authors": "Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L\u00e9lio Renard Lavaud, Louis Ternon, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Th\u00e9ophile Gervet, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, William El Sayed.",
                "publication_date": "2023-12-11",
                "reference": "Mixtral of experts: A high quality Sparse Mixture-of-Experts.",
                "link": "https://mistral.ai/news/mixtral-of-experts/, https://arxiv.org/abs/2401.04088",
                "citations": null,
                "notability_criteria": "Significant use",
                "parameters": 46700000000.0,
                "training_compute_(flop)": 7.74e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Speculative",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "France",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "France",
                "domain_group": "Language",
                "export_controls_sum": 6.0,
                "publication_count": 2.320372806963785,
                "log_params": 10.669316880566113,
                "log_compute": 23.88874096068289,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "Llama Guard",
                "domain": "Language",
                "task": "Chat",
                "organization": "Meta AI",
                "authors": "Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Davide Testuggine, Madian Khabsa",
                "publication_date": "2023-12-07",
                "reference": "Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations",
                "link": "https://arxiv.org/abs/2312.06674",
                "citations": 617.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 7000000000.0,
                "training_compute_(flop)": 1.6e+23,
                "training_dataset_size_(gradients)": 4096000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100 SXM4 80 GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Open weights (restricted use)",
                "country": "United States",
                "base_model": "Llama 2-7B",
                "finetune_compute_(flop)": 1.7e+17,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:09:07+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 9.845098040014257,
                "log_compute": 23.204119982655925,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "Mamba-2.8B",
                "domain": "Language",
                "task": "Language generation,Question answering",
                "organization": "Carnegie Mellon University (CMU),Princeton University",
                "authors": "Albert Gu, Tri Dao",
                "publication_date": "2023-12-01",
                "reference": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
                "link": "https://arxiv.org/abs/2312.00752",
                "citations": 3858.0,
                "notability_criteria": null,
                "parameters": 2800000000.0,
                "training_compute_(flop)": 5.400000000000001e+21,
                "training_dataset_size_(gradients)": 300000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:09:06+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 9.44715803134222,
                "log_compute": 21.73239375982297,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "Granite 13B",
                "domain": "Language",
                "task": "Chat,Language modeling/generation,Question answering,Text summarization",
                "organization": "IBM",
                "authors": "Unknown",
                "publication_date": "2023-11-30",
                "reference": "Granite Foundation Models",
                "link": "https://www.ibm.com/downloads/cas/X9W4O6BM",
                "citations": null,
                "notability_criteria": null,
                "parameters": 13000000000.0,
                "training_compute_(flop)": 2.44e+23,
                "training_dataset_size_(gradients)": 2500000000000.0,
                "training_time_(hours)": 2208.0,
                "training_hardware": "NVIDIA A100",
                "approach": null,
                "confidence": "Likely",
                "epochs": 1.0,
                "model_accessibility": "API access",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware,Operation counting",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 10.113943352306837,
                "log_compute": 23.38738982633873,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "StripedHyena-Hessian-7B",
                "domain": "Language",
                "task": "Chat,Language modeling/generation",
                "organization": "Together,Nous Research",
                "authors": "Michael Poli, Jue Wang, Stefano Massaroli, Jeffrey Quesnelle, Ryan Carlow, Eric Nguyen, Armin Thomas",
                "publication_date": "2023-11-27",
                "reference": "StripedHyena-Hessian-7B (SH 7B) ",
                "link": "https://huggingface.co/togethercomputer/StripedHyena-Hessian-7B\nhttps://github.com/togethercomputer/stripedhyena\nhttps://www.together.ai/blog/stripedhyena-7b",
                "citations": null,
                "notability_criteria": null,
                "parameters": 7000000000.0,
                "training_compute_(flop)": 8e+19,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Reported",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 9.845098040014257,
                "log_compute": 19.903089986991944,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "Orca 2-13B",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering",
                "organization": "Microsoft Research",
                "authors": "Arindam Mitra, Luciano Del Corro, Shweti Mahajan, Andres Codas, Clarisse Simoes, Sahaj Agarwal, Xuxi Chen, Anastasia Razdaibiedina, Erik Jones, Kriti Aggarwal, Hamid Palangi, Guoqing Zheng, Corby Rosset, Hamed Khanpour, Ahmed Awadallah",
                "publication_date": "2023-11-21",
                "reference": "Orca 2: Teaching Small Language Models How to Reason",
                "link": "https://arxiv.org/abs/2311.11045, https://huggingface.co/microsoft/Orca-2-13b",
                "citations": 163.0,
                "notability_criteria": null,
                "parameters": 13000000000.0,
                "training_compute_(flop)": 4.6e+22,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 80.0,
                "training_hardware": "NVIDIA A100 SXM4 80 GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "Multinational",
                "base_model": "LLaMA-13B",
                "finetune_compute_(flop)": 8.6e+20,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 18:03:40+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 10.113943352306837,
                "log_compute": 22.662757831681574,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "Nemotron-3-8B",
                "domain": "Language",
                "task": "Chat,Language generation,Language modeling/generation,Translation,Code generation,Question answering",
                "organization": "NVIDIA",
                "authors": "Unknown",
                "publication_date": "2023-11-15",
                "reference": "NVIDIA AI Foundation Models: Build Custom Enterprise Chatbots and Co-Pilots with Production-Ready LLMs",
                "link": "https://developer.nvidia.com/blog/nvidia-ai-foundation-models-build-custom-enterprise-chatbots-and-co-pilots-with-production-ready-llms/\n\nhttps://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/nemotron-3-8b-base-4k",
                "citations": null,
                "notability_criteria": "SOTA improvement",
                "parameters": 8000000000.0,
                "training_compute_(flop)": 1.8e+23,
                "training_dataset_size_(gradients)": 3800000000000.0,
                "training_time_(hours)": 456.0,
                "training_hardware": "NVIDIA A100",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 1024.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": 214467.02013524104,
                "frontier_model": false,
                "training_power_draw_(w)": 812476.3359553809,
                "training_compute_estimation_method": "Operation counting,Hardware",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 9.903089986991944,
                "log_compute": 23.255272505103306,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "Prithvi-100M",
                "domain": "Earth science",
                "task": "Cloud monitoring / analysis,Flood Mapping,Wildfire Mapping,Crop Mapping / Segmentation",
                "organization": "IBM,NASA",
                "authors": "Johannes Jakubik, Sujit Roy, C. E. Phillips, Paolo Fraccaro, Denys Godwin, Bianca Zadrozny, Daniela Szwarcman, Carlos Gomes, Gabby Nyirjesy, Blair Edwards, Daiki Kimura, Naomi Simumba, Linsong Chu, S. Karthik Mukkavilli, Devyani Lambhate, Kamal Das, Ranjini Bangalore, Dario Oliveira, Michal Muszynski, Kumar Ankur, Muthukumaran Ramasubramanian, Iksha Gurung, Sam Khallaghi, Hanxi (Steve) Li, Michael Cecil, Maryam Ahmadi, Fatemeh Kordi, Hamed Alemohammad, Manil Maskey, Raghu Ganti, Kommy Weldemariam, Rahul Ramachandran",
                "publication_date": "2023-11-08",
                "reference": "Foundation Models for Generalist Geospatial Artificial Intelligence",
                "link": "https://arxiv.org/abs/2310.18660",
                "citations": null,
                "notability_criteria": null,
                "parameters": 100000000.0,
                "training_compute_(flop)": 2.299133952e+21,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100",
                "approach": null,
                "confidence": "Speculative",
                "epochs": 1000.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 64.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": "IBM",
                "batch_size": 1024.0,
                "organization_categorization": "Industry, Government",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 50787.68744924102,
                "training_compute_estimation_method": "Hardware",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Earth science",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 8.0,
                "log_compute": 21.36156427484463,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "Whisper v3",
                "domain": "Speech",
                "task": "Speech recognition (ASR)",
                "organization": "OpenAI",
                "authors": "Unknown",
                "publication_date": "2023-11-06",
                "reference": null,
                "link": "https://huggingface.co/openai/whisper-large-v3",
                "citations": null,
                "notability_criteria": null,
                "parameters": 1550000000.0,
                "training_compute_(flop)": 2.7e+23,
                "training_dataset_size_(gradients)": 80000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": "Supervised",
                "confidence": "Likely",
                "epochs": 2.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-08-05 19:36:07+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Comparison with other models",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Audio",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 9.190331698170292,
                "log_compute": 23.431363764158988,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "LLaVA 1.5",
                "domain": "Multimodal,Language,Vision",
                "task": "Chat,Question answering,Visual question answering",
                "organization": "University of Wisconsin Madison,Microsoft Research",
                "authors": "Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee",
                "publication_date": "2023-11-05",
                "reference": "Improved Baselines with Visual Instruction Tuning",
                "link": "https://arxiv.org/abs/2310.03744,\n",
                "citations": 3544.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 13000000000.0,
                "training_compute_(flop)": 7.807e+22,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 24.0,
                "training_hardware": "NVIDIA A100",
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Open weights (restricted use)",
                "country": "Multinational",
                "base_model": "Vicuna-13B v0",
                "finetune_compute_(flop)": 7.008768e+19,
                "hardware_quantity": 8.0,
                "last_modified": "2025-10-16 13:08:52+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": 192.0,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 6348.88507402357,
                "training_compute_estimation_method": "Hardware",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 10.113943352306837,
                "log_compute": 22.892484179364686,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "Grok-1",
                "domain": "Language",
                "task": "Language modeling,Chat",
                "organization": "xAI",
                "authors": "Unknown",
                "publication_date": "2023-11-04",
                "reference": "Announcing Grok",
                "link": "https://x.ai/model-card/, https://x.ai/blog/grok-os",
                "citations": null,
                "notability_criteria": "Training cost",
                "parameters": 314000000000.0,
                "training_compute_(flop)": 2.90000000001e+24,
                "training_dataset_size_(gradients)": 6200000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": true,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Benchmarks",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 11.496929648073214,
                "log_compute": 24.462397997900453,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "CODEFUSION (Python)",
                "domain": "Language",
                "task": "Code generation",
                "organization": "Microsoft,Microsoft Research",
                "authors": "Mukul Singh, Jos\u00e9 Cambronero, Sumit Gulwani, Vu Le, Carina Negreanu, Gust Verbruggen",
                "publication_date": "2023-10-26",
                "reference": "CODEFUSION: A Pre-trained Diffusion Model for Code Generation",
                "link": "https://arxiv.org/abs/2310.17680 (was withdrawn)\n\nhttps://www.microsoft.com/en-us/research/wp-content/uploads/2023/11/CodeFusion-Revised-CameraReady.pdf",
                "citations": 38.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 75000000.0,
                "training_compute_(flop)": 7.92e+18,
                "training_dataset_size_(gradients)": 4390400.0,
                "training_time_(hours)": 11.0,
                "training_hardware": "NVIDIA V100",
                "approach": "Self-supervised learning",
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 4.0,
                "last_modified": "2025-10-14 18:03:41+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 8.542235671062665,
                "frontier_model": false,
                "training_power_draw_(w)": 2381.36215809478,
                "training_compute_estimation_method": "Hardware",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 7.8750612633917,
                "log_compute": 18.898725181589494,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "Llemma 34B",
                "domain": "Mathematics,Language",
                "task": "Mathematical reasoning,Language modeling/generation,Question answering,Code generation",
                "organization": "Princeton University,University of Toronto,Vector Institute,University of Cambridge,Carnegie Mellon University (CMU),University of Washington,EleutherAI",
                "authors": "Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q. Jiang, Jia Deng, Stella Biderman, Sean Welleck",
                "publication_date": "2023-10-16",
                "reference": "Llemma: An Open Language Model For Mathematics",
                "link": "https://arxiv.org/abs/2310.10631",
                "citations": 344.0,
                "notability_criteria": null,
                "parameters": 34000000000.0,
                "training_compute_(flop)": 5.4270979e+23,
                "training_dataset_size_(gradients)": 55000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100 SXM4 40 GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": 0.9,
                "model_accessibility": "Open weights (restricted use)",
                "country": "Multinational",
                "base_model": "Code Llama-34B",
                "finetune_compute_(flop)": 1.2709785e+22,
                "hardware_quantity": 256.0,
                "last_modified": "2025-10-16 13:08:52+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 4000000.0,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": 47000.0,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 203254.8293570902,
                "training_compute_estimation_method": "Operation counting,Hardware",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 10.531478917042255,
                "log_compute": 23.73456765593479,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "Llemma 7B",
                "domain": "Mathematics,Language",
                "task": "Mathematical reasoning,Language modeling/generation,Question answering,Code generation",
                "organization": "Princeton University,EleutherAI,University of Toronto,Vector Institute,University of Cambridge,Carnegie Mellon University (CMU),University of Washington",
                "authors": "Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q. Jiang, Jia Deng, Stella Biderman, Sean Welleck",
                "publication_date": "2023-10-16",
                "reference": "Llemma: An Open Language Model For Mathematics",
                "link": "https://arxiv.org/abs/2310.10631",
                "citations": 344.0,
                "notability_criteria": null,
                "parameters": 7000000000.0,
                "training_compute_(flop)": 1.180685e+23,
                "training_dataset_size_(gradients)": 55000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100 SXM4 40 GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": 3.64,
                "model_accessibility": "Open weights (restricted use)",
                "country": "Multinational",
                "base_model": "Code Llama-7B",
                "finetune_compute_(flop)": 8.0684987e+21,
                "hardware_quantity": 256.0,
                "last_modified": "2025-10-16 13:08:52+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 4000000.0,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": 23000.0,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 203254.8293570902,
                "training_compute_estimation_method": "Operation counting,Hardware",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 9.845098040014257,
                "log_compute": 23.07213404578517,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "FinGPT-13B",
                "domain": "Language",
                "task": "Named entity recognition (NER),Sentiment classification,Language modeling/generation,Financial management",
                "organization": "University of California Los Angeles (UCLA),Columbia University,New York University (NYU)",
                "authors": "Neng Wang, Hongyang Yang, Christina Dan Wang",
                "publication_date": "2023-10-07",
                "reference": "FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets",
                "link": "https://arxiv.org/abs/2310.04793; https://github.com/AI4Finance-Foundation/FinGPT",
                "citations": 69.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 13000000000.0,
                "training_compute_(flop)": 1.6e+23,
                "training_dataset_size_(gradients)": 76800.0,
                "training_time_(hours)": 17.25,
                "training_hardware": "NVIDIA GeForce RTX 3090",
                "approach": "Supervised",
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": "Llama 2-13B",
                "finetune_compute_(flop)": 6.532488e+17,
                "hardware_quantity": 1.0,
                "last_modified": "2025-10-14 18:03:42+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 381.7900590727425,
                "training_compute_estimation_method": "Hardware",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 10.113943352306837,
                "log_compute": 23.204119982655925,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "Phi-1",
                "domain": "Language",
                "task": "Language modeling/generation,Code generation",
                "organization": "Microsoft Research",
                "authors": "Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C\u00e9sar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, S\u00e9bastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, Yuanzhi Li",
                "publication_date": "2023-10-02",
                "reference": "Textbooks Are All You Need",
                "link": "https://arxiv.org/abs/2306.11644",
                "citations": null,
                "notability_criteria": null,
                "parameters": 1300000000.0,
                "training_compute_(flop)": 3.3234195e+20,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 103.0,
                "training_hardware": "NVIDIA A100",
                "approach": null,
                "confidence": "Confident",
                "epochs": 7.3,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 8.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 6353.694007439068,
                "training_compute_estimation_method": "Operation counting,Hardware",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 9.113943352306837,
                "log_compute": 20.521585163739257,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "Wuerstchen",
                "domain": "Image generation",
                "task": "Text-to-image,Image generation",
                "organization": "Technische Hochschule Ingolstadt,University of Montreal / Universit\u00e9 de Montr\u00e9al,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms),Polytechnique Montreal,Wand Technologies",
                "authors": "Pablo Pernias, Dominic Rampas, Mats L. Richter, Christopher J. Pal, Marc Aubreville",
                "publication_date": "2023-09-29",
                "reference": "Wuerstchen: An Efficient Architecture for Large-Scale Text-to-Image Diffusion Models",
                "link": "https://arxiv.org/abs/2306.00637",
                "citations": null,
                "notability_criteria": null,
                "parameters": 1000000000.0,
                "training_compute_(flop)": 8.2898899e+21,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "Germany",
                "domain_group": "Vision",
                "export_controls_sum": 6.0,
                "publication_count": 3.4737849918245405,
                "log_params": 9.0,
                "log_compute": 21.91854876262024,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "Amazon Titan",
                "domain": "Language,Image generation",
                "task": "Semantic search,Image generation,Language modeling/generation,Code generation,Chat,Text-to-image,Translation",
                "organization": "Amazon",
                "authors": "Unknown",
                "publication_date": "2023-09-28",
                "reference": null,
                "link": "https://aws.amazon.com/bedrock/titan/",
                "citations": null,
                "notability_criteria": "Training cost",
                "parameters": 200000000000.0,
                "training_compute_(flop)": 4.7999999999999996e+24,
                "training_dataset_size_(gradients)": 4000000000000.0,
                "training_time_(hours)": 1152.0,
                "training_hardware": "NVIDIA A100",
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "API access",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 13760.0,
                "last_modified": "2025-10-16 15:12:24+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": 7933464.673729055,
                "frontier_model": true,
                "training_power_draw_(w)": 10929327.206416875,
                "training_compute_estimation_method": "Hardware,Operation counting",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 11.301029995663981,
                "log_compute": 24.681241237375588,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "Phi-1.5",
                "domain": "Language",
                "task": "Language generation",
                "organization": "Microsoft",
                "authors": "Yuanzhi Li, S\u00e9bastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, Yin Tat Lee",
                "publication_date": "2023-09-11",
                "reference": "Textbooks Are All You Need II: phi-1.5 technical report",
                "link": "https://arxiv.org/abs/2309.05463, https://huggingface.co/microsoft/phi-1_5",
                "citations": 539.0,
                "notability_criteria": null,
                "parameters": 1300000000.0,
                "training_compute_(flop)": 1.17e+21,
                "training_dataset_size_(gradients)": 30000000000.0,
                "training_time_(hours)": 192.0,
                "training_hardware": "NVIDIA A100 SXM4 40 GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": 5.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:08:52+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": true,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting,Hardware",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 9.113943352306837,
                "log_compute": 21.068185861746162,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "MADLAD-400 10B",
                "domain": "Language",
                "task": "Translation",
                "organization": "Google DeepMind,Google Research",
                "authors": "Sneha Kudugunta, Isaac Caswell, Biao Zhang, Xavier Garcia, Christopher A. Choquette-Choo, Katherine Lee, Derrick Xin, Aditya Kusupati, Romi Stella, Ankur Bapna, Orhan Firat",
                "publication_date": "2023-09-09",
                "reference": "MADLAD-400: A Multilingual And Document-Level Large Audited Dataset",
                "link": "https://arxiv.org/abs/2309.04662",
                "citations": 165.0,
                "notability_criteria": null,
                "parameters": 10700000000.0,
                "training_compute_(flop)": 1.605e+22,
                "training_dataset_size_(gradients)": 250000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:09:07+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 10.02938377768521,
                "log_compute": 22.20547503674089,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "Persimmon-8B",
                "domain": "Language",
                "task": "Language modeling",
                "organization": "Adept",
                "authors": "Erich Elsen, Augustus Odena, Maxwell Nye, Sa\u011fnak Ta\u015f\u0131rlar, Tri Dao, Curtis Hawthorne, Deepak Moparthi, Arushi Somani",
                "publication_date": "2023-09-07",
                "reference": "Releasing Persimmon-8B",
                "link": "https://www.adept.ai/blog/persimmon-8b",
                "citations": null,
                "notability_criteria": null,
                "parameters": 9300000000.0,
                "training_compute_(flop)": 4.11246e+22,
                "training_dataset_size_(gradients)": 737000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 9.968482948553936,
                "log_compute": 22.61410168679663,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "XGen-7B",
                "domain": "Language",
                "task": "Language generation",
                "organization": "Salesforce",
                "authors": "Erik Nijkamp, Tian Xie, Hiroaki Hayashi, Bo Pang, Congying Xia, Chen Xing, Jesse Vig, Semih Yavuz, Philippe Laban, Ben Krause, Senthil Purushwalkam, Tong Niu, Wojciech Kryscinski, Lidiya Murakhovs\u2019ka, Prafulla Kumar Choubey, Alex Fabbri, Ye Liu, Rui Meng, Lifu Tu, Meghana Bhat, Chien-Sheng Wu, Silvio Savarese, Yingbo Zhou, Shafiq Joty, Caiming Xiong",
                "publication_date": "2023-09-07",
                "reference": "XGen-7B Technical Report",
                "link": "https://arxiv.org/abs/2309.03450",
                "citations": 14.0,
                "notability_criteria": null,
                "parameters": 6700000000.0,
                "training_compute_(flop)": 8.02e+22,
                "training_dataset_size_(gradients)": 1429520000000.0,
                "training_time_(hours)": null,
                "training_hardware": "Google TPU v4",
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 18:02:45+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 1048576.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": 270336.0,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware,Operation counting",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 9.826074802700827,
                "log_compute": 22.904174368284163,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "Code Llama-34B",
                "domain": "Language",
                "task": "Code generation",
                "organization": "Meta AI",
                "authors": "Baptiste Rozi\u00e8re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Ellen Tan, Yossef (Yossi) Adi, Jingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Defossez, Jade Copet, Faisal Azhar, Hugo Touvron, Gabriel Synnaeve, Louis Martin, Nicolas Usunier, Thomas Scialom",
                "publication_date": "2023-08-14",
                "reference": "Code Llama: Open Foundation Models for Code",
                "link": "https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/\nhttps://arxiv.org/abs/2308.12950",
                "citations": 2432.0,
                "notability_criteria": null,
                "parameters": 34000000000.0,
                "training_compute_(flop)": 5.3e+23,
                "training_dataset_size_(gradients)": 600000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100 SXM4 80 GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "United States",
                "base_model": "Llama 2-34B",
                "finetune_compute_(flop)": 1.22e+23,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:08:38+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 4000000.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 10.531478917042255,
                "log_compute": 23.72427586960079,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "Code Llama-7B",
                "domain": "Language",
                "task": "Code generation",
                "organization": "Meta AI",
                "authors": "Baptiste Rozi\u00e8re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Ellen Tan, Yossef (Yossi) Adi, Jingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Defossez, Jade Copet, Faisal Azhar, Hugo Touvron, Gabriel Synnaeve, Louis Martin, Nicolas Usunier, Thomas Scialom",
                "publication_date": "2023-08-14",
                "reference": "Code Llama: Open Foundation Models for Code",
                "link": "https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/\nhttps://arxiv.org/abs/2308.12950",
                "citations": 2432.0,
                "notability_criteria": null,
                "parameters": 7000000000.0,
                "training_compute_(flop)": 1.1e+23,
                "training_dataset_size_(gradients)": 600000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100 SXM4 80 GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "United States",
                "base_model": "Llama 2-7B",
                "finetune_compute_(flop)": 2.5e+22,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:08:38+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 4000000.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 9.845098040014257,
                "log_compute": 23.041392685158225,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "SS-pLM",
                "domain": "Biology",
                "task": "Protein or nucleotide language model (pLM/nLM)",
                "organization": "Nostrum Biodiscovery,Barcelona Supercomputing Center,Institucio Catalana de Recerca i Estudis Avanc\u00e7ats",
                "authors": "Yaiza Serrano, Sergi Roda, Victor Guallar, Alexis Molina",
                "publication_date": "2023-08-06",
                "reference": "Efficient and accurate sequence generation with small-scale protein language models",
                "link": "https://www.biorxiv.org/content/10.1101/2023.08.04.551626.abstract",
                "citations": 5.0,
                "notability_criteria": null,
                "parameters": 14800000.0,
                "training_compute_(flop)": 2.28096e+19,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 24.0,
                "training_hardware": "NVIDIA A30 PCIe",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unknown",
                "country": "Spain",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 4.0,
                "last_modified": "2025-10-14 21:50:12+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Government",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 1312.1138696054595,
                "training_compute_estimation_method": "Hardware",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "Spain",
                "domain_group": "Biology",
                "export_controls_sum": 6.0,
                "publication_count": 1.2574715250483772,
                "log_params": 7.1702617153949575,
                "log_compute": 19.358117669348726,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "Llama 2-70B",
                "domain": "Language",
                "task": "Language modeling,Language modeling/generation,Question answering",
                "organization": "Meta AI",
                "authors": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom\n",
                "publication_date": "2023-07-18",
                "reference": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
                "link": "https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/\nhttps://arxiv.org/abs/2307.09288",
                "citations": 13685.0,
                "notability_criteria": "Historical significance,Significant use,Highly cited,Training cost",
                "parameters": 70000000000.0,
                "training_compute_(flop)": 8.1e+23,
                "training_dataset_size_(gradients)": 2000000000000.0,
                "training_time_(hours)": 1728.0,
                "training_hardware": "NVIDIA A100 SXM4 80 GB",
                "approach": "Supervised",
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Open weights (restricted use)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 1000.0,
                "last_modified": "2025-10-16 13:08:38+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 4000000.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": 1720320.0,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": 1102561.194,
                "frontier_model": false,
                "training_power_draw_(w)": 795557.0703894598,
                "training_compute_estimation_method": "Hardware,Operation counting",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 10.845098040014257,
                "log_compute": 23.90848501887865,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "Llama 2-34B",
                "domain": "Language",
                "task": "Language modeling",
                "organization": "Meta AI",
                "authors": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom\n",
                "publication_date": "2023-07-18",
                "reference": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
                "link": "https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/\nhttps://arxiv.org/abs/2307.09288",
                "citations": 13685.0,
                "notability_criteria": "Highly cited",
                "parameters": 34000000000.0,
                "training_compute_(flop)": 4.08e+23,
                "training_dataset_size_(gradients)": 2000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100 SXM4 80 GB",
                "approach": "Supervised",
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Unreleased",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:08:38+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 4000000.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": 600469.8720490151,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware,Operation counting",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 10.531478917042255,
                "log_compute": 23.61066016308988,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "Llama 2-7B",
                "domain": "Language",
                "task": "Language modeling",
                "organization": "Meta AI",
                "authors": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom\n",
                "publication_date": "2023-07-18",
                "reference": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
                "link": "https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/",
                "citations": 13685.0,
                "notability_criteria": "Historical significance,Significant use,Highly cited",
                "parameters": 7000000000.0,
                "training_compute_(flop)": 8.4e+22,
                "training_dataset_size_(gradients)": 2000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100 SXM4 80 GB",
                "approach": "Supervised",
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Open weights (restricted use)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:08:38+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 4000000.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": 184320.0,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 114259.38527188863,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware,Operation counting",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 9.845098040014257,
                "log_compute": 22.924279286061882,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "Llama 2-13B",
                "domain": "Language",
                "task": "Language modeling",
                "organization": "Meta AI",
                "authors": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom\n",
                "publication_date": "2023-07-18",
                "reference": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
                "link": "https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/\nhttps://arxiv.org/abs/2307.09288",
                "citations": 13685.0,
                "notability_criteria": "Historical significance,Significant use,Highly cited",
                "parameters": 13000000000.0,
                "training_compute_(flop)": 1.6e+23,
                "training_dataset_size_(gradients)": 2000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100 SXM4 80 GB",
                "approach": "Supervised",
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Open weights (restricted use)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:08:39+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 4000000.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": 235478.3811956922,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware,Operation counting",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 10.113943352306837,
                "log_compute": 23.204119982655925,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "RetNet",
                "domain": "Language",
                "task": "Language modeling",
                "organization": "Microsoft Research,Tsinghua University",
                "authors": "Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, Furu Wei",
                "publication_date": "2023-07-17",
                "reference": "Retentive Network: A Successor to Transformer for Large Language Models",
                "link": "https://arxiv.org/abs/2307.08621",
                "citations": 419.0,
                "notability_criteria": null,
                "parameters": 6700000000.0,
                "training_compute_(flop)": 4.02e+21,
                "training_dataset_size_(gradients)": 100000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:08:38+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 4000000.0,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 9.826074802700827,
                "log_compute": 21.60422605308447,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "CodeGen2.5",
                "domain": "Language",
                "task": "Code generation",
                "organization": "Salesforce",
                "authors": "Erik Nijkamp, Hiroaki Hayashi, Yingbo Zhou, Caiming Xiong",
                "publication_date": "2023-07-06",
                "reference": "CodeGen2.5: Small, but mighty",
                "link": "https://blog.salesforceairesearch.com/codegen25/",
                "citations": null,
                "notability_criteria": null,
                "parameters": 7000000000.0,
                "training_compute_(flop)": 5.9e+22,
                "training_dataset_size_(gradients)": 300000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": 4.66,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 9.845098040014257,
                "log_compute": 22.770852011642145,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "LongNet",
                "domain": "Language",
                "task": "Language modeling/generation",
                "organization": "Microsoft,Xi\u2019an Jiaotong University",
                "authors": "Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng, Furu Wei\n",
                "publication_date": "2023-07-05",
                "reference": "LongNet: Scaling Transformers to 1,000,000,000 Tokens",
                "link": "https://arxiv.org/abs/2307.02486",
                "citations": 194.0,
                "notability_criteria": null,
                "parameters": 2700000000.0,
                "training_compute_(flop)": 4.86e+21,
                "training_dataset_size_(gradients)": 300000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 18:02:44+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 9.431363764158988,
                "log_compute": 21.686636269262294,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "Multilingual-E5-large",
                "domain": "Language",
                "task": "Semantic embedding",
                "organization": "Microsoft",
                "authors": "Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei",
                "publication_date": "2023-06-30",
                "reference": "Multilingual E5 Text Embeddings: A Technical Report",
                "link": "https://arxiv.org/abs/2402.05672\nhttps://huggingface.co/intfloat/multilingual-e5-large",
                "citations": null,
                "notability_criteria": null,
                "parameters": 560000000.0,
                "training_compute_(flop)": 3.370752e+18,
                "training_dataset_size_(gradients)": 1000160000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 8.7481880270062,
                "log_compute": 18.527726800876486,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "HyenaDNA",
                "domain": "Biology",
                "task": "Protein or nucleotide language model (pLM/nLM)",
                "organization": "Stanford University,Harvard University,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms),University of Montreal / Universit\u00e9 de Montr\u00e9al",
                "authors": "Eric Nguyen, Michael Poli, Marjan Faizi, Armin W. Thomas, Callum Birch Sykes, Michael Wornow, Aman Patel, Clayton Rabideau, Stefano Massaroli, Yoshua Bengio, Stefano Ermon, Stephen A. Baccus, Christopher R\u00e9",
                "publication_date": "2023-06-27",
                "reference": "HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution",
                "link": "https://arxiv.org/abs/2306.15794",
                "citations": 318.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 6600000.0,
                "training_compute_(flop)": 1.811e+21,
                "training_dataset_size_(gradients)": 2945000000.0,
                "training_time_(hours)": 672.0,
                "training_hardware": "NVIDIA A100",
                "approach": null,
                "confidence": "Confident",
                "epochs": 679.117147708,
                "model_accessibility": "Open weights (restricted use)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 8.0,
                "last_modified": "2025-10-16 13:08:38+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 64000000.0,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 5000.0,
                "frontier_model": false,
                "training_power_draw_(w)": 6367.433640556241,
                "training_compute_estimation_method": "Hardware",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Biology",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 6.819543935541868,
                "log_compute": 21.257918450314058,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "Kosmos-2",
                "domain": "Language,Vision,Multimodal",
                "task": "Visual question answering,Image captioning,Named entity recognition (NER),Character recognition (OCR),Document representation",
                "organization": "Microsoft",
                "authors": "Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, Furu Wei",
                "publication_date": "2023-06-26",
                "reference": "Kosmos-2: Grounding Multimodal Large Language Models to the World",
                "link": "https://arxiv.org/abs/2306.14824",
                "citations": 884.0,
                "notability_criteria": null,
                "parameters": 1600000000.0,
                "training_compute_(flop)": 4.5500354284e+20,
                "training_dataset_size_(gradients)": 25000000000.0,
                "training_time_(hours)": 24.0,
                "training_hardware": "NVIDIA V100",
                "approach": "Self-supervised learning",
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 256.0,
                "last_modified": "2025-10-16 13:08:38+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": 6144.0,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 152821.8105810948,
                "training_compute_estimation_method": "Operation counting,Hardware",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 9.204119982655925,
                "log_compute": 20.658014778261226,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "MPT-30B",
                "domain": "Language",
                "task": "Language generation,Code generation",
                "organization": "MosaicML",
                "authors": "Unknown",
                "publication_date": "2023-06-22",
                "reference": null,
                "link": "https://huggingface.co/mosaicml/mpt-30b",
                "citations": null,
                "notability_criteria": null,
                "parameters": 30000000000.0,
                "training_compute_(flop)": 1.8900000000001e+23,
                "training_dataset_size_(gradients)": 1050000000000.0,
                "training_time_(hours)": 278.4,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 512.0,
                "last_modified": "2025-08-04 19:57:52+00:00",
                "training_cloud_compute_vendor": "Databricks",
                "batch_size": 4096000.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": 142541.0,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 713231.9794598748,
                "training_compute_estimation_method": "Operation counting",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 10.477121254719663,
                "log_compute": 23.276461804173266,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "WizardCoder-15.5B",
                "domain": "Language",
                "task": "Code generation",
                "organization": "Microsoft",
                "authors": "Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Daxin Jiang",
                "publication_date": "2023-06-14",
                "reference": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct",
                "link": "https://arxiv.org/abs/2306.08568",
                "citations": 773.0,
                "notability_criteria": null,
                "parameters": 15500000000.0,
                "training_compute_(flop)": 1.12e+23,
                "training_dataset_size_(gradients)": 209715200.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (restricted use)",
                "country": "Multinational",
                "base_model": "StarCoder",
                "finetune_compute_(flop)": 1.95035136e+19,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:08:38+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 10.190331698170292,
                "log_compute": 23.049218022670182,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "RedPajama-INCITE-7B-Base",
                "domain": "Language",
                "task": "Chat",
                "organization": "Together",
                "authors": "Unknown",
                "publication_date": "2023-06-06",
                "reference": null,
                "link": "https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Base",
                "citations": null,
                "notability_criteria": null,
                "parameters": 6900000000.0,
                "training_compute_(flop)": 4.1e+22,
                "training_dataset_size_(gradients)": 1001000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA V100",
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 3072.0,
                "last_modified": "2025-08-05 19:36:03+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 4000000.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 1834678.6878449952,
                "training_compute_estimation_method": "Operation counting",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 9.838849090737256,
                "log_compute": 22.612783856719737,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "GELU for CIFAR-10",
                "domain": "Vision",
                "task": "Image classification",
                "organization": "University of California (UC) Berkeley,Toyota Technological Institute at Chicago",
                "authors": "Dan Hendrycks, Kevin Gimpel",
                "publication_date": "2023-06-06",
                "reference": "Gaussian Error Linear Units (GELUs)",
                "link": "https://arxiv.org/abs/1606.08415",
                "citations": null,
                "notability_criteria": null,
                "parameters": 9888.0,
                "training_compute_(flop)": 741600000000.0,
                "training_dataset_size_(gradients)": 50000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA GeForce GTX TITAN X",
                "approach": null,
                "confidence": "Speculative",
                "epochs": 250.0,
                "model_accessibility": "Unknown",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Vision",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 3.9951084577447404,
                "log_compute": 11.87016972113644,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "TransAct",
                "domain": "Recommendation",
                "task": "Recommender system",
                "organization": "Pinterest",
                "authors": "Xue Xia, Pong Eksombatchai, Nikil Pancha, Dhruvil Deven Badani, Po-Wei Wang, Neng Gu, Saurabh Vishwas Joshi, Nazanin Farahpour, Zhiyuan Zhang, Andrew Zhai",
                "publication_date": "2023-05-31",
                "reference": "TransAct: Transformer-based Realtime User Action Model for Recommendation at Pinterest",
                "link": "https://arxiv.org/abs/2306.00248",
                "citations": null,
                "notability_criteria": null,
                "parameters": 92000000.0,
                "training_compute_(flop)": 1.656e+20,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Recommendation",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 7.963787827345556,
                "log_compute": 20.219060332448862,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "Guanaco-65B",
                "domain": "Language",
                "task": "Chat",
                "organization": "University of Washington",
                "authors": "Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer",
                "publication_date": "2023-05-23",
                "reference": "QLoRA: Efficient Finetuning of Quantized LLMs",
                "link": "https://arxiv.org/abs/2305.14314; https://github.com/artidoro/qlora",
                "citations": 3124.0,
                "notability_criteria": null,
                "parameters": 65000000000.0,
                "training_compute_(flop)": 5.5e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 24.0,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "United States",
                "base_model": "LLaMA-65B",
                "finetune_compute_(flop)": 8e+18,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:08:52+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 10.812913356642856,
                "log_compute": 23.740362689494244,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "LIMA",
                "domain": "Language",
                "task": "Chat,Language modeling/generation",
                "organization": "Meta AI,Carnegie Mellon University (CMU),University of Southern California,Tel Aviv University",
                "authors": "Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, Omer Levy",
                "publication_date": "2023-05-18",
                "reference": "LIMA: Less Is More for Alignment",
                "link": "https://arxiv.org/abs/2305.11206",
                "citations": 1081.0,
                "notability_criteria": null,
                "parameters": 65000000000.0,
                "training_compute_(flop)": 5.5000439e+23,
                "training_dataset_size_(gradients)": 685300.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": 15.0,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": "LLaMA-65B",
                "finetune_compute_(flop)": 4.39e+20,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 10.812913356642856,
                "log_compute": 23.740366155940002,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "InstructBLIP",
                "domain": "Multimodal,Language,Vision",
                "task": "Visual question answering,Chat",
                "organization": "Salesforce Research,Hong Kong University of Science and Technology (HKUST),Nanyang Technological University",
                "authors": "Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi",
                "publication_date": "2023-05-11",
                "reference": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning",
                "link": "https://arxiv.org/abs/2305.06500",
                "citations": 2528.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 13000000000.0,
                "training_compute_(flop)": 1.94e+20,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 36.0,
                "training_hardware": "NVIDIA A100 SXM4 40 GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "Multinational",
                "base_model": "Vicuna-13B v0",
                "finetune_compute_(flop)": 1.9408896e+20,
                "hardware_quantity": 16.0,
                "last_modified": "2025-10-16 13:08:52+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": 576.0,
                "training_code_accessibility": "Open (non-commercial)",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 12748.20334097663,
                "training_compute_estimation_method": "Hardware",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 10.113943352306837,
                "log_compute": 20.287801729930226,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "PaLM 2",
                "domain": "Language",
                "task": "Language modeling,Language modeling/generation",
                "organization": "Google",
                "authors": "Andrew M. Dai, David R. So, Dmitry Lepikhin, Jonathan H. Clark, Maxim Krikun, Melvin Johnson, Nan Du, Rohan Anil, Siamak Shakeri, Xavier Garcia, Yanping Huang, Yi Tay, Yong Cheng, Yonghui Wu, Yuanzhong Xu, Yujing Zhang, Zachary Nado, Bryan Richter, Alex Polozov, Andrew Nystrom, Fangxiaoyu Feng, Hanzhao Lin, Jacob Austin, Jacob Devlin, Kefan Xiao, Orhan Firat, Parker Riley, Steven Zheng, Yuhuai Wu, Zhongtao Liu, Jiahui Yu, Guy Gur-Ari, Weikang Zhou, Sneha Kudugunta, Sunipa Dev, Frederick Liu, Gustavo Hernandez Abrego, Kelvin Xu, Abe Ittycheriah, Daniel Sohn, John Nham, Le Hou, Siyuan Qiao, Pidong Wang, Zirui Wang, Laurent El Shafey, Hyeontaek Lim, Marcello Maggioni, Michael Isard, Paul Barham, Qiao Zhang, Tao Wang, Yash Katariya, Aurko Roy, Benjamin Lee, Brennan Saeta, Ce Zheng, Hadi Hashemi, Junwhan Ahn, Rajkumar Samuel, Steven Hand, Zhifeng Chen, Kiran Vodrahalli, Aakanksha Chowdhery, Ethan Dyer, Emanuel Taropa, Vlad Feinberg, James Bradbury, Reiner Pope, Wei Li, YaGuang Li, Eric Chu, Jeffrey Hui, Joshua Howland, Vlad Fienber, Aroma Mahendru, Michele Catasta, Vedant Misra, Kevin Robinson, Maysam Moussalem, Sebastian Ruder, Erica Moreira, Eric Ni, Paige Bailey, Lucas Gonzalez, Alexandre Passos, Slav Petrov, Gaurav Mishra, Mark Omernick, Ambrose Slone, Andrea Hu, Colin Cherry, Denny Zhou, Jan Botha, John Wieting, Joshua Maynez, Kathleen Kenealy, Kevin Brooks, Linting Xue, Markus Freitag, Martin Polacek, Pengcheng Yin, Sebastian Gehrmann, Xuezhi Wang, Kathy Meier-Hellstern, Christopher A. Choquette-Choo, Daniel Smilkov, Emily Reif, Alicia Parrish, Alex Castro Ros, Cl\u00e9ment Crepy, Dasha Valter, Jeremy Hurwitz, Katherine Lee, Mark D\u00edaz, Marie Pellat, Matthew Jagielski, Renee Shelby, Shachi Dave",
                "publication_date": "2023-05-10",
                "reference": "PaLM 2 Technical Report",
                "link": "https://arxiv.org/abs/2305.10403",
                "citations": 1734.0,
                "notability_criteria": "SOTA improvement,Training cost,Significant use,Highly cited",
                "parameters": 340000000000.0,
                "training_compute_(flop)": 7.34e+24,
                "training_dataset_size_(gradients)": 3600000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "Google TPU v4",
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "API access",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 15:12:27+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": true,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": 4804456.380612964,
                "frontier_model": true,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 11.531478917042255,
                "log_compute": 24.865696059916072,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "MPT-7B",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering",
                "organization": "MosaicML",
                "authors": "MosaicML NLP Team",
                "publication_date": "2023-05-05",
                "reference": "\"Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs\"",
                "link": "https://www.mosaicml.com/blog/mpt-7b",
                "citations": null,
                "notability_criteria": null,
                "parameters": 7000000000.0,
                "training_compute_(flop)": 4.2e+22,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 228.0,
                "training_hardware": "NVIDIA A100 SXM4 40 GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 440.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": 100320.0,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 350622.43759455526,
                "training_compute_estimation_method": "Hardware",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 9.845098040014257,
                "log_compute": 22.6232492903979,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "MosaicML Diffusion",
                "domain": "Image generation",
                "task": "Image generation,Text-to-image",
                "organization": "Databricks",
                "authors": "Mihir Patel, Erica Ji Yuen, Cory Stephenson, Landan Seguin",
                "publication_date": "2023-04-28",
                "reference": "Training Stable Diffusion from Scratch for <$50k with MosaicML",
                "link": "https://www.databricks.com/blog/stable-diffusion-2\nhttps://www.databricks.com/blog/diffusion\nhttps://github.com/mosaicml/diffusion",
                "citations": null,
                "notability_criteria": null,
                "parameters": 1289952427.0,
                "training_compute_(flop)": 1.07085888e+22,
                "training_dataset_size_(gradients)": 790000000.0,
                "training_time_(hours)": 186.0,
                "training_hardware": "NVIDIA A100",
                "approach": "Self-supervised learning",
                "confidence": "Confident",
                "epochs": 1.3797,
                "model_accessibility": "Unreleased",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 128.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 102015.15602722247,
                "training_compute_estimation_method": "Hardware",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Vision",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 9.110573693964085,
                "log_compute": 22.029732242376753,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "WizardLM-7B",
                "domain": "Language",
                "task": "Language modeling",
                "organization": "Microsoft,Peking University",
                "authors": "Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Daxin Jiang",
                "publication_date": "2023-04-24",
                "reference": "WizardLM: Empowering Large Language Models to Follow Complex Instructions",
                "link": "https://arxiv.org/abs/2304.12244",
                "citations": 1132.0,
                "notability_criteria": null,
                "parameters": 6700000000.0,
                "training_compute_(flop)": 4.02e+22,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 70.0,
                "training_hardware": "NVIDIA V100",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "Multinational",
                "base_model": "LLaMA-7B",
                "finetune_compute_(flop)": null,
                "hardware_quantity": 8.0,
                "last_modified": "2025-10-14 18:02:44+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 46907.42757520694,
                "frontier_model": false,
                "training_power_draw_(w)": 4782.3864227583035,
                "training_compute_estimation_method": "Hardware",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 9.826074802700827,
                "log_compute": 22.60422605308447,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "LLaVA",
                "domain": "Multimodal,Vision,Language",
                "task": "Chat,Question answering,Visual question answering",
                "organization": "University of Wisconsin Madison,Microsoft Research,Columbia University",
                "authors": "Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee",
                "publication_date": "2023-04-17",
                "reference": "Visual Instruction Tuning",
                "link": "https://arxiv.org/abs/2304.08485",
                "citations": 6185.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 13000000000.0,
                "training_compute_(flop)": 7.8049e+22,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 10.0,
                "training_hardware": "NVIDIA A100",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": "Vicuna-13B v0",
                "finetune_compute_(flop)": 4.9e+19,
                "hardware_quantity": 8.0,
                "last_modified": "2025-10-16 13:08:52+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": 80.0,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 42.46267260692187,
                "frontier_model": false,
                "training_power_draw_(w)": 6377.509314719864,
                "training_compute_estimation_method": "Hardware",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 10.113943352306837,
                "log_compute": 22.89236734305168,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "DINOv2",
                "domain": "Vision",
                "task": "Image representation,Image classification",
                "organization": "Facebook AI Research,INRIA",
                "authors": "Maxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herv\u00e9 Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, Piotr Bojanowski",
                "publication_date": "2023-04-14",
                "reference": "DINOv2: Learning Robust Visual Features without Supervision",
                "link": "https://arxiv.org/abs/2304.07193",
                "citations": 4783.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 1140000000.0,
                "training_compute_(flop)": 7.41851136e+21,
                "training_dataset_size_(gradients)": 36380002816.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100 SXM4 40 GB",
                "approach": "Self-supervised learning",
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:08:51+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 10203.60518105836,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Vision",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 9.056904851336473,
                "log_compute": 21.87031676606081,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "Incoder-6.7B",
                "domain": "Language",
                "task": "Code generation",
                "organization": "Facebook AI Research,University of Washington,University of California (UC) Berkeley,Carnegie Mellon University (CMU),Toyota Technological Institute at Chicago",
                "authors": "Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, Mike Lewis",
                "publication_date": "2023-04-09",
                "reference": "InCoder: A Generative Model for Code Infilling and Synthesis",
                "link": "https://arxiv.org/abs/2204.05999",
                "citations": 724.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 6700000000.0,
                "training_compute_(flop)": 3.00001e+21,
                "training_dataset_size_(gradients)": 52000000000.0,
                "training_time_(hours)": 576.0,
                "training_hardware": "NVIDIA V100",
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 18:01:53+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 3129.0771365574788,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Reported",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 9.826074802700827,
                "log_compute": 21.477122702365524,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "gLM",
                "domain": "Biology",
                "task": "Protein or nucleotide language model (pLM/nLM)",
                "organization": "Harvard University",
                "authors": "Yunha Hwang, Andre L. Cornman, Sergey Ovchinnikov, Peter R. Girguis",
                "publication_date": "2023-04-08",
                "reference": "Deep learning of genomic contexts predicts protein co-regulation and function",
                "link": "https://www.biorxiv.org/content/10.1101/2023.04.07.536042v1.full\nhttps://github.com/y-hwang/gLM",
                "citations": null,
                "notability_criteria": null,
                "parameters": 1000000000.0,
                "training_compute_(flop)": 2.26437e+20,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100",
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "United States",
                "base_model": "ESM2-650M",
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open (non-commercial)",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Biology",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 9.0,
                "log_compute": 20.35494739240547,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "DiffDock-PP",
                "domain": "Biology",
                "task": "Protein interaction prediction",
                "organization": "Technical University of Munich,Massachusetts Institute of Technology (MIT)",
                "authors": "Mohamed Amine Ketata, Cedrik Laue, Ruslan Mammadov, Hannes St\u00e4rk, Menghua Wu, Gabriele Corso, C\u00e9line Marquet, Regina Barzilay, Tommi S. Jaakkola",
                "publication_date": "2023-04-08",
                "reference": "DiffDock-PP: Rigid Protein-Protein Docking with Diffusion Models",
                "link": "https://arxiv.org/abs/2304.03889",
                "citations": 54.0,
                "notability_criteria": null,
                "parameters": 1620000.0,
                "training_compute_(flop)": 3.5382841e+16,
                "training_dataset_size_(gradients)": 42826.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Likely",
                "epochs": 170.0,
                "model_accessibility": "Unknown",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 18:01:53+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": null,
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "Germany",
                "domain_group": "Biology",
                "export_controls_sum": 6.0,
                "publication_count": 3.4737849918245405,
                "log_params": 6.209515014542631,
                "log_compute": 16.548792700848654,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "Segment Anything Model",
                "domain": "Vision",
                "task": "Image segmentation",
                "organization": "Meta AI",
                "authors": "Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll\u00e1r, Ross Girshick",
                "publication_date": "2023-04-05",
                "reference": "Segment Anything",
                "link": "https://arxiv.org/abs/2304.02643",
                "citations": 9166.0,
                "notability_criteria": "Highly cited,Historical significance",
                "parameters": 636000000.0,
                "training_compute_(flop)": 7.8e+21,
                "training_dataset_size_(gradients)": 1100000000.0,
                "training_time_(hours)": 68.0,
                "training_hardware": "NVIDIA A100",
                "approach": "Supervised",
                "confidence": "Confident",
                "epochs": 2.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": "ViT-Huge/14",
                "finetune_compute_(flop)": 7.8e+21,
                "hardware_quantity": 256.0,
                "last_modified": "2025-10-16 13:08:52+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 15888.411228475235,
                "frontier_model": false,
                "training_power_draw_(w)": 204134.84223782964,
                "training_compute_estimation_method": "Hardware",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Vision",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 8.803457115648413,
                "log_compute": 21.89209460269048,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "BloombergGPT",
                "domain": "Language",
                "task": "Language modeling,Language modeling/generation,Question answering,Financial management,Text classification",
                "organization": "Bloomberg,Johns Hopkins University",
                "authors": "Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, Gideon Mann",
                "publication_date": "2023-03-30",
                "reference": "BloombergGPT: A Large Language Model for Finance",
                "link": "https://arxiv.org/abs/2303.17564",
                "citations": 974.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 50558868480.0,
                "training_compute_(flop)": 2.36e+23,
                "training_dataset_size_(gradients)": 569000000000.0,
                "training_time_(hours)": 1270.0,
                "training_hardware": "NVIDIA A100",
                "approach": "Self-supervised learning",
                "confidence": "Confident",
                "epochs": 0.8,
                "model_accessibility": "Unreleased",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 512.0,
                "last_modified": "2025-10-14 18:01:51+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 4200000.0,
                "organization_categorization": "Academia, Industry",
                "foundation_model": true,
                "training_chip-hours": 650240.0,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": 369586.1352802876,
                "frontier_model": false,
                "training_power_draw_(w)": 408324.2395754059,
                "training_compute_estimation_method": "Reported,Hardware",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 10.703797345765011,
                "log_compute": 23.372912002970107,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "SigLIP 400M",
                "domain": "Vision",
                "task": "Image classification,Image embedding",
                "organization": "Google DeepMind",
                "authors": "Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, Lucas Beyer",
                "publication_date": "2023-03-27",
                "reference": "Sigmoid Loss for Language Image Pre-Training",
                "link": "https://arxiv.org/abs/2303.15343",
                "citations": null,
                "notability_criteria": "Significant use",
                "parameters": 400000000.0,
                "training_compute_(flop)": 4.9467301e+21,
                "training_dataset_size_(gradients)": 6705000000000.0,
                "training_time_(hours)": 120.0,
                "training_hardware": "Google TPU v4",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 32.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 32000.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": 3840.0,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 10846.837246253186,
                "training_compute_estimation_method": "Hardware,Operation counting",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Vision",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 8.602059991327963,
                "log_compute": 21.69431821534127,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "LightOn Mini",
                "domain": "Language",
                "task": "Language modeling/generation,Chat",
                "organization": "LightOn",
                "authors": "Unknown",
                "publication_date": "2023-03-21",
                "reference": "LightOn's Large Language Model of 40 billion parameters: MINI",
                "link": "https://www.lighton.ai/blog/lighton-s-blog-4/lighton-s-large-language-model-of-40-billion-parameters-mini-19",
                "citations": null,
                "notability_criteria": null,
                "parameters": 40000000000.0,
                "training_compute_(flop)": 2.4e+23,
                "training_dataset_size_(gradients)": 1000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": "Self-supervised learning",
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Hosted access (no API)",
                "country": "France",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "France",
                "domain_group": "Language",
                "export_controls_sum": 6.0,
                "publication_count": 2.320372806963785,
                "log_params": 10.602059991327963,
                "log_compute": 23.380211241711606,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "GPT-4",
                "domain": "Multimodal,Language,Vision",
                "task": "Language modeling,Language modeling/generation,Question answering,Visual question answering",
                "organization": "OpenAI",
                "authors": "OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Sim\u00f3n Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain et al. (181 additional authors not shown)",
                "publication_date": "2023-03-15",
                "reference": "GPT-4 Technical Report",
                "link": "https://arxiv.org/abs/2303.08774",
                "citations": 17779.0,
                "notability_criteria": "Highly cited,SOTA improvement,Training cost",
                "parameters": 1800000000000.0,
                "training_compute_(flop)": 2.1e+25,
                "training_dataset_size_(gradients)": 5416666666666.667,
                "training_time_(hours)": 2280.0,
                "training_hardware": "NVIDIA A100 SXM4 40 GB",
                "approach": "Self-supervised learning",
                "confidence": "Likely",
                "epochs": 2.0,
                "model_accessibility": "API access",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 25000.0,
                "last_modified": "2025-10-16 15:12:25+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": true,
                "training_chip-hours": 57000000.0,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": 42166697.17422247,
                "frontier_model": true,
                "training_power_draw_(w)": 19944368.12599428,
                "training_compute_estimation_method": "Hardware",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 12.255272505103306,
                "log_compute": 25.32221929473392,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "VALL-E X",
                "domain": "Speech",
                "task": "Translation,Speech synthesis,Speech recognition (ASR),Speech-to-speech",
                "organization": "Microsoft",
                "authors": "Ziqiang Zhang, Long Zhou, Chengyi Wang, Sanyuan Chen, Yu Wu, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, Lei He, Sheng Zhao, Furu Wei",
                "publication_date": "2023-03-07",
                "reference": "Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling",
                "link": "https://arxiv.org/abs/2303.03926",
                "citations": 215.0,
                "notability_criteria": null,
                "parameters": 700000000.0,
                "training_compute_(flop)": 1.2e+21,
                "training_dataset_size_(gradients)": 151200000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA V100",
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 18:02:46+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Audio",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 8.845098040014257,
                "log_compute": 21.079181246047625,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "AudioGen",
                "domain": "Audio",
                "task": "Audio generation",
                "organization": "Meta AI,Hebrew University of Jerusalem",
                "authors": "Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer, Alexandre D\u00e9fossez, Jade Copet, Devi Parikh, Yaniv Taigman, Yossi Adi",
                "publication_date": "2023-03-05",
                "reference": "AudioGen: Textually Guided Audio Generation",
                "link": "https://arxiv.org/abs/2209.15352",
                "citations": 359.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 1000000000.0,
                "training_compute_(flop)": 9.5e+21,
                "training_dataset_size_(gradients)": 230400000000.0,
                "training_time_(hours)": 168.0,
                "training_hardware": "NVIDIA A100",
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:08:20+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": true,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 9429.74091062958,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Audio",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 9.0,
                "log_compute": 21.977723605288848,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "DiT-XL/2",
                "domain": "Image generation",
                "task": "Image generation",
                "organization": "New York University (NYU),University of California (UC) Berkeley",
                "authors": "William Peebles, Saining Xie",
                "publication_date": "2023-03-02",
                "reference": "Scalable Diffusion Models with Transformers",
                "link": "https://arxiv.org/abs/2212.09748",
                "citations": 3338.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 675000000.0,
                "training_compute_(flop)": 6e+20,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "Google TPU v3",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "United States",
                "base_model": "Stable Diffusion (LDM-KL-8-G)",
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:08:20+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open (non-commercial)",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 111048.19613085664,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware,Other",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Vision",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 8.829303772831025,
                "log_compute": 20.778151250383644,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "Palmyra Large 20B",
                "domain": "Language",
                "task": "Language modeling",
                "organization": "Writer",
                "authors": "Sam Julien / Writer",
                "publication_date": "2023-03-01",
                "reference": null,
                "link": "https://huggingface.co/Writer/palmyra-large",
                "citations": null,
                "notability_criteria": null,
                "parameters": 20000000000.0,
                "training_compute_(flop)": 9.6e+22,
                "training_dataset_size_(gradients)": 800000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Speculative",
                "epochs": 0.8,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-08-01 16:22:55+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 10.301029995663981,
                "log_compute": 22.98227123303957,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "Kosmos-1",
                "domain": "Multimodal,Language,Vision",
                "task": "Visual question answering,Image captioning,Language modeling/generation,Chat,Question answering,Document classification,Image classification,Visual puzzles",
                "organization": "Microsoft",
                "authors": "Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, Furu Wei",
                "publication_date": "2023-03-01",
                "reference": "Language Is Not All You Need: Aligning Perception with Language Models",
                "link": "https://arxiv.org/abs/2302.14045",
                "citations": null,
                "notability_criteria": null,
                "parameters": 1600000000.0,
                "training_compute_(flop)": 3.456e+21,
                "training_dataset_size_(gradients)": 360000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": "ViT-G/14 (LiT)",
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 1200000.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 9.204119982655925,
                "log_compute": 21.538573733806857,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "LLaMA-13B",
                "domain": "Language",
                "task": "Language modeling,Code generation",
                "organization": "Meta AI",
                "authors": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample",
                "publication_date": "2023-02-27",
                "reference": "LLaMA: Open and Efficient Foundation Language Models",
                "link": "https://arxiv.org/abs/2302.13971",
                "citations": 15589.0,
                "notability_criteria": "Highly cited",
                "parameters": 13000000000.0,
                "training_compute_(flop)": 7.8e+22,
                "training_dataset_size_(gradients)": 1000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100",
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:08:20+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 4000000.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 61300.86591335317,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 10.113943352306837,
                "log_compute": 22.89209460269048,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "LLaMA-33B",
                "domain": "Language",
                "task": "Language modeling,Code generation,Language modeling/generation,Question answering",
                "organization": "Meta AI",
                "authors": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample",
                "publication_date": "2023-02-27",
                "reference": "LLaMA: Open and Efficient Foundation Language Models",
                "link": "https://arxiv.org/abs/2302.13971",
                "citations": 15589.0,
                "notability_criteria": null,
                "parameters": 32500000000.0,
                "training_compute_(flop)": 2.7300000000001e+23,
                "training_dataset_size_(gradients)": 1400000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.04,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:08:20+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 4000000.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 10.511883360978874,
                "log_compute": 23.43616264704077,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "LLaMA-65B",
                "domain": "Language",
                "task": "Language modeling,Code generation",
                "organization": "Meta AI",
                "authors": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample",
                "publication_date": "2023-02-24",
                "reference": "LLaMA: Open and Efficient Foundation Language Models",
                "link": "https://arxiv.org/abs/2302.13971",
                "citations": 15589.0,
                "notability_criteria": "Historical significance,Highly cited",
                "parameters": 65200000000.0,
                "training_compute_(flop)": 5.5e+23,
                "training_dataset_size_(gradients)": 1400000000000.0,
                "training_time_(hours)": 500.0,
                "training_hardware": "NVIDIA A100",
                "approach": "Supervised",
                "confidence": "Confident",
                "epochs": 1.09,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 2048.0,
                "last_modified": "2025-10-16 13:08:20+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 4000000.0,
                "organization_categorization": "Industry",
                "foundation_model": true,
                "training_chip-hours": 1024000.0,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": 578026.3043,
                "frontier_model": false,
                "training_power_draw_(w)": 1634534.091472035,
                "training_compute_estimation_method": "Operation counting",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 10.81424759573192,
                "log_compute": 23.740362689494244,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "LLaMA-7B",
                "domain": "Language",
                "task": "Language modeling,Code generation",
                "organization": "Meta AI",
                "authors": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample",
                "publication_date": "2023-02-24",
                "reference": "LLaMA: Open and Efficient Foundation Language Models",
                "link": "https://arxiv.org/abs/2302.13971",
                "citations": 15589.0,
                "notability_criteria": "Significant use,Highly cited",
                "parameters": 6700000000.0,
                "training_compute_(flop)": 4.00000001e+22,
                "training_dataset_size_(gradients)": 1000000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100",
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:08:20+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 4000000.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": 82432.0,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 46889.820096866126,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 9.826074802700827,
                "log_compute": 22.6020599924137,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "Hyena 1.3B",
                "domain": "Language",
                "task": "Language modeling/generation",
                "organization": "Stanford University,University of Montreal / Universit\u00e9 de Montr\u00e9al,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms)",
                "authors": "Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y. Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, Christopher R\u00e9",
                "publication_date": "2023-02-21",
                "reference": "Hyena Hierarchy: Towards Larger Convolutional Language Models",
                "link": "https://arxiv.org/abs/2302.10866",
                "citations": 365.0,
                "notability_criteria": null,
                "parameters": 1300000000.0,
                "training_compute_(flop)": 4.76e+19,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100 SXM4 80 GB",
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 8.0,
                "last_modified": "2025-10-16 13:08:39+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 256.0,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 6385.325372107343,
                "training_compute_estimation_method": "Comparison with other models",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 9.113943352306837,
                "log_compute": 19.67760695272049,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "Hyena-2 355M",
                "domain": "Language",
                "task": "Language modeling/generation",
                "organization": "Stanford University,University of Montreal / Universit\u00e9 de Montr\u00e9al,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms)",
                "authors": "Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y. Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, Christopher R\u00e9",
                "publication_date": "2023-02-21",
                "reference": "Hyena Hierarchy: Towards Larger Convolutional Language Models",
                "link": "https://arxiv.org/abs/2302.10866",
                "citations": 365.0,
                "notability_criteria": null,
                "parameters": 355000000.0,
                "training_compute_(flop)": 3.93e+19,
                "training_dataset_size_(gradients)": 15000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100 SXM4 80 GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 8.0,
                "last_modified": "2025-10-16 13:08:38+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 256.0,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 6385.325372107343,
                "training_compute_estimation_method": "Reported,Operation counting",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 8.550228353055093,
                "log_compute": 19.594392550375428,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "Hyena-2 153M",
                "domain": "Language",
                "task": "Language modeling/generation",
                "organization": "Stanford University,University of Montreal / Universit\u00e9 de Montr\u00e9al,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms)",
                "authors": "Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y. Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, Christopher R\u00e9",
                "publication_date": "2023-02-21",
                "reference": "Hyena Hierarchy: Towards Larger Convolutional Language Models",
                "link": "https://arxiv.org/abs/2302.10866",
                "citations": 365.0,
                "notability_criteria": null,
                "parameters": 153000000.0,
                "training_compute_(flop)": 1.87e+19,
                "training_dataset_size_(gradients)": 15000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100 SXM4 80 GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 8.0,
                "last_modified": "2025-10-16 13:08:38+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 256.0,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 6385.325372107343,
                "training_compute_estimation_method": "Reported,Operation counting",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 8.1846914308176,
                "log_compute": 19.271841606536498,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "ViT-22B",
                "domain": "Vision",
                "task": "Object detection,Image classification",
                "organization": "Google",
                "authors": "Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, Rodolphe Jenatton, Lucas Beyer, Michael Tschannen, Anurag Arnab, Xiao Wang, Carlos Riquelme, Matthias Minderer, Joan Puigcerver, Utku Evci, Manoj Kumar, Sjoerd van Steenkiste, Gamaleldin F. Elsayed, Aravindh Mahendran, Fisher Yu, Avital Oliver, Fantine Huot, Jasmijn Bastings, Mark Patrick Collier, Alexey Gritsenko, Vighnesh Birodkar, Cristina Vasconcelos, Yi Tay, Thomas Mensink, Alexander Kolesnikov, Filip Paveti\u0107, Dustin Tran, Thomas Kipf, Mario Lu\u010di\u0107, Xiaohua Zhai, Daniel Keysers, Jeremiah Harmsen, Neil Houlsby",
                "publication_date": "2023-02-10",
                "reference": "Scaling Vision Transformers to 22 Billion Parameters",
                "link": "https://arxiv.org/abs/2302.05442v1",
                "citations": 687.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 21743000000.0,
                "training_compute_(flop)": 1.93248e+23,
                "training_dataset_size_(gradients)": 4000000000.0,
                "training_time_(hours)": 347.4,
                "training_hardware": "Google TPU v4",
                "approach": null,
                "confidence": "Confident",
                "epochs": 2.9,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 1024.0,
                "last_modified": "2025-10-16 13:08:38+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": 285555.57016183576,
                "frontier_model": false,
                "training_power_draw_(w)": 347446.80145890714,
                "training_compute_estimation_method": "Hardware",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Vision",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 10.337319465856927,
                "log_compute": 23.286115007928224,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "BLIP-2 (Q-Former)",
                "domain": "Vision,Language",
                "task": "Visual question answering,Image captioning",
                "organization": "Salesforce Research",
                "authors": "Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi",
                "publication_date": "2023-01-30",
                "reference": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
                "link": "https://arxiv.org/abs/2301.12597",
                "citations": 5696.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 1480000000.0,
                "training_compute_(flop)": 1.20000000001e+21,
                "training_dataset_size_(gradients)": 2322000000.0,
                "training_time_(hours)": 200.0,
                "training_hardware": "NVIDIA A100 SXM4 40 GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 16.0,
                "last_modified": "2025-10-16 13:08:38+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": 3200.0,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 1960.8225382725811,
                "frontier_model": false,
                "training_power_draw_(w)": 12776.908953102382,
                "training_compute_estimation_method": "Hardware",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 9.170261715394957,
                "log_compute": 21.079181246051245,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "Genie-SCOPe (bio)",
                "domain": "Biology",
                "task": "Protein design",
                "organization": "Columbia University",
                "authors": "Yeqing Lin, Mohammed AlQuraishi",
                "publication_date": "2023-01-29",
                "reference": "Generating Novel, Designable, and Diverse Protein Structures by Equivariantly Diffusing Oriented Residue Clouds",
                "link": "https://arxiv.org/abs/2301.12485",
                "citations": null,
                "notability_criteria": null,
                "parameters": 4100000.0,
                "training_compute_(flop)": 1.81149696e+21,
                "training_dataset_size_(gradients)": 1753200.0,
                "training_time_(hours)": 336.0,
                "training_hardware": "NVIDIA A100",
                "approach": null,
                "confidence": "Confident",
                "epochs": 30000.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 12.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 9582.89511749987,
                "training_compute_estimation_method": "Hardware",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Biology",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 6.6127838567197355,
                "log_compute": 21.25803760955116,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "Ankh_large",
                "domain": "Biology",
                "task": "Protein generation,Proteins,Protein or nucleotide language model (pLM/nLM),Protein contact and distance prediction,Protein classification,Protein localization prediction,Protein fold classification",
                "organization": "Technical University of Munich,Columbia University",
                "authors": "Ahmed Elnaggar, Hazem Essam, Wafaa Salah-Eldin, Walid Moustafa, Mohamed Elkerdawy, Charlotte Rochereau, Burkhard Rost",
                "publication_date": "2023-01-16",
                "reference": "Ankh: Optimized Protein Language Model Unlocks General-Purpose Modelling",
                "link": "https://arxiv.org/abs/2301.06568",
                "citations": 120.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 1900000000.0,
                "training_compute_(flop)": 6.5e+21,
                "training_dataset_size_(gradients)": 14000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "Google TPU v4",
                "approach": null,
                "confidence": "Confident",
                "epochs": 68.0,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 64.0,
                "last_modified": "2025-10-16 13:08:39+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 524288.0,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 4802.398249072418,
                "frontier_model": false,
                "training_power_draw_(w)": 21727.518178781513,
                "training_compute_estimation_method": "Operation counting,Third-party estimation",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "Germany",
                "domain_group": "Biology",
                "export_controls_sum": 6.0,
                "publication_count": 3.4737849918245405,
                "log_params": 9.278753600952829,
                "log_compute": 21.812913356642856,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "Ankh_base",
                "domain": "Biology",
                "task": "Protein generation,Proteins,Protein or nucleotide language model (pLM/nLM),Protein contact and distance prediction,Protein classification,Protein localization prediction,Protein fold classification",
                "organization": "Technical University of Munich,Columbia University",
                "authors": "Ahmed Elnaggar, Hazem Essam, Wafaa Salah-Eldin, Walid Moustafa, Mohamed Elkerdawy, Charlotte Rochereau, Burkhard Rost",
                "publication_date": "2023-01-16",
                "reference": "Ankh: Optimized Protein Language Model Unlocks General-Purpose Modelling",
                "link": "https://arxiv.org/abs/2301.06568",
                "citations": 120.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 740000000.0,
                "training_compute_(flop)": 2.6e+21,
                "training_dataset_size_(gradients)": 14000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "Google TPU v4",
                "approach": null,
                "confidence": "Confident",
                "epochs": 68.0,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:08:38+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 1920.959299628968,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Third-party estimation",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "Germany",
                "domain_group": "Biology",
                "export_controls_sum": 6.0,
                "publication_count": 3.4737849918245405,
                "log_params": 8.869231719730976,
                "log_compute": 21.414973347970818,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "Nucleotide Transformer",
                "domain": "Biology",
                "task": "Protein or nucleotide language model (pLM/nLM),Nucleotide generation",
                "organization": "NVIDIA,Technical University of Munich,InstaDeep",
                "authors": "Hugo Dalla-Torre, Liam Gonzalez, Javier Mendoza Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, Evan Trop, Hassan Sirelkhatim, Guillaume Richard, Marcin Skwark, Karim Beguir,\nMarie Lopez, Thomas Pierrot",
                "publication_date": "2023-01-15",
                "reference": "The Nucleotide Transformer: Building and Evaluating Robust\nFoundation Models for Human Genomics",
                "link": "https://www.biorxiv.org/content/10.1101/2023.01.11.523679v1.full.pdf",
                "citations": 22.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 2500000000.0,
                "training_compute_(flop)": 8.08e+21,
                "training_dataset_size_(gradients)": 300000000000.0,
                "training_time_(hours)": 672.0,
                "training_hardware": "NVIDIA A100",
                "approach": "Self-supervised learning",
                "confidence": "Likely",
                "epochs": 1.0,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": 9.35e+17,
                "hardware_quantity": 128.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": 86016.0,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 51064.70621394041,
                "frontier_model": false,
                "training_power_draw_(w)": 102249.42137571,
                "training_compute_estimation_method": "Operation counting,Hardware",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Biology",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 9.397940008672037,
                "log_compute": 21.907411360774585,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "VALL-E",
                "domain": "Audio,Speech",
                "task": "Speech synthesis,Text-to-speech (TTS)",
                "organization": "Microsoft",
                "authors": "Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, Lei He, Sheng Zhao, Furu Wei",
                "publication_date": "2023-01-05",
                "reference": "Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers",
                "link": "https://arxiv.org/abs/2301.02111",
                "citations": 900.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 353000000.0,
                "training_compute_(flop)": 1.01e+19,
                "training_dataset_size_(gradients)": 76800000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA V100",
                "approach": "Supervised",
                "confidence": "Speculative",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:08:39+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": true,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 11.40575196486363,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2023,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Audio",
                "export_controls_sum": 22.0,
                "publication_count": 12.024618272999188,
                "log_params": 8.547774705387823,
                "log_compute": 19.00432137378264,
                "pub_bin": "Low",
                "export_bin": "Medium"
            },
            {
                "model": "Hybrid H3-2.7B",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering",
                "organization": "Stanford University,University at Buffalo",
                "authors": "Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, Christopher R\u00e9",
                "publication_date": "2022-12-28",
                "reference": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
                "link": "https://arxiv.org/abs/2212.14052",
                "citations": 484.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 2700000000.0,
                "training_compute_(flop)": 6.48e+21,
                "training_dataset_size_(gradients)": 400000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100 SXM4 80 GB",
                "approach": null,
                "confidence": "Likely",
                "epochs": 509.02,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 8.0,
                "last_modified": "2025-10-16 13:08:38+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 6393.151008587547,
                "training_compute_estimation_method": "Operation counting",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 9.431363764158988,
                "log_compute": 21.811575005870594,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "OPT-IML (175B)",
                "domain": "Language",
                "task": "Language modeling",
                "organization": "Meta AI",
                "authors": "Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, Xian Li, Brian O'Horo, Gabriel Pereyra, Jeff Wang, Christopher Dewan, Asli Celikyilmaz, Luke Zettlemoyer, Ves Stoyanov",
                "publication_date": "2022-12-22",
                "reference": "OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization",
                "link": "https://arxiv.org/abs/2212.12017",
                "citations": 284.0,
                "notability_criteria": null,
                "parameters": 175000000000.0,
                "training_compute_(flop)": 4.3e+23,
                "training_dataset_size_(gradients)": 2000000000.0,
                "training_time_(hours)": 72.0,
                "training_hardware": "NVIDIA A100",
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "United States",
                "base_model": "OPT-175B",
                "finetune_compute_(flop)": 2.1e+21,
                "hardware_quantity": 128.0,
                "last_modified": "2025-10-14 18:01:53+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 125.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": 9216.0,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 102304.08471008124,
                "training_compute_estimation_method": "Operation counting",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 11.243038048686294,
                "log_compute": 23.633468455579585,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "Whisper v2",
                "domain": "Speech",
                "task": "Speech recognition (ASR)",
                "organization": "OpenAI",
                "authors": "Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever",
                "publication_date": "2022-12-05",
                "reference": "Robust Speech Recognition via Large-Scale Weak Supervision",
                "link": "https://huggingface.co/openai/whisper-large-v2\n\nhttps://arxiv.org/abs/2212.04356",
                "citations": 4738.0,
                "notability_criteria": null,
                "parameters": 1550000000.0,
                "training_compute_(flop)": 1.1e+23,
                "training_dataset_size_(gradients)": 12403200000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": "Self-supervised learning",
                "confidence": "Likely",
                "epochs": 7.5,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:07:57+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 1024.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Comparison with other models",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Audio",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 9.190331698170292,
                "log_compute": 23.041392685158225,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "Galactica",
                "domain": "Language,Biology",
                "task": "Language modeling,Question answering,Mathematical reasoning,Medical diagnosis,Language modeling/generation",
                "organization": "Meta AI",
                "authors": "Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic",
                "publication_date": "2022-11-16",
                "reference": "Galactica: A Large Language Model for Science",
                "link": "https://arxiv.org/abs/2211.09085",
                "citations": 842.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 120000000000.0,
                "training_compute_(flop)": 3.24e+23,
                "training_dataset_size_(gradients)": 106000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100 SXM4 80 GB",
                "approach": "Self-supervised learning",
                "confidence": "Likely",
                "epochs": 4.0,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 128.0,
                "last_modified": "2025-10-16 13:08:20+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 2000000.0,
                "organization_categorization": "Industry",
                "foundation_model": true,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": 591076.8943544837,
                "frontier_model": false,
                "training_power_draw_(w)": 102386.13451047534,
                "training_compute_estimation_method": "Operation counting",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 11.079181246047625,
                "log_compute": 23.510545010206613,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "eDiff-I",
                "domain": "Image generation",
                "task": "Image generation,Text-to-image",
                "organization": "NVIDIA",
                "authors": "Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, Tero Karras, Ming-Yu Liu",
                "publication_date": "2022-11-02",
                "reference": "eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers",
                "link": "https://arxiv.org/abs/2211.01324",
                "citations": 909.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 9100000000.0,
                "training_compute_(flop)": 5.46e+19,
                "training_dataset_size_(gradients)": 1556275200000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100",
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 18:01:51+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Vision",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 9.959041392321094,
                "log_compute": 19.737192642704738,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "XY-LENTXL",
                "domain": "Language",
                "task": "Representation learning,Language modeling",
                "organization": "Microsoft",
                "authors": "Barun Patra, Saksham Singhal, Shaohan Huang, Zewen Chi, Li Dong, Furu Wei, Vishrav Chaudhary, Xia Song",
                "publication_date": "2022-10-26",
                "reference": "Beyond English-Centric Bitexts for Better Multilingual Language Representation Learning",
                "link": "https://arxiv.org/abs/2210.14867",
                "citations": null,
                "notability_criteria": null,
                "parameters": 2000000000.0,
                "training_compute_(flop)": 7.2e+21,
                "training_dataset_size_(gradients)": 600000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA H100 SXM5 80GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 512.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 4000000.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 717038.1909599457,
                "training_compute_estimation_method": "Operation counting",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 9.301029995663981,
                "log_compute": 21.85733249643127,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "Verbatim Memory Transformer (108M)",
                "domain": "Language",
                "task": "Language modeling",
                "organization": "Johns Hopkins University,New York University (NYU)",
                "authors": "Kristijan Armeni, Christopher Honey, Tal Linzen",
                "publication_date": "2022-10-24",
                "reference": "Characterizing Verbatim Short-Term Memory in Neural Language Models",
                "link": "https://arxiv.org/abs/2210.13569",
                "citations": null,
                "notability_criteria": null,
                "parameters": 107700000.0,
                "training_compute_(flop)": 9.864288e+17,
                "training_dataset_size_(gradients)": 40000000.0,
                "training_time_(hours)": 12.0,
                "training_hardware": "NVIDIA Quadro RTX 8000",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 1.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 285.82195776886823,
                "training_compute_estimation_method": null,
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 8.032215703297982,
                "log_compute": 17.994065743533426,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "U-PaLM (540B)",
                "domain": "Language",
                "task": "Language generation,Language modeling/generation,Question answering,Mathematical reasoning,Quantitative reasoning",
                "organization": "Google",
                "authors": "Yi Tay, Jason Wei, Hyung Won Chung, Vinh Q. Tran, David R. So, Siamak Shakeri, Xavier Garcia, Huaixiu Steven Zheng, Jinfeng Rao, Aakanksha Chowdhery, Denny Zhou, Donald Metzler, Slav Petrov, Neil Houlsby, Quoc V. Le, Mostafa Dehghani",
                "publication_date": "2022-10-20",
                "reference": "Transcending Scaling Laws with 0.1% Extra Compute",
                "link": "https://arxiv.org/abs/2210.11399",
                "citations": 73.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 540000000000.0,
                "training_compute_(flop)": 2.53e+24,
                "training_dataset_size_(gradients)": 1300000000.0,
                "training_time_(hours)": 120.0,
                "training_hardware": "Google TPU v4",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": "PaLM (540B)",
                "finetune_compute_(flop)": 4e+21,
                "hardware_quantity": 512.0,
                "last_modified": "2025-10-14 18:01:52+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": 61440.0,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": true,
                "training_power_draw_(w)": 174161.11557264757,
                "training_compute_estimation_method": "Comparison with other models",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 11.732393759822969,
                "log_compute": 24.40312052117582,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "Flan-PaLM 540B",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Mathematical reasoning",
                "organization": "Google",
                "authors": "Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, Jason Wei",
                "publication_date": "2022-10-20",
                "reference": "Scaling Instruction-Finetuned Language Models",
                "link": "https://arxiv.org/abs/2210.11416",
                "citations": 3502.0,
                "notability_criteria": "Highly cited,SOTA improvement",
                "parameters": 540000000000.0,
                "training_compute_(flop)": 2.540000000001e+24,
                "training_dataset_size_(gradients)": 1400000000.0,
                "training_time_(hours)": 37.0,
                "training_hardware": "Google TPU v4",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": "PaLM (540B)",
                "finetune_compute_(flop)": 5.6e+21,
                "hardware_quantity": 512.0,
                "last_modified": "2025-10-16 13:08:21+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": 18944.0,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": true,
                "training_power_draw_(w)": 174161.11557264757,
                "training_compute_estimation_method": "Reported,Hardware",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 11.732393759822969,
                "log_compute": 24.40483371662011,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "Flan-T5 11B",
                "domain": "Language",
                "task": "Language modeling/generation",
                "organization": "Google",
                "authors": "Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, Jason Wei",
                "publication_date": "2022-10-20",
                "reference": "Scaling Instruction-Finetuned Language Models",
                "link": "https://arxiv.org/abs/2210.11416, https://huggingface.co/google/flan-t5-xxl",
                "citations": 3502.0,
                "notability_criteria": null,
                "parameters": 11000000000.0,
                "training_compute_(flop)": 3.3e+22,
                "training_dataset_size_(gradients)": 100000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "Google TPU v4",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": "T5-11B",
                "finetune_compute_(flop)": 7.6e+19,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:08:20+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 98374.29476250126,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Reported",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 10.041392685158225,
                "log_compute": 22.518513939877888,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "GenSLM",
                "domain": "Biology",
                "task": "Protein or nucleotide language model (pLM/nLM)",
                "organization": "University of Chicago,NVIDIA,Harvard University,Cerebras Systems,Technical University of Munich,California Institute of Technology",
                "authors": "Maxim Zvyagin, Alexander Brace, Kyle Hippe, Yuntian Deng, Bin Zhang, Cindy Orozco Bohorquez, Austin Clyde, Bharat Kale, Danilo Perez-Rivera, Heng Ma, Carla M. Mann, Michael Irvin, J. Gregory Pauloski, Logan Ward, Valerie Hayot, Murali Emani, Sam Foreman, Zhen Xie, Diangen Lin, Maulik Shukla, Weili Nie, Josh Romero, Christian Dallago, Arash Vahdat, Chaowei Xiao, Thomas Gibbs, Ian Foster, James J. Davis, Michael E. Papka, Thomas Brettin, Rick Stevens, Anima Anandkumar, Venkatram Vishwanath, Arvind Ramanathan",
                "publication_date": "2022-10-11",
                "reference": "GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics",
                "link": "https://www.biorxiv.org/content/biorxiv/early/2022/10/11/2022.10.10.511571.full.pdf",
                "citations": 104.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 25000000000.0,
                "training_compute_(flop)": 1.42e+21,
                "training_dataset_size_(gradients)": 225280000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 21:49:55+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": true,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Reported",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Biology",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 10.397940008672037,
                "log_compute": 21.152288344383056,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "DiffDock",
                "domain": "Biology",
                "task": "Proteins",
                "organization": "Massachusetts Institute of Technology (MIT)",
                "authors": "Gabriele Corso, Hannes St\u00e4rk, Bowen Jing, Regina Barzilay, Tommi Jaakkola",
                "publication_date": "2022-10-04",
                "reference": "DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking",
                "link": "https://arxiv.org/abs/2210.01776, https://docs.nvidia.com/bionemo-framework/latest/models/diffdock.html",
                "citations": 516.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 20240000.0,
                "training_compute_(flop)": 7.2e+19,
                "training_dataset_size_(gradients)": 4352000.0,
                "training_time_(hours)": 432.0,
                "training_hardware": "NVIDIA RTX A6000",
                "approach": null,
                "confidence": "Likely",
                "epochs": 850.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 18:01:52+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Biology",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 7.306210508167761,
                "log_compute": 19.85733249643127,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "NMST+GPT-2",
                "domain": "Language",
                "task": "Language modeling",
                "organization": "New York University (NYU)",
                "authors": "Eugene Choi, Cheolhyoung Lee, Kyunghyun Cho",
                "publication_date": "2022-10-03",
                "reference": "A Non-monotonic Self-terminating Language Model",
                "link": "https://arxiv.org/abs/2210.00660",
                "citations": 0.0,
                "notability_criteria": null,
                "parameters": 124000000.0,
                "training_compute_(flop)": 1.20380928e+20,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": 4.97,
                "model_accessibility": "Unreleased",
                "country": "United States",
                "base_model": "GPT-2 (124M)",
                "finetune_compute_(flop)": 3.80928e+17,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": null,
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 8.093421685162236,
                "log_compute": 20.080557686918045,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "Whisper",
                "domain": "Speech",
                "task": "Speech recognition (ASR)",
                "organization": "OpenAI",
                "authors": "Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever",
                "publication_date": "2022-09-21",
                "reference": "Robust Speech Recognition via Large-Scale Weak Supervision",
                "link": "https://cdn.openai.com/papers/whisper.pdf\n\nhttps://arxiv.org/abs/2212.04356",
                "citations": 4738.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 1550000000.0,
                "training_compute_(flop)": 4.2072663e+21,
                "training_dataset_size_(gradients)": 12403200000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": "Self-supervised learning",
                "confidence": "Likely",
                "epochs": 3.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:07:57+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 256.0,
                "organization_categorization": "Industry",
                "foundation_model": true,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Audio",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 9.190331698170292,
                "log_compute": 21.624000001667657,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "PaLI",
                "domain": "Language,Vision,Multimodal",
                "task": "Visual question answering,Language modeling/generation,Image captioning",
                "organization": "Google",
                "authors": "Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, Radu Soricut",
                "publication_date": "2022-09-14",
                "reference": "PaLI: A Jointly-Scaled Multilingual Language-Image Model",
                "link": "https://arxiv.org/abs/2209.06794v4",
                "citations": 831.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 16900000000.0,
                "training_compute_(flop)": 1.69e+23,
                "training_dataset_size_(gradients)": 143507000000.0,
                "training_time_(hours)": 240.0,
                "training_hardware": "Google TPU v4",
                "approach": null,
                "confidence": "Likely",
                "epochs": 1.0,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 1024.0,
                "last_modified": "2025-10-16 13:08:20+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": true,
                "training_chip-hours": 172032.0,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": 50878.10777366616,
                "frontier_model": false,
                "training_power_draw_(w)": 348601.5921270125,
                "training_compute_estimation_method": "Operation counting,Hardware",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 10.227886704613674,
                "log_compute": 23.227886704613674,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "BEIT-3",
                "domain": "Multimodal,Vision,Language",
                "task": "Object detection,Semantic segmentation,Image classification,Visual question answering,Image captioning,Language generation",
                "organization": "Microsoft",
                "authors": "Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks",
                "publication_date": "2022-08-22",
                "reference": "Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks",
                "link": "https://arxiv.org/abs/2208.10442",
                "citations": 679.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 1900000000.0,
                "training_compute_(flop)": 7e+19,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 18:01:53+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 9.278753600952829,
                "log_compute": 19.845098040014257,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "Luminous-supreme",
                "domain": "Language",
                "task": "Language generation",
                "organization": "Aleph Alpha",
                "authors": "Unknown",
                "publication_date": "2022-08-15",
                "reference": "Model Card Luminous",
                "link": "https://docs.aleph-alpha.com/docs/introduction/model-card/",
                "citations": null,
                "notability_criteria": null,
                "parameters": 70000000000.0,
                "training_compute_(flop)": 3.5461e+23,
                "training_dataset_size_(gradients)": 1069300000000.0,
                "training_time_(hours)": 2016.0,
                "training_hardware": "NVIDIA A100 SXM4 40 GB,NVIDIA A100 SXM4 80 GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "API access",
                "country": "Germany",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 512.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware,Operation counting",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "Germany",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 3.437316959598853,
                "log_params": 10.845098040014257,
                "log_compute": 23.549750978539528,
                "pub_bin": "Low",
                "export_bin": "Low"
            },
            {
                "model": "Luminous-extended",
                "domain": "Language",
                "task": "Language modeling/generation",
                "organization": "Aleph Alpha",
                "authors": "Unknown",
                "publication_date": "2022-08-15",
                "reference": null,
                "link": "https://docs.aleph-alpha.com/docs/Deprecated%20Luminous/Deprecated-Luminous/model-card/",
                "citations": null,
                "notability_criteria": null,
                "parameters": 30000000000.0,
                "training_compute_(flop)": 1.0019457e+23,
                "training_dataset_size_(gradients)": 460000000000.0,
                "training_time_(hours)": 1344.0,
                "training_hardware": "NVIDIA A100 SXM4 40 GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "API access",
                "country": "Germany",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 512.0,
                "last_modified": "2025-08-01 16:22:40+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": 360000.0,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 410393.6048104728,
                "training_compute_estimation_method": "Operation counting,Hardware",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "Germany",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 3.437316959598853,
                "log_params": 10.477121254719663,
                "log_compute": 23.000844185773374,
                "pub_bin": "Low",
                "export_bin": "Low"
            },
            {
                "model": "Luminous-base",
                "domain": "Language",
                "task": "Language modeling/generation",
                "organization": "Aleph Alpha",
                "authors": "Unknown",
                "publication_date": "2022-08-15",
                "reference": null,
                "link": "https://docs.aleph-alpha.com/docs/Deprecated%20Luminous/Deprecated-Luminous/model-card/",
                "citations": null,
                "notability_criteria": null,
                "parameters": 13000000000.0,
                "training_compute_(flop)": 3.1673782e+22,
                "training_dataset_size_(gradients)": 402000000000.0,
                "training_time_(hours)": 1344.0,
                "training_hardware": "NVIDIA A100 SXM4 40 GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "API access",
                "country": "Germany",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 128.0,
                "last_modified": "2025-08-01 16:22:40+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": 95000.0,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 102598.4012026182,
                "training_compute_estimation_method": "Operation counting,Hardware",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "Germany",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 3.437316959598853,
                "log_params": 10.113943352306837,
                "log_compute": 22.50069992329188,
                "pub_bin": "Low",
                "export_bin": "Low"
            },
            {
                "model": "PeTriBERT",
                "domain": "Biology",
                "task": "Protein generation",
                "organization": "University of Montpellier,BionomeeX",
                "authors": "Baldwin Dumortier, Antoine Liutkus, Cl\u00e9ment Carr\u00e9, Gabriel Krouk",
                "publication_date": "2022-08-13",
                "reference": "PeTriBERT : Augmenting BERT with tridimensional encoding for inverse protein folding and design",
                "link": "https://www.biorxiv.org/content/10.1101/2022.08.10.503344v1.abstract",
                "citations": 14.0,
                "notability_criteria": null,
                "parameters": 40000000.0,
                "training_compute_(flop)": 1e+20,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 70.0,
                "training_hardware": "NVIDIA Tesla V100 DGXS 32 GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unknown",
                "country": "France",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 8.0,
                "last_modified": "2025-10-14 21:49:55+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 4007.928551099969,
                "training_compute_estimation_method": "Hardware",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "France",
                "domain_group": "Biology",
                "export_controls_sum": 1.0,
                "publication_count": 2.3327744955886316,
                "log_params": 7.6020599913279625,
                "log_compute": 20.0,
                "pub_bin": "Low",
                "export_bin": "Low"
            },
            {
                "model": "AlexaTM 20B",
                "domain": "Language",
                "task": "Language modeling,Translation,Question answering",
                "organization": "Amazon",
                "authors": "Saleh Soltan, Shankar Ananthakrishnan, Jack FitzGerald, Rahul Gupta, Wael Hamza, Haidar Khan, Charith Peris, Stephen Rawls, Andy Rosenbaum, Anna Rumshisky, Chandana Satya Prakash, Mukund Sridhar, Fabian Triefenbach, Apurv Verma, Gokhan Tur, Prem Natarajan",
                "publication_date": "2022-08-02",
                "reference": "AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model",
                "link": "https://arxiv.org/abs/2208.01448",
                "citations": 86.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 19750000000.0,
                "training_compute_(flop)": 2.04374016e+23,
                "training_dataset_size_(gradients)": 1319000000000.0,
                "training_time_(hours)": 2880.0,
                "training_hardware": "NVIDIA A100",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "API access",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 128.0,
                "last_modified": "2025-10-14 18:01:51+00:00",
                "training_cloud_compute_vendor": "AWS",
                "batch_size": 2000000.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": 368640.0,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": 267943.21130997164,
                "frontier_model": false,
                "training_power_draw_(w)": 102628.10792703854,
                "training_compute_estimation_method": "Hardware",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 10.29556709996248,
                "log_compute": 23.310425679010848,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "ProtGPT2",
                "domain": "Biology",
                "task": "Proteins,Protein generation,Protein or nucleotide language model (pLM/nLM)",
                "organization": "University of Bayreuth",
                "authors": "Noelia Ferruz, Steffen Schmidt, Birte H\u00f6cker ",
                "publication_date": "2022-07-27",
                "reference": "ProtGPT2 is a deep unsupervised language model for protein design",
                "link": "https://www.nature.com/articles/s41467-022-32007-7",
                "citations": 618.0,
                "notability_criteria": null,
                "parameters": 738000000.0,
                "training_compute_(flop)": 4.1e+21,
                "training_dataset_size_(gradients)": 25535777280.0,
                "training_time_(hours)": 96.0,
                "training_hardware": "NVIDIA A100",
                "approach": "Unsupervised",
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Germany",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 128.0,
                "last_modified": "2025-10-14 21:49:55+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": 12288.0,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 102641.82162383666,
                "training_compute_estimation_method": "Hardware,Third-party estimation",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "Germany",
                "domain_group": "Biology",
                "export_controls_sum": 1.0,
                "publication_count": 3.437316959598853,
                "log_params": 8.868056361823042,
                "log_compute": 21.612783856719737,
                "pub_bin": "Low",
                "export_bin": "Low"
            },
            {
                "model": "OmegaPLM",
                "domain": "Biology",
                "task": "Proteins,Protein folding prediction",
                "organization": "Massachusetts Institute of Technology (MIT),Westlake University",
                "authors": "Ruidong Wu, Fan Ding, Rui Wang, Rui Shen, Xiwen Zhang, Shitong Luo, Chenpeng Su, Zuofan Wu, Qi Xie, Bonnie Berger, Jianzhu Ma, Jian Peng",
                "publication_date": "2022-07-22",
                "reference": "High-resolution de novo structure prediction from primary sequence",
                "link": "https://www.biorxiv.org/content/10.1101/2022.07.21.500999v1",
                "citations": 395.0,
                "notability_criteria": "Historical significance",
                "parameters": 670000000.0,
                "training_compute_(flop)": 1.03514112e+22,
                "training_dataset_size_(gradients)": 1258291200000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100 SXM4 80 GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:08:20+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 2097152.0,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": 61440.0,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 52400.32118528479,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Biology",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 8.826074802700827,
                "log_compute": 22.014999560864865,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "ESM2-15B",
                "domain": "Biology",
                "task": "Proteins,Protein or nucleotide language model (pLM/nLM),Protein folding prediction",
                "organization": "Meta AI,New York University (NYU),Stanford University,Massachusetts Institute of Technology (MIT)",
                "authors": "Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives",
                "publication_date": "2022-07-21",
                "reference": "Evolutionary-scale prediction of atomic-level protein structure with a language model",
                "link": "https://www.science.org/doi/abs/10.1126/science.ade2574\nhttps://www.biorxiv.org/content/10.1101/2022.07.20.500902v2",
                "citations": 636.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 15000000000.0,
                "training_compute_(flop)": 7.35000000001e+22,
                "training_dataset_size_(gradients)": 15360000000.0,
                "training_time_(hours)": 1440.0,
                "training_hardware": "NVIDIA V100",
                "approach": "Unsupervised",
                "confidence": "Confident",
                "epochs": 72.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 512.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 163467.82019979745,
                "frontier_model": false,
                "training_power_draw_(w)": 307966.61145938886,
                "training_compute_estimation_method": "Hardware,Third-party estimation",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Biology",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 10.176091259055681,
                "log_compute": 22.866287339084785,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "ESM2-3B",
                "domain": "Biology",
                "task": "Proteins,Protein or nucleotide language model (pLM/nLM),Protein folding prediction",
                "organization": "Meta AI,New York University (NYU),Stanford University,Massachusetts Institute of Technology (MIT)",
                "authors": "Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives",
                "publication_date": "2022-07-21",
                "reference": "Evolutionary-scale prediction of atomic-level protein structure with a language model",
                "link": "https://www.science.org/doi/abs/10.1126/science.ade2574",
                "citations": 636.0,
                "notability_criteria": null,
                "parameters": 3000000000.0,
                "training_compute_(flop)": 3.000000001e+22,
                "training_dataset_size_(gradients)": 15360000000.0,
                "training_time_(hours)": 720.0,
                "training_hardware": null,
                "approach": "Unsupervised",
                "confidence": "Confident",
                "epochs": 83.3,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware,Third-party estimation",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Biology",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 9.477121254719663,
                "log_compute": 22.47712125486443,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "ESM2-650M",
                "domain": "Biology",
                "task": "Proteins,Protein or nucleotide language model (pLM/nLM),Protein folding prediction",
                "organization": "Meta AI,New York University (NYU),Stanford University,Massachusetts Institute of Technology (MIT)",
                "authors": "Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives",
                "publication_date": "2022-07-21",
                "reference": "Evolutionary-scale prediction of atomic-level protein structure with a language model",
                "link": "https://www.science.org/doi/abs/10.1126/science.ade2574",
                "citations": 636.0,
                "notability_criteria": null,
                "parameters": 650000000.0,
                "training_compute_(flop)": 7.560000000001e+21,
                "training_dataset_size_(gradients)": 15360000000.0,
                "training_time_(hours)": 192.0,
                "training_hardware": "NVIDIA V100",
                "approach": "Unsupervised",
                "confidence": "Confident",
                "epochs": 83.3,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 512.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 20972.27663102349,
                "frontier_model": false,
                "training_power_draw_(w)": 307966.61145938886,
                "training_compute_estimation_method": "Hardware,Third-party estimation",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Biology",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 8.812913356642856,
                "log_compute": 21.878521795501264,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "ESM2-150M",
                "domain": "Biology",
                "task": "Proteins,Protein or nucleotide language model (pLM/nLM),Protein folding prediction",
                "organization": "Meta AI,New York University (NYU),Stanford University,Massachusetts Institute of Technology (MIT)",
                "authors": "Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives",
                "publication_date": "2022-07-21",
                "reference": "Evolutionary-scale prediction of atomic-level protein structure with a language model",
                "link": "https://www.science.org/doi/abs/10.1126/science.ade2574",
                "citations": 636.0,
                "notability_criteria": null,
                "parameters": 150000000.0,
                "training_compute_(flop)": 1.1e+21,
                "training_dataset_size_(gradients)": 15360000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": "Unsupervised",
                "confidence": "Confident",
                "epochs": 83.3,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Third-party estimation",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Biology",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 8.176091259055681,
                "log_compute": 21.041392685158225,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "ESM2-35M",
                "domain": "Biology",
                "task": "Proteins,Protein or nucleotide language model (pLM/nLM),Protein folding prediction",
                "organization": "Meta AI,New York University (NYU),Stanford University,Massachusetts Institute of Technology (MIT)",
                "authors": "Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives",
                "publication_date": "2022-07-21",
                "reference": "Evolutionary-scale prediction of atomic-level protein structure with a language model",
                "link": "https://www.science.org/doi/abs/10.1126/science.ade2574",
                "citations": 636.0,
                "notability_criteria": null,
                "parameters": 35000000.0,
                "training_compute_(flop)": 2.1e+20,
                "training_dataset_size_(gradients)": 15360000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": "Unsupervised",
                "confidence": "Confident",
                "epochs": 83.3,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": null,
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Biology",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 7.544068044350276,
                "log_compute": 20.32221929473392,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "ESM2-8M",
                "domain": "Biology",
                "task": "Proteins,Protein or nucleotide language model (pLM/nLM),Protein folding prediction",
                "organization": "Meta AI,New York University (NYU),Stanford University,Massachusetts Institute of Technology (MIT)",
                "authors": "Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives",
                "publication_date": "2022-07-21",
                "reference": "Evolutionary-scale prediction of atomic-level protein structure with a language model",
                "link": "https://www.science.org/doi/abs/10.1126/science.ade2574",
                "citations": 636.0,
                "notability_criteria": null,
                "parameters": 8000000.0,
                "training_compute_(flop)": 4.8e+19,
                "training_dataset_size_(gradients)": 15360000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": "Unsupervised",
                "confidence": "Confident",
                "epochs": 83.3,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": null,
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Biology",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 6.903089986991944,
                "log_compute": 19.681241237375588,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "Rita-XLarge",
                "domain": "Biology",
                "task": "Proteins,Protein or nucleotide language model (pLM/nLM)",
                "organization": "LightOn,Harvard University,University of Oxford",
                "authors": "Daniel Hesslow, Niccolo Zanichelli, Pascal Notin, Iacopo Poli, Debora Marks ",
                "publication_date": "2022-07-14",
                "reference": "RITA: a Study on Scaling Up Generative Protein Sequence Models",
                "link": "https://arxiv.org/abs/2205.05789",
                "citations": 111.0,
                "notability_criteria": null,
                "parameters": 1200000000.0,
                "training_compute_(flop)": 8.64e+20,
                "training_dataset_size_(gradients)": 150000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA V100",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 18:00:48+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Reported",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "France",
                "domain_group": "Biology",
                "export_controls_sum": 1.0,
                "publication_count": 2.3327744955886316,
                "log_params": 9.079181246047625,
                "log_compute": 20.936513742478894,
                "pub_bin": "Low",
                "export_bin": "Low"
            },
            {
                "model": "NLLB",
                "domain": "Language",
                "task": "Translation",
                "organization": "Meta AI",
                "authors": "Marta R. Costa-juss\u00e0, James Cross, Onur \u00c7elebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco (Paco) Guzm\u00e1n, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, Jeff Wang",
                "publication_date": "2022-07-06",
                "reference": "No Language Left Behind: Scaling Human-Centered Machine Translation",
                "link": "https://research.facebook.com/publications/no-language-left-behind/",
                "citations": 1418.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 54500000000.0,
                "training_compute_(flop)": 1.751113728e+22,
                "training_dataset_size_(gradients)": 300000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100 SXM4 80 GB",
                "approach": "Self-supervised learning",
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:08:20+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 1000000.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": 59168.0,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 50667.25034038439,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 10.736396502276643,
                "log_compute": 22.243314352730454,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "CodeT5-large",
                "domain": "Language",
                "task": "Code generation",
                "organization": "Salesforce",
                "authors": "Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, Steven C.H. Hoi ",
                "publication_date": "2022-07-05",
                "reference": "CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning",
                "link": "https://arxiv.org/abs/2207.01780",
                "citations": 327.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 770000000.0,
                "training_compute_(flop)": 2.72e+21,
                "training_dataset_size_(gradients)": 10500000000.0,
                "training_time_(hours)": 504.0,
                "training_hardware": "NVIDIA A100",
                "approach": null,
                "confidence": "Likely",
                "epochs": 150.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 18:00:46+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 4478.145684414144,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 8.886490725172482,
                "log_compute": 21.434568904034197,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "Minerva (540B)",
                "domain": "Language",
                "task": "Quantitative reasoning,Mathematical reasoning,Language modeling/generation,Question answering",
                "organization": "Google",
                "authors": "Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, Vedant Misra",
                "publication_date": "2022-06-29",
                "reference": "Solving Quantitative Reasoning Problems with Language Models",
                "link": "https://arxiv.org/abs/2206.14858",
                "citations": 1126.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 540350000000.0,
                "training_compute_(flop)": 2.7415e+24,
                "training_dataset_size_(gradients)": 26000000000.0,
                "training_time_(hours)": 696.0,
                "training_hardware": "Google TPU v4",
                "approach": "Self-supervised learning",
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": "PaLM (540B)",
                "finetune_compute_(flop)": 2.1429e+23,
                "hardware_quantity": 1024.0,
                "last_modified": "2025-10-16 13:07:57+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": true,
                "training_chip-hours": 712704.0,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": true,
                "training_power_draw_(w)": 349199.866571188,
                "training_compute_estimation_method": "Hardware",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 11.732675155803872,
                "log_compute": 24.4379882502195,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "ProGen2-xlarge",
                "domain": "Biology",
                "task": "Proteins,Protein generation,Protein or nucleotide language model (pLM/nLM)",
                "organization": "Salesforce Research,Columbia University,Johns Hopkins University",
                "authors": "Erik Nijkamp, Jeffrey Ruffolo, Eli N. Weinstein, Nikhil Naik, Ali Madani\n",
                "publication_date": "2022-06-27",
                "reference": "ProGen2: Exploring the Boundaries of Protein Language Models",
                "link": "https://arxiv.org/abs/2206.13517",
                "citations": 362.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 6400000000.0,
                "training_compute_(flop)": 1.35e+22,
                "training_dataset_size_(gradients)": 350000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "Google TPU v3",
                "approach": "Self-supervised learning",
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 18:00:48+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 11850.178410269727,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware,Third-party estimation",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Biology",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 9.806179973983888,
                "log_compute": 22.130333768495007,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "ProGen2-base",
                "domain": "Biology",
                "task": "Protein or nucleotide language model (pLM/nLM)",
                "organization": "Salesforce Research,Columbia University,Johns Hopkins University",
                "authors": "Erik Nijkamp, Jeffrey Ruffolo, Eli N. Weinstein, Nikhil Naik, Ali Madani\n",
                "publication_date": "2022-06-27",
                "reference": "ProGen2: Exploring the Boundaries of Protein Language Models",
                "link": "https://arxiv.org/abs/2206.13517",
                "citations": 362.0,
                "notability_criteria": null,
                "parameters": 764000000.0,
                "training_compute_(flop)": 1.1e+21,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "Google TPU v3",
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 18:00:48+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting,Third-party estimation",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Biology",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 8.88309335857569,
                "log_compute": 21.041392685158225,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "OPT-6.7B",
                "domain": "Language",
                "task": "Language modeling,Chat,Language modeling/generation,Question answering",
                "organization": "Meta AI",
                "authors": "Susan Zhang\u2217 , Stephen Roller\u2217 , Naman Goyal\u2217 , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott\u2020 , Sam Shleifer\u2020 , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",
                "publication_date": "2022-06-21",
                "reference": "OPT: Open Pre-trained Transformer Language Models",
                "link": "https://arxiv.org/abs/2205.01068",
                "citations": 4020.0,
                "notability_criteria": null,
                "parameters": 6700000000.0,
                "training_compute_(flop)": 1.2060000000001e+22,
                "training_dataset_size_(gradients)": 180000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.67,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:07:58+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 2000000.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Comparison with other models",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 9.826074802700827,
                "log_compute": 22.08134730780417,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "OPT-66B",
                "domain": "Language",
                "task": "Language modeling,Chat,Language modeling/generation,Question answering",
                "organization": "Meta AI",
                "authors": "Susan Zhang\u2217 , Stephen Roller\u2217 , Naman Goyal\u2217 , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott\u2020 , Sam Shleifer\u2020 , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",
                "publication_date": "2022-06-21",
                "reference": "OPT: Open Pre-trained Transformer Language Models",
                "link": "https://arxiv.org/abs/2205.01068",
                "citations": 4020.0,
                "notability_criteria": null,
                "parameters": 66000000000.0,
                "training_compute_(flop)": 1.100000000001e+23,
                "training_dataset_size_(gradients)": 180000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100 SXM4 80 GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.67,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:07:58+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 2000000.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 10.81954393554187,
                "log_compute": 23.04139268515862,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "OPT-30B",
                "domain": "Language",
                "task": "Language modeling,Chat,Language modeling/generation,Question answering",
                "organization": "Meta AI",
                "authors": "Susan Zhang\u2217 , Stephen Roller\u2217 , Naman Goyal\u2217 , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott\u2020 , Sam Shleifer\u2020 , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",
                "publication_date": "2022-06-21",
                "reference": "OPT: Open Pre-trained Transformer Language Models",
                "link": "https://arxiv.org/abs/2205.01068",
                "citations": 4020.0,
                "notability_criteria": null,
                "parameters": 30000000000.0,
                "training_compute_(flop)": 5.4000000000001e+22,
                "training_dataset_size_(gradients)": 180000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.67,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:07:56+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 4000000.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 10.477121254719663,
                "log_compute": 22.732393759822976,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "Unified-IO (XL)",
                "domain": "Multimodal,Vision,Language",
                "task": "Object detection,Language modeling/generation,Image generation,Visual question answering,Image classification,Image captioning,Text classification,Text summarization,Question answering",
                "organization": "Allen Institute for AI,University of Washington",
                "authors": "Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, Aniruddha Kembhavi",
                "publication_date": "2022-06-17",
                "reference": "Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks",
                "link": "https://arxiv.org/abs/2206.08916",
                "citations": 447.0,
                "notability_criteria": null,
                "parameters": 2925000000.0,
                "training_compute_(flop)": 3.5e+21,
                "training_dataset_size_(gradients)": 74880000000.0,
                "training_time_(hours)": null,
                "training_hardware": "Google TPU v4",
                "approach": null,
                "confidence": "Speculative",
                "epochs": 7.88,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 18:00:49+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 9.4661258704182,
                "log_compute": 21.544068044350276,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "EGRU (WT2)",
                "domain": "Language",
                "task": "Language modeling",
                "organization": "Ruhr University Bochum,Technische Universit\u00e4t Dresden,University of London",
                "authors": "Anand Subramoney, Khaleelulla Khan Nazeer, Mark Sch\u00f6ne, Christian Mayr, David Kappel",
                "publication_date": "2022-06-13",
                "reference": "Efficient recurrent architectures through activity sparsity and sparse back-propagation through time",
                "link": "https://arxiv.org/abs/2206.06178v3",
                "citations": 26.0,
                "notability_criteria": null,
                "parameters": 74000000.0,
                "training_compute_(flop)": 2.22e+18,
                "training_dataset_size_(gradients)": 2000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100",
                "approach": null,
                "confidence": "Confident",
                "epochs": 2500.0,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 18:00:47+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 8576.0,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "Germany",
                "domain_group": "Language",
                "export_controls_sum": 1.0,
                "publication_count": 3.437316959598853,
                "log_params": 7.869231719730976,
                "log_compute": 18.34635297445064,
                "pub_bin": "Low",
                "export_bin": "Low"
            },
            {
                "model": "BIG-G 137B",
                "domain": "Language",
                "task": "Language modeling/generation",
                "organization": "Google",
                "authors": "Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adri\u00e0 Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman S. Iyer, Anders Andreassen, Andrea Madotto, Andrea Santilli, Andreas Stuhlm\u00fcller, Andrew Dai, Andrew La, Andrew Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karaka\u015f, B. Ryan Roberts, Bao Sheng Loe, Barret Zoph, Bart\u0142omiej Bojanowski, Batuhan \u00d6zyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno Stein, Berk Ekmekci, Bill Yuchen Lin, Blake Howald, Bryan Orinion, Cameron Diao, Cameron Dour, Catherine Stinson, Cedrick Argueta, C\u00e9sar Ferri Ram\u00edrez, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu, Chris Callison-Burch, Chris Waites, Christian Voigt, Christopher D. Manning, Christopher Potts, Cindy Ramirez, Clara E. Rivera, Clemencia Siro, Colin Raffel, Courtney Ashcraft, Cristina Garbacea, Damien Sileo, Dan Garrette, Dan Hendrycks, Dan Kilman, Dan Roth, Daniel Freeman, Daniel Khashabi, Daniel Levy, Daniel Mosegu\u00ed Gonz\u00e1lez, Danielle Perszyk, Danny Hernandez, Danqi Chen, Daphne Ippolito et al. (351 additional authors not shown)",
                "publication_date": "2022-06-09",
                "reference": "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models",
                "link": "https://arxiv.org/abs/2206.04615",
                "citations": 1937.0,
                "notability_criteria": null,
                "parameters": 137000000000.0,
                "training_compute_(flop)": 5.6e+23,
                "training_dataset_size_(gradients)": 681200000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 18:00:48+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 262000.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 11.136720567156408,
                "log_compute": 23.7481880270062,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "LIMoE-H/14",
                "domain": "Multimodal,Vision,Language",
                "task": "Image classification",
                "organization": "Google",
                "authors": "Basil Mustafa, Carlos Riquelme, Joan Puigcerver, Rodolphe Jenatton, Neil Houlsby",
                "publication_date": "2022-06-06",
                "reference": "Multimodal Contrastive Learning with LIMoE: the Language-Image Mixture of Experts",
                "link": "https://arxiv.org/abs/2206.02770",
                "citations": 243.0,
                "notability_criteria": null,
                "parameters": 5600000000.0,
                "training_compute_(flop)": 1.8e+22,
                "training_dataset_size_(gradients)": 4575625612000.0,
                "training_time_(hours)": null,
                "training_hardware": "Google TPU v2,Google TPU v3,Google TPU v4",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:07:57+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 9.7481880270062,
                "log_compute": 22.255272505103306,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "GPT-2 Medium (FlashAttention)",
                "domain": "Language",
                "task": "Language modeling/generation",
                "organization": "Stanford University,University at Buffalo",
                "authors": "Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, Christopher R\u00e9",
                "publication_date": "2022-05-27",
                "reference": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
                "link": "https://arxiv.org/abs/2205.14135",
                "citations": 2777.0,
                "notability_criteria": "Highly cited,Historical significance",
                "parameters": 355000000.0,
                "training_compute_(flop)": 8.9280922e+20,
                "training_dataset_size_(gradients)": 10130000000.0,
                "training_time_(hours)": 165.6,
                "training_hardware": "NVIDIA A100 SXM4 40 GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 8.0,
                "last_modified": "2025-10-16 13:07:58+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 512.0,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 6423.834266872993,
                "training_compute_estimation_method": "Comparison with other models,Hardware",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 8.550228353055093,
                "log_compute": 20.950758666555924,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "TRIMELMext (247M)",
                "domain": "Language",
                "task": "Language modeling/generation,Translation",
                "organization": "Princeton University",
                "authors": "Zexuan Zhong, Tao Lei, Danqi Chen",
                "publication_date": "2022-05-25",
                "reference": "Training Language Models with Memory Augmentation",
                "link": "https://arxiv.org/abs/2205.12674",
                "citations": 138.0,
                "notability_criteria": null,
                "parameters": 247000000.00000003,
                "training_compute_(flop)": 3.12e+19,
                "training_dataset_size_(gradients)": 21086208000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": 204.72,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 18:00:48+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open (non-commercial)",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": null,
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 8.392696953259666,
                "log_compute": 19.494154594018443,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "TRIMELMext (7M)",
                "domain": "Language",
                "task": "Language modeling/generation,Translation",
                "organization": "Princeton University",
                "authors": "Zexuan Zhong, Tao Lei, Danqi Chen",
                "publication_date": "2022-05-25",
                "reference": "Training Language Models with Memory Augmentation",
                "link": "https://arxiv.org/abs/2205.12674",
                "citations": 138.0,
                "notability_criteria": null,
                "parameters": 7000000.0,
                "training_compute_(flop)": 2.06e+17,
                "training_dataset_size_(gradients)": 4915200000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": 47.72,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 18:00:48+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open (non-commercial)",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": null,
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 6.845098040014257,
                "log_compute": 17.313867220369154,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "TRIMELMlong (150M)",
                "domain": "Language",
                "task": "Language modeling/generation,Translation",
                "organization": "Princeton University",
                "authors": "Zexuan Zhong, Tao Lei, Danqi Chen",
                "publication_date": "2022-05-25",
                "reference": "Training Language Models with Memory Augmentation",
                "link": "https://arxiv.org/abs/2205.12674",
                "citations": 138.0,
                "notability_criteria": null,
                "parameters": 150000000.0,
                "training_compute_(flop)": 6.48e+18,
                "training_dataset_size_(gradients)": 7200000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": 139.81,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 18:00:47+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open (non-commercial)",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": null,
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 8.176091259055681,
                "log_compute": 18.811575005870594,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "Imagen",
                "domain": "Image generation",
                "task": "Text-to-image,Image generation",
                "organization": "Google Brain",
                "authors": "Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, Mohammad Norouzi",
                "publication_date": "2022-05-23",
                "reference": "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding",
                "link": "https://arxiv.org/abs/2205.11487",
                "citations": 6802.0,
                "notability_criteria": "Significant use,SOTA improvement,Highly cited",
                "parameters": 7762000000.0,
                "training_compute_(flop)": 1.46e+22,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 96.0,
                "training_hardware": "Google TPU v4",
                "approach": "Self-supervised learning",
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "API access",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 256.0,
                "last_modified": "2025-10-16 13:07:57+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": true,
                "training_chip-hours": 24576.0,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 7915.823806150154,
                "frontier_model": false,
                "training_power_draw_(w)": 87371.9285545078,
                "training_compute_estimation_method": "Hardware",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Vision",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 9.889973638403996,
                "log_compute": 22.164352855784436,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "OPT-175B",
                "domain": "Language",
                "task": "Language modeling,Chat,Language modeling/generation,Question answering",
                "organization": "Meta AI",
                "authors": "Susan Zhang\u2217 , Stephen Roller\u2217 , Naman Goyal\u2217 , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott\u2020 , Sam Shleifer\u2020 , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",
                "publication_date": "2022-05-02",
                "reference": "OPT: Open Pre-trained Transformer Language Models",
                "link": "https://arxiv.org/abs/2205.01068",
                "citations": 4020.0,
                "notability_criteria": "Significant use,Highly cited",
                "parameters": 175000000000.0,
                "training_compute_(flop)": 4.3e+23,
                "training_dataset_size_(gradients)": 180000000000.0,
                "training_time_(hours)": 793.5,
                "training_hardware": "NVIDIA A100 SXM4 80 GB",
                "approach": "Self-supervised learning",
                "confidence": "Confident",
                "epochs": 1.6667,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 1024.0,
                "last_modified": "2025-10-16 13:07:57+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 2000000.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": 812544.0,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": 733634.637,
                "frontier_model": false,
                "training_power_draw_(w)": 822708.6888139298,
                "training_compute_estimation_method": "Reported",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 11.243038048686294,
                "log_compute": 23.633468455579585,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "OPT-13B",
                "domain": null,
                "task": null,
                "organization": "Meta AI",
                "authors": "Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",
                "publication_date": "2022-05-02",
                "reference": null,
                "link": "https://arxiv.org/abs/2205.01068",
                "citations": null,
                "notability_criteria": null,
                "parameters": 13000000000.0,
                "training_compute_(flop)": 3.53e+23,
                "training_dataset_size_(gradients)": 180000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Unknown",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-08-15 20:40:03+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": null,
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": null,
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 10.113943352306837,
                "log_compute": 23.547774705387823,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "Sparse all-MLP",
                "domain": "Language",
                "task": "Language modeling",
                "organization": "Meta AI",
                "authors": "Ping Yu, Mikel Artexte, Myle Ott, Sam Shleifer, Hongyu Gong, Ves Stoyanov, Xian Li",
                "publication_date": "2022-04-14",
                "reference": "Efficient Language Modeling with Sparse all-MLP",
                "link": "https://arxiv.org/abs/2203.06850",
                "citations": 13.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 9410000000.0,
                "training_compute_(flop)": 5.32224e+20,
                "training_dataset_size_(gradients)": 100000000000.0,
                "training_time_(hours)": 112.0,
                "training_hardware": "NVIDIA V100",
                "approach": "Self-supervised learning",
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 18:00:48+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 9.973589623427257,
                "log_compute": 20.72609445464332,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "Stable Diffusion (LDM-KL-8-G)",
                "domain": "Image generation",
                "task": "Image generation,Text-to-image",
                "organization": "Runway,Ludwig Maximilian University of Munich,Heidelberg University",
                "authors": "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj\u00f6rn Ommer",
                "publication_date": "2022-04-13",
                "reference": "High-Resolution Image Synthesis with Latent Diffusion Models",
                "link": "https://arxiv.org/abs/2112.10752",
                "citations": 18496.0,
                "notability_criteria": "Significant use,Highly cited",
                "parameters": 1450000000.0,
                "training_compute_(flop)": 5e+22,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 585.9375,
                "training_hardware": "NVIDIA A100",
                "approach": "Self-supervised learning",
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": "Stable Diffusion 1.2",
                "finetune_compute_(flop)": null,
                "hardware_quantity": 256.0,
                "last_modified": "2025-10-16 13:07:39+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": 150000.0,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 111248.21698072633,
                "frontier_model": false,
                "training_power_draw_(w)": 205764.21634205984,
                "training_compute_estimation_method": "Hardware",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Vision",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 9.161368002234974,
                "log_compute": 22.69897000433602,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "BERT-RBP",
                "domain": "Biology",
                "task": "Proteins,Protein interaction prediction,RNA-Protein interaction prediction",
                "organization": "Waseda University",
                "authors": "Keisuke Yamada, Michiaki Hamada",
                "publication_date": "2022-04-07",
                "reference": "Prediction of RNA\u2013protein interactions using a nucleotide language model",
                "link": "https://academic.oup.com/bioinformaticsadvances/article/2/1/vbac023/6564689",
                "citations": 60.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 110000000.0,
                "training_compute_(flop)": 1.4e+20,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": 3.0,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "Japan",
                "base_model": "DNABERT",
                "finetune_compute_(flop)": 2.2e+16,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 21:49:33+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open (non-commercial)",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "Japan",
                "domain_group": "Biology",
                "export_controls_sum": 1.0,
                "publication_count": 2.8473564742897706,
                "log_params": 8.041392685158225,
                "log_compute": 20.146128035678238,
                "pub_bin": "Low",
                "export_bin": "Low"
            },
            {
                "model": "DALL\u00b7E 2",
                "domain": "Image generation",
                "task": "Text-to-image,Image generation",
                "organization": "OpenAI",
                "authors": "Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen",
                "publication_date": "2022-04-06",
                "reference": "Hierarchical Text-Conditional Image Generation with CLIP Latents",
                "link": "https://cdn.openai.com/papers/dall-e-2.pdf",
                "citations": 7582.0,
                "notability_criteria": "Highly cited,SOTA improvement",
                "parameters": 3500000000.0,
                "training_compute_(flop)": 3.3695784e+23,
                "training_dataset_size_(gradients)": 167050000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": "Self-supervised learning",
                "confidence": "Speculative",
                "epochs": null,
                "model_accessibility": "API access",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:07:57+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": true,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Third-party estimation",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Vision",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 9.544068044350276,
                "log_compute": 23.527575565557484,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "Monarch-GPT-2-Medium",
                "domain": "Language",
                "task": "Language modeling",
                "organization": "Stanford University,University at Buffalo,University of Michigan",
                "authors": "Tri Dao, Beidi Chen, Nimit Sohoni, Arjun Desai, Michael Poli, Jessica Grogan, Alexander Liu, Aniruddh Rao, Atri Rudra, Christopher R\u00e9",
                "publication_date": "2022-04-01",
                "reference": "Monarch: Expressive Structured Matrices for Efficient and Accurate Training",
                "link": "https://arxiv.org/abs/2204.00595",
                "citations": 102.0,
                "notability_criteria": null,
                "parameters": 165000000.0,
                "training_compute_(flop)": 4.36e+20,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA V100",
                "approach": null,
                "confidence": "Confident",
                "epochs": 110.0,
                "model_accessibility": "Unreleased",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 17:39:13+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 8.217483944213907,
                "log_compute": 20.639486489268585,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "Monarch-GPT-2-Small",
                "domain": "Language",
                "task": "Language modeling",
                "organization": "Stanford University,University at Buffalo,University of Michigan",
                "authors": "Tri Dao, Beidi Chen, Nimit Sohoni, Arjun Desai, Michael Poli, Jessica Grogan, Alexander Liu, Aniruddh Rao, Atri Rudra, Christopher R\u00e9",
                "publication_date": "2022-04-01",
                "reference": "Monarch: Expressive Structured Matrices for Efficient and Accurate Training",
                "link": "https://arxiv.org/abs/2204.00595",
                "citations": 102.0,
                "notability_criteria": null,
                "parameters": 72000000.0,
                "training_compute_(flop)": 9e+19,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA V100",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 17:39:15+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 7.857332496431268,
                "log_compute": 19.954242509439325,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "Make-A-Scene",
                "domain": "Image generation",
                "task": "Image generation,Text-to-image",
                "organization": "Meta AI",
                "authors": "Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, Yaniv Taigman",
                "publication_date": "2022-03-24",
                "reference": "Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors",
                "link": "https://arxiv.org/abs/2203.13131",
                "citations": null,
                "notability_criteria": "SOTA improvement",
                "parameters": 4000000000.0,
                "training_compute_(flop)": 6.4172851e+21,
                "training_dataset_size_(gradients)": 267386880000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Vision",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 9.602059991327963,
                "log_compute": 21.80735133407167,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "MemSizer (language modeling)",
                "domain": "Language",
                "task": "Language modeling",
                "organization": "Meta AI,Chinese University of Hong Kong (CUHK)",
                "authors": "Yizhe Zhang, Deng Cai",
                "publication_date": "2022-03-23",
                "reference": "Linearizing Transformer with Key-Value Memory",
                "link": "https://arxiv.org/abs/2203.12644",
                "citations": 6.0,
                "notability_criteria": null,
                "parameters": 357000000.0,
                "training_compute_(flop)": 7.3e+18,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open (non-commercial)",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 8.552668216112194,
                "log_compute": 18.863322860120455,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "Segatron-XL large, M=384 + HCP",
                "domain": "Language",
                "task": "Language modeling",
                "organization": "Microsoft Research,University of Waterloo",
                "authors": "He Bai, Tong Wang, Alessandro Sordoni, Peng Shi",
                "publication_date": "2022-03-21",
                "reference": "Better Language Model with Hypernym Class Prediction",
                "link": "https://arxiv.org/abs/2203.10692",
                "citations": 16.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 256999999.99999997,
                "training_compute_(flop)": 2.65e+19,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": 167.02,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 17:39:14+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open (non-commercial)",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 8.409933123331294,
                "log_compute": 19.423245873936807,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "Segatron -XL base, M=150 + HCP",
                "domain": "Language",
                "task": "Language modeling",
                "organization": "Microsoft Research,University of Waterloo",
                "authors": "He Bai, Tong Wang, Alessandro Sordoni, Peng Shi",
                "publication_date": "2022-03-21",
                "reference": "Better Language Model with Hypernym Class Prediction",
                "link": "https://arxiv.org/abs/2203.10692",
                "citations": 16.0,
                "notability_criteria": null,
                "parameters": 151000000.0,
                "training_compute_(flop)": 1.74e+18,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": 18.64,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 17:39:14+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open (non-commercial)",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": null,
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 8.178976947293169,
                "log_compute": 18.2405492482826,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "ViT-G (model soup)",
                "domain": "Vision",
                "task": "Image classification",
                "organization": "University of Washington,Columbia University,Google,Meta AI,Tel Aviv University",
                "authors": "Mitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, Ludwig Schmidt",
                "publication_date": "2022-03-10",
                "reference": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time",
                "link": "https://arxiv.org/abs/2203.05482v3",
                "citations": 1157.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 1843000000.0,
                "training_compute_(flop)": 3.4e+21,
                "training_dataset_size_(gradients)": 1752320000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": 8.0,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 17:39:14+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Vision",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 9.265525335219074,
                "log_compute": 21.531478917042254,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "GPT3-6.7B + muP",
                "domain": "Language",
                "task": "Language modeling/generation",
                "organization": "Microsoft,OpenAI",
                "authors": "Greg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, Jianfeng Gao",
                "publication_date": "2022-03-07",
                "reference": "Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer",
                "link": "https://arxiv.org/abs/2203.03466",
                "citations": 197.0,
                "notability_criteria": null,
                "parameters": 6700000000.0,
                "training_compute_(flop)": 1.28e+22,
                "training_dataset_size_(gradients)": 300000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": 8.4e+20,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 17:31:41+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Comparison with other models,Operation counting",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 9.826074802700827,
                "log_compute": 22.10720996964787,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "PolyCoder",
                "domain": "Language",
                "task": "Code generation",
                "organization": "Carnegie Mellon University (CMU)",
                "authors": "Frank F. Xu, Uri Alon, Graham Neubig, Vincent J. Hellendoorn",
                "publication_date": "2022-02-26",
                "reference": "A Systematic Evaluation of Large Language Models of Code",
                "link": "https://arxiv.org/abs/2202.13169",
                "citations": 725.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 2700000000.0,
                "training_compute_(flop)": 1.1e+21,
                "training_dataset_size_(gradients)": 39300000000.0,
                "training_time_(hours)": 1000.0,
                "training_hardware": "NVIDIA Quadro RTX 8000",
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 17:39:14+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": true,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 9.431363764158988,
                "log_compute": 21.041392685158225,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "ST-MoE",
                "domain": "Language",
                "task": "Language modeling/generation",
                "organization": "Google,Google Brain,Google Research",
                "authors": "Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, William Fedus",
                "publication_date": "2022-02-17",
                "reference": "ST-MoE: Designing Stable and Transferable Sparse Expert Models",
                "link": "https://arxiv.org/abs/2202.08906v2",
                "citations": 266.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 269000000000.0,
                "training_compute_(flop)": 2.9e+23,
                "training_dataset_size_(gradients)": 1500000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Likely",
                "epochs": 0.84,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 17:39:15+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 1000000.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 11.429752280002408,
                "log_compute": 23.462397997898957,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "LaMDA",
                "domain": "Language",
                "task": "Language modeling",
                "organization": "Google",
                "authors": "Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi, Quoc Le",
                "publication_date": "2022-02-10",
                "reference": "LaMDA: Language Models for Dialog Applications",
                "link": "https://arxiv.org/abs/2201.08239",
                "citations": 1688.0,
                "notability_criteria": "Historical significance",
                "parameters": 137000000000.0,
                "training_compute_(flop)": 3.55e+23,
                "training_dataset_size_(gradients)": 2080000000000.0,
                "training_time_(hours)": 1385.0,
                "training_hardware": "Google TPU v3",
                "approach": "Self-supervised learning",
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 1024.0,
                "last_modified": "2025-10-14 17:39:14+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 256000.0,
                "organization_categorization": "Industry",
                "foundation_model": true,
                "training_chip-hours": 1418240.0,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": 229949.98625999544,
                "frontier_model": false,
                "training_power_draw_(w)": 453306.7251313544,
                "training_compute_estimation_method": "Hardware",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 11.136720567156408,
                "log_compute": 23.550228353055093,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "InstructGPT 175B",
                "domain": "Language",
                "task": "Language modeling/generation",
                "organization": "OpenAI",
                "authors": "Long Ouyang, Pamela Mishkin, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright,John Schulman, Amanda Askell, Fraser Kelton, Peter Welinder, Luke Miller, Maddie Simens, Paul Christiano, Ryan Lowe, Chong Zhang, Jacob Hilton, Sandhini Agarwal, Katarina Slama, Alex Ray, Jan Leike",
                "publication_date": "2022-01-27",
                "reference": "Training language models to follow instructions with human feedback",
                "link": "https://arxiv.org/pdf/2203.02155",
                "citations": 15376.0,
                "notability_criteria": "Historical significance,Highly cited",
                "parameters": 175000000000.0,
                "training_compute_(flop)": 3.19181e+23,
                "training_dataset_size_(gradients)": 16969897.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": "Self-supervised learning",
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "API access",
                "country": "United States",
                "base_model": "GPT-3 175B (davinci)",
                "finetune_compute_(flop)": 5.181e+21,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:07:39+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": true,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Reported",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 11.243038048686294,
                "log_compute": 23.50403703104727,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "Primer (GPT-3 XL-like 1.9B)",
                "domain": "Language",
                "task": "Language modeling/generation",
                "organization": "Google Brain",
                "authors": "DavidR.So, WojciechMan \u0301ke, HanxiaoLiu, ZihangDai, NoamShazeer, QuocV.Le",
                "publication_date": "2022-01-24",
                "reference": "Primer: Searching for Efficient Transformers for Language Modeling",
                "link": "https://arxiv.org/abs/2109.08668",
                "citations": 172.0,
                "notability_criteria": null,
                "parameters": 1900000000.0,
                "training_compute_(flop)": 2.2049963e+22,
                "training_dataset_size_(gradients)": 2000000000000.0,
                "training_time_(hours)": 140.0,
                "training_hardware": "Google TPU v4",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 512.0,
                "last_modified": "2025-10-14 17:39:14+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 2000000.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 175207.5521103308,
                "training_compute_estimation_method": "Hardware,Operation counting",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 9.278753600952829,
                "log_compute": 22.343407865055134,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "Detic",
                "domain": "Vision",
                "task": "Object detection,Image classification",
                "organization": "Meta AI,University of Texas at Austin",
                "authors": "Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp Kr\u00e4henb\u00fchl, Ishan Misra",
                "publication_date": "2022-01-07",
                "reference": "Detecting Twenty-thousand Classes using Image-level Supervision",
                "link": "https://arxiv.org/abs/2201.02605",
                "citations": 686.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 88000000.0,
                "training_compute_(flop)": 2.34399744e+19,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 24.0,
                "training_hardware": "NVIDIA V100",
                "approach": null,
                "confidence": "Speculative",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 32.0,
                "last_modified": "2025-10-14 17:39:13+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": 768.0,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 191.44581825616137,
                "frontier_model": false,
                "training_power_draw_(w)": 19331.67955389363,
                "training_compute_estimation_method": "Hardware",
                "year": 2022,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Vision",
                "export_controls_sum": 14.0,
                "publication_count": 12.360967021321876,
                "log_params": 7.944482672150168,
                "log_compute": 19.369957133031,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "GLIDE",
                "domain": "Image generation",
                "task": "Image generation,Text-to-image",
                "organization": "OpenAI",
                "authors": "Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, Mark Chen",
                "publication_date": "2021-12-20",
                "reference": "GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models",
                "link": "https://arxiv.org/abs/2112.10741",
                "citations": 3985.0,
                "notability_criteria": null,
                "parameters": 3500000000.0,
                "training_compute_(flop)": 4.7e+22,
                "training_dataset_size_(gradients)": 7602176000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Speculative",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:07:38+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Comparison with other models",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Vision",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 9.544068044350276,
                "log_compute": 22.672097857935718,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "MoE-1.1T",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering",
                "organization": "Meta AI",
                "authors": "Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, Giri Anantharaman, Xian Li, Shuohui Chen, Halil Akin, Mandeep Baines, Louis Martin, Xing Zhou, Punit Singh Koura, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Mona Diab, Zornitsa Kozareva, Ves Stoyanov",
                "publication_date": "2021-12-20",
                "reference": "Efficient Large Scale Language Modeling with Mixtures of Experts",
                "link": "https://arxiv.org/abs/2112.10684",
                "citations": 212.0,
                "notability_criteria": null,
                "parameters": 1100000000000.0,
                "training_compute_(flop)": 2.227e+22,
                "training_dataset_size_(gradients)": 112000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100",
                "approach": null,
                "confidence": "Confident",
                "epochs": 2.68,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 17:39:14+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Reported",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 12.041392685158225,
                "log_compute": 22.347720217034038,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "Fairseq-dense 13B",
                "domain": "Language",
                "task": "Language modeling/generation",
                "organization": "Meta AI",
                "authors": "Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, Giri Anantharaman, Xian Li, Shuohui Chen, Halil Akin, Mandeep Baines, Louis Martin, Xing Zhou, Punit Singh Koura, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Mona Diab, Zornitsa Kozareva, Ves Stoyanov",
                "publication_date": "2021-12-20",
                "reference": "Efficient Large Scale Language Modeling with Mixtures of Experts",
                "link": "https://arxiv.org/abs/2112.10684",
                "citations": 212.0,
                "notability_criteria": null,
                "parameters": 13000000000.0,
                "training_compute_(flop)": 3.267e+22,
                "training_dataset_size_(gradients)": 112000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100",
                "approach": null,
                "confidence": "Likely",
                "epochs": 2.68,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 17:39:14+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Reported",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 10.113943352306837,
                "log_compute": 22.51414913447544,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "XGLM-7.5B",
                "domain": "Language",
                "task": "Translation,Question answering,Language modeling/generation",
                "organization": "Meta AI,Facebook AI Research",
                "authors": "Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, Xian Li",
                "publication_date": "2021-12-20",
                "reference": "Few-shot Learning with Multilingual Language Models",
                "link": "https://arxiv.org/abs/2112.10668",
                "citations": 340.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 7500000000.0,
                "training_compute_(flop)": 2.25e+22,
                "training_dataset_size_(gradients)": 500000000000.0,
                "training_time_(hours)": 504.0,
                "training_hardware": "NVIDIA A100",
                "approach": "Self-supervised learning",
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 256.0,
                "last_modified": "2025-10-16 13:07:38+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": 129024.0,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 104152.22590136188,
                "frontier_model": false,
                "training_power_draw_(w)": 206287.25531193183,
                "training_compute_estimation_method": "Operation counting,Hardware",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 9.8750612633917,
                "log_compute": 22.352182518111363,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "HSO",
                "domain": "Language",
                "task": "Language modeling/generation",
                "organization": "Toyota Technological Institute at Chicago",
                "authors": "Davis Yoshida, Kevin Gimpel",
                "publication_date": "2021-12-16",
                "reference": "Reconsidering the Past: Optimizing Hidden States in Language Models",
                "link": "https://arxiv.org/abs/2112.08653",
                "citations": 3.0,
                "notability_criteria": null,
                "parameters": 345000000.0,
                "training_compute_(flop)": 2.272000000071e+21,
                "training_dataset_size_(gradients)": 0.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Speculative",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "United States",
                "base_model": "GPT-2 (355M)",
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 17:39:14+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Comparison with other models",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 8.537819095073274,
                "log_compute": 21.356408327052552,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "Contriever",
                "domain": "Language",
                "task": "Language modeling",
                "organization": "Meta AI,University College London (UCL),PSL University,Universit\u00e9 Grenoble Alpes",
                "authors": "Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, Edouard Grave",
                "publication_date": "2021-12-16",
                "reference": "Unsupervised Dense Information Retrieval with Contrastive Learning",
                "link": "https://arxiv.org/abs/2112.09118",
                "citations": 1073.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 110000000.0,
                "training_compute_(flop)": 1.57e+20,
                "training_dataset_size_(gradients)": 262144000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "Multinational",
                "base_model": "BERT-Large",
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:07:39+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open (non-commercial)",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 8.041392685158225,
                "log_compute": 20.195899652409235,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "GLaM",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering",
                "organization": "Google",
                "authors": "Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten Bosma, Zongwei Zhou, Tao Wang, Yu Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc V Le, Yonghui Wu, Zhifeng Chen, Claire Cui",
                "publication_date": "2021-12-13",
                "reference": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts",
                "link": "https://arxiv.org/abs/2112.06905",
                "citations": 929.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 1200000000000.0,
                "training_compute_(flop)": 3.6363112434e+23,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 1366.0,
                "training_hardware": "Google TPU v4",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 1024.0,
                "last_modified": "2025-10-16 13:07:39+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 1000000.0,
                "organization_categorization": "Industry",
                "foundation_model": true,
                "training_chip-hours": 1398784.0,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": 541437.4162400038,
                "frontier_model": false,
                "training_power_draw_(w)": 350743.0055524121,
                "training_compute_estimation_method": "Operation counting,Hardware",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 12.079181246047625,
                "log_compute": 23.560661048781533,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "CTR-BERT",
                "domain": "Recommendation",
                "task": "Click-through rate prediction",
                "organization": "Amazon",
                "authors": "Aashiq Muhamed, Iman Keivanloo, Sujan Perera, James Mracek, Yi Xu, Qingjun Cui, Santosh Rajagopalan, Belinda Zeng, Trishul Chilimb\n",
                "publication_date": "2021-12-06",
                "reference": "CTR-BERT: Cost-effective knowledge distillation for billion-parameter teacher models",
                "link": "https://neurips2021-nlp.github.io/papers/20/CameraReady/camera_ready_final.pdf",
                "citations": 48.0,
                "notability_criteria": null,
                "parameters": 70000000.0,
                "training_compute_(flop)": 6.469632e+19,
                "training_dataset_size_(gradients)": 1200000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100",
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 8.0,
                "last_modified": "2025-10-14 21:48:05+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 6448.4868676049155,
                "training_compute_estimation_method": "Hardware",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Recommendation",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 7.845098040014257,
                "log_compute": 19.810879578208944,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "GPT-2-Medium+Pixelfly",
                "domain": "Language",
                "task": "Language modeling,Text classification",
                "organization": "Stanford University,SambaNova Systems, Inc,Peking University,Adobe,University at Buffalo",
                "authors": "Tri Dao, Beidi Chen, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, Christopher R\u00e9",
                "publication_date": "2021-11-30",
                "reference": "Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models",
                "link": "https://arxiv.org/abs/2112.00029",
                "citations": 84.0,
                "notability_criteria": null,
                "parameters": 202999999.99999997,
                "training_compute_(flop)": 1.25454e+19,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA V100",
                "approach": null,
                "confidence": "Confident",
                "epochs": 100.0,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 17:39:15+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 8.307496037913213,
                "log_compute": 19.098484513002028,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "GPT-2-Small+Pixelfly",
                "domain": "Language",
                "task": "Language modeling",
                "organization": "Stanford University,SambaNova Systems, Inc,Peking University,Adobe,University at Buffalo",
                "authors": "Tri Dao, Beidi Chen, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, Christopher R\u00e9",
                "publication_date": "2021-11-30",
                "reference": "Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models",
                "link": "https://arxiv.org/abs/2112.00029",
                "citations": 84.0,
                "notability_criteria": null,
                "parameters": 68000000.0,
                "training_compute_(flop)": 4.2024e+18,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA V100",
                "approach": null,
                "confidence": "Confident",
                "epochs": 100.0,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 17:39:14+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 7.832508912706237,
                "log_compute": 18.62349738779505,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "N\u00dcWA",
                "domain": "Multimodal,Vision,Image generation,Video,Language",
                "task": "Image generation,Video generation,Text-to-image,Text-to-video",
                "organization": "Microsoft Research,Peking University",
                "authors": "Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, Nan Duan",
                "publication_date": "2021-11-24",
                "reference": "N\u00dcWA: Visual Synthesis Pre-training for Neural visUal World creAtion",
                "link": "https://arxiv.org/abs/2111.12417",
                "citations": 316.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 870000000.0,
                "training_compute_(flop)": 7.24598784e+21,
                "training_dataset_size_(gradients)": 5547780000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100",
                "approach": "Self-supervised learning",
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 17:39:14+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 8.939519252618618,
                "log_compute": 21.860097600879122,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "Florence",
                "domain": "Vision",
                "task": "Image captioning,Visual question answering,Image classification,Object detection",
                "organization": "Microsoft",
                "authors": "Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, Ce Liu, Mengchen Liu, Zicheng Liu, Yumao Lu, Yu Shi, Lijuan Wang, JianFeng Wang, Bin Xiao, Zhen Xiao, Jianwei Yang, Michael Zeng, Luowei Zhou, Pengchuan Zhang",
                "publication_date": "2021-11-22",
                "reference": "Florence: A New Foundation Model for Computer Vision",
                "link": "https://arxiv.org/abs/2111.11432v1",
                "citations": 971.0,
                "notability_criteria": "Historical significance,SOTA improvement",
                "parameters": 893000000.0,
                "training_compute_(flop)": 4.831e+22,
                "training_dataset_size_(gradients)": 7500000000.0,
                "training_time_(hours)": 240.0,
                "training_hardware": "NVIDIA A100 SXM4 40 GB",
                "approach": "Supervised",
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 512.0,
                "last_modified": "2025-10-16 13:07:38+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": true,
                "training_chip-hours": 122880.0,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 106950.61569328008,
                "frontier_model": false,
                "training_power_draw_(w)": 412831.8485448412,
                "training_compute_estimation_method": "Hardware",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Vision",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 8.950851458888547,
                "log_compute": 22.68403703748652,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "BASIC-L",
                "domain": "Vision",
                "task": "Image classification",
                "organization": "Google",
                "authors": "Hieu Pham, Zihang Dai, Golnaz Ghiasi, Kenji Kawaguchi, Hanxiao Liu, Adams Wei Yu, Jiahui Yu, Yi-Ting Chen, Minh-Thang Luong, Yonghui Wu, Mingxing Tan, Quoc V. Le",
                "publication_date": "2021-11-19",
                "reference": "Combined Scaling for Zero-shot Transfer Learning",
                "link": "https://arxiv.org/abs/2111.10050",
                "citations": 217.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 3070000000.0,
                "training_compute_(flop)": 4.12e+22,
                "training_dataset_size_(gradients)": 8905031712000.0,
                "training_time_(hours)": null,
                "training_hardware": "Google TPU v4",
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 17:39:13+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": 4350.0,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 1684.770712126102,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Vision",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 9.487138375477187,
                "log_compute": 22.614897216033135,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "DeBERTaV3large",
                "domain": "Language",
                "task": "Question answering,Language modeling/generation",
                "organization": "Microsoft Research",
                "authors": "Pengcheng He, Jianfeng Gao, Weizhu Chen",
                "publication_date": "2021-11-18",
                "reference": "DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing",
                "link": "https://arxiv.org/abs/2111.09543",
                "citations": null,
                "notability_criteria": null,
                "parameters": 418000000.0,
                "training_compute_(flop)": 1.07008e+20,
                "training_dataset_size_(gradients)": 42666666667.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 8.621176281775035,
                "log_compute": 20.029416247086886,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "ESM1v",
                "domain": "Biology",
                "task": "Proteins,Protein or nucleotide language model (pLM/nLM),Protein pathogenicity prediction",
                "organization": "Facebook AI Research,New York University (NYU),University of California (UC) Berkeley",
                "authors": "Joshua Meier, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu, Alexander Rives",
                "publication_date": "2021-11-17",
                "reference": "Language models enable zero-shot prediction of the effects of mutations on protein function",
                "link": "https://www.biorxiv.org/content/10.1101/2021.07.09.450648v2",
                "citations": 619.0,
                "notability_criteria": null,
                "parameters": 650000000.0,
                "training_compute_(flop)": 1.3500000000000008e+20,
                "training_dataset_size_(gradients)": 22050000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA V100",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 64.0,
                "last_modified": "2025-10-16 13:07:39+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 38707.29549681129,
                "training_compute_estimation_method": "Hardware",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Biology",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 8.812913356642856,
                "log_compute": 20.130333768495007,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "Masked Autoencoders ViT-H",
                "domain": "Vision",
                "task": "Semantic segmentation,Image classification,Image generation",
                "organization": "Facebook AI Research",
                "authors": "Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, Ross Girshick",
                "publication_date": "2021-11-11",
                "reference": "Masked Autoencoders Are Scalable Vision Learners",
                "link": "https://arxiv.org/abs/2111.06377",
                "citations": 8884.0,
                "notability_criteria": "Highly cited,SOTA improvement",
                "parameters": 632000000.0,
                "training_compute_(flop)": 4.6e+20,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 69.0,
                "training_hardware": null,
                "approach": "Self-supervised learning",
                "confidence": "Speculative",
                "epochs": 1600.0,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "Multinational",
                "base_model": "ViT-Huge/14",
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:07:38+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open (non-commercial)",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware,Operation counting",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Vision",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 8.800717078282386,
                "log_compute": 20.662757831681574,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "CodeT5-base",
                "domain": "Language",
                "task": "Code generation",
                "organization": "Salesforce,Nanyang Technological University",
                "authors": "Yue Wang, Weishi Wang, Shafiq Joty, Steven C.H. Hoi",
                "publication_date": "2021-11-01",
                "reference": "CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation",
                "link": "https://aclanthology.org/2021.emnlp-main.685/",
                "citations": 1808.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 220000000.0,
                "training_compute_(flop)": 1.56e+21,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 288.0,
                "training_hardware": "NVIDIA A100",
                "approach": null,
                "confidence": "Likely",
                "epochs": 150.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 21:48:05+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 3114.869094617447,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 8.342422680822207,
                "log_compute": 21.193124598354462,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "S4",
                "domain": "Language",
                "task": "Language modeling/generation",
                "organization": "Stanford University",
                "authors": "Albert Gu, Karan Goel, Christopher R\u00e9",
                "publication_date": "2021-10-31",
                "reference": "Efficiently Modeling Long Sequences with Structured State Spaces",
                "link": "https://arxiv.org/abs/2111.00396",
                "citations": 2301.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 249000000.00000003,
                "training_compute_(flop)": 7.8328627e+19,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA A100",
                "approach": null,
                "confidence": "Likely",
                "epochs": 509.02,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 8.0,
                "last_modified": "2025-10-16 13:07:39+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 6453.658675376135,
                "training_compute_estimation_method": null,
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 8.396199347095736,
                "log_compute": 19.89392051398612,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "GPT-2 (fine-tuned with HYDRA)",
                "domain": "Language",
                "task": "Language modeling/generation",
                "organization": "University of California San Diego",
                "authors": "Kabir Nagrecha, Arun Kumar",
                "publication_date": "2021-10-16",
                "reference": "Hydra: A System for Large Multi-Model Deep Learning",
                "link": "https://arxiv.org/abs/2110.08633",
                "citations": 5.0,
                "notability_criteria": null,
                "parameters": 1540000000.0,
                "training_compute_(flop)": 1.92e+21,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Unreleased",
                "country": "United States",
                "base_model": "GPT-2 (1.5B)",
                "finetune_compute_(flop)": 1.848e+16,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 17:37:15+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 9.187520720836464,
                "log_compute": 21.28330122870355,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "TOME",
                "domain": "Language",
                "task": "Question answering",
                "organization": "University of Southern California,Google",
                "authors": "Michiel de Jong, Yury Zemlyanskiy, Nicholas FitzGerald, Fei Sha, William Cohen",
                "publication_date": "2021-10-12",
                "reference": "Mention Memory: incorporating textual knowledge into Transformers through entity mention attention",
                "link": "https://arxiv.org/abs/2110.06176v2",
                "citations": 50.0,
                "notability_criteria": null,
                "parameters": 220000000.0,
                "training_compute_(flop)": 1.03809024e+21,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Likely",
                "epochs": 157.2864,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 17:37:15+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 524288.0,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 8.342422680822207,
                "log_compute": 21.016235107877172,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "Megatron-Turing NLG 530B",
                "domain": "Language",
                "task": "Language modeling,Language modeling/generation,Question answering",
                "organization": "Microsoft,NVIDIA",
                "authors": "Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, Bryan Catanzaro",
                "publication_date": "2021-10-11",
                "reference": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",
                "link": "https://arxiv.org/abs/2201.11990",
                "citations": 784.0,
                "notability_criteria": "SOTA improvement,Training cost",
                "parameters": 530000000000.0,
                "training_compute_(flop)": 8.586e+23,
                "training_dataset_size_(gradients)": 270000000000.0,
                "training_time_(hours)": 770.0,
                "training_hardware": "NVIDIA A100 SXM4 80 GB",
                "approach": "Self-supervised learning",
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 4480.0,
                "last_modified": "2025-10-16 15:12:25+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 3932160.0,
                "organization_categorization": "Industry",
                "foundation_model": true,
                "training_chip-hours": 3449600.0,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": 3848505.6256471737,
                "frontier_model": true,
                "training_power_draw_(w)": 3615658.868639838,
                "training_compute_estimation_method": "Third-party estimation",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 11.724275869600788,
                "log_compute": 23.93379088414342,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "Turing ULRv5",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering,Translation",
                "organization": "Microsoft",
                "authors": "Unknown",
                "publication_date": "2021-09-28",
                "reference": "Microsoft Turing Universal Language Representation model, T-ULRv5, tops XTREME leaderboard and trains 100x faster",
                "link": "https://www.microsoft.com/en-us/research/blog/microsoft-turing-universal-language-representation-model-t-ulrv5-tops-xtreme-leaderboard-and-trains-100x-faster/",
                "citations": null,
                "notability_criteria": "SOTA improvement",
                "parameters": 2200000000.0,
                "training_compute_(flop)": 2.8983951e+22,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 336.0,
                "training_hardware": "NVIDIA A100",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Hosted access (no API)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 256.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 206668.900572444,
                "training_compute_estimation_method": "Hardware",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 9.342422680822207,
                "log_compute": 22.462157586812857,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "DLRM-2022",
                "domain": "Recommendation",
                "task": "Recommender system",
                "organization": "Facebook",
                "authors": "D Mudigere, Y Hao, J Huang, A Tulloch",
                "publication_date": "2021-09-15",
                "reference": "Software-Hardware Co-design for Fast and Scalable Training of Deep Learning Recommendation Models",
                "link": "https://arxiv.org/abs/2104.05158",
                "citations": 170.0,
                "notability_criteria": null,
                "parameters": 3000000000000.0,
                "training_compute_(flop)": 1.1e+21,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA Tesla V100 DGXS 32 GB,NVIDIA A100",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:07:09+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Reported",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Recommendation",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 12.477121254719663,
                "log_compute": 21.041392685158225,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "MegaMolBART",
                "domain": "Biology",
                "task": "Drug discovery",
                "organization": "NVIDIA",
                "authors": "Unknown",
                "publication_date": "2021-09-14",
                "reference": "MegaMolBART",
                "link": "https://docs.nvidia.com/bionemo-framework/0.4.0/models/megamolbart.html, https://github.com/NVIDIA/MegaMolBART",
                "citations": null,
                "notability_criteria": null,
                "parameters": 45000000.0,
                "training_compute_(flop)": 7.2e+20,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 80.0,
                "training_hardware": "NVIDIA V100",
                "approach": null,
                "confidence": "Likely",
                "epochs": 8.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Biology",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 7.653212513775344,
                "log_compute": 20.85733249643127,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "NLM",
                "domain": "Language",
                "task": "Language modeling",
                "organization": "Carnegie Mellon University (CMU),University of California San Diego",
                "authors": "Junxian He, Graham Neubig, Taylor Berg-Kirkpatrick",
                "publication_date": "2021-09-09",
                "reference": "Efficient Nearest Neighbor Language Models",
                "link": "https://arxiv.org/abs/2109.04212",
                "citations": 113.0,
                "notability_criteria": null,
                "parameters": 247000512.0,
                "training_compute_(flop)": 2.783e+19,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA GeForce RTX 3090",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 1.0,
                "last_modified": "2025-10-14 17:37:15+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 388.2894463979599,
                "training_compute_estimation_method": "Operation counting",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 8.392697853496687,
                "log_compute": 19.444513206334044,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "ALiBi (L=3072, Lvalid = 3072)",
                "domain": "Language",
                "task": "Language modeling",
                "organization": "University of Washington,Facebook AI Research,Allen Institute for AI",
                "authors": "Ofir Press, Noah A. Smith, Mike Lewis",
                "publication_date": "2021-08-27",
                "reference": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
                "link": "https://arxiv.org/abs/2108.12409",
                "citations": 889.0,
                "notability_criteria": null,
                "parameters": 1300000000.0,
                "training_compute_(flop)": 8.1e+20,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": 205.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:07:24+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 9.113943352306837,
                "log_compute": 20.90848501887865,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "XLMR-XXL",
                "domain": "Language",
                "task": "Translation,Language modeling/generation",
                "organization": "Facebook AI Research",
                "authors": "Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, Alexis Conneau",
                "publication_date": "2021-08-17",
                "reference": "Larger-Scale Transformers for Multilingual Masked Language Modeling",
                "link": "https://arxiv.org/abs/2105.00572",
                "citations": 137.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 10700000000.0,
                "training_compute_(flop)": 3.366e+22,
                "training_dataset_size_(gradients)": 167000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": 3.1394491018,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 17:37:14+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 1048576.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 10.02938377768521,
                "log_compute": 22.527114111639804,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "DNABERT",
                "domain": "Biology",
                "task": "Protein or nucleotide language model (pLM/nLM)",
                "organization": "Northeastern University",
                "authors": "Yanrong Ji, Zhihan Zhou, Han Liu, Ramana V Davuluri",
                "publication_date": "2021-08-15",
                "reference": "DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome",
                "link": "https://academic.oup.com/bioinformatics/article/37/15/2112/6128680",
                "citations": 835.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 110000000.0,
                "training_compute_(flop)": 1.07e+20,
                "training_dataset_size_(gradients)": 1444128539.3656242,
                "training_time_(hours)": 600.0,
                "training_hardware": "NVIDIA GeForce RTX 2080 Ti 11GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": 4.04,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:07:24+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware,Operation counting",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Biology",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 8.041392685158225,
                "log_compute": 20.029383777685208,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "GPT-2 (1.5B, Curriculum Learning 45K)",
                "domain": "Language",
                "task": "Language modeling/generation",
                "organization": "Microsoft",
                "authors": "Conglong Li, Minjia Zhang, Yuxiong He",
                "publication_date": "2021-08-13",
                "reference": "Curriculum Learning: A Regularization Method for Efficient and Stable Billion-Scale GPT Model Pre-Training",
                "link": "https://arxiv.org/abs/2108.06084",
                "citations": 41.0,
                "notability_criteria": null,
                "parameters": 1500000000.0,
                "training_compute_(flop)": 2.3811e+21,
                "training_dataset_size_(gradients)": 157000000000.0,
                "training_time_(hours)": 155.0,
                "training_hardware": "NVIDIA V100",
                "approach": null,
                "confidence": "Likely",
                "epochs": 1.6,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 128.0,
                "last_modified": "2025-10-14 17:37:16+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 4000000.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 77580.26955910088,
                "training_compute_estimation_method": "Comparison with other models,Hardware,Operation counting",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 9.176091259055681,
                "log_compute": 21.3767776350253,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "GPT-2 (117M, SLW 110K)",
                "domain": "Language",
                "task": "Language modeling/generation",
                "organization": "Microsoft",
                "authors": "Conglong Li, Minjia Zhang, Yuxiong He",
                "publication_date": "2021-08-13",
                "reference": "Curriculum Learning: A Regularization Method for Efficient and Stable Billion-Scale GPT Model Pre-Training",
                "link": "https://arxiv.org/abs/2108.06084",
                "citations": 41.0,
                "notability_criteria": null,
                "parameters": 117000000.0,
                "training_compute_(flop)": 1.10214e+20,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA V100",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 128.0,
                "last_modified": "2025-10-14 17:37:13+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 77580.26955910088,
                "training_compute_estimation_method": "Operation counting",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 8.068185861746162,
                "log_compute": 20.042236764539037,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "FMMformer (2-kernel fast weight + Band20)",
                "domain": "Language",
                "task": "Language modeling,Text classification",
                "organization": "University of California Los Angeles (UCLA),University of Utah",
                "authors": "Tan M. Nguyen, Vai Suliafu, Stanley J. Osher, Long Chen, Bao Wang",
                "publication_date": "2021-08-05",
                "reference": "FMMformer: Efficient and Flexible Transformer via Decomposed Near-field and Far-field Attention",
                "link": "https://proceedings.neurips.cc/paper/2021/file/f621585df244e9596dc70a39b579efb1-Paper.pdf",
                "citations": 37.0,
                "notability_criteria": null,
                "parameters": 40000000.0,
                "training_compute_(flop)": 2.472e+16,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA GeForce RTX 3090 Ti",
                "approach": null,
                "confidence": "Speculative",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 4.0,
                "last_modified": "2025-10-14 21:47:50+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 3637.223067232183,
                "training_compute_estimation_method": "Operation counting",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 7.6020599913279625,
                "log_compute": 16.39304846641678,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "SEER",
                "domain": "Vision",
                "task": "Image embedding,Image classification",
                "organization": "Facebook AI Research,INRIA",
                "authors": "Priya Goyal, Mathilde Caron, Benjamin Lefaudeux, Min Xu, Pengchao Wang, Vivek Pai, Mannat Singh, Vitaliy Liptchinsky, Ishan Misra, Armand Joulin, Piotr Bojanowski",
                "publication_date": "2021-07-29",
                "reference": "Self-supervised Pretraining of Visual Features in the Wild",
                "link": "https://arxiv.org/abs/2103.01988",
                "citations": 284.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 1300000000.0,
                "training_compute_(flop)": 1.8e+22,
                "training_dataset_size_(gradients)": 1000000000.0,
                "training_time_(hours)": 195.5,
                "training_hardware": "NVIDIA V100",
                "approach": "Self-supervised learning",
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 512.0,
                "last_modified": "2025-10-14 17:37:13+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": 98304.0,
                "training_code_accessibility": "Open (non-commercial)",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 34114.252463690456,
                "frontier_model": false,
                "training_power_draw_(w)": 310424.75538121304,
                "training_compute_estimation_method": "Hardware",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Vision",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 9.113943352306837,
                "log_compute": 22.255272505103306,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "HuBERT",
                "domain": "Speech",
                "task": "Speech recognition (ASR)",
                "organization": "Facebook AI Research",
                "authors": "Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed",
                "publication_date": "2021-07-27",
                "reference": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units",
                "link": "https://arxiv.org/abs/2106.07447",
                "citations": 3517.0,
                "notability_criteria": "Highly cited,SOTA improvement",
                "parameters": 1000000000.0,
                "training_compute_(flop)": 5.54e+21,
                "training_dataset_size_(gradients)": 864000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": "Self-supervised learning",
                "confidence": "Speculative",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:07:24+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Audio",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 9.0,
                "log_compute": 21.74350976472843,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "Codex",
                "domain": "Language",
                "task": "Code autocompletion",
                "organization": "OpenAI",
                "authors": "Mark Chen , Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger,  Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji,  Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, Wojciech Zaremba ",
                "publication_date": "2021-07-07",
                "reference": "Evaluating Large Language Models Trained on Code",
                "link": "https://openai.com/blog/openai-codex/\nhttps://arxiv.org/abs/2107.03374",
                "citations": 6731.0,
                "notability_criteria": "Significant use,Discretionary",
                "parameters": 12000000000.0,
                "training_compute_(flop)": 7.344e+22,
                "training_dataset_size_(gradients)": 52788000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": "Self-supervised learning",
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "API access",
                "country": "United States",
                "base_model": "GPT-3 13B",
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-17 18:59:33+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": true,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": null,
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 10.079181246047625,
                "log_compute": 22.865932668193185,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "DEQ-Transformer (Post-LN) + Jacobian Regularisation",
                "domain": "Language",
                "task": "Language modeling",
                "organization": "Carnegie Mellon University (CMU),Intel Labs",
                "authors": "Shaojie Bai, Vladlen Koltun, J. Zico Kolter",
                "publication_date": "2021-06-28",
                "reference": "Stabilizing Equilibrium Models by Jacobian Regularization",
                "link": "https://arxiv.org/abs/2106.14342",
                "citations": 68.0,
                "notability_criteria": null,
                "parameters": 98000000.0,
                "training_compute_(flop)": 2.9e+19,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 250.0,
                "training_hardware": "NVIDIA GeForce RTX 2080 Ti 11GB",
                "approach": null,
                "confidence": "Likely",
                "epochs": 23.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 4.0,
                "last_modified": "2025-10-14 17:37:15+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 2022.3901774609187,
                "training_compute_estimation_method": "Operation counting,Hardware",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 7.991226075692495,
                "log_compute": 19.462397997898957,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "Fold2Seq",
                "domain": "Biology",
                "task": "Proteins,Protein generation,Protein inverse folding,Protein fold classification",
                "organization": "IBM,Texas A&M",
                "authors": "Yue Cao, Payel Das, Vijil Chenthamarakshan, Pin-Yu Chen, Igor Melnyk, Yang Shen",
                "publication_date": "2021-06-24",
                "reference": "Fold2Seq: A Joint Sequence(1D)-Fold(3D) Embedding-based Generative Model for Protein Design",
                "link": "https://arxiv.org/abs/2106.13058",
                "citations": 50.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 12427904.0,
                "training_compute_(flop)": 1.4e+17,
                "training_dataset_size_(gradients)": 45995.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA Tesla K80",
                "approach": null,
                "confidence": "Speculative",
                "epochs": 200.0,
                "model_accessibility": "Unreleased",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 2.0,
                "last_modified": "2025-10-14 17:37:15+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 128.0,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 1213.542200949413,
                "training_compute_estimation_method": "Operation counting",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Biology",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 7.0943978898652595,
                "log_compute": 17.146128035678238,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "EfficientNetV2-XL",
                "domain": "Vision",
                "task": "Image classification,Neural Architecture Search - NAS",
                "organization": "Google,Google Brain",
                "authors": "Mingxing Tan, Quoc V. Le",
                "publication_date": "2021-06-23",
                "reference": "EfficientNetV2: Smaller Models and Faster Training",
                "link": "https://arxiv.org/abs/2104.00298",
                "citations": 3099.0,
                "notability_criteria": "Highly cited,SOTA improvement",
                "parameters": 208000000.0,
                "training_compute_(flop)": 9.56e+19,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 45.0,
                "training_hardware": "Google TPU v3",
                "approach": "Supervised",
                "confidence": "Confident",
                "epochs": 30.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 16.0,
                "last_modified": "2025-10-16 13:07:24+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 4096.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 104.34013401561587,
                "frontier_model": false,
                "training_power_draw_(w)": 7119.606126290003,
                "training_compute_estimation_method": "Hardware",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Vision",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 8.318063334962762,
                "log_compute": 19.9804578922761,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "StyleGAN3-T",
                "domain": "Image generation",
                "task": "Image generation",
                "organization": "NVIDIA,Aalto University",
                "authors": "Tero Karras, Miika Aittala, Samuli Laine, Erik H\u00e4rk\u00f6nen, Janne Hellsten, Jaakko Lehtinen, Timo Aila",
                "publication_date": "2021-06-21",
                "reference": "Alias-Free Generative Adversarial Networks",
                "link": "https://arxiv.org/abs/2106.12423",
                "citations": 2031.0,
                "notability_criteria": "Historical significance,Highly cited",
                "parameters": 2230000.0,
                "training_compute_(flop)": 1.70208e+21,
                "training_dataset_size_(gradients)": 50000000.0,
                "training_time_(hours)": 1576.0,
                "training_hardware": "NVIDIA V100",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 8.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 32.0,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open (non-commercial)",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 4854.493112492733,
                "training_compute_estimation_method": "Hardware",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Vision",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 6.348304863048161,
                "log_compute": 21.230979968640487,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "StyleGAN3-R",
                "domain": "Image generation",
                "task": "Image generation",
                "organization": "NVIDIA,Aalto University",
                "authors": "Tero Karras, Miika Aittala, Samuli Laine, Erik H\u00e4rk\u00f6nen, Janne Hellsten, Jaakko Lehtinen, Timo Aila",
                "publication_date": "2021-06-21",
                "reference": "Alias-Free Generative Adversarial Networks",
                "link": "https://arxiv.org/pdf/2106.12423",
                "citations": 2031.0,
                "notability_criteria": "Historical significance,Highly cited",
                "parameters": 1580000.0,
                "training_compute_(flop)": 2.42784e+21,
                "training_dataset_size_(gradients)": 50000000.0,
                "training_time_(hours)": 2248.0,
                "training_hardware": "NVIDIA V100",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 8.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 32.0,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open (non-commercial)",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 4854.493112492733,
                "training_compute_estimation_method": "Hardware",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Vision",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 6.198657086954422,
                "log_compute": 21.385220062383972,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "Denoising Diffusion Probabilistic Models (LSUN Bedroom)",
                "domain": "Vision",
                "task": "Image generation",
                "organization": "University of California (UC) Berkeley",
                "authors": "Jonathan Ho, Ajay Jain, Pieter Abbeel",
                "publication_date": "2021-06-11",
                "reference": "Denoising Diffusion Probabilistic Models",
                "link": "https://arxiv.org/abs/2006.11239",
                "citations": 22043.0,
                "notability_criteria": "Highly cited,SOTA improvement",
                "parameters": 256000000.0,
                "training_compute_(flop)": 7.840125000000001e+19,
                "training_dataset_size_(gradients)": 596320321536.0,
                "training_time_(hours)": null,
                "training_hardware": "Google TPU v3",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:07:24+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 436.308484475363,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Vision",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 8.40823996531185,
                "log_compute": 19.89432298696728,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "DeBERTa",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering",
                "organization": "Microsoft",
                "authors": "Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen",
                "publication_date": "2021-06-10",
                "reference": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention",
                "link": "https://arxiv.org/abs/2006.03654",
                "citations": 3010.0,
                "notability_criteria": "Highly cited,SOTA improvement",
                "parameters": 1500000000.0,
                "training_compute_(flop)": 2.588e+22,
                "training_dataset_size_(gradients)": 20800000000.0,
                "training_time_(hours)": 720.0,
                "training_hardware": "NVIDIA V100",
                "approach": "Self-supervised learning",
                "confidence": "Confident",
                "epochs": 49.2,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 256.0,
                "last_modified": "2025-10-16 13:07:24+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 6682.2289986716,
                "frontier_model": false,
                "training_power_draw_(w)": 155381.83775233384,
                "training_compute_estimation_method": "Hardware",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 9.176091259055681,
                "log_compute": 22.412964271996664,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "CoAtNet",
                "domain": "Vision",
                "task": "Image classification",
                "organization": "Google,Google Research,Google Brain",
                "authors": "Zihang Dai, Hanxiao Liu, Quoc V. Le, Mingxing Tan",
                "publication_date": "2021-06-09",
                "reference": "CoAtNet: Marrying Convolution and Attention\nfor All Data Sizes",
                "link": "https://arxiv.org/abs/2106.04803v2",
                "citations": 1304.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 2440000000.0,
                "training_compute_(flop)": 4.27e+22,
                "training_dataset_size_(gradients)": 88779000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "Google TPU v3",
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.83,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 17:37:15+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": 10050.0,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 1887.1635925610951,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Vision",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 9.38738982633873,
                "log_compute": 22.630427875025024,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "ViT-G/14",
                "domain": "Vision",
                "task": "Image classification",
                "organization": "Google Brain,Google Research",
                "authors": "X Zhai, A Kolesnikov, N Houlsby, L Beyer",
                "publication_date": "2021-06-08",
                "reference": "Scaling Vision Transformers",
                "link": "https://arxiv.org/abs/2106.04560",
                "citations": 1200.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 1843000000.0,
                "training_compute_(flop)": 5.85e+22,
                "training_dataset_size_(gradients)": 3000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "Google TPU v3",
                "approach": "Self-supervised learning",
                "confidence": "Confident",
                "epochs": 54.6,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 2048.0,
                "last_modified": "2025-10-16 13:07:24+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 32768.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 3847.8613922131217,
                "frontier_model": false,
                "training_power_draw_(w)": 911614.0493863056,
                "training_compute_estimation_method": "Hardware,Operation counting",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Vision",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 9.265525335219074,
                "log_compute": 22.76715586608218,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "GPT2-Large+LHOPT",
                "domain": "Language",
                "task": "Language modeling/generation",
                "organization": "OpenAI",
                "authors": "Diogo Almeida, Clemens Winter, Jie Tang, Wojciech Zaremba",
                "publication_date": "2021-06-02",
                "reference": "A Generalizable Approach to Learning Optimizers",
                "link": "https://arxiv.org/abs/2106.00958",
                "citations": 32.0,
                "notability_criteria": null,
                "parameters": 760000000.0,
                "training_compute_(flop)": 4.9540697e+21,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Speculative",
                "epochs": 1.0,
                "model_accessibility": "Unreleased",
                "country": "United States",
                "base_model": "GPT-2 (774M)",
                "finetune_compute_(flop)": 4.6968e+17,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 17:35:05+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 13000.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 8.880813592280791,
                "log_compute": 21.694962112486486,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "ByT5-XXL",
                "domain": "Language",
                "task": "Language modeling",
                "organization": "Google,Google Research",
                "authors": "Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel",
                "publication_date": "2021-05-28",
                "reference": "ByT5: Towards a token-free future with pre-trained byte-to-byte models",
                "link": "https://arxiv.org/abs/2105.13626",
                "citations": 568.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 12900000000.0,
                "training_compute_(flop)": 8.1e+22,
                "training_dataset_size_(gradients)": 1048576000000.0,
                "training_time_(hours)": null,
                "training_hardware": "Google TPU v3",
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:07:24+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 1048576.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 92453.37635669748,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 10.11058971029925,
                "log_compute": 22.90848501887865,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "ADM",
                "domain": "Image generation",
                "task": "Image generation,Text-to-image",
                "organization": "OpenAI",
                "authors": "Prafulla Dhariwal, Alex Nichol",
                "publication_date": "2021-05-11",
                "reference": "Diffusion Models Beat GANs on Image Synthesis",
                "link": "https://arxiv.org/abs/2105.05233",
                "citations": 9136.0,
                "notability_criteria": "Highly cited,SOTA improvement",
                "parameters": 559000000.0,
                "training_compute_(flop)": 6.2e+21,
                "training_dataset_size_(gradients)": 130191200000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA V100",
                "approach": null,
                "confidence": "Confident",
                "epochs": 381.0,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:07:09+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 11274.484326547095,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Vision",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 8.747411807886424,
                "log_compute": 21.792391689498253,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "DiffQ Transformer (16L)",
                "domain": "Language",
                "task": "Language modeling",
                "organization": "Meta AI",
                "authors": "Alexandre D\u00e9fossez, Yossi Adi, Gabriel Synnaeve",
                "publication_date": "2021-04-20",
                "reference": "Differentiable Model Compression via Pseudo Quantization Noise",
                "link": "https://arxiv.org/abs/2104.09987",
                "citations": 52.0,
                "notability_criteria": null,
                "parameters": 247000000.0,
                "training_compute_(flop)": 3.02e+19,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA V100",
                "approach": "Self-supervised learning",
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 17:35:05+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open (non-commercial)",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 8.392696953259666,
                "log_compute": 19.48000694295715,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "Transformer-C",
                "domain": "Language",
                "task": "Language modeling",
                "organization": "University of Massachusetts Amherst",
                "authors": "Simeng Sun, Mohit Iyyer",
                "publication_date": "2021-04-08",
                "reference": "Revisiting Simple Neural Probabilistic Language Models",
                "link": "https://arxiv.org/abs/2104.03474",
                "citations": 15.0,
                "notability_criteria": null,
                "parameters": 148000000.0,
                "training_compute_(flop)": 1.8877734e+18,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": 40.0,
                "training_hardware": "NVIDIA GeForce GTX 1080 Ti",
                "approach": null,
                "confidence": "Confident",
                "epochs": 19.88,
                "model_accessibility": "Unreleased",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 4.0,
                "last_modified": "2025-10-14 17:35:05+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 10240.0,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 2026.0414956053005,
                "training_compute_estimation_method": "Operation counting,Hardware",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 8.170261715394957,
                "log_compute": 18.275949862294794,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "T2R + Random Init",
                "domain": "Language",
                "task": "Language modeling",
                "organization": "University of Washington,Microsoft,DeepMind,Allen Institute for AI",
                "authors": "Jungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu Chen, Noah A. Smith",
                "publication_date": "2021-03-24",
                "reference": "Finetuning Pretrained Transformers into RNNs",
                "link": "https://arxiv.org/abs/2103.13076",
                "citations": 75.0,
                "notability_criteria": null,
                "parameters": 450000000.0,
                "training_compute_(flop)": 2.749544e+19,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA V100",
                "approach": null,
                "confidence": "Likely",
                "epochs": 205.48,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 8.0,
                "last_modified": "2025-10-14 17:35:05+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 74000.0,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": 98.0,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 4864.124132899583,
                "training_compute_estimation_method": "Operation counting,Hardware",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 8.653212513775344,
                "log_compute": 19.439260673937632,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "T2R 75% + Pretrain (WT-103)",
                "domain": "Language",
                "task": "Language modeling",
                "organization": "University of Washington,Microsoft,DeepMind",
                "authors": "Jungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu Chen, Noah A. Smith",
                "publication_date": "2021-03-24",
                "reference": "Finetuning Pretrained Transformers into RNNs",
                "link": "https://arxiv.org/abs/2103.13076",
                "citations": 75.0,
                "notability_criteria": null,
                "parameters": 668893184.0,
                "training_compute_(flop)": 1.3517515512289972e+19,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 11.8,
                "training_hardware": "NVIDIA V100",
                "approach": null,
                "confidence": "Confident",
                "epochs": 34.47,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 8.0,
                "last_modified": "2025-10-14 17:35:06+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 4864.124132899583,
                "training_compute_estimation_method": "Operation counting,Hardware",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 8.8253567705284,
                "log_compute": 19.130896876631095,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "T2R + Pretrain",
                "domain": "Language",
                "task": "Language modeling",
                "organization": "University of Washington,Microsoft,DeepMind",
                "authors": "Jungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu Chen, Noah A. Smith",
                "publication_date": "2021-03-24",
                "reference": "Finetuning Pretrained Transformers into RNNs",
                "link": "https://arxiv.org/abs/2103.13076",
                "citations": 75.0,
                "notability_criteria": null,
                "parameters": 668893184.0,
                "training_compute_(flop)": 1.372929105052406e+19,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 12.25,
                "training_hardware": "NVIDIA V100",
                "approach": null,
                "confidence": "Confident",
                "epochs": 34.47,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 8.0,
                "last_modified": "2025-10-14 17:35:04+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 4864.124132899583,
                "training_compute_estimation_method": "Operation counting,Hardware",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 8.8253567705284,
                "log_compute": 19.13764811183292,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "Very Deep VAEs (ImageNet-64)",
                "domain": "Image generation",
                "task": "Image generation,Image representation",
                "organization": "OpenAI",
                "authors": "Rewon Child",
                "publication_date": "2021-03-16",
                "reference": "Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them on Images",
                "link": "https://arxiv.org/abs/2011.10650",
                "citations": null,
                "notability_criteria": null,
                "parameters": 125000000.0,
                "training_compute_(flop)": 1.8144e+21,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 420.0,
                "training_hardware": "NVIDIA V100",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 32.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 19459.963111943263,
                "training_compute_estimation_method": "Hardware",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Vision",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 8.096910013008056,
                "log_compute": 21.258733037212814,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "ResNet-RS",
                "domain": "Vision",
                "task": "Image classification",
                "organization": "Google Brain,University of California (UC) Berkeley",
                "authors": "Irwan Bello, William Fedus, Xianzhi Du, Ekin D. Cubuk, Aravind Srinivas, Tsung-Yi Lin, Jonathon Shlens, Barret Zoph",
                "publication_date": "2021-03-13",
                "reference": "Revisiting ResNets: Improved Training and Scaling Strategies",
                "link": "https://arxiv.org/abs/2103.07579",
                "citations": 307.0,
                "notability_criteria": null,
                "parameters": 192000000.0,
                "training_compute_(flop)": 1.763328e+22,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": "Google TPU v3",
                "approach": null,
                "confidence": "Confident",
                "epochs": 350.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 17:35:07+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Vision",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 8.28330122870355,
                "log_compute": 22.246333103757447,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "Generative BST",
                "domain": "Language",
                "task": "Language modeling/generation,Chat,Question answering",
                "organization": "Facebook AI Research",
                "authors": "Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston",
                "publication_date": "2021-03-05",
                "reference": "Recipes for building an open-domain chatbot",
                "link": "https://arxiv.org/abs/2004.13637",
                "citations": 1058.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 9431810048.0,
                "training_compute_(flop)": 1.449e+22,
                "training_dataset_size_(gradients)": 56800000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.7605633803,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 17:35:05+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 9.974595045698555,
                "log_compute": 22.161068385471175,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "RFA-GATE-Gaussian-Stateful Big",
                "domain": "Language",
                "task": "Language modeling/generation,Translation",
                "organization": "University of Washington,DeepMind,Allen Institute for AI,Hebrew University of Jerusalem,The University of Hong Kong",
                "authors": "Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A. Smith, Lingpeng Kong",
                "publication_date": "2021-03-03",
                "reference": "Random Feature Attention",
                "link": "https://arxiv.org/abs/2103.02143",
                "citations": 382.0,
                "notability_criteria": null,
                "parameters": 242000000.0,
                "training_compute_(flop)": 7.14e+18,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": 3.36,
                "training_hardware": "Google TPU v3",
                "approach": null,
                "confidence": "Confident",
                "epochs": 47.72,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 16.0,
                "last_modified": "2025-10-14 17:35:04+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 32768.0,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 7137.385794869192,
                "training_compute_estimation_method": "Operation counting",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 8.383815365980432,
                "log_compute": 18.853698211776173,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "Meta Pseudo Labels",
                "domain": "Vision",
                "task": "Image classification",
                "organization": "Google Brain,Google AI",
                "authors": "Hieu Pham, Zihang Dai, Qizhe Xie, Minh-Thang Luong, and Quoc V. Le",
                "publication_date": "2021-03-01",
                "reference": "Meta pseudo labels",
                "link": "https://arxiv.org/abs/2003.10580",
                "citations": 699.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 480000000.0,
                "training_compute_(flop)": 4.79e+22,
                "training_dataset_size_(gradients)": 131280000.0,
                "training_time_(hours)": 264.0,
                "training_hardware": "Google TPU v3",
                "approach": "Self-supervised learning",
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 1024.0,
                "last_modified": "2025-10-14 17:35:04+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": 270336.0,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 53844.28059190435,
                "frontier_model": false,
                "training_power_draw_(w)": 456813.03629676264,
                "training_compute_estimation_method": "Hardware",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Vision",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 8.681241237375588,
                "log_compute": 22.680335513414562,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "SRU++ Large",
                "domain": "Language",
                "task": "Language modeling",
                "organization": "ASAPP",
                "authors": "Tao Lei",
                "publication_date": "2021-02-24",
                "reference": "When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute",
                "link": "https://arxiv.org/abs/2102.12459",
                "citations": 53.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 234000000.0,
                "training_compute_(flop)": 2.1173704e+19,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA V100",
                "approach": null,
                "confidence": "Confident",
                "epochs": 254.5,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 17:35:05+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 65536.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": 360.0,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting,Hardware",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 8.369215857410143,
                "log_compute": 19.325796837521114,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "SRU++ Base",
                "domain": "Language",
                "task": "Language modeling",
                "organization": "ASAPP",
                "authors": "Tao Lei",
                "publication_date": "2021-02-24",
                "reference": "When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute",
                "link": "https://arxiv.org/abs/2102.12459",
                "citations": 53.0,
                "notability_criteria": null,
                "parameters": 148000000.0,
                "training_compute_(flop)": 2.592e+19,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 24.0,
                "training_hardware": "NVIDIA V100",
                "approach": null,
                "confidence": "Confident",
                "epochs": 25.56,
                "model_accessibility": "Unreleased",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 8.0,
                "last_modified": "2025-10-14 17:35:07+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 4867.158066309197,
                "training_compute_estimation_method": "Hardware",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 8.170261715394957,
                "log_compute": 19.413634997198557,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "SRU++ Large only 2 attention layers (k=5) (WT103)",
                "domain": "Language",
                "task": "Language modeling",
                "organization": "ASAPP",
                "authors": "Tao Lei",
                "publication_date": "2021-02-24",
                "reference": "When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute",
                "link": "https://arxiv.org/abs/2102.12459",
                "citations": 53.0,
                "notability_criteria": null,
                "parameters": 225000000.0,
                "training_compute_(flop)": 3.564e+19,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 33.0,
                "training_hardware": "NVIDIA V100",
                "approach": null,
                "confidence": "Confident",
                "epochs": 34.08,
                "model_accessibility": "Unreleased",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 8.0,
                "last_modified": "2025-10-14 17:35:05+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 4867.158066309197,
                "training_compute_estimation_method": "Hardware",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 8.352182518111363,
                "log_compute": 19.55193769536484,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "MSA Transformer",
                "domain": "Biology",
                "task": "Proteins,Protein or nucleotide language model (pLM/nLM),Protein contact and distance prediction,Protein folding prediction",
                "organization": "Facebook AI Research,University of California (UC) Berkeley,New York University (NYU)",
                "authors": "Roshan Rao, Jason Liu, Robert Verkuil, Joshua Meier, John F. Canny, Pieter Abbeel, Tom Sercu, Alexander Rives",
                "publication_date": "2021-02-13",
                "reference": "MSA Transformer",
                "link": "https://proceedings.mlr.press/v139/rao21a/rao21a.pdf",
                "citations": 579.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 100000000.0,
                "training_compute_(flop)": 5.49e+21,
                "training_dataset_size_(gradients)": 1395000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA Tesla V100 DGXS 32 GB",
                "approach": null,
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 32.0,
                "last_modified": "2025-10-16 13:07:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 13256.937301517895,
                "frontier_model": false,
                "training_power_draw_(w)": 16227.834954676886,
                "training_compute_estimation_method": "Operation counting",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Biology",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 8.0,
                "log_compute": 21.739572344450092,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "DLWP",
                "domain": "Earth science",
                "task": "Weather forecasting",
                "organization": "University of Washington,Microsoft Research",
                "authors": "Jonathan A. Weyn, Dale R. Durran, Rich Caruana, Nathaniel Cresswell-Clay",
                "publication_date": "2021-02-09",
                "reference": "Sub-seasonal forecasting with a large ensemble of deep-learning weather prediction models",
                "link": "https://arxiv.org/abs/2102.05107",
                "citations": null,
                "notability_criteria": "SOTA improvement",
                "parameters": 2676376.0,
                "training_compute_(flop)": 5.6845152e+18,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 168.0,
                "training_hardware": "NVIDIA Tesla V100 DGXS 32 GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 1.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 278.6620974208019,
                "training_compute_estimation_method": "Hardware",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Earth science",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 6.427547126754285,
                "log_compute": 18.754693432094516,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "DeiT-B",
                "domain": "Vision",
                "task": "Image classification",
                "organization": "Meta AI,Sorbonne University",
                "authors": "Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Herv\u00e9 J\u00e9gou",
                "publication_date": "2021-01-15",
                "reference": "Training data-efficient image transformers & distillation through attention",
                "link": "https://arxiv.org/abs/2012.12877",
                "citations": 7446.0,
                "notability_criteria": "Highly cited",
                "parameters": 86000000.0,
                "training_compute_(flop)": 7.884e+19,
                "training_dataset_size_(gradients)": 3840000.0,
                "training_time_(hours)": 53.0,
                "training_hardware": "NVIDIA V100",
                "approach": null,
                "confidence": "Confident",
                "epochs": 300.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:07:09+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Vision",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 7.9344984512435675,
                "log_compute": 19.896746615607405,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "Switch",
                "domain": "Language",
                "task": "Text autocompletion",
                "organization": "Google",
                "authors": "William Fedus, Barret Zoph, Noam Shazeer",
                "publication_date": "2021-01-11",
                "reference": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity",
                "link": "https://arxiv.org/abs/2101.03961",
                "citations": 2642.0,
                "notability_criteria": "Highly cited,SOTA improvement",
                "parameters": 1571000000000.0,
                "training_compute_(flop)": 8.22e+22,
                "training_dataset_size_(gradients)": 86400000000.0,
                "training_time_(hours)": 648.0,
                "training_hardware": "Google TPU v3",
                "approach": "Self-supervised learning",
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 1024.0,
                "last_modified": "2025-10-16 15:12:26+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": 663552.0,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 136886.37981186676,
                "frontier_model": true,
                "training_power_draw_(w)": 457311.78237926256,
                "training_compute_estimation_method": "Third-party estimation",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 12.196176185039974,
                "log_compute": 22.91487181754005,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "DALL-E",
                "domain": "Image generation",
                "task": "Text-to-image,Image generation",
                "organization": "OpenAI",
                "authors": "Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever",
                "publication_date": "2021-01-05",
                "reference": "Zero-Shot Text-to-Image Generation",
                "link": "https://openai.com/blog/dall-e/\n\nhttps://arxiv.org/abs/2102.12092",
                "citations": 5468.0,
                "notability_criteria": "Significant use,Highly cited,Historical significance",
                "parameters": 12000000000.0,
                "training_compute_(flop)": 4.7e+22,
                "training_dataset_size_(gradients)": 320000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA Tesla V100 DGXS 16 GB",
                "approach": "Self-supervised learning",
                "confidence": "Likely",
                "epochs": 1.76,
                "model_accessibility": "API access",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 1024.0,
                "last_modified": "2025-10-16 13:07:09+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 125818.6901,
                "frontier_model": false,
                "training_power_draw_(w)": 519741.92129196384,
                "training_compute_estimation_method": "Third-party estimation",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Vision",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 10.079181246047625,
                "log_compute": 22.672097857935718,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "CLIP (ViT L/14@336px)",
                "domain": "Multimodal,Vision,Language,Video",
                "task": "Zero-shot image classification,Character recognition (OCR),Video description",
                "organization": "OpenAI",
                "authors": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever",
                "publication_date": "2021-01-05",
                "reference": "Learning Transferable Visual Models From Natural Language Supervision",
                "link": "https://arxiv.org/abs/2103.00020\nhttps://huggingface.co/openai/clip-vit-large-patch14-336",
                "citations": 35452.0,
                "notability_criteria": "Highly cited,SOTA improvement",
                "parameters": 370000000.0,
                "training_compute_(flop)": 1.05e+22,
                "training_dataset_size_(gradients)": 400000000.0,
                "training_time_(hours)": 288.0,
                "training_hardware": "NVIDIA V100",
                "approach": "Self-supervised learning",
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 256.0,
                "last_modified": "2025-10-16 13:07:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 24638.51841340951,
                "frontier_model": false,
                "training_power_draw_(w)": 155922.57638758916,
                "training_compute_estimation_method": "Third-party estimation",
                "year": 2021,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Multimodal",
                "export_controls_sum": 4.0,
                "publication_count": 16.799695057021165,
                "log_params": 8.568201724066995,
                "log_compute": 22.02118929906994,
                "pub_bin": "Medium",
                "export_bin": "Medium"
            },
            {
                "model": "Shortformer",
                "domain": "Language",
                "task": "Language modeling",
                "organization": "University of Washington,Facebook AI Research,Allen Institute for AI",
                "authors": "Ofir Press, Noah A. Smith, Mike Lewis",
                "publication_date": "2020-12-31",
                "reference": "Shortformer: Better Language Modeling using Shorter Inputs",
                "link": "https://arxiv.org/abs/2012.15832",
                "citations": 92.0,
                "notability_criteria": null,
                "parameters": 247000000.0,
                "training_compute_(flop)": 3.04e+18,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": 205.0,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 17:35:07+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2020,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 18.419496055176168,
                "log_params": 8.392696953259666,
                "log_compute": 18.482873583608754,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "CT-MoS (WT2)",
                "domain": "Language",
                "task": "Language modeling",
                "organization": "Google,National Tsing Hua University",
                "authors": "Pei-Hsin Wang, Sheng-Iou Hsieh, Shih-Chieh Chang, Yu-Ting Chen, Jia-Yu Pan, Wei Wei, Da-Chang Juan",
                "publication_date": "2020-12-25",
                "reference": "Contextual Temperature for Language Modeling",
                "link": "https://arxiv.org/abs/2012.13575",
                "citations": 30.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 45000000.0,
                "training_compute_(flop)": 5.4e+17,
                "training_dataset_size_(gradients)": 2000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA GeForce GTX 1080 Ti",
                "approach": null,
                "confidence": "Confident",
                "epochs": 1000.0,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 4.0,
                "last_modified": "2025-10-14 17:31:39+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 2030.739275278243,
                "training_compute_estimation_method": "Operation counting",
                "year": 2020,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 18.419496055176168,
                "log_params": 7.653212513775344,
                "log_compute": 17.73239375982297,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "ESM1b",
                "domain": "Biology",
                "task": "Proteins,Protein or nucleotide language model (pLM/nLM)",
                "organization": "Facebook AI Research,New York University (NYU)",
                "authors": "Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C. Lawrence Zitnick, Jerry Ma, and Rob Fergus",
                "publication_date": "2020-12-15",
                "reference": "Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences",
                "link": "https://www.pnas.org/doi/abs/10.1073/pnas.2016239118",
                "citations": 2392.0,
                "notability_criteria": "Highly cited,SOTA improvement",
                "parameters": 652400000.0,
                "training_compute_(flop)": 5.1e+21,
                "training_dataset_size_(gradients)": 27750400000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA V100",
                "approach": null,
                "confidence": "Confident",
                "epochs": 56.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 128.0,
                "last_modified": "2025-10-16 13:06:54+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 924.3248891034536,
                "frontier_model": false,
                "training_power_draw_(w)": 77997.75584661258,
                "training_compute_estimation_method": "Hardware,Operation counting",
                "year": 2020,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Biology",
                "export_controls_sum": 2.0,
                "publication_count": 18.419496055176168,
                "log_params": 8.814513952368237,
                "log_compute": 21.707570176097935,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "ViT-Huge/14",
                "domain": "Vision",
                "task": "Image representation",
                "organization": "Google Brain,Google Research",
                "authors": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby",
                "publication_date": "2020-10-22",
                "reference": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
                "link": "https://arxiv.org/abs/2010.11929",
                "citations": 47228.0,
                "notability_criteria": "Highly cited",
                "parameters": 632000000.0,
                "training_compute_(flop)": 4.262e+21,
                "training_dataset_size_(gradients)": 303000000.0,
                "training_time_(hours)": null,
                "training_hardware": "Google TPU v3",
                "approach": null,
                "confidence": "Confident",
                "epochs": 14.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:06:54+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": 30000.0,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 6724.023241261178,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware",
                "year": 2020,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Vision",
                "export_controls_sum": 2.0,
                "publication_count": 18.419496055176168,
                "log_params": 8.800717078282386,
                "log_compute": 21.629613445378183,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "wave2vec 2.0 LARGE",
                "domain": "Speech",
                "task": "Speech completion",
                "organization": "Facebook",
                "authors": "Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli",
                "publication_date": "2020-10-22",
                "reference": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations",
                "link": "https://arxiv.org/abs/2006.11477",
                "citations": 6617.0,
                "notability_criteria": "Highly cited,SOTA improvement",
                "parameters": 317000000.0,
                "training_compute_(flop)": 3.87072e+21,
                "training_dataset_size_(gradients)": 4598395200.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA Tesla V100 DGXS 32 GB",
                "approach": "Self-supervised learning",
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:06:55+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 5021.241075362514,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware",
                "year": 2020,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Audio",
                "export_controls_sum": 2.0,
                "publication_count": 18.419496055176168,
                "log_params": 8.501059262217751,
                "log_compute": 21.58779175647704,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Conformer + Wav2vec 2.0 + Noisy Student",
                "domain": "Speech",
                "task": "Speech recognition (ASR)",
                "organization": "Google,Google Research,Google Brain",
                "authors": "Yu Zhang, James Qin, Daniel S. Park, Wei Han, Chung-Cheng Chiu, Ruoming Pang, Quoc V. Le, Yonghui Wu",
                "publication_date": "2020-10-20",
                "reference": "Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition",
                "link": "https://arxiv.org/abs/2010.10504v2",
                "citations": 316.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 1000000000.0,
                "training_compute_(flop)": 7.6e+21,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 168.0,
                "training_hardware": "Google TPU v3",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 256.0,
                "last_modified": "2025-10-14 17:31:40+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 9449.541661137231,
                "frontier_model": false,
                "training_power_draw_(w)": 114539.4599635263,
                "training_compute_estimation_method": "Hardware",
                "year": 2020,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Audio",
                "export_controls_sum": 2.0,
                "publication_count": 18.419496055176168,
                "log_params": 9.0,
                "log_compute": 21.88081359228079,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "mT5-XXL",
                "domain": "Language",
                "task": "Language modeling,Translation,Language modeling/generation,Question answering",
                "organization": "Google,Google Research",
                "authors": "Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel",
                "publication_date": "2020-10-20",
                "reference": "mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer",
                "link": "https://aclanthology.org/2021.naacl-main.41/",
                "citations": 2736.0,
                "notability_criteria": "Highly cited,SOTA improvement",
                "parameters": 13000000000.0,
                "training_compute_(flop)": 8.2e+22,
                "training_dataset_size_(gradients)": 150000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 15:12:25+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 1048576.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 192595.40371209933,
                "frontier_model": true,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2020,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 18.419496055176168,
                "log_params": 10.113943352306837,
                "log_compute": 22.913813852383715,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Memformer (4 encoder + 16 decoder)",
                "domain": "Language",
                "task": "Language modeling",
                "organization": "UC Davis,Westlake University,Facebook AI",
                "authors": "Qingyang Wu, Zhenzhong Lan, Kun Qian, Jing Gu, Alborz Geramifard, Zhou Yu",
                "publication_date": "2020-10-14",
                "reference": "Memformer: A Memory-Augmented Transformer for Sequence Modeling",
                "link": "https://arxiv.org/abs/2010.06891",
                "citations": 59.0,
                "notability_criteria": null,
                "parameters": 76200000.0,
                "training_compute_(flop)": 1.2e+19,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": 96.0,
                "training_hardware": "NVIDIA Tesla V100 DGXS 16 GB,NVIDIA GeForce RTX 2080 Ti 11GB",
                "approach": null,
                "confidence": "Likely",
                "epochs": 11.93,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 4.0,
                "last_modified": "2025-10-16 13:06:54+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware",
                "year": 2020,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 18.419496055176168,
                "log_params": 7.8819549713396,
                "log_compute": 19.079181246047625,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "LUKE",
                "domain": "Language",
                "task": "Question answering,Relation extraction,Named entity recognition (NER)",
                "organization": "University of Washington,National Institute of Informatics",
                "authors": "Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, Yuji Matsumoto",
                "publication_date": "2020-10-02",
                "reference": "LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention",
                "link": "https://arxiv.org/abs/2010.01057v1",
                "citations": 700.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 483000000.0,
                "training_compute_(flop)": 1.8144e+22,
                "training_dataset_size_(gradients)": 3511000000.0,
                "training_time_(hours)": 720.0,
                "training_hardware": "NVIDIA V100",
                "approach": null,
                "confidence": "Likely",
                "epochs": 119.46,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": "RoBERTa Large",
                "finetune_compute_(flop)": null,
                "hardware_quantity": 16.0,
                "last_modified": "2025-10-14 17:31:40+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 2048.0,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": 11520.0,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 4186.383565732907,
                "frontier_model": false,
                "training_power_draw_(w)": 9765.799615782104,
                "training_compute_estimation_method": "Hardware",
                "year": 2020,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 18.419496055176168,
                "log_params": 8.683947130751513,
                "log_compute": 22.258733037212814,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "ProBERTa",
                "domain": "Biology",
                "task": "Proteins,Protein representation learning,Protein classification,Protein interaction prediction",
                "organization": "University of Illinois Urbana-Champaign (UIUC),Reed College",
                "authors": "Ananthan Nambiar, Maeve Heflin, Simon Liu, Sergei Maslov, Mark Hopkins, Anna Ritz",
                "publication_date": "2020-09-01",
                "reference": "Transforming the Language of Life: Transformer Neural Networks for Protein Prediction Tasks",
                "link": "https://dl.acm.org/doi/10.1145/3388440.3412467",
                "citations": 97.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 44000000.0,
                "training_compute_(flop)": 9.72e+18,
                "training_dataset_size_(gradients)": 58320000.0,
                "training_time_(hours)": 18.0,
                "training_hardware": "NVIDIA V100",
                "approach": "Self-supervised learning",
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (non-commercial)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 4.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open (non-commercial)",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 26.206992435694065,
                "frontier_model": false,
                "training_power_draw_(w)": 2443.135942042391,
                "training_compute_estimation_method": "Hardware",
                "year": 2020,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Biology",
                "export_controls_sum": 2.0,
                "publication_count": 18.419496055176168,
                "log_params": 7.643452676486188,
                "log_compute": 18.987666264926276,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "ESM1-670M (UR50/S)",
                "domain": "Biology",
                "task": "Proteins,Protein or nucleotide language model (pLM/nLM),Mutation prediction,Protein contact and distance prediction",
                "organization": "Facebook AI Research,New York University (NYU)",
                "authors": "Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C. Lawrence Zitnick, Jerry Ma, and Rob Fergus",
                "publication_date": "2020-08-31",
                "reference": "Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences",
                "link": "https://www.pnas.org/doi/abs/10.1073/pnas.2016239118",
                "citations": 2392.0,
                "notability_criteria": "Highly cited,SOTA improvement",
                "parameters": 669200000.0,
                "training_compute_(flop)": 4.4e+20,
                "training_dataset_size_(gradients)": 27750400000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA V100",
                "approach": null,
                "confidence": "Confident",
                "epochs": 4.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 128.0,
                "last_modified": "2025-10-16 13:06:54+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 966.9178746226474,
                "frontier_model": false,
                "training_power_draw_(w)": 78182.09119198628,
                "training_compute_estimation_method": "Operation counting",
                "year": 2020,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Biology",
                "export_controls_sum": 2.0,
                "publication_count": 18.419496055176168,
                "log_params": 8.825555932290357,
                "log_compute": 20.643452676486188,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "ESM1-670M (UR50/D)",
                "domain": "Biology",
                "task": "Proteins,Protein or nucleotide language model (pLM/nLM)",
                "organization": "Facebook AI Research,New York University (NYU)",
                "authors": "Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C. Lawrence Zitnick, Jerry Ma, and Rob Fergus",
                "publication_date": "2020-08-31",
                "reference": "Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences",
                "link": "https://www.pnas.org/doi/abs/10.1073/pnas.2016239118",
                "citations": 2392.0,
                "notability_criteria": "Highly cited",
                "parameters": 669200000.0,
                "training_compute_(flop)": 4.8e+20,
                "training_dataset_size_(gradients)": 33847900000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA V100",
                "approach": null,
                "confidence": "Confident",
                "epochs": 3.51,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 128.0,
                "last_modified": "2025-10-16 13:06:54+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 131072.0,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 1054.8194995883428,
                "frontier_model": false,
                "training_power_draw_(w)": 78182.09119198628,
                "training_compute_estimation_method": "Operation counting",
                "year": 2020,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Biology",
                "export_controls_sum": 2.0,
                "publication_count": 18.419496055176168,
                "log_params": 8.825555932290357,
                "log_compute": 20.681241237375588,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "ESM1-670M (UR100)",
                "domain": "Biology",
                "task": "Proteins,Protein or nucleotide language model (pLM/nLM)",
                "organization": "Facebook AI Research,New York University (NYU)",
                "authors": "Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C. Lawrence Zitnick, Jerry Ma, and Rob Fergus",
                "publication_date": "2020-08-31",
                "reference": "Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences",
                "link": "https://www.pnas.org/doi/abs/10.1073/pnas.2016239118",
                "citations": 2392.0,
                "notability_criteria": "Highly cited",
                "parameters": 669200000.0,
                "training_compute_(flop)": 1.4e+20,
                "training_dataset_size_(gradients)": 127897600000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA V100",
                "approach": null,
                "confidence": "Confident",
                "epochs": 0.3,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 128.0,
                "last_modified": "2025-10-16 13:06:54+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 307.6556873799333,
                "frontier_model": false,
                "training_power_draw_(w)": 78182.09119198628,
                "training_compute_estimation_method": "Operation counting",
                "year": 2020,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Biology",
                "export_controls_sum": 2.0,
                "publication_count": 18.419496055176168,
                "log_params": 8.825555932290357,
                "log_compute": 20.146128035678238,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "ESM1-85M",
                "domain": "Biology",
                "task": "Proteins,Protein or nucleotide language model (pLM/nLM)",
                "organization": "Facebook AI Research,New York University (NYU)",
                "authors": "Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C. Lawrence Zitnick, Jerry Ma, and Rob Fergus",
                "publication_date": "2020-08-31",
                "reference": "Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences",
                "link": "https://www.pnas.org/doi/abs/10.1073/pnas.2016239118",
                "citations": 2392.0,
                "notability_criteria": "Highly cited",
                "parameters": 85100000.0,
                "training_compute_(flop)": 5.6e+19,
                "training_dataset_size_(gradients)": 27750400000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA V100",
                "approach": null,
                "confidence": "Confident",
                "epochs": 4.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 128.0,
                "last_modified": "2025-10-16 13:06:55+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 123.06227495197334,
                "frontier_model": false,
                "training_power_draw_(w)": 78182.09119198628,
                "training_compute_estimation_method": "Operation counting",
                "year": 2020,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Biology",
                "export_controls_sum": 2.0,
                "publication_count": 18.419496055176168,
                "log_params": 7.929929560084588,
                "log_compute": 19.7481880270062,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "ESM1-43M",
                "domain": "Biology",
                "task": "Proteins,Protein or nucleotide language model (pLM/nLM)",
                "organization": "Facebook AI Research,New York University (NYU)",
                "authors": "Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C. Lawrence Zitnick, Jerry Ma, and Rob Fergus",
                "publication_date": "2020-08-31",
                "reference": "Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences",
                "link": "https://www.pnas.org/doi/abs/10.1073/pnas.2016239118",
                "citations": 2392.0,
                "notability_criteria": "Highly cited",
                "parameters": 42600000.0,
                "training_compute_(flop)": 2.8e+19,
                "training_dataset_size_(gradients)": 27750400000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA V100",
                "approach": null,
                "confidence": "Confident",
                "epochs": 4.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 128.0,
                "last_modified": "2025-10-16 13:06:54+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 61.53113747598667,
                "frontier_model": false,
                "training_power_draw_(w)": 78182.09119198628,
                "training_compute_estimation_method": "Operation counting",
                "year": 2020,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Biology",
                "export_controls_sum": 2.0,
                "publication_count": 18.419496055176168,
                "log_params": 7.629409599102719,
                "log_compute": 19.44715803134222,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Transformer+Recurrent Windows of Context",
                "domain": "Language",
                "task": "Language modeling",
                "organization": "Toyota Technological Institute at Chicago,University of Chicago",
                "authors": "Davis Yoshida, Allyson Ettinger, Kevin Gimpel",
                "publication_date": "2020-08-16",
                "reference": "Adding Recurrence to Pretrained Transformers for Improved Efficiency and Context Size",
                "link": "https://arxiv.org/abs/2008.07027",
                "citations": 7.0,
                "notability_criteria": null,
                "parameters": 124000000.0,
                "training_compute_(flop)": 7.9375326e+20,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Speculative",
                "epochs": 2.0,
                "model_accessibility": "Unreleased",
                "country": "United States",
                "base_model": "GPT-2 (124M)",
                "finetune_compute_(flop)": 1.53264e+17,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2020,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 18.419496055176168,
                "log_params": 8.093421685162236,
                "log_compute": 20.89968552198142,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "DeLighT",
                "domain": "Language",
                "task": "Language modeling,Translation",
                "organization": "University of Washington,Allen Institute for AI,Facebook AI Research",
                "authors": "Sachin Mehta, Marjan Ghazvininejad, Srinivasan Iyer, Luke Zettlemoyer, Hannaneh Hajishirzi",
                "publication_date": "2020-08-03",
                "reference": "DeLighT: Deep and Light-weight Transformer",
                "link": "https://arxiv.org/abs/2008.00623",
                "citations": 98.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 99000000.0,
                "training_compute_(flop)": 3.8016e+18,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": 30.0,
                "training_hardware": "NVIDIA V100",
                "approach": null,
                "confidence": "Likely",
                "epochs": 62.14,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 8.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 64000.0,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 4889.428515149248,
                "training_compute_estimation_method": "Operation counting,Hardware",
                "year": 2020,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 18.419496055176168,
                "log_params": 7.99563519459755,
                "log_compute": 18.579966418965082,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "mBART-50",
                "domain": "Language",
                "task": "Translation",
                "organization": "Facebook AI",
                "authors": "Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, Angela Fan",
                "publication_date": "2020-08-02",
                "reference": "Multilingual Translation with Extensible Multilingual Pretraining and Finetuning\n",
                "link": "https://arxiv.org/abs/2008.00401\n\nhttps://huggingface.co/facebook/mbart-large-50",
                "citations": 477.0,
                "notability_criteria": null,
                "parameters": 610000000.0,
                "training_compute_(flop)": 1.45152e+22,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 420.0,
                "training_hardware": "NVIDIA V100",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 256.0,
                "last_modified": "2025-10-14 17:31:39+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": 107520.0,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 156465.19682753857,
                "training_compute_estimation_method": "Hardware",
                "year": 2020,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 18.419496055176168,
                "log_params": 8.785329835010767,
                "log_compute": 22.161823024204757,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "DLRM-2021",
                "domain": "Recommendation",
                "task": "Recommender system",
                "organization": "Meta AI",
                "authors": "Dheevatsa Mudigere, Yuchen Hao, Jianyu Huang, Zhihao Jia, Andrew Tulloch,\nSrinivas Sridharan, Xing Liu, Mustafa Ozdal, Jade Nie, Jongsoo Park, Liang Luo, Jie (Amy) Yang, Leon Gao, Dmytro Ivchenko, Aarti Basant, Yuxi Hu, Jiyan Yang, Ehsan K. Ardestani, Xiaodong Wang, Rakesh Komuravelli, Ching-Hsiang Chu, Serhat Yilmaz, Huayu Li, Jiyuan Qian, Zhuobo Feng, Yinbin Ma, Junjie Yang, Ellie Wen, Hong Li, Lin Yang, Chonglin Sun, Whitney Zhao, Dimitry Melts, Krishna Dhulipala, KR Kishore, Tyler Graf, Assaf Eisenman, Kiran Kumar Matam, Adi Gangidi, Guoqiang Jerry Chen, Manoj Krishnan, Avinash Nayak, Krishnakumar Nair, Bharath Muthiah, Mahmoud khorashadi, Pallab Bhattacharya, Petr Lapukhov, Maxim Naumov, Ajit Mathews, Lin Qiao, Mikhail Smelyanskiy, Bill Jia, Vijay Rao",
                "publication_date": "2020-07-01",
                "reference": "High-performance, Distributed Training of Large scale Deep Learning Recommendation Models",
                "link": "https://arxiv.org/abs/2104.05158",
                "citations": 170.0,
                "notability_criteria": null,
                "parameters": 1000000000000.0,
                "training_compute_(flop)": 3e+20,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:06:55+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Reported",
                "year": 2020,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Recommendation",
                "export_controls_sum": 2.0,
                "publication_count": 18.419496055176168,
                "log_params": 12.0,
                "log_compute": 20.477121254719663,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "GShard (dense)",
                "domain": "Language",
                "task": "Translation",
                "organization": "Google",
                "authors": "Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, Zhifeng Chen",
                "publication_date": "2020-06-30",
                "reference": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding",
                "link": "https://arxiv.org/abs/2006.16668",
                "citations": 1395.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 2300000000.0,
                "training_compute_(flop)": 4.765e+22,
                "training_dataset_size_(gradients)": 346666666667.0,
                "training_time_(hours)": 1008.0,
                "training_hardware": "Google TPU v3",
                "approach": "Self-supervised learning",
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 1024.0,
                "last_modified": "2025-10-16 15:12:26+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 4000000.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": 1032192.0,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 254267.11053357512,
                "frontier_model": true,
                "training_power_draw_(w)": 459301.98946084874,
                "training_compute_estimation_method": "Operation counting,Hardware",
                "year": 2020,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 18.419496055176168,
                "log_params": 9.361727836017593,
                "log_compute": 22.678062904974347,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "GShard (600B)",
                "domain": "Language",
                "task": "Translation",
                "organization": "Google",
                "authors": "Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, Zhifeng Chen",
                "publication_date": "2020-06-30",
                "reference": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding",
                "link": "https://arxiv.org/abs/2006.16668",
                "citations": 1395.0,
                "notability_criteria": null,
                "parameters": 600000000000.0,
                "training_compute_(flop)": 1.33e+22,
                "training_dataset_size_(gradients)": 1000000000000.0,
                "training_time_(hours)": 96.0,
                "training_hardware": "Google TPU v3",
                "approach": "Self-supervised learning",
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:06:54+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": 96360.0,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Third-party estimation,Hardware",
                "year": 2020,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 18.419496055176168,
                "log_params": 11.778151250383644,
                "log_compute": 22.123851640967086,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "GPT-3 6.7B",
                "domain": "Language",
                "task": "Text autocompletion,Language modeling/generation",
                "organization": "OpenAI",
                "authors": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei",
                "publication_date": "2020-06-22",
                "reference": "Language Models are Few-Shot Learners",
                "link": "https://arxiv.org/abs/2005.14165",
                "citations": 47064.0,
                "notability_criteria": null,
                "parameters": 6660000000.0,
                "training_compute_(flop)": 1.2e+22,
                "training_dataset_size_(gradients)": 300000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA Tesla V100 DGXS 32 GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unknown",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:06:55+00:00",
                "training_cloud_compute_vendor": "Microsoft",
                "batch_size": 2000000.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Reported",
                "year": 2020,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 18.419496055176168,
                "log_params": 9.823474229170301,
                "log_compute": 22.079181246047625,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "GPT-3 XL",
                "domain": "Language",
                "task": "Text autocompletion,Language modeling/generation",
                "organization": "OpenAI",
                "authors": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei",
                "publication_date": "2020-06-22",
                "reference": "Language Models are Few-Shot Learners",
                "link": "https://arxiv.org/abs/2005.14165",
                "citations": 47064.0,
                "notability_criteria": null,
                "parameters": 1320000000.0,
                "training_compute_(flop)": 2.38e+21,
                "training_dataset_size_(gradients)": 300000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA Tesla V100 DGXS 32 GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unknown",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:06:54+00:00",
                "training_cloud_compute_vendor": "Microsoft",
                "batch_size": 1000000.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Reported",
                "year": 2020,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 18.419496055176168,
                "log_params": 9.12057393120585,
                "log_compute": 21.37657695705651,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "GPT-3 Small",
                "domain": "Language",
                "task": "Text autocompletion,Language modeling/generation",
                "organization": "OpenAI",
                "authors": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei",
                "publication_date": "2020-06-22",
                "reference": "Language Models are Few-Shot Learners",
                "link": "https://arxiv.org/abs/2005.14165",
                "citations": 47064.0,
                "notability_criteria": null,
                "parameters": 125000000.0,
                "training_compute_(flop)": 2.25e+20,
                "training_dataset_size_(gradients)": 300000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA Tesla V100 DGXS 32 GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unknown",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:06:54+00:00",
                "training_cloud_compute_vendor": "Microsoft",
                "batch_size": 500000.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Reported",
                "year": 2020,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 18.419496055176168,
                "log_params": 8.096910013008056,
                "log_compute": 20.352182518111363,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "GPT-3 Medium",
                "domain": "Language",
                "task": "Text autocompletion,Language modeling/generation",
                "organization": "OpenAI",
                "authors": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei",
                "publication_date": "2020-06-22",
                "reference": "Language Models are Few-Shot Learners",
                "link": "https://arxiv.org/abs/2005.14165",
                "citations": 47064.0,
                "notability_criteria": null,
                "parameters": 356000000.0,
                "training_compute_(flop)": 6.41e+20,
                "training_dataset_size_(gradients)": 300000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA Tesla V100 DGXS 32 GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unknown",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:06:54+00:00",
                "training_cloud_compute_vendor": "Microsoft",
                "batch_size": 500000.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Reported",
                "year": 2020,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 18.419496055176168,
                "log_params": 8.551449997972876,
                "log_compute": 20.806858029518818,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "GPT-3 Large",
                "domain": "Language",
                "task": "Text autocompletion,Language modeling/generation",
                "organization": "OpenAI",
                "authors": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei",
                "publication_date": "2020-06-22",
                "reference": "Language Models are Few-Shot Learners",
                "link": "https://arxiv.org/abs/2005.14165",
                "citations": 47064.0,
                "notability_criteria": null,
                "parameters": 760000000.0,
                "training_compute_(flop)": 1.37e+21,
                "training_dataset_size_(gradients)": 300000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA Tesla V100 DGXS 32 GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unknown",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:06:54+00:00",
                "training_cloud_compute_vendor": "Microsoft",
                "batch_size": 500000.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Reported",
                "year": 2020,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 18.419496055176168,
                "log_params": 8.880813592280791,
                "log_compute": 21.136720567156406,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "GPT-3 2.7B",
                "domain": "Language",
                "task": "Text autocompletion,Language modeling/generation",
                "organization": "OpenAI",
                "authors": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei",
                "publication_date": "2020-06-22",
                "reference": "Language Models are Few-Shot Learners",
                "link": "https://arxiv.org/abs/2005.14165",
                "citations": 47064.0,
                "notability_criteria": null,
                "parameters": 2650000000.0,
                "training_compute_(flop)": 4.77e+21,
                "training_dataset_size_(gradients)": 300000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA Tesla V100 DGXS 32 GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unknown",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:06:54+00:00",
                "training_cloud_compute_vendor": "Microsoft",
                "batch_size": 2000000.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Reported",
                "year": 2020,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 18.419496055176168,
                "log_params": 9.423245873936807,
                "log_compute": 21.678518379040113,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "GPT-3 13B",
                "domain": "Language",
                "task": "Text autocompletion,Language modeling/generation",
                "organization": "OpenAI",
                "authors": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei",
                "publication_date": "2020-06-22",
                "reference": "Language Models are Few-Shot Learners",
                "link": "https://arxiv.org/abs/2005.14165",
                "citations": 47064.0,
                "notability_criteria": null,
                "parameters": 12850000000.0,
                "training_compute_(flop)": 2.31e+22,
                "training_dataset_size_(gradients)": 300000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA Tesla V100 DGXS 32 GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Unknown",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:06:54+00:00",
                "training_cloud_compute_vendor": "Microsoft",
                "batch_size": 1000000.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Reported",
                "year": 2020,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 18.419496055176168,
                "log_params": 10.108903127667313,
                "log_compute": 22.363611979892145,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "iGPT-XL",
                "domain": "Vision,Image generation",
                "task": "Image completion",
                "organization": "OpenAI",
                "authors": "Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, Ilya Sutskever",
                "publication_date": "2020-06-17",
                "reference": "Generative Pretraining from Pixels",
                "link": "https://openai.com/research/image-gpt",
                "citations": 1620.0,
                "notability_criteria": null,
                "parameters": 6801000000.0,
                "training_compute_(flop)": 3.3e+22,
                "training_dataset_size_(gradients)": 4718592000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA Tesla V100 DGXS 32 GB",
                "approach": "Self-supervised learning",
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 15:12:27+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 108390.73774775192,
                "frontier_model": true,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Third-party estimation",
                "year": 2020,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Vision",
                "export_controls_sum": 2.0,
                "publication_count": 18.419496055176168,
                "log_params": 9.832572774846179,
                "log_compute": 22.518513939877888,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "iGPT-L",
                "domain": "Image generation,Vision",
                "task": "Image completion",
                "organization": "OpenAI",
                "authors": "Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, Ilya Sutskever",
                "publication_date": "2020-06-17",
                "reference": "Generative Pretraining from Pixels",
                "link": "https://openai.com/blog/image-gpt/",
                "citations": 1620.0,
                "notability_criteria": null,
                "parameters": 1362000000.0,
                "training_compute_(flop)": 8.91e+21,
                "training_dataset_size_(gradients)": 2654208000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA Tesla V100 DGXS 32 GB",
                "approach": "Self-supervised learning",
                "confidence": "Confident",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:06:54+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": 60000.0,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 30093.444683107013,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware",
                "year": 2020,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Vision",
                "export_controls_sum": 2.0,
                "publication_count": 18.419496055176168,
                "log_params": 9.134177107576766,
                "log_compute": 21.949877704036876,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "6-Layer-Tensor-Transformer+AdaHessian",
                "domain": "Language",
                "task": "Language modeling",
                "organization": "NERSC, Lawrence Berkeley National Laboratory,University of California (UC) Berkeley",
                "authors": "Zhewei Yao, Amir Gholami, Sheng Shen, Mustafa Mustafa, Kurt Keutzer, Michael W. Mahoney",
                "publication_date": "2020-06-01",
                "reference": "ADAHESSIAN: An Adaptive Second Order Optimizer for Machine Learning",
                "link": "https://arxiv.org/abs/2006.00719",
                "citations": 313.0,
                "notability_criteria": null,
                "parameters": 85500000.0,
                "training_compute_(flop)": 1.58e+18,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": 30.0,
                "model_accessibility": "Unreleased",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:06:55+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Government",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2020,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 18.419496055176168,
                "log_params": 7.931966114728173,
                "log_compute": 18.198657086954423,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "GPT-3 175B (davinci)",
                "domain": "Language",
                "task": "Text autocompletion,Language modeling/generation",
                "organization": "OpenAI",
                "authors": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei",
                "publication_date": "2020-05-28",
                "reference": "Language Models are Few-Shot Learners",
                "link": "https://arxiv.org/abs/2005.14165",
                "citations": 47064.0,
                "notability_criteria": "Highly cited,Training cost",
                "parameters": 174600000000.0,
                "training_compute_(flop)": 3.14e+23,
                "training_dataset_size_(gradients)": 238000000000.0,
                "training_time_(hours)": 355.2,
                "training_hardware": "NVIDIA Tesla V100 DGXS 32 GB",
                "approach": "Self-supervised learning",
                "confidence": "Confident",
                "epochs": 0.6,
                "model_accessibility": "API access",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 10000.0,
                "last_modified": "2025-10-16 15:12:25+00:00",
                "training_cloud_compute_vendor": "Microsoft",
                "batch_size": 3200000.0,
                "organization_categorization": "Industry",
                "foundation_model": true,
                "training_chip-hours": 3552000.0,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": 2277557.456144878,
                "frontier_model": true,
                "training_power_draw_(w)": 5100759.605963441,
                "training_compute_estimation_method": "Reported",
                "year": 2020,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 18.419496055176168,
                "log_params": 11.242044239369552,
                "log_compute": 23.496929648073216,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "GPT3-6.7B (rerun of original)",
                "domain": "Language",
                "task": "Language modeling/generation",
                "organization": "Microsoft,OpenAI",
                "authors": "Greg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, Jianfeng Gao",
                "publication_date": "2020-05-28",
                "reference": "Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer",
                "link": "https://arxiv.org/abs/2203.03466",
                "citations": 197.0,
                "notability_criteria": null,
                "parameters": 6700000000.0,
                "training_compute_(flop)": 1.2e+22,
                "training_dataset_size_(gradients)": 300000000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA V100",
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.0,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 17:31:38+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2020,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 18.419496055176168,
                "log_params": 9.826074802700827,
                "log_compute": 22.079181246047625,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "DETR",
                "domain": "Vision",
                "task": "Object detection",
                "organization": "Facebook",
                "authors": "Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko",
                "publication_date": "2020-05-26",
                "reference": "End-to-End Object Detection with Transformers",
                "link": "https://arxiv.org/abs/2005.12872",
                "citations": 14440.0,
                "notability_criteria": "Highly cited",
                "parameters": 60000000.0,
                "training_compute_(flop)": 4e+20,
                "training_dataset_size_(gradients)": 826000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA V100",
                "approach": "Supervised",
                "confidence": "Confident",
                "epochs": 500.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:06:54+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 64.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 959.9097627206426,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware",
                "year": 2020,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Vision",
                "export_controls_sum": 2.0,
                "publication_count": 18.419496055176168,
                "log_params": 7.778151250383644,
                "log_compute": 20.602059991327963,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "rTop-k(distributed setting)",
                "domain": "Language",
                "task": "Language modeling",
                "organization": "Stanford University",
                "authors": "Leighton Pate Barnes, Huseyin A. Inan, Berivan Isik, Ayfer Ozgur",
                "publication_date": "2020-05-21",
                "reference": "rTop-k: A Statistical Estimation Approach to Distributed SGD",
                "link": "https://arxiv.org/abs/2005.10761",
                "citations": 65.0,
                "notability_criteria": null,
                "parameters": 69000000.0,
                "training_compute_(flop)": 1.4352996e+16,
                "training_dataset_size_(gradients)": 912344.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Likely",
                "epochs": 38.0,
                "model_accessibility": "Unreleased",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 17:31:38+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2020,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 18.419496055176168,
                "log_params": 7.838849090737256,
                "log_compute": 16.15694256381982,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "UnifiedQA",
                "domain": "Language",
                "task": "Question answering",
                "organization": "Allen Institute for AI,University of Washington",
                "authors": "Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, Hannaneh Hajishirzi",
                "publication_date": "2020-05-02",
                "reference": "UnifiedQA: Crossing Format Boundaries With a Single QA System",
                "link": "https://arxiv.org/abs/2005.00700v3",
                "citations": 766.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 11000000000.0,
                "training_compute_(flop)": 1.65e+19,
                "training_dataset_size_(gradients)": 2619750.0,
                "training_time_(hours)": 36.0,
                "training_hardware": "Google TPU v3",
                "approach": null,
                "confidence": "Confident",
                "epochs": 1.88,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": "T5-11B",
                "finetune_compute_(flop)": 3.8e+19,
                "hardware_quantity": 8.0,
                "last_modified": "2025-10-14 17:31:40+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 27.0,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 59.122086907352475,
                "frontier_model": false,
                "training_power_draw_(w)": 3593.01452898213,
                "training_compute_estimation_method": "Operation counting,Hardware",
                "year": 2020,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 18.419496055176168,
                "log_params": 10.041392685158225,
                "log_compute": 19.217483944213907,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Once for All",
                "domain": "Vision",
                "task": "Image classification",
                "organization": "MIT-IBM Watson AI Lab,Massachusetts Institute of Technology (MIT),IBM",
                "authors": "Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han",
                "publication_date": "2020-04-29",
                "reference": "Once for all: Train one network and specialize it for efficient deployment.",
                "link": "https://arxiv.org/abs/1908.09791",
                "citations": 1352.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 7700000.0,
                "training_compute_(flop)": 6.237e+20,
                "training_dataset_size_(gradients)": 1280000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA V100",
                "approach": null,
                "confidence": "Confident",
                "epochs": 180.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-16 13:06:54+00:00",
                "training_cloud_compute_vendor": "AWS",
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": 4200.0,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 1753.9255676777682,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware",
                "year": 2020,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Vision",
                "export_controls_sum": 2.0,
                "publication_count": 18.419496055176168,
                "log_params": 6.886490725172482,
                "log_compute": 20.794975744051133,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "DiffStk-MRNN",
                "domain": "Language",
                "task": "Language modeling",
                "organization": "Pennsylvania State University,Rochester Institute of Technology",
                "authors": "Ankur Mali, Alexander Ororbia, Daniel Kifer, Clyde Lee Giles",
                "publication_date": "2020-04-04",
                "reference": "Recognizing Long Grammatical Sequences Using Recurrent Networks Augmented With An External Differentiable Stack",
                "link": "https://arxiv.org/abs/2004.07623",
                "citations": 13.0,
                "notability_criteria": null,
                "parameters": 1010000.0,
                "training_compute_(flop)": 276440230000000.0,
                "training_dataset_size_(gradients)": 912344.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Likely",
                "epochs": 50.0,
                "model_accessibility": "Unreleased",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2020,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 18.419496055176168,
                "log_params": 6.004321373782642,
                "log_compute": 14.441601245632807,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "MetNet",
                "domain": "Earth science",
                "task": "Weather forecasting",
                "organization": "Google",
                "authors": "Casper Kaae S\u00f8nderby, Lasse Espeholt, Jonathan Heek, Mostafa Dehghani, Avital Oliver, Tim Salimans, Shreya Agrawal, Jason Hickey, Nal Kalchbrenner",
                "publication_date": "2020-03-24",
                "reference": "MetNet: A Neural Weather Model for Precipitation Forecasting",
                "link": "https://arxiv.org/abs/2003.12140",
                "citations": 291.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 225000000.0,
                "training_compute_(flop)": 9.510912e+18,
                "training_dataset_size_(gradients)": 7045120000.0,
                "training_time_(hours)": null,
                "training_hardware": "Google TPU v3",
                "approach": null,
                "confidence": "Speculative",
                "epochs": null,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 256.0,
                "last_modified": "2025-10-14 17:23:46+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 115076.36599329064,
                "training_compute_estimation_method": "Operation counting",
                "year": 2020,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Earth science",
                "export_controls_sum": 2.0,
                "publication_count": 18.419496055176168,
                "log_params": 8.352182518111363,
                "log_compute": 18.97822216337033,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "ELECTRA",
                "domain": "Language",
                "task": "Text autocompletion",
                "organization": "Stanford University,Google,Google Brain",
                "authors": "Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning",
                "publication_date": "2020-03-23",
                "reference": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators",
                "link": "https://arxiv.org/abs/2003.10555v1",
                "citations": 2968.0,
                "notability_criteria": "Discretionary",
                "parameters": 335000000.0,
                "training_compute_(flop)": 3.1e+21,
                "training_dataset_size_(gradients)": 33000000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": "Self-supervised learning",
                "confidence": "Likely",
                "epochs": null,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 262144.0,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Reported",
                "year": 2020,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 18.419496055176168,
                "log_params": 8.525044807036846,
                "log_compute": 21.49136169383427,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Tensor-Transformer(1core)+PN (WT103)",
                "domain": "Language",
                "task": "Language modeling",
                "organization": "University of California (UC) Berkeley",
                "authors": "Sheng Shen, Zhewei Yao, Amir Gholami, Michael W. Mahoney, Kurt Keutzer",
                "publication_date": "2020-03-17",
                "reference": "PowerNorm: Rethinking Batch Normalization in Transformers",
                "link": "https://arxiv.org/abs/2003.07845",
                "citations": 60.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 85300000.0,
                "training_compute_(flop)": 1.58e+18,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": null,
                "confidence": "Confident",
                "epochs": 30.0,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Operation counting",
                "year": 2020,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 18.419496055176168,
                "log_params": 7.930949031167523,
                "log_compute": 18.198657086954423,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "WDC20 / DLWP",
                "domain": "Earth science",
                "task": "Weather forecasting",
                "organization": "University of Washington,Microsoft Research",
                "authors": "Jonathan A. Weyn, Dale R. Durran, Rich Caruana",
                "publication_date": "2020-03-15",
                "reference": "Improving data-driven global weather prediction using deep convolutional neural networks on a cubed sphere",
                "link": "https://arxiv.org/abs/2003.11927",
                "citations": null,
                "notability_criteria": null,
                "parameters": 672080.0,
                "training_compute_(flop)": 2.4362208e+18,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 72.0,
                "training_hardware": "NVIDIA Tesla V100 DGXS 32 GB",
                "approach": null,
                "confidence": "Confident",
                "epochs": 100.0,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 1.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 280.72374990639975,
                "training_compute_estimation_method": "Hardware",
                "year": 2020,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Earth science",
                "export_controls_sum": 2.0,
                "publication_count": 18.419496055176168,
                "log_params": 5.827420971700622,
                "log_compute": 18.386716646799922,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "ProGen",
                "domain": "Biology",
                "task": "Protein generation,Proteins,Protein or nucleotide language model (pLM/nLM)",
                "organization": "Salesforce Research,Stanford University",
                "authors": "Ali Madani, Bryan McCann, Nikhil Naik, Nitish Shirish Keskar, Namrata Anand, Raphael R. Eguchi,  View ORCID ProfilePo-Ssu Huang, Richard Socher",
                "publication_date": "2020-03-13",
                "reference": "ProGen: Language Modeling for Protein Generation",
                "link": "https://www.biorxiv.org/content/10.1101/2020.03.07.982272v2",
                "citations": 301.0,
                "notability_criteria": null,
                "parameters": 1200000000.0,
                "training_compute_(flop)": 3.7e+20,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_hardware": null,
                "approach": "Self-supervised learning",
                "confidence": "Likely",
                "epochs": 5.0,
                "model_accessibility": "Unknown",
                "country": "United States",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": null,
                "last_modified": "2025-10-14 21:46:05+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": null,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": null,
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": null,
                "training_compute_estimation_method": "Hardware,Third-party estimation",
                "year": 2020,
                "era": "Deep learning era",
                "notable_model": false,
                "country_first": "United States",
                "domain_group": "Biology",
                "export_controls_sum": 2.0,
                "publication_count": 18.419496055176168,
                "log_params": 9.079181246047625,
                "log_compute": 20.568201724066995,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "TransformerXL + spectrum control",
                "domain": "Language",
                "task": "Language modeling",
                "organization": "University of California Los Angeles (UCLA),JD.com",
                "authors": "Lingxiao Wang, Jing Huang, Kevin Huang, Ziniu Hu, Guangtao Wang, Quanquan Gu",
                "publication_date": "2020-03-11",
                "reference": "Improving Neural Language Generation with Spectrum Control",
                "link": "https://openreview.net/forum?id=ByxY8CNtvr",
                "citations": 87.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 151000000.0,
                "training_compute_(flop)": 2.6289761e+19,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": 219.0,
                "training_hardware": "NVIDIA V100",
                "approach": null,
                "confidence": "Speculative",
                "epochs": 250.0,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 4.0,
                "last_modified": "2025-10-14 21:46:05+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 61440.0,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": null,
                "frontier_model": false,
                "training_power_draw_(w)": 2452.6211427753756,
                "training_compute_estimation_method": "Operation counting,Hardware",
                "year": 2020,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 18.419496055176168,
                "log_params": 8.178976947293169,
                "log_compute": 19.41978663795676,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Turing-NLG",
                "domain": "Language",
                "task": "Text autocompletion,Language generation,Text summarization",
                "organization": "Microsoft",
                "authors": "Corby Rosset",
                "publication_date": "2020-02-13",
                "reference": "Turing-NLG: A 17-billion-parameter language model by Microsoft",
                "link": "https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/",
                "citations": 114.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 17000000000.0,
                "training_compute_(flop)": 1.57e+22,
                "training_dataset_size_(gradients)": 46400000000.0,
                "training_time_(hours)": null,
                "training_hardware": "NVIDIA Tesla V100 DGXS 32 GB",
                "approach": "Self-supervised learning",
                "confidence": "Likely",
                "epochs": 3.39,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 256.0,
                "last_modified": "2025-09-28 01:00:10+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 524288.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 51659.713290894986,
                "frontier_model": false,
                "training_power_draw_(w)": 130885.13499433055,
                "training_compute_estimation_method": "Third-party estimation,Operation counting",
                "year": 2020,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 18.419496055176168,
                "log_params": 10.230448921378274,
                "log_compute": 22.195899652409235,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "ALBERT-xxlarge",
                "domain": "Language",
                "task": "Language modeling/generation,Question answering",
                "organization": "Toyota Technological Institute at Chicago,Google",
                "authors": "Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut",
                "publication_date": "2020-02-09",
                "reference": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.",
                "link": "https://arxiv.org/abs/1909.11942",
                "citations": 6702.0,
                "notability_criteria": "Highly cited",
                "parameters": 235000000.0,
                "training_compute_(flop)": 2.39e+21,
                "training_dataset_size_(gradients)": 3300000000.0,
                "training_time_(hours)": 32.0,
                "training_hardware": "Google TPU v3",
                "approach": "Self-supervised learning",
                "confidence": "Speculative",
                "epochs": 79.4,
                "model_accessibility": "Open weights (unrestricted)",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 512.0,
                "last_modified": "2025-10-16 13:06:40+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 2097152.0,
                "organization_categorization": "Academia, Industry",
                "foundation_model": false,
                "training_chip-hours": null,
                "training_code_accessibility": "Open source",
                "possibly_over_1e23_flop": false,
                "training_compute_cost_(2023_usd)": 4439.921298510215,
                "frontier_model": false,
                "training_power_draw_(w)": 230378.35820081632,
                "training_compute_estimation_method": "Hardware,Third-party estimation,Operation counting",
                "year": 2020,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 18.419496055176168,
                "log_params": 8.371067862271737,
                "log_compute": 21.378397900948137,
                "pub_bin": "High",
                "export_bin": "Low"
            },
            {
                "model": "Meena",
                "domain": "Language",
                "task": "Text autocompletion,Chat",
                "organization": "Google Brain",
                "authors": "Dongling Xiao, Han Zhang, Yukun Li, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang",
                "publication_date": "2020-01-28",
                "reference": "Towards a Human-like Open-Domain Chatbot",
                "link": "https://arxiv.org/abs/2001.09977",
                "citations": 964.0,
                "notability_criteria": "SOTA improvement",
                "parameters": 2600000000.0,
                "training_compute_(flop)": 1.12e+23,
                "training_dataset_size_(gradients)": 53333333333.333336,
                "training_time_(hours)": 720.0,
                "training_hardware": "Google TPU v3",
                "approach": "Self-supervised learning",
                "confidence": "Confident",
                "epochs": 164.0,
                "model_accessibility": "Unreleased",
                "country": "Multinational",
                "base_model": null,
                "finetune_compute_(flop)": null,
                "hardware_quantity": 1024.0,
                "last_modified": "2025-10-16 15:12:27+00:00",
                "training_cloud_compute_vendor": null,
                "batch_size": 82655.0,
                "organization_categorization": "Industry",
                "foundation_model": false,
                "training_chip-hours": 737280.0,
                "training_code_accessibility": "Unreleased",
                "possibly_over_1e23_flop": true,
                "training_compute_cost_(2023_usd)": 206226.21339944057,
                "frontier_model": true,
                "training_power_draw_(w)": 460879.8620037727,
                "training_compute_estimation_method": "Hardware,Operation counting,Third-party estimation",
                "year": 2020,
                "era": "Deep learning era",
                "notable_model": true,
                "country_first": "United States",
                "domain_group": "Language",
                "export_controls_sum": 2.0,
                "publication_count": 18.419496055176168,
                "log_params": 9.414973347970818,
                "log_compute": 23.049218022670182,
                "pub_bin": "High",
                "export_bin": "Low"
            }
        ]
    }
}
{
    "config": {
        "view": {
            "continuousWidth": 300,
            "continuousHeight": 300
        }
    },
    "hconcat": [
        {
            "mark": {
                "type": "line",
                "point": true
            },
            "encoding": {
                "color": {
                    "field": "access_group",
                    "scale": {
                        "domain": [
                            "Open",
                            "Closed"
                        ],
                        "range": [
                            "#BF903D",
                            "#45A7CE"
                        ]
                    },
                    "title": "Accessibiility",
                    "type": "nominal"
                },
                "tooltip": [
                    {
                        "field": "publication_date",
                        "timeUnit": "year",
                        "title": "Publication year",
                        "type": "temporal"
                    },
                    {
                        "field": "access_group",
                        "title": "Accessibility",
                        "type": "nominal"
                    },
                    {
                        "aggregate": "mean",
                        "field": "training_compute_(flop)",
                        "title": "Log-mean compute",
                        "type": "quantitative"
                    }
                ],
                "x": {
                    "axis": {
                        "format": "d"
                    },
                    "field": "year",
                    "title": "Publication year",
                    "type": "quantitative"
                },
                "y": {
                    "aggregate": "mean",
                    "field": "training_compute_(flop)",
                    "scale": {
                        "type": "log"
                    },
                    "title": "Training compute (FLOPs)",
                    "type": "quantitative"
                }
            },
            "height": 300,
            "name": "view_4",
            "title": {
                "text": "Open vs. Closed Model Log-Mean Compute",
                "anchor": "start",
                "frame": "group",
                "offset": 10
            },
            "width": 500
        },
        {
            "layer": [
                {
                    "mark": {
                        "type": "bar"
                    },
                    "encoding": {
                        "color": {
                            "field": "model_accessibility",
                            "scale": {
                                "domain": [
                                    "Open weights (unrestricted)",
                                    "Open weights (restricted use)",
                                    "Open weights (non-commercial)",
                                    "API access",
                                    "Hosted access (no API)",
                                    "Unreleased",
                                    "Unknown"
                                ],
                                "range": [
                                    "#C7E5F0",
                                    "#ABD7E9",
                                    "#8FCAE2",
                                    "#6AB9D8",
                                    "#45A7CE",
                                    "#2984A8",
                                    "#1D617B"
                                ]
                            },
                            "title": "Accessibility level",
                            "type": "ordinal"
                        },
                        "opacity": {
                            "condition": [
                                {
                                    "param": "param_4",
                                    "value": 0.95
                                }
                            ],
                            "value": 0.3
                        },
                        "stroke": {
                            "condition": {
                                "param": "param_5",
                                "value": "gray",
                                "empty": false
                            },
                            "value": "transparent"
                        },
                        "strokeWidth": {
                            "condition": {
                                "param": "param_5",
                                "value": 2,
                                "empty": false
                            },
                            "value": 0
                        },
                        "tooltip": [
                            {
                                "aggregate": "count",
                                "title": "Model count",
                                "type": "quantitative"
                            }
                        ],
                        "x": {
                            "aggregate": "count",
                            "title": "Count",
                            "type": "quantitative"
                        },
                        "y": {
                            "field": "model_accessibility",
                            "sort": [
                                "Open weights (unrestricted)",
                                "Open weights (restricted use)",
                                "Open weights (non-commercial)",
                                "API access",
                                "Hosted access (no API)",
                                "Unreleased",
                                "Unknown"
                            ],
                            "title": "Accessibility level",
                            "type": "ordinal"
                        }
                    },
                    "name": "view_3",
                    "title": {
                        "text": "Accessibility Levels for Year Range",
                        "anchor": "start",
                        "frame": "group",
                        "offset": 10
                    },
                    "transform": [
                        {
                            "filter": {
                                "param": "brush"
                            }
                        }
                    ]
                },
                {
                    "layer": [
                        {
                            "mark": {
                                "type": "text",
                                "align": "left",
                                "baseline": "middle",
                                "dx": -100,
                                "dy": {
                                    "expr": "datum.line_index * 15 + 10"
                                }
                            },
                            "encoding": {
                                "text": {
                                    "field": "model_and_compute",
                                    "type": "nominal"
                                },
                                "y": {
                                    "field": "model_accessibility",
                                    "sort": [
                                        "Open weights (unrestricted)",
                                        "Open weights (restricted use)",
                                        "Open weights (non-commercial)",
                                        "API access",
                                        "Hosted access (no API)",
                                        "Unreleased",
                                        "Unknown"
                                    ],
                                    "type": "ordinal"
                                }
                            },
                            "transform": [
                                {
                                    "filter": {
                                        "param": "param_5",
                                        "empty": false
                                    }
                                },
                                {
                                    "filter": {
                                        "param": "brush"
                                    }
                                },
                                {
                                    "window": [
                                        {
                                            "op": "rank",
                                            "field": "-training_compute_(flop)",
                                            "as": "rank"
                                        }
                                    ]
                                },
                                {
                                    "filter": "(datum.rank <= 1)"
                                },
                                {
                                    "calculate": "datum.rank - 1",
                                    "as": "line_index"
                                }
                            ]
                        },
                        {
                            "mark": {
                                "type": "text",
                                "align": "left",
                                "baseline": "top",
                                "dx": -100,
                                "dy": -10,
                                "fontSize": 12,
                                "fontWeight": "bold"
                            },
                            "encoding": {
                                "text": {
                                    "field": "title_text",
                                    "type": "nominal"
                                },
                                "y": {
                                    "field": "model_accessibility",
                                    "sort": [
                                        "Open weights (unrestricted)",
                                        "Open weights (restricted use)",
                                        "Open weights (non-commercial)",
                                        "API access",
                                        "Hosted access (no API)",
                                        "Unreleased",
                                        "Unknown"
                                    ],
                                    "type": "ordinal"
                                }
                            },
                            "transform": [
                                {
                                    "filter": {
                                        "param": "param_5",
                                        "empty": false
                                    }
                                },
                                {
                                    "filter": {
                                        "param": "brush"
                                    }
                                },
                                {
                                    "window": [
                                        {
                                            "op": "rank",
                                            "field": "-training_compute_(flop)",
                                            "as": "rank"
                                        }
                                    ]
                                },
                                {
                                    "filter": "(datum.rank <= 1)"
                                },
                                {
                                    "calculate": "\"Top model (compute) - \" + datum.year",
                                    "as": "title_text"
                                }
                            ]
                        }
                    ]
                }
            ],
            "height": 300,
            "width": 300
        }
    ],
    "data": {
        "name": "data-200e30c290d570f26fbd5c3580158f33"
    },
    "params": [
        {
            "name": "brush",
            "select": {
                "type": "interval",
                "encodings": [
                    "x"
                ]
            },
            "value": {
                "x": [
                    2006,
                    2016
                ]
            },
            "views": [
                "view_4",
                "view_3"
            ]
        },
        {
            "name": "param_4",
            "select": {
                "type": "point",
                "fields": [
                    "model_accessibility"
                ]
            },
            "bind": "legend",
            "views": [
                "view_3"
            ]
        },
        {
            "name": "param_5",
            "select": {
                "type": "point",
                "fields": [
                    "model_accessibility"
                ]
            },
            "views": [
                "view_3"
            ]
        }
    ],
    "resolve": {
        "legend": {
            "color": "independent"
        },
        "scale": {
            "color": "independent"
        }
    },
    "title": {
        "text": "Model Accessibility Over Time",
        "anchor": "start",
        "fontSize": 16,
        "frame": "group",
        "offset": 10
    },
    "$schema": "https://vega.github.io/schema/vega-lite/v5.20.1.json",
    "datasets": {
        "data-200e30c290d570f26fbd5c3580158f33": [
            {
                "model": "Odyssey 102B",
                "training_compute_(flop)": 1.1e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Industry",
                "publication_date": "2025-10-18T00:00:00",
                "link": "https://www.biorxiv.org/content/10.1101/2025.10.15.682677v1",
                "reference": "Odyssey: reconstructing evolution through emergent consensus in the global proteome",
                "organization": "Anthrogen",
                "parameters": 102000000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Biology",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Odyssey 102B - 1.10e+23 FLOPs"
            },
            {
                "model": "Ling-1T",
                "training_compute_(flop)": 6.000001e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 20000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-10-10T00:00:00",
                "link": "https://huggingface.co/inclusionAI/Ling-1T",
                "reference": "Ling-1T",
                "organization": "Ant Group",
                "parameters": 1000000000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Ling-1T - 6.00e+24 FLOPs"
            },
            {
                "model": "Tiny Recursive Model (TRM-Att)",
                "training_compute_(flop)": 3.07742976e+20,
                "training_power_draw_(w)": 5469.225394941245,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 72.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2025-10-06T00:00:00",
                "link": "https://arxiv.org/abs/2510.04871",
                "reference": "Less is More: Recursive Reasoning with Tiny Networks",
                "organization": "Samsung SAIT AI Lab",
                "parameters": 7000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Tiny Recursive Model (TRM-Att) - 3.08e+20 FLOPs"
            },
            {
                "model": "Granite-4.0-H-Tiny",
                "training_compute_(flop)": 1.35e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-10-02T00:00:00",
                "link": "https://www.ibm.com/new/announcements/ibm-granite-4-0-hyper-efficient-high-performance-hybrid-models#:~:text=We're%20launching%20Granite%204,costs%20compared%20to%20conventional%20LLMs.",
                "reference": "IBM Granite 4.0: hyper-efficient, high performance hybrid models for enterprise",
                "organization": "IBM",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Granite-4.0-H-Tiny - 1.35e+23 FLOPs"
            },
            {
                "model": "Granite-4.0-H-Micro",
                "training_compute_(flop)": 3.15e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-10-02T00:00:00",
                "link": "https://www.ibm.com/new/announcements/ibm-granite-4-0-hyper-efficient-high-performance-hybrid-models#:~:text=We're%20launching%20Granite%204,costs%20compared%20to%20conventional%20LLMs.",
                "reference": "IBM Granite 4.0: hyper-efficient, high performance hybrid models for enterprise",
                "organization": "IBM",
                "parameters": 3000000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Granite-4.0-H-Micro - 3.15e+23 FLOPs"
            },
            {
                "model": "Granite-4.0-H-Small",
                "training_compute_(flop)": 1.215e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-10-02T00:00:00",
                "link": "https://www.ibm.com/new/announcements/ibm-granite-4-0-hyper-efficient-high-performance-hybrid-models#:~:text=We're%20launching%20Granite%204,costs%20compared%20to%20conventional%20LLMs.",
                "reference": "IBM Granite 4.0: hyper-efficient, high performance hybrid models for enterprise",
                "organization": "IBM",
                "parameters": 32000000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Granite-4.0-H-Small - 1.22e+24 FLOPs"
            },
            {
                "model": "GLM 4.6",
                "training_compute_(flop)": 4.42e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 23000000000000.0,
                "training_time_(hours)": 2880.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2025-09-30T00:00:00",
                "link": "https://z.ai/blog/glm-4.6",
                "reference": "GLM-4.6: Advanced Agentic, Reasoning and Coding Capabilities",
                "organization": "Zhipu AI,Tsinghua University",
                "parameters": 357000000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "GLM 4.6 - 4.42e+24 FLOPs"
            },
            {
                "model": "DeepSeek-V3.2-Exp",
                "training_compute_(flop)": 3.8035594e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 943700000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-09-29T00:00:00",
                "link": "https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/DeepSeek_V3_2.pdf",
                "reference": "Introducing DeepSeek-V3.2-Exp",
                "organization": "DeepSeek",
                "parameters": 671000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "DeepSeek-V3.2-Exp - 3.80e+24 FLOPs"
            },
            {
                "model": "SimpleFold",
                "training_compute_(flop)": 2e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2227200000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Industry",
                "publication_date": "2025-09-23T00:00:00",
                "link": "https://arxiv.org/abs/2509.18480v1",
                "reference": "SimpleFold: Folding Proteins is Simpler than You Think",
                "organization": "Apple",
                "parameters": 3000000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Biology",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "SimpleFold - 2.00e+21 FLOPs"
            },
            {
                "model": "DeepSeek-V3.1-Terminus",
                "training_compute_(flop)": 3.594058e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 839000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-09-22T00:00:00",
                "link": "https://api-docs.deepseek.com/news/news250922",
                "reference": "The latest update builds on V3.1\u2019s strengths while addressing key user feedback.",
                "organization": "DeepSeek",
                "parameters": 671000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "DeepSeek-V3.1-Terminus - 3.59e+24 FLOPs"
            },
            {
                "model": "Qwen3-Omni-Flash",
                "training_compute_(flop)": 3.6e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2025-09-22T00:00:00",
                "link": "https://arxiv.org/abs/2509.17765",
                "reference": "Qwen3-Omni Technical Report",
                "organization": "Alibaba",
                "parameters": 35300000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Unreleased",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Qwen3-Omni-Flash - 3.60e+22 FLOPs"
            },
            {
                "model": "Qwen3-Omni-30B-A3B",
                "training_compute_(flop)": 3.6e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2025-09-22T00:00:00",
                "link": "https://arxiv.org/abs/2509.17765",
                "reference": "Qwen3-Omni Technical Report",
                "organization": "Alibaba",
                "parameters": 35300000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Qwen3-Omni-30B-A3B - 3.60e+22 FLOPs"
            },
            {
                "model": "AgentFounder-30B",
                "training_compute_(flop)": 6.5367e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 315000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-09-16T00:00:00",
                "link": "https://arxiv.org/abs/2509.13310",
                "reference": "Scaling Agents via Continual Pre-training",
                "organization": "Alibaba",
                "parameters": 30000000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "AgentFounder-30B - 6.54e+23 FLOPs"
            },
            {
                "model": "Qwen3-Next-80B-A3B",
                "training_compute_(flop)": 2.7e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 15000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-09-10T00:00:00",
                "link": "https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&from=research.latest-advancements-list",
                "reference": "Qwen3-Next: Towards Ultimate Training & Inference Efficiency",
                "organization": "Alibaba",
                "parameters": 80000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Qwen3-Next-80B-A3B - 2.70e+23 FLOPs"
            },
            {
                "model": "Qwen3-Max",
                "training_compute_(flop)": 1.512e+25,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 36000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-09-05T00:00:00",
                "link": "https://modelstudio.console.alibabacloud.com/?tab=doc#/doc/?type=model&url=2840914_2&modelId=qwen3-max-preview\n\nhttps://qwen.ai/blog?id=87dc93fc8a590dc718c77e1f6e84c07b474f6c5a&from=home.latest-research-list",
                "reference": "Introducing Qwen3-Max-Preview (Instruct) \u2014 our biggest model yet, with over 1 trillion parameters! ",
                "organization": "Alibaba",
                "parameters": 1000000000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "API access",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Qwen3-Max - 1.51e+25 FLOPs"
            },
            {
                "model": "Apertus 70B",
                "training_compute_(flop)": 6.74e+24,
                "training_power_draw_(w)": 5604728.8670533225,
                "training_dataset_size_(gradients)": 15000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2025-09-02T00:00:00",
                "link": "https://ethz.ch/en/news-and-events/eth-news/news/2025/09/press-release-apertus-a-fully-open-transparent-multilingual-language-model.html\n\nhttps://github.com/swiss-ai/apertus-tech-report/blob/main/Apertus_Tech_Report.pdf",
                "reference": "Apertus: a fully open, transparent, multilingual language model",
                "organization": "ETH Zurich,Ecole Polytechnique F\u00b4ed\u00b4erale de Lausanne (EPFL),Swiss National Supercomputing Centre (CSCS),Swisscom",
                "parameters": 70000000000.0,
                "notable_model": false,
                "country": "Switzerland",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "Apertus 70B - 6.74e+24 FLOPs"
            },
            {
                "model": "LongCat-Flash",
                "training_compute_(flop)": 3.726e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 23000000000000.0,
                "training_time_(hours)": 720.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-09-01T00:00:00",
                "link": "https://arxiv.org/abs/2509.01322",
                "reference": "LongCat-Flash Technical Report",
                "organization": "Meituan Inc",
                "parameters": 560000000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "LongCat-Flash - 3.73e+24 FLOPs"
            },
            {
                "model": "DeepSeek-V3.1",
                "training_compute_(flop)": 3.594058e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 840000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-08-21T00:00:00",
                "link": "https://api-docs.deepseek.com/news/news250821\n\nhttps://huggingface.co/deepseek-ai/DeepSeek-V3.1",
                "reference": "Introducing DeepSeek-V3.1: our first step toward the agent era!",
                "organization": "DeepSeek",
                "parameters": 671000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "DeepSeek-V3.1 - 3.59e+24 FLOPs"
            },
            {
                "model": "Seed-OSS-36B-Base",
                "training_compute_(flop)": 2.592e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 12000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-08-21T00:00:00",
                "link": "https://seed.bytedance.com/en/blog/seed-oss-open-source-models-release",
                "reference": "Seed-OSS Open-Source Models Release",
                "organization": "ByteDance",
                "parameters": null,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Seed-OSS-36B-Base - 2.59e+24 FLOPs"
            },
            {
                "model": "Teuken 7B",
                "training_compute_(flop)": 2.1444092e+23,
                "training_power_draw_(w)": 400444.7737609557,
                "training_dataset_size_(gradients)": 4000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Government",
                "publication_date": "2025-08-21T00:00:00",
                "link": "https://arxiv.org/abs/2410.03730",
                "reference": "Teuken-7B-Base & Teuken-7B-Instruct: Towards European LLMs",
                "organization": "OpenGPT-X,Fraunhofer Institute for Algorithms and Scientific Computing,Forschungszentrum Julich,Technische Universit\u00e4t Dresden",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "Germany",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Germany",
                "domain_top4": "Language",
                "org_top5": "Other",
                "access_group": "Open",
                "model_and_compute": "Teuken 7B - 2.14e+23 FLOPs"
            },
            {
                "model": "Surya",
                "training_compute_(flop)": 2.9474214e+21,
                "training_power_draw_(w)": 100113.42287839846,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Earth science",
                "organization_categorization": "Academia, Industry, Government",
                "publication_date": "2025-08-20T00:00:00",
                "link": "https://arxiv.org/abs/2508.14112",
                "reference": "Surya: Foundation Model for Heliophysics",
                "organization": "NASA,University of Alabama,IBM Research",
                "parameters": 366000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Academia, Industry, Government",
                "access_group": "Open",
                "model_and_compute": "Surya - 2.95e+21 FLOPs"
            },
            {
                "model": "Gemma-SEA-LION-v4-27B-IT",
                "training_compute_(flop)": 2.349e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 500000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Government",
                "publication_date": "2025-08-20T00:00:00",
                "link": "https://sea-lion.ai/our-models/#sealionv4",
                "reference": "SEA-LION v4",
                "organization": "AI Singapore",
                "parameters": 27000000000.0,
                "notable_model": false,
                "country": "Singapore",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Multimodal",
                "org_top5": "Government",
                "access_group": "Open",
                "model_and_compute": "Gemma-SEA-LION-v4-27B-IT - 2.35e+24 FLOPs"
            },
            {
                "model": "FlowER",
                "training_compute_(flop)": 5.88e+16,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1400000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Materials science",
                "organization_categorization": "Academia",
                "publication_date": "2025-08-20T00:00:00",
                "link": "https://www.nature.com/articles/s41586-025-09426-9",
                "reference": "Electron flow matching for generative reaction mechanism prediction",
                "organization": "Massachusetts Institute of Technology (MIT)",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "FlowER - 5.88e+16 FLOPs"
            },
            {
                "model": "NVIDIA-Nemotron-Nano-9B-v2",
                "training_compute_(flop)": 1.53e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 21100000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-08-18T00:00:00",
                "link": "https://build.nvidia.com/nvidia/nvidia-nemotron-nano-9b-v2/modelcard",
                "reference": "NVIDIA-Nemotron-Nano-9B-v2 Overview",
                "organization": "NVIDIA",
                "parameters": 9000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "NVIDIA-Nemotron-Nano-9B-v2 - 1.53e+24 FLOPs"
            },
            {
                "model": "NVIDIA-Nemotron-Nano-12B-v2",
                "training_compute_(flop)": 1.5192e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 21100000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-08-18T00:00:00",
                "link": "https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-12B-v2",
                "reference": "NVIDIA-Nemotron-Nano-12B-v2",
                "organization": "NVIDIA",
                "parameters": 12000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "NVIDIA-Nemotron-Nano-12B-v2 - 1.52e+24 FLOPs"
            },
            {
                "model": "GLM-4.5V",
                "training_compute_(flop)": 1.8e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2013265900000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2025-08-15T00:00:00",
                "link": "https://arxiv.org/abs/2507.01006",
                "reference": "GLM-4.5V and GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning",
                "organization": "Zhipu AI,Tsinghua University",
                "parameters": 108000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Multimodal",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "GLM-4.5V - 1.80e+24 FLOPs"
            },
            {
                "model": "GLM-4.1V-Thinking",
                "training_compute_(flop)": 9.18e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2025-08-15T00:00:00",
                "link": "https://arxiv.org/abs/2507.01006",
                "reference": "GLM-4.5V and GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning",
                "organization": "Zhipu AI,Tsinghua University",
                "parameters": 9000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Multimodal",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "GLM-4.1V-Thinking - 9.18e+23 FLOPs"
            },
            {
                "model": "Gemma 3 270M",
                "training_compute_(flop)": 9.72e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 6000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-08-14T00:00:00",
                "link": "https://developers.googleblog.com/en/introducing-gemma-3-270m/",
                "reference": "Introducing Gemma 3 270M: The compact model for hyper-efficient AI",
                "organization": "Google DeepMind",
                "parameters": 270000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Gemma 3 270M - 9.72e+21 FLOPs"
            },
            {
                "model": "gpt-oss-120b",
                "training_compute_(flop)": 4.94e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-08-05T00:00:00",
                "link": "https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf",
                "reference": "gpt-oss-120b & gpt-oss-20b Model Card",
                "organization": "OpenAI",
                "parameters": 116830000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "gpt-oss-120b - 4.94e+24 FLOPs"
            },
            {
                "model": "gpt-oss-20b",
                "training_compute_(flop)": 5.49e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-08-05T00:00:00",
                "link": "https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf",
                "reference": "gpt-oss-120b & gpt-oss-20b Model Card",
                "organization": "OpenAI",
                "parameters": 20910000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "gpt-oss-20b - 5.49e+23 FLOPs"
            },
            {
                "model": "GLM 4.5",
                "training_compute_(flop)": 4.42e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 23100000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2025-08-05T00:00:00",
                "link": "https://arxiv.org/abs/2508.06471",
                "reference": "GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models",
                "organization": "Zhipu AI,Tsinghua University",
                "parameters": 355000000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "GLM 4.5 - 4.42e+24 FLOPs"
            },
            {
                "model": "GLM-4.5-Air",
                "training_compute_(flop)": 1.656e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2025-08-05T00:00:00",
                "link": "https://arxiv.org/abs/2508.06471",
                "reference": "GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models",
                "organization": "Zhipu AI,Tsinghua University",
                "parameters": 106000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "GLM-4.5-Air - 1.66e+24 FLOPs"
            },
            {
                "model": "Tri-21B",
                "training_compute_(flop)": 2.95e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2300000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-08-01T00:00:00",
                "link": "https://huggingface.co/trillionlabs/Tri-21B",
                "reference": null,
                "organization": "Trillion Labs",
                "parameters": 20730000000.0,
                "notable_model": false,
                "country": "South Korea",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "South Korea",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Tri-21B - 2.95e+23 FLOPs"
            },
            {
                "model": "AlphaEarth Foundations (AEF)",
                "training_compute_(flop)": 2.36544e+18,
                "training_power_draw_(w)": 170272.4293355278,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 56.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Earth science",
                "organization_categorization": "Industry",
                "publication_date": "2025-07-30T00:00:00",
                "link": "https://arxiv.org/abs/2507.22291",
                "reference": "AlphaEarth Foundations: An embedding field model for accurate and efficient global mapping from sparse label data",
                "organization": "Google DeepMind,Google",
                "parameters": 480000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Hosted access (no API)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "AlphaEarth Foundations (AEF) - 2.37e+18 FLOPs"
            },
            {
                "model": "Wan 2.2 14B T2V",
                "training_compute_(flop)": 4.2e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 5000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2025-07-28T00:00:00",
                "link": "https://huggingface.co/Wan-AI/Wan2.2-T2V-A14B",
                "reference": "We are excited to introduce Wan2.2, a major upgrade to our foundational video models.",
                "organization": "Alibaba",
                "parameters": 14000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Wan 2.2 14B T2V - 4.20e+23 FLOPs"
            },
            {
                "model": "Aeneas",
                "training_compute_(flop)": 2.2875955e+21,
                "training_power_draw_(w)": 17530.77677895151,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 168.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2025-07-23T00:00:00",
                "link": "https://www.nature.com/articles/s41586-025-09292-5",
                "reference": "Contextualizing ancient texts with generative neural networks",
                "organization": "Google DeepMind,University of Nottingham,University of Warwick,Athens University of Economics and Business,Google,University of Oxford",
                "parameters": null,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Hosted access (no API)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "Aeneas - 2.29e+21 FLOPs"
            },
            {
                "model": "Qwen3-Coder-480B-A35B",
                "training_compute_(flop)": 1.575e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 7500000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-07-22T00:00:00",
                "link": "https://qwenlm.github.io/blog/qwen3-coder/",
                "reference": "Qwen3-Coder: Agentic Coding in the World",
                "organization": "Alibaba",
                "parameters": 480000000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Qwen3-Coder-480B-A35B - 1.58e+24 FLOPs"
            },
            {
                "model": "EXAONE 4.0 (32B)",
                "training_compute_(flop)": 2.69000000000001e+24,
                "training_power_draw_(w)": 701356.0100875632,
                "training_dataset_size_(gradients)": 14000000000000.0,
                "training_time_(hours)": 3192.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-07-15T00:00:00",
                "link": "https://arxiv.org/abs/2507.11407",
                "reference": "EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and Reasoning Modes",
                "organization": "LG AI Research",
                "parameters": 32000000000.0,
                "notable_model": true,
                "country": "South Korea",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "South Korea",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "EXAONE 4.0 (32B) - 2.69e+24 FLOPs"
            },
            {
                "model": "EXAONE 4.0 (1.2B)",
                "training_compute_(flop)": 8.65e+22,
                "training_power_draw_(w)": 701356.0100875632,
                "training_dataset_size_(gradients)": 12000000000000.0,
                "training_time_(hours)": 336.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-07-15T00:00:00",
                "link": "https://arxiv.org/abs/2507.11407",
                "reference": "EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and Reasoning Modes",
                "organization": "LG AI Research",
                "parameters": 1200000000.0,
                "notable_model": false,
                "country": "South Korea",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "South Korea",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "EXAONE 4.0 (1.2B) - 8.65e+22 FLOPs"
            },
            {
                "model": "Kimi K2",
                "training_compute_(flop)": 2.976e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 15500000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-07-11T00:00:00",
                "link": "https://moonshotai.github.io/Kimi-K2/",
                "reference": "Kimi K2: Open Agentic Intelligence",
                "organization": "Moonshot",
                "parameters": 1000000000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Kimi K2 - 2.98e+24 FLOPs"
            },
            {
                "model": "Grok 4",
                "training_compute_(flop)": 5.0000000000001e+26,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 387830439.15113616,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2025-07-09T00:00:00",
                "link": "https://x.ai/news/grok-4",
                "reference": "Grok 4",
                "organization": "xAI",
                "parameters": null,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "API access",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Grok 4 - 5.00e+26 FLOPs"
            },
            {
                "model": "dots.llm1",
                "training_compute_(flop)": 1.2164856e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 11328000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-07-06T00:00:00",
                "link": "https://www.arxiv.org/abs/2506.05767",
                "reference": "dots.llm1 Technical Report",
                "organization": "Rednote",
                "parameters": 142000000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "dots.llm1 - 1.22e+24 FLOPs"
            },
            {
                "model": "ERNIE-4.5-300B-A47B",
                "training_compute_(flop)": 2.82e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-06-29T00:00:00",
                "link": "https://yiyan.baidu.com/blog/publication/ERNIE_Technical_Report.pdf",
                "reference": "ERNIE 4.5 Technical Report",
                "organization": "Baidu",
                "parameters": 300000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "ERNIE-4.5-300B-A47B - 2.82e+24 FLOPs"
            },
            {
                "model": "ERNIE-4.5-21B-A3B",
                "training_compute_(flop)": 1.8e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-06-29T00:00:00",
                "link": "https://yiyan.baidu.com/blog/publication/ERNIE_Technical_Report.pdf",
                "reference": "ERNIE 4.5 Technical Report",
                "organization": "Baidu",
                "parameters": 21000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "ERNIE-4.5-21B-A3B - 1.80e+23 FLOPs"
            },
            {
                "model": "AlphaGenome",
                "training_compute_(flop)": 1.362969e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Industry",
                "publication_date": "2025-06-25T00:00:00",
                "link": "https://storage.googleapis.com/deepmind-media/papers/alphagenome.pdf",
                "reference": "AlphaGenome: advancing regulatory variant effect prediction with a unified DNA sequence model",
                "organization": "Google DeepMind",
                "parameters": 450000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "API access",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "AlphaGenome - 1.36e+22 FLOPs"
            },
            {
                "model": "MiniMax-M1-80k",
                "training_compute_(flop)": 4.3240062e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 7500000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-06-13T00:00:00",
                "link": "https://arxiv.org/abs/2506.13585",
                "reference": "MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention",
                "organization": "MiniMax",
                "parameters": 456000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "MiniMax-M1-80k - 4.32e+24 FLOPs"
            },
            {
                "model": "MiniMax-M1-40k",
                "training_compute_(flop)": 4.1861931e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 7500000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-06-13T00:00:00",
                "link": "https://arxiv.org/abs/2506.13585",
                "reference": "MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention",
                "organization": "MiniMax",
                "parameters": 456000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "MiniMax-M1-40k - 4.19e+24 FLOPs"
            },
            {
                "model": "FGN",
                "training_compute_(flop)": 9.61895088e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 72.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Earth science",
                "organization_categorization": "Industry",
                "publication_date": "2025-06-12T00:00:00",
                "link": "https://arxiv.org/abs/2506.10772v1",
                "reference": "Skillful joint probabilistic weather forecasting from marginals",
                "organization": "Google DeepMind",
                "parameters": 720000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "FGN - 9.62e+21 FLOPs"
            },
            {
                "model": "V-JEPA 2",
                "training_compute_(flop)": 9.05969664e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2025-06-11T00:00:00",
                "link": "https://arxiv.org/abs/2506.09985",
                "reference": "V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning",
                "organization": "Facebook AI Research",
                "parameters": 1000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "V-JEPA 2 - 9.06e+21 FLOPs"
            },
            {
                "model": "MiMo-7B-Base",
                "training_compute_(flop)": 1.05e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 25000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-06-05T00:00:00",
                "link": "https://arxiv.org/abs/2505.07608",
                "reference": "MiMo: Unlocking the Reasoning Potential of Language Model -- From Pretraining to Posttraining",
                "organization": "Xiaomi Corp",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "MiMo-7B-Base - 1.05e+24 FLOPs"
            },
            {
                "model": "MiMo-VL-7B-SFT",
                "training_compute_(flop)": 1.1508e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2400000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2025-06-04T00:00:00",
                "link": "https://arxiv.org/abs/2506.03569",
                "reference": "MiMo-VL Technical Report\n",
                "organization": "Xiaomi Corp",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "MiMo-VL-7B-SFT - 1.15e+24 FLOPs"
            },
            {
                "model": "OpenAudio-S1",
                "training_compute_(flop)": 3.36e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1400000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Audio",
                "organization_categorization": "Industry",
                "publication_date": "2025-06-03T00:00:00",
                "link": "https://openaudio.com/blogs/s1",
                "reference": "Our cutting-edge text-to-speech model that performs like voice actors",
                "organization": "Fish Audio",
                "parameters": 4000000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "API access",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "OpenAudio-S1 - 3.36e+22 FLOPs"
            },
            {
                "model": "Pangu Pro MoE",
                "training_compute_(flop)": 1.287e+24,
                "training_power_draw_(w)": 2429161.768914584,
                "training_dataset_size_(gradients)": 13000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-05-28T00:00:00",
                "link": "https://arxiv.org/abs/2505.21411",
                "reference": "Pangu Pro MoE: Mixture of Grouped Experts for Efficient Sparsity",
                "organization": "Huawei",
                "parameters": 71990000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Pangu Pro MoE - 1.29e+24 FLOPs"
            },
            {
                "model": "DataRater test model (1B)",
                "training_compute_(flop)": 1.2e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-05-23T00:00:00",
                "link": "https://arxiv.org/abs/2505.17895",
                "reference": "DataRater: Meta-Learned Dataset Curation",
                "organization": "Google DeepMind",
                "parameters": 1000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "DataRater test model (1B) - 1.20e+20 FLOPs"
            },
            {
                "model": "Reason-ModernColBERT",
                "training_compute_(flop)": 3.6832407e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 2.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-05-22T00:00:00",
                "link": "https://huggingface.co/lightonai/Reason-ModernColBERT",
                "reference": "LightOn Unlocks Agentic RAG with new SOTA Model Reason-ModernColBERT",
                "organization": "LightOn",
                "parameters": 150000000.0,
                "notable_model": false,
                "country": "France",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Reason-ModernColBERT - 3.68e+21 FLOPs"
            },
            {
                "model": "Falcon-H1",
                "training_compute_(flop)": 3.672e+24,
                "training_power_draw_(w)": 5617724.544289504,
                "training_dataset_size_(gradients)": 18000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Government",
                "publication_date": "2025-05-21T00:00:00",
                "link": "https://arxiv.org/abs/2507.22448",
                "reference": "Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency and Performance",
                "organization": "Technology Innovation Institute",
                "parameters": 34000000000.0,
                "notable_model": false,
                "country": "United Arab Emirates",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Government",
                "access_group": "Open",
                "model_and_compute": "Falcon-H1 - 3.67e+24 FLOPs"
            },
            {
                "model": "Gemma 3n",
                "training_compute_(flop)": 5.181e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 11000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2025-05-20T00:00:00",
                "link": "https://developers.googleblog.com/en/introducing-gemma-3n/",
                "reference": "Announcing Gemma 3n preview: powerful, efficient, mobile-first AI",
                "organization": "Google",
                "parameters": 7850000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Gemma 3n - 5.18e+23 FLOPs"
            },
            {
                "model": "Marin 8B",
                "training_compute_(flop)": 6.12e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": null,
                "publication_date": "2025-05-19T00:00:00",
                "link": "https://marin.readthedocs.io/en/latest/reports/marin-8b-retro/",
                "reference": "Marin 8B Retrospective",
                "organization": "Marin",
                "parameters": 8000000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Other",
                "access_group": "Open",
                "model_and_compute": "Marin 8B - 6.12e+23 FLOPs"
            },
            {
                "model": "Seed1.5-VL",
                "training_compute_(flop)": 1.388556e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 3000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2025-05-11T00:00:00",
                "link": "https://arxiv.org/abs/2505.07062",
                "reference": "Seed1.5-VL Technical Report",
                "organization": "ByteDance",
                "parameters": null,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "API access",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Seed1.5-VL - 1.39e+24 FLOPs"
            },
            {
                "model": "Earth-2 (cBottle-SR)",
                "training_compute_(flop)": 1.6014864e+21,
                "training_power_draw_(w)": 87798.45074975025,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2025-05-10T00:00:00",
                "link": "https://arxiv.org/abs/2505.06474v1",
                "reference": "Climate in a Bottle: Towards a Generative Foundation Model for the Kilometer-Scale Global Atmosphere",
                "organization": "NVIDIA",
                "parameters": 330000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Earth-2 (cBottle-SR) - 1.60e+21 FLOPs"
            },
            {
                "model": "Pangu Ultra MoE",
                "training_compute_(flop)": 3.0888e+24,
                "training_power_draw_(w)": 4703802.674709778,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-05-07T00:00:00",
                "link": "https://arxiv.org/abs/2505.04519",
                "reference": "Pangu Ultra MoE: How to Train Your Big MoE on Ascend NPUs",
                "organization": "Huawei",
                "parameters": 718000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Hosted access (no API)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Pangu Ultra MoE - 3.09e+24 FLOPs"
            },
            {
                "model": "Apriel Nemotron 15B",
                "training_compute_(flop)": 9.00001e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 100000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-05-06T00:00:00",
                "link": "https://www.servicenow.com/company/media/press-room/nvidia-enterprise-ai-agents.html",
                "reference": "New Apriel Nemotron 15B reasoning model delivers lower latency, lower inference costs, and faster agentic AI\u2014purpose built for performance, cost, and scale",
                "organization": "NVIDIA,ServiceNow",
                "parameters": 15000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Apriel Nemotron 15B - 9.00e+21 FLOPs"
            },
            {
                "model": "Phi-4-Reasoning",
                "training_compute_(flop)": 9.3368077e+23,
                "training_power_draw_(w)": 43909.002544977615,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 60.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-04-30T00:00:00",
                "link": "https://arxiv.org/abs/2504.21318",
                "reference": "Phi-4-reasoning Technical Report",
                "organization": "Microsoft",
                "parameters": 14000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Phi-4-Reasoning - 9.34e+23 FLOPs"
            },
            {
                "model": "GTE-ModernColBERT-v1",
                "training_compute_(flop)": 3.6832407e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-04-30T00:00:00",
                "link": "https://huggingface.co/lightonai/GTE-ModernColBERT-v1",
                "reference": "GTE-ModernColBERT-v1",
                "organization": "LightOn",
                "parameters": 149000000.0,
                "notable_model": false,
                "country": "France",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "GTE-ModernColBERT-v1 - 3.68e+21 FLOPs"
            },
            {
                "model": "Qwen3-235B-A22B",
                "training_compute_(flop)": 4.752e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-04-29T00:00:00",
                "link": "https://qwenlm.github.io/blog/qwen3/",
                "reference": "Qwen3: Think Deeper, Act Faster",
                "organization": "Alibaba",
                "parameters": 235000000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Qwen3-235B-A22B - 4.75e+24 FLOPs"
            },
            {
                "model": "Qwen3-30B-A3B",
                "training_compute_(flop)": 6.48e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-04-29T00:00:00",
                "link": "https://qwenlm.github.io/blog/qwen3/",
                "reference": "Qwen3: Think Deeper, Act Faster",
                "organization": "Alibaba",
                "parameters": 30000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Qwen3-30B-A3B - 6.48e+23 FLOPs"
            },
            {
                "model": "Qwen3-32B",
                "training_compute_(flop)": 7.0848e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-04-29T00:00:00",
                "link": "https://qwenlm.github.io/blog/qwen3/",
                "reference": "Qwen3: Think Deeper, Act Faster",
                "organization": "Alibaba",
                "parameters": 32800000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Qwen3-32B - 7.08e+24 FLOPs"
            },
            {
                "model": "Qwen3-14B",
                "training_compute_(flop)": 3.1968e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-04-29T00:00:00",
                "link": "https://qwenlm.github.io/blog/qwen3/",
                "reference": "Qwen3: Think Deeper, Act Faster",
                "organization": "Alibaba",
                "parameters": 14800000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Qwen3-14B - 3.20e+24 FLOPs"
            },
            {
                "model": "Qwen3-8B",
                "training_compute_(flop)": 1.7712e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-04-29T00:00:00",
                "link": "https://qwenlm.github.io/blog/qwen3/",
                "reference": "Qwen3: Think Deeper, Act Faster",
                "organization": "Alibaba",
                "parameters": 8200000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Qwen3-8B - 1.77e+24 FLOPs"
            },
            {
                "model": "Qwen3-4B",
                "training_compute_(flop)": 8.64e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-04-29T00:00:00",
                "link": "https://qwenlm.github.io/blog/qwen3/",
                "reference": "Qwen3: Think Deeper, Act Faster",
                "organization": "Alibaba",
                "parameters": 4000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Qwen3-4B - 8.64e+23 FLOPs"
            },
            {
                "model": "Qwen3-1.7B",
                "training_compute_(flop)": 3.672e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-04-29T00:00:00",
                "link": "https://qwenlm.github.io/blog/qwen3/",
                "reference": "Qwen3: Think Deeper, Act Faster",
                "organization": "Alibaba",
                "parameters": 1700000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Qwen3-1.7B - 3.67e+23 FLOPs"
            },
            {
                "model": "Qwen3-0.6B",
                "training_compute_(flop)": 1.296e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-04-29T00:00:00",
                "link": "https://qwenlm.github.io/blog/qwen3/",
                "reference": "Qwen3: Think Deeper, Act Faster",
                "organization": "Alibaba",
                "parameters": 600000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Qwen3-0.6B - 1.30e+23 FLOPs"
            },
            {
                "model": "Foundation-sec-8b",
                "training_compute_(flop)": 1.4688e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 5100000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-04-28T00:00:00",
                "link": "https://arxiv.org/abs/2504.21039",
                "reference": "Llama-3.1-FoundationAI-SecurityLLM-Base-8B Technical Report",
                "organization": "Cisco",
                "parameters": 8000000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Foundation-sec-8b - 1.47e+24 FLOPs"
            },
            {
                "model": "Pleias-RAG-350m",
                "training_compute_(flop)": 2.7186806e+21,
                "training_power_draw_(w)": 21956.945973291175,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-04-25T00:00:00",
                "link": "https://arxiv.org/abs/2504.18225",
                "reference": "Even Small Reasoners Should Quote Their Sources: Introducing the Pleias-RAG Model Family",
                "organization": "PleIAs",
                "parameters": 350000000.0,
                "notable_model": false,
                "country": "France",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Pleias-RAG-350m - 2.72e+21 FLOPs"
            },
            {
                "model": "Pleias-RAG-1B",
                "training_compute_(flop)": 2.9907184e+22,
                "training_power_draw_(w)": 21956.945973291175,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-04-25T00:00:00",
                "link": "https://arxiv.org/abs/2504.18225",
                "reference": "Even Small Reasoners Should Quote Their Sources: Introducing the Pleias-RAG Model Family",
                "organization": "PleIAs",
                "parameters": 1200000000.0,
                "notable_model": false,
                "country": "France",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Pleias-RAG-1B - 2.99e+22 FLOPs"
            },
            {
                "model": "Trillion-7B",
                "training_compute_(flop)": 9.3e+22,
                "training_power_draw_(w)": 351342.43087893666,
                "training_dataset_size_(gradients)": 2000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 139200.0,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-04-21T00:00:00",
                "link": "https://arxiv.org/abs/2504.15431",
                "reference": "Trillion 7B Technical Report",
                "organization": "Trillion Labs",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "South Korea",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "South Korea",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Trillion-7B - 9.30e+22 FLOPs"
            },
            {
                "model": "Demist-2",
                "training_compute_(flop)": 4.579186e+20,
                "training_power_draw_(w)": 6274.530874518676,
                "training_dataset_size_(gradients)": 351000000000.0,
                "training_time_(hours)": 216.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-04-17T00:00:00",
                "link": "https://www.darktrace.com/research/demist-2-darktrace-embedding-model-for-investigation-of-security-threats",
                "reference": "DEMIST-2: Darktrace Embedding Model for Investigation of Security Threats",
                "organization": "Darktrace",
                "parameters": 95000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Hosted access (no API)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Demist-2 - 4.58e+20 FLOPs"
            },
            {
                "model": "TerraMind",
                "training_compute_(flop)": 3.10542336e+21,
                "training_power_draw_(w)": 25099.241361829627,
                "training_dataset_size_(gradients)": 500000000000.0,
                "training_time_(hours)": 288.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry, Government",
                "publication_date": "2025-04-15T00:00:00",
                "link": "https://arxiv.org/abs/2504.11171",
                "reference": "TerraMind: Large-Scale Generative Multimodality for Earth Observation",
                "organization": "IBM,Forschungszentrum Julich,European Space Agency (ESA),NASA",
                "parameters": null,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Other",
                "access_group": "Open",
                "model_and_compute": "TerraMind - 3.11e+21 FLOPs"
            },
            {
                "model": "Nemotron-H 8B",
                "training_compute_(flop)": 7.2e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-04-14T00:00:00",
                "link": "https://arxiv.org/abs/2504.03624",
                "reference": "Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models",
                "organization": "NVIDIA",
                "parameters": 8000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Nemotron-H 8B - 7.20e+23 FLOPs"
            },
            {
                "model": "Nemotron-H 56B",
                "training_compute_(flop)": 6.72e+24,
                "training_power_draw_(w)": 8433532.904958995,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-04-14T00:00:00",
                "link": "https://arxiv.org/abs/2504.03624",
                "reference": "Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models",
                "organization": "NVIDIA",
                "parameters": 56000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Nemotron-H 56B - 6.72e+24 FLOPs"
            },
            {
                "model": "GLM-Z1-Rumination-32B-0414",
                "training_compute_(flop)": 2.88e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2025-04-14T00:00:00",
                "link": "https://huggingface.co/THUDM/GLM-Z1-Rumination-32B-0414",
                "reference": "GLM-4-Z1-Rumination-32B-0414",
                "organization": "Tsinghua University",
                "parameters": 32000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "GLM-Z1-Rumination-32B-0414 - 2.88e+24 FLOPs"
            },
            {
                "model": "GLM-4-9B-0414",
                "training_compute_(flop)": 8.1e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2025-04-14T00:00:00",
                "link": "https://huggingface.co/THUDM/GLM-4-9B-0414",
                "reference": "GLM-4-9B-0414",
                "organization": "Tsinghua University",
                "parameters": 9000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "GLM-4-9B-0414 - 8.10e+23 FLOPs"
            },
            {
                "model": "Seaweed-7B",
                "training_compute_(flop)": 9.0007697e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2025-04-11T00:00:00",
                "link": "https://arxiv.org/abs/2504.08685",
                "reference": "Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model\n",
                "organization": "ByteDance",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Unreleased",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Seaweed-7B - 9.00e+23 FLOPs"
            },
            {
                "model": "Pangu Ultra",
                "training_compute_(flop)": 1.0692e+25,
                "training_power_draw_(w)": 6426121.277196856,
                "training_dataset_size_(gradients)": 13200000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-04-10T00:00:00",
                "link": "https://arxiv.org/abs/2504.07866",
                "reference": "Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs",
                "organization": "Huawei",
                "parameters": 135000000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Hosted access (no API)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Pangu Ultra - 1.07e+25 FLOPs"
            },
            {
                "model": "AMIE (Articulate Medical Intelligence Explorer)",
                "training_compute_(flop)": 7.34e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2025-04-09T00:00:00",
                "link": "https://www.nature.com/articles/s41586-025-08866-7?linkId=13898052\nhttps://www.nature.com/articles/s41586-025-08869-4?linkId=13898054",
                "reference": "Towards conversational diagnostic artificial intelligence",
                "organization": "Google DeepMind,Google Research",
                "parameters": 340000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "AMIE (Articulate Medical Intelligence Explorer) - 7.34e+24 FLOPs"
            },
            {
                "model": "TxGemma 27B",
                "training_compute_(flop)": 2.116854e+24,
                "training_power_draw_(w)": 85350.72454206382,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2025-04-08T00:00:00",
                "link": "https://arxiv.org/abs/2504.06196",
                "reference": "TxGemma: Efficient and Agentic LLMs for Therapeutics",
                "organization": "Google DeepMind,Google Research",
                "parameters": 27000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "TxGemma 27B - 2.12e+24 FLOPs"
            },
            {
                "model": "TxGemma 9B",
                "training_compute_(flop)": 4.35618e+23,
                "training_power_draw_(w)": 85350.72454206382,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2025-04-08T00:00:00",
                "link": "https://arxiv.org/abs/2504.06196",
                "reference": "TxGemma: Efficient and Agentic LLMs for Therapeutics",
                "organization": "Google DeepMind,Google Research",
                "parameters": 9000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "TxGemma 9B - 4.36e+23 FLOPs"
            },
            {
                "model": "TxGemma 2B",
                "training_compute_(flop)": 3.22452e+22,
                "training_power_draw_(w)": 85350.72454206382,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2025-04-08T00:00:00",
                "link": "https://arxiv.org/abs/2504.06196",
                "reference": "TxGemma: Efficient and Agentic LLMs for Therapeutics",
                "organization": "Google DeepMind,Google Research",
                "parameters": 2600000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "TxGemma 2B - 3.22e+22 FLOPs"
            },
            {
                "model": "Llama 4 Scout",
                "training_compute_(flop)": 4.08e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 30000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2025-04-05T00:00:00",
                "link": "https://ai.meta.com/blog/llama-4-multimodal-intelligence/",
                "reference": "The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation",
                "organization": "Meta AI",
                "parameters": 109000000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Llama 4 Scout - 4.08e+24 FLOPs"
            },
            {
                "model": "Llama 4 Maverick",
                "training_compute_(flop)": 2.244000000001e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 30000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2025-04-05T00:00:00",
                "link": "https://ai.meta.com/blog/llama-4-multimodal-intelligence/",
                "reference": "The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation",
                "organization": "Meta AI",
                "parameters": 400000000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Llama 4 Maverick - 2.24e+24 FLOPs"
            },
            {
                "model": "Llama 4 Behemoth (preview)",
                "training_compute_(flop)": 5.18400000000001e+25,
                "training_power_draw_(w)": 43933454.99810563,
                "training_dataset_size_(gradients)": 30000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 44588963.624007165,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2025-04-05T00:00:00",
                "link": "https://ai.meta.com/blog/llama-4-multimodal-intelligence/",
                "reference": "The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation",
                "organization": "Meta AI",
                "parameters": 2000000000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Llama 4 Behemoth (preview) - 5.18e+25 FLOPs"
            },
            {
                "model": "Lumina-Image-2.0",
                "training_compute_(flop)": 4.7794406e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2025-03-27T00:00:00",
                "link": "https://arxiv.org/abs/2503.21758",
                "reference": "Lumina-Image 2.0: A Unified and Efficient Image Generative Framework",
                "organization": "Shanghai AI Lab,University of Sydney,Chinese University of Hong Kong (CUHK),Shanghai Jiao Tong University,Krea AI",
                "parameters": 2600000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "Lumina-Image-2.0 - 4.78e+21 FLOPs"
            },
            {
                "model": "GAIA-2",
                "training_compute_(flop)": 7.5609676e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2025-03-26T00:00:00",
                "link": "https://arxiv.org/abs/2503.20523v1",
                "reference": "GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving",
                "organization": "Wayve",
                "parameters": 8685000000.0,
                "notable_model": false,
                "country": "United Kingdom",
                "model_accessibility": "Unreleased",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "GAIA-2 - 7.56e+22 FLOPs"
            },
            {
                "model": "Llama Nemotron Ultra 253B",
                "training_compute_(flop)": 3.911001e+25,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 603000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-03-18T00:00:00",
                "link": "https://arxiv.org/abs/2505.00949",
                "reference": "Ultra is 253B distilled from Llama 3.1 405B for maximum agentic accuracy on multi-GPU data center servers.",
                "organization": "NVIDIA",
                "parameters": 253000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Llama Nemotron Ultra 253B - 3.91e+25 FLOPs"
            },
            {
                "model": "GR00T N1 2B",
                "training_compute_(flop)": 7.12368e+22,
                "training_power_draw_(w)": 1406434.214264239,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 48.828125,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2025-03-18T00:00:00",
                "link": "https://arxiv.org/abs/2503.14734v2\nhttps://arxiv.org/abs/2503.14734v1\nhttps://github.com/NVIDIA/Isaac-GR00T\nhttps://nvidianews.nvidia.com/news/nvidia-isaac-gr00t-n1-open-humanoid-robot-foundation-model-simulation-frameworks\nhttps://developer.nvidia.com/blog/accelerate-generalist-humanoid-robot-development-with-nvidia-isaac-gr00t-n1/\nhttps://huggingface.co/nvidia/GR00T-N1-2B\nhttps://huggingface.co/datasets/nvidia/PhysicalAI-Robotics-GR00T-X-Embodiment-Sim\n\n",
                "reference": "GR00T N1: An Open Foundation Model for Generalist Humanoid Robots",
                "organization": "NVIDIA",
                "parameters": 2190000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "GR00T N1 2B - 7.12e+22 FLOPs"
            },
            {
                "model": "Cosmos-Transfer1-7B",
                "training_compute_(flop)": 3.6059017e+24,
                "training_power_draw_(w)": 1406434.214264239,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 2016.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2025-03-18T00:00:00",
                "link": "https://arxiv.org/abs/2503.14492",
                "reference": "Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal Control",
                "organization": "NVIDIA",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Cosmos-Transfer1-7B - 3.61e+24 FLOPs"
            },
            {
                "model": "EXAONE Deep 32B",
                "training_compute_(flop)": 1.26e+24,
                "training_power_draw_(w)": 703248.4282353076,
                "training_dataset_size_(gradients)": 12000000000.0,
                "training_time_(hours)": 2160.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-03-16T00:00:00",
                "link": "https://arxiv.org/abs/2503.12524",
                "reference": "EXAONE Deep: LLMs with Enhanced Reasoning Performance",
                "organization": "LG AI Research",
                "parameters": 32000000000.0,
                "notable_model": true,
                "country": "South Korea",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "South Korea",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "EXAONE Deep 32B - 1.26e+24 FLOPs"
            },
            {
                "model": "EXAONE Deep 7.8B",
                "training_compute_(flop)": 4.23e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 12000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-03-16T00:00:00",
                "link": "https://arxiv.org/abs/2503.12524",
                "reference": "EXAONE Deep: LLMs with Enhanced Reasoning Performance",
                "organization": "LG AI Research",
                "parameters": 7800000000.0,
                "notable_model": false,
                "country": "South Korea",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "South Korea",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "EXAONE Deep 7.8B - 4.23e+23 FLOPs"
            },
            {
                "model": "EXAONE Deep 2.4B",
                "training_compute_(flop)": 9.41e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 12000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-03-16T00:00:00",
                "link": "https://arxiv.org/abs/2503.12524",
                "reference": "EXAONE Deep: LLMs with Enhanced Reasoning Performance",
                "organization": "LG AI Research",
                "parameters": 2400000000.0,
                "notable_model": false,
                "country": "South Korea",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "South Korea",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "EXAONE Deep 2.4B - 9.41e+22 FLOPs"
            },
            {
                "model": "EXAONE 3.5-R 2.4B",
                "training_compute_(flop)": 9.504e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-03-14T00:00:00",
                "link": null,
                "reference": null,
                "organization": "LG AI Research",
                "parameters": 2400000000.0,
                "notable_model": false,
                "country": "South Korea",
                "model_accessibility": "Unreleased",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "South Korea",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "EXAONE 3.5-R 2.4B - 9.50e+22 FLOPs"
            },
            {
                "model": "EXAONE 3.5-R 32B",
                "training_compute_(flop)": 1.2692e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-03-14T00:00:00",
                "link": null,
                "reference": null,
                "organization": "LG AI Research",
                "parameters": 32000000000.0,
                "notable_model": false,
                "country": "South Korea",
                "model_accessibility": "Unreleased",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "South Korea",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "EXAONE 3.5-R 32B - 1.27e+24 FLOPs"
            },
            {
                "model": "EXAONE 3.5-R 7.8B",
                "training_compute_(flop)": 4.2568e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-03-14T00:00:00",
                "link": null,
                "reference": null,
                "organization": "LG AI Research",
                "parameters": 7800000000.0,
                "notable_model": false,
                "country": "South Korea",
                "model_accessibility": "Unreleased",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "South Korea",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "EXAONE 3.5-R 7.8B - 4.26e+23 FLOPs"
            },
            {
                "model": "Meissonic",
                "training_compute_(flop)": 1.2292301e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2025-03-13T00:00:00",
                "link": "https://arxiv.org/abs/2410.08261",
                "reference": "Meissonic: Revitalizing Masked Generative Transformers for Efficient High-Resolution Text-to-Image Synthesis",
                "organization": "National University of Singapore,Skywork AI,Hong Kong University of Science and Technology (HKUST),University of California (UC) Berkeley,Zhejiang University (ZJU)",
                "parameters": 1000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "Meissonic - 1.23e+21 FLOPs"
            },
            {
                "model": "OLMo 2 32B",
                "training_compute_(flop)": 1.3e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 4000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": null,
                "publication_date": "2025-03-13T00:00:00",
                "link": "https://allenai.org/blog/olmo2-32B",
                "reference": "OLMo 2 32B: First fully open model to outperform GPT 3.5 and GPT 4o mini",
                "organization": "Allen Institute for AI",
                "parameters": 32000000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Other",
                "access_group": "Open",
                "model_and_compute": "OLMo 2 32B - 1.30e+24 FLOPs"
            },
            {
                "model": "Gemma 3 27B",
                "training_compute_(flop)": 2.268e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 14000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2025-03-12T00:00:00",
                "link": "https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf",
                "reference": "Gemma 3 Technical Report\n",
                "organization": "Google DeepMind",
                "parameters": 27000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Gemma 3 27B - 2.27e+24 FLOPs"
            },
            {
                "model": "Gemma 3 12B",
                "training_compute_(flop)": 8.64e+23,
                "training_power_draw_(w)": 2049649.4174839524,
                "training_dataset_size_(gradients)": 12000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2025-03-12T00:00:00",
                "link": "https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf",
                "reference": "Gemma 3 Technical Report\n",
                "organization": "Google DeepMind",
                "parameters": 12000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Gemma 3 12B - 8.64e+23 FLOPs"
            },
            {
                "model": "Gemma 3 4B",
                "training_compute_(flop)": 9.6e+22,
                "training_power_draw_(w)": 562648.8597014771,
                "training_dataset_size_(gradients)": 4000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2025-03-12T00:00:00",
                "link": "https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf",
                "reference": "Gemma 3 Technical Report\n",
                "organization": "Google DeepMind",
                "parameters": 4000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Gemma 3 4B - 9.60e+22 FLOPs"
            },
            {
                "model": "Gemma 3 1B",
                "training_compute_(flop)": 1.2e+22,
                "training_power_draw_(w)": 140662.21492536928,
                "training_dataset_size_(gradients)": 2000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-03-12T00:00:00",
                "link": "https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf",
                "reference": "Gemma 3 Technical Report\n",
                "organization": "Google DeepMind",
                "parameters": 1000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Gemma 3 1B - 1.20e+22 FLOPs"
            },
            {
                "model": "Ling-lite-1.5 (\"Bailing\")",
                "training_compute_(flop)": 1.485e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 9000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-03-10T00:00:00",
                "link": "https://arxiv.org/abs/2503.05139",
                "reference": "Every FLOP Counts: Scaling a 300B Mixture-of-Experts LING LLM without Premium GPUs",
                "organization": "Ant Group",
                "parameters": 16800000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Ling-lite-1.5 (\"Bailing\") - 1.49e+23 FLOPs"
            },
            {
                "model": "Ling-Plus (\"Bailing\")",
                "training_compute_(flop)": 1.5552e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 9000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-03-10T00:00:00",
                "link": "https://arxiv.org/abs/2503.05139",
                "reference": "Every FLOP Counts: Scaling a 300B Mixture-of-Experts LING LLM without Premium GPUs",
                "organization": "Ant Group",
                "parameters": 290000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Ling-Plus (\"Bailing\") - 1.56e+24 FLOPs"
            },
            {
                "model": "QwQ-32B",
                "training_compute_(flop)": 3.51e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-03-06T00:00:00",
                "link": "https://qwenlm.github.io/blog/qwq-32b/",
                "reference": "QwQ-32B: Embracing the Power of Reinforcement Learning",
                "organization": "Alibaba",
                "parameters": 32500000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "QwQ-32B - 3.51e+24 FLOPs"
            },
            {
                "model": "Phi-4 Mini",
                "training_compute_(flop)": 9.9561596e+22,
                "training_power_draw_(w)": 401972.5996644288,
                "training_dataset_size_(gradients)": 5000000000000.0,
                "training_time_(hours)": 504.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-03-03T00:00:00",
                "link": "https://arxiv.org/abs/2503.01743",
                "reference": "Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs",
                "organization": "Microsoft",
                "parameters": 3800000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Phi-4 Mini - 9.96e+22 FLOPs"
            },
            {
                "model": "Phi-4-Multimodal",
                "training_compute_(flop)": 1.1852519e+23,
                "training_power_draw_(w)": 401972.5996644288,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 672.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2025-03-03T00:00:00",
                "link": "https://arxiv.org/abs/2503.01743",
                "reference": "Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs",
                "organization": "Microsoft",
                "parameters": 5600000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Phi-4-Multimodal - 1.19e+23 FLOPs"
            },
            {
                "model": "GPT-4.5",
                "training_compute_(flop)": 2.1000001e+26,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 187871247.85797697,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2025-02-27T00:00:00",
                "link": "https://openai.com/index/introducing-gpt-4-5/",
                "reference": "Introducing GPT-4.5",
                "organization": "OpenAI",
                "parameters": null,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "API access",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "GPT-4.5 - 2.10e+26 FLOPs"
            },
            {
                "model": "Wan 2.1 14B I2V",
                "training_compute_(flop)": 2.5e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 3000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2025-02-25T00:00:00",
                "link": "https://arxiv.org/abs/2503.20314\n",
                "reference": "Wan 2.1 by Wan AI :best cost efficient video generation model Now Available",
                "organization": "Alibaba",
                "parameters": 14000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Wan 2.1 14B I2V - 2.50e+23 FLOPs"
            },
            {
                "model": "YandexGPT 5 Lite",
                "training_compute_(flop)": 7.3536e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 15320000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-02-25T00:00:00",
                "link": "https://ya.ru/ai/gpt\n\nhttps://habr.com/ru/companies/yandex/articles/885218/",
                "reference": "The new generation: better at addressing user and business needs, solving problems, and writing code.",
                "organization": "Yandex",
                "parameters": 8000000000.0,
                "notable_model": false,
                "country": "Russia",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "YandexGPT 5 Lite - 7.35e+23 FLOPs"
            },
            {
                "model": "Claude 3.7 Sonnet",
                "training_compute_(flop)": 3.3499999999999998e+25,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2025-02-24T00:00:00",
                "link": "https://www.anthropic.com/news/claude-3-7-sonnet",
                "reference": "Claude 3.7 Sonnet",
                "organization": "Anthropic",
                "parameters": null,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "API access",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Claude 3.7 Sonnet - 3.35e+25 FLOPs"
            },
            {
                "model": "Step-Video-T2V",
                "training_compute_(flop)": 4.1015808e+24,
                "training_power_draw_(w)": 6870719.882854385,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 720.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2025-02-24T00:00:00",
                "link": "https://arxiv.org/abs/2502.10248",
                "reference": "Step-Video-T2V Technical Report: The Practice, Challenges, and Future of Video Foundation Model",
                "organization": "StepFun",
                "parameters": 30000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Step-Video-T2V - 4.10e+24 FLOPs"
            },
            {
                "model": "SigLIP 2",
                "training_compute_(flop)": 8.208e+22,
                "training_power_draw_(w)": 562899.512243166,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2025-02-20T00:00:00",
                "link": "https://arxiv.org/abs/2502.14786",
                "reference": "SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features",
                "organization": "Google DeepMind",
                "parameters": 1140000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "SigLIP 2 - 8.21e+22 FLOPs"
            },
            {
                "model": "Evo 2 40B",
                "training_compute_(flop)": 2.25e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 9300000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2025-02-19T00:00:00",
                "link": "https://arcinstitute.org/manuscripts/Evo2",
                "reference": "Genome modeling and design across all domains of life with Evo 2",
                "organization": "Arc Institute,Stanford University,NVIDIA,Liquid,University of California (UC) Berkeley,Goodfire,Columbia University,University of California San Francisco",
                "parameters": 40300000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "Evo 2 40B - 2.25e+24 FLOPs"
            },
            {
                "model": "Evo 2 7B",
                "training_compute_(flop)": 1.008e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2400000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2025-02-19T00:00:00",
                "link": "https://arcinstitute.org/manuscripts/Evo2",
                "reference": "Genome modeling and design across all domains of life with Evo 2",
                "organization": "Arc Institute,Stanford University,NVIDIA,Liquid,University of California (UC) Berkeley,Goodfire,Columbia University,University of California San Francisco",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "Evo 2 7B - 1.01e+23 FLOPs"
            },
            {
                "model": "PaliGemma 2 3B Mix 224",
                "training_compute_(flop)": 4.0397694e+22,
                "training_power_draw_(w)": 70364.00597512173,
                "training_dataset_size_(gradients)": 256000000000.0,
                "training_time_(hours)": 72.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2025-02-19T00:00:00",
                "link": "https://developers.googleblog.com/en/introducing-paligemma-2-mix/\n\nhttps://arxiv.org/abs/2412.03555",
                "reference": "Introducing PaliGemma 2 mix: A vision-language model for multiple tasks",
                "organization": "Google",
                "parameters": 2920000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "PaliGemma 2 3B Mix 224 - 4.04e+22 FLOPs"
            },
            {
                "model": "Qwen2.5-VL-72B",
                "training_compute_(flop)": 9.5712e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2025-02-19T00:00:00",
                "link": "https://arxiv.org/abs/2502.13923",
                "reference": "Qwen2.5-VL Technical Report",
                "organization": "Alibaba",
                "parameters": 72000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Qwen2.5-VL-72B - 9.57e+24 FLOPs"
            },
            {
                "model": "Qwen2.5-VL-7B ",
                "training_compute_(flop)": 9.9408e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2025-02-19T00:00:00",
                "link": "https://arxiv.org/abs/2502.13923",
                "reference": "Qwen2.5-VL Technical Report",
                "organization": "Alibaba",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Qwen2.5-VL-7B  - 9.94e+23 FLOPs"
            },
            {
                "model": "Qwen2.5-VL-3B",
                "training_compute_(flop)": 4.0752e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2025-02-19T00:00:00",
                "link": "https://arxiv.org/abs/2502.13923",
                "reference": "Qwen2.5-VL Technical Report",
                "organization": "Alibaba",
                "parameters": 3000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Qwen2.5-VL-3B - 4.08e+23 FLOPs"
            },
            {
                "model": "Brain2Qwerty",
                "training_compute_(flop)": 1.62e+18,
                "training_power_draw_(w)": 323.6250227878313,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 12.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2025-02-18T00:00:00",
                "link": "https://ai.meta.com/research/publications/brain-to-text-decoding-a-non-invasive-approach-via-typing/",
                "reference": "Brain-to-Text Decoding: A Non-invasive Approach via Typing",
                "organization": "Meta AI,Universite de Technologie de Compi\u00e8gne \u2013 CNRS,Basque Center on Cognition",
                "parameters": 400000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "Brain2Qwerty - 1.62e+18 FLOPs"
            },
            {
                "model": "Step-Audio-Chat 130B",
                "training_compute_(flop)": 1.92504e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1668000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Audio",
                "organization_categorization": "Industry",
                "publication_date": "2025-02-18T00:00:00",
                "link": "https://arxiv.org/abs/2502.11946",
                "reference": "Step-Audio: Unified Understanding and Generation in Intelligent Speech Interaction",
                "organization": "StepFun",
                "parameters": 130000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Step-Audio-Chat 130B - 1.93e+24 FLOPs"
            },
            {
                "model": "Step-1",
                "training_compute_(flop)": 6.24e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 800000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-02-18T00:00:00",
                "link": "https://arxiv.org/abs/2502.11946",
                "reference": "Step-Audio: Unified Understanding and Generation in Intelligent Speech Interaction",
                "organization": "StepFun",
                "parameters": 130000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Unreleased",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Step-1 - 6.24e+23 FLOPs"
            },
            {
                "model": "Step-Omni",
                "training_compute_(flop)": 2.54904e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2468000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2025-02-18T00:00:00",
                "link": "https://arxiv.org/abs/2502.11946",
                "reference": "Step-Audio: Unified Understanding and Generation in Intelligent Speech Interaction",
                "organization": "StepFun",
                "parameters": 130000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Unreleased",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Step-Omni - 2.55e+24 FLOPs"
            },
            {
                "model": "Grok 3",
                "training_compute_(flop)": 3.5000000000000006e+26,
                "training_power_draw_(w)": 109948656.20196916,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 2160.0,
                "training_compute_cost_(2023_usd)": 217835545.5267339,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2025-02-17T00:00:00",
                "link": "https://x.ai/blog/grok-3",
                "reference": "Grok 3 Beta \u2014 The Age of Reasoning Agents",
                "organization": "xAI",
                "parameters": null,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "API access",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Grok 3 - 3.50e+26 FLOPs"
            },
            {
                "model": "HAMSTER VLM",
                "training_compute_(flop)": 2.4081408e+21,
                "training_power_draw_(w)": 6284.039702711232,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 30.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Robotics",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2025-02-08T00:00:00",
                "link": "https://arxiv.org/abs/2502.05485\nhttps://hamster-robot.github.io/\nhttps://github.com/liyi14/HAMSTER_beta\nhttps://huggingface.co/yili18/Hamster_dev",
                "reference": "HAMSTER: Hierarchical Action Models For Open-World Robot Manipulation",
                "organization": "NVIDIA,University of Washington,University of Southern California",
                "parameters": 13493916736.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "HAMSTER VLM - 2.41e+21 FLOPs"
            },
            {
                "model": "Prithvi-EO-2.0 600M",
                "training_compute_(flop)": 1.954368e+22,
                "training_power_draw_(w)": 188542.18349202207,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Earth science",
                "organization_categorization": "Academia, Industry, Government",
                "publication_date": "2025-02-03T00:00:00",
                "link": "https://arxiv.org/abs/2412.02732",
                "reference": "Prithvi-EO-2.0: A Versatile Multi-Temporal Foundation Model for Earth Observation Applications",
                "organization": "IBM Research,NASA,University of Alabama,University of Iceland,Forschungszentrum Julich,Virginia Tech (Virginia Polytechnic Institute and State University),Arizona State University,Oregon State University,Boston University,University of California (UC) Berkeley,Julich Supercomputing Center",
                "parameters": 600000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Academia, Industry, Government",
                "access_group": "Open",
                "model_and_compute": "Prithvi-EO-2.0 600M - 1.95e+22 FLOPs"
            },
            {
                "model": "Prithvi-EO-2.0 300M",
                "training_compute_(flop)": 7.07616e+21,
                "training_power_draw_(w)": 62847.3944973407,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Earth science",
                "organization_categorization": "Academia, Industry, Government",
                "publication_date": "2025-02-03T00:00:00",
                "link": "https://arxiv.org/abs/2412.02732",
                "reference": "Prithvi-EO-2.0: A Versatile Multi-Temporal Foundation Model for Earth Observation Applications",
                "organization": "IBM Research,NASA,University of Alabama,University of Iceland,Forschungszentrum Julich,Virginia Tech (Virginia Polytechnic Institute and State University),Arizona State University,Oregon State University,Boston University,University of California (UC) Berkeley,Julich Supercomputing Center",
                "parameters": 300000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Academia, Industry, Government",
                "access_group": "Open",
                "model_and_compute": "Prithvi-EO-2.0 300M - 7.08e+21 FLOPs"
            },
            {
                "model": "Mistral Small 3",
                "training_compute_(flop)": 1.152e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 8000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-01-30T00:00:00",
                "link": "https://mistral.ai/news/mistral-small-3/",
                "reference": "Mistral Small 3, a latency-optimized 24B-parameter model released under the Apache 2.0 license.",
                "organization": "Mistral AI",
                "parameters": 24000000000.0,
                "notable_model": false,
                "country": "France",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Mistral Small 3 - 1.15e+24 FLOPs"
            },
            {
                "model": "Baichuan-Omni-1.5",
                "training_compute_(flop)": 3.3e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 500000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2025-01-26T00:00:00",
                "link": "https://arxiv.org/abs/2501.15368",
                "reference": "Baichuan-Omni-1.5 Technical Report",
                "organization": "Baichuan",
                "parameters": 11000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Baichuan-Omni-1.5 - 3.30e+22 FLOPs"
            },
            {
                "model": "gte-modernbert",
                "training_compute_(flop)": 3.676128e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1028000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-01-22T00:00:00",
                "link": "https://huggingface.co/Alibaba-NLP/gte-modernbert-base",
                "reference": "gte-modernbert-base",
                "organization": "Alibaba",
                "parameters": 149000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "gte-modernbert - 3.68e+21 FLOPs"
            },
            {
                "model": "DeepSeek-R1",
                "training_compute_(flop)": 4.020010000000001e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 6770000.0,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-01-20T00:00:00",
                "link": "https://api-docs.deepseek.com/news/news250120",
                "reference": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
                "organization": "DeepSeek",
                "parameters": 671000000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "DeepSeek-R1 - 4.02e+24 FLOPs"
            },
            {
                "model": "Eagle 2",
                "training_compute_(flop)": 4.7156e+22,
                "training_power_draw_(w)": 352055.152733459,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 130.5,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2025-01-20T00:00:00",
                "link": "arxiv.org/abs/2501.14818",
                "reference": "Eagle 2: Building Post-Training Data Strategies from Scratch for Frontier Vision-Language Models",
                "organization": "NVIDIA,Nanjing University,Tsinghua University,Hong Kong Polytechnic University,Johns Hopkins University,New York University (NYU)",
                "parameters": 8930000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "Eagle 2 - 4.72e+22 FLOPs"
            },
            {
                "model": "Zero-shot Monocular Scene Flow (ZeroMSF)",
                "training_compute_(flop)": 3.234816e+19,
                "training_power_draw_(w)": 6286.699155954625,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 12.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2025-01-20T00:00:00",
                "link": "https://arxiv.org/abs/2501.10357",
                "reference": "Zero-Shot Monocular Scene Flow Estimation in the Wild",
                "organization": "NVIDIA,Brown University",
                "parameters": null,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "Zero-shot Monocular Scene Flow (ZeroMSF) - 3.23e+19 FLOPs"
            },
            {
                "model": "MatterGen",
                "training_compute_(flop)": 2.69568e+19,
                "training_power_draw_(w)": 6287.259184247614,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 10.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Materials science",
                "organization_categorization": "Industry",
                "publication_date": "2025-01-16T00:00:00",
                "link": "https://www.nature.com/articles/s41586-025-08628-5",
                "reference": "A generative model for inorganic materials design",
                "organization": "Microsoft Research AI for Science",
                "parameters": 46800000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "MatterGen - 2.70e+19 FLOPs"
            },
            {
                "model": "MiniMax-Text-01",
                "training_compute_(flop)": 1.98288e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 7200000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2025-01-14T00:00:00",
                "link": "https://arxiv.org/abs/2501.08313",
                "reference": "MiniMax-01: Scaling Foundation Models with Lightning Attention",
                "organization": "MiniMax",
                "parameters": 456000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "MiniMax-Text-01 - 1.98e+24 FLOPs"
            },
            {
                "model": "MiniMax-VL-01",
                "training_compute_(flop)": 2.1238848e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2025-01-14T00:00:00",
                "link": "https://arxiv.org/abs/2501.08313",
                "reference": "MiniMax-01: Scaling Foundation Models with Lightning Attention",
                "organization": "MiniMax",
                "parameters": null,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "MiniMax-VL-01 - 2.12e+24 FLOPs"
            },
            {
                "model": "Cosmos-1.0-\nDiffusion-14B Video2World",
                "training_compute_(flop)": 2.7999999999999996e+24,
                "training_power_draw_(w)": 13756136.253818648,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2025-01-07T00:00:00",
                "link": "https://arxiv.org/abs/2501.03575",
                "reference": "Cosmos World Foundation Model Platform for Physical AI",
                "organization": "NVIDIA",
                "parameters": 14000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Cosmos-1.0-\nDiffusion-14B Video2World - 2.80e+24 FLOPs"
            },
            {
                "model": "Cosmos-Predict1-7b-Video2World",
                "training_compute_(flop)": 1.3999999999999998e+24,
                "training_power_draw_(w)": 13756136.253818648,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2025-01-07T00:00:00",
                "link": "https://arxiv.org/abs/2501.03575",
                "reference": "Cosmos World Foundation Model Platform for Physical AI",
                "organization": "NVIDIA",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Cosmos-Predict1-7b-Video2World - 1.40e+24 FLOPs"
            },
            {
                "model": "Cosmos-Predict1-14b-Video2World",
                "training_compute_(flop)": 2.7999999999999996e+24,
                "training_power_draw_(w)": 13756136.253818648,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2025-01-07T00:00:00",
                "link": "https://arxiv.org/abs/2501.03575",
                "reference": "Cosmos World Foundation Model Platform for Physical AI",
                "organization": "NVIDIA",
                "parameters": 14000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2025,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Cosmos-Predict1-14b-Video2World - 2.80e+24 FLOPs"
            },
            {
                "model": "OLMo 2 Furious 7B",
                "training_compute_(flop)": 1.8e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2024-12-31T00:00:00",
                "link": "https://arxiv.org/abs/2501.00656",
                "reference": "2 OLMo 2 Furious",
                "organization": "Allen Institute for AI,University of Washington,New York University (NYU)",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "OLMo 2 Furious 7B - 1.80e+23 FLOPs"
            },
            {
                "model": "OLMo 2 Furious 13B",
                "training_compute_(flop)": 4.6e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2024-12-31T00:00:00",
                "link": "https://arxiv.org/abs/2501.00656",
                "reference": "2 OLMo 2 Furious",
                "organization": "Allen Institute for AI,University of Washington,New York University (NYU)",
                "parameters": 13000000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "OLMo 2 Furious 13B - 4.60e+23 FLOPs"
            },
            {
                "model": "DeepSeek-V3",
                "training_compute_(flop)": 3.4078e+24,
                "training_power_draw_(w)": 2818135.1812142837,
                "training_dataset_size_(gradients)": 14800000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 5390000.0,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-12-24T00:00:00",
                "link": "https://arxiv.org/abs/2412.19437",
                "reference": "DeepSeek-V3 Technical Report",
                "organization": "DeepSeek",
                "parameters": 671000000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "DeepSeek-V3 - 3.41e+24 FLOPs"
            },
            {
                "model": "SEA-LION V3 Gemma2 9B",
                "training_compute_(flop)": 4.484146e+23,
                "training_power_draw_(w)": 88076.53091181243,
                "training_dataset_size_(gradients)": 200000000000.0,
                "training_time_(hours)": 240.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Government",
                "publication_date": "2024-12-19T00:00:00",
                "link": "https://huggingface.co/aisingapore/gemma2-9b-cpt-sea-lionv3-base",
                "reference": "SEA-LION V3",
                "organization": "AI Singapore",
                "parameters": 9000000000.0,
                "notable_model": false,
                "country": "Singapore",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Government",
                "access_group": "Open",
                "model_and_compute": "SEA-LION V3 Gemma2 9B - 4.48e+23 FLOPs"
            },
            {
                "model": "SEA-LION V3 Llama3.1 8B",
                "training_compute_(flop)": 1.23330162e+24,
                "training_power_draw_(w)": 88076.53091181243,
                "training_dataset_size_(gradients)": 200000000000.0,
                "training_time_(hours)": 136.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Government",
                "publication_date": "2024-12-19T00:00:00",
                "link": "https://huggingface.co/aisingapore/llama3.1-8b-cpt-sea-lionv3-base",
                "reference": "SEA-LION V3",
                "organization": "AI Singapore",
                "parameters": 8000000000.0,
                "notable_model": false,
                "country": "Singapore",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Government",
                "access_group": "Open",
                "model_and_compute": "SEA-LION V3 Llama3.1 8B - 1.23e+24 FLOPs"
            },
            {
                "model": "SEA-LION V3 Llama3.1 70B",
                "training_compute_(flop)": 8.0103891e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 200000000000.0,
                "training_time_(hours)": 136.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Government",
                "publication_date": "2024-12-19T00:00:00",
                "link": "https://huggingface.co/aisingapore/Llama-SEA-LION-v3-70B",
                "reference": "SEA-LION V3",
                "organization": "AI Singapore",
                "parameters": 70000000000.0,
                "notable_model": false,
                "country": "Singapore",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Government",
                "access_group": "Open",
                "model_and_compute": "SEA-LION V3 Llama3.1 70B - 8.01e+24 FLOPs"
            },
            {
                "model": "Granite 3.1 2B",
                "training_compute_(flop)": 1.8e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 12000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-12-18T00:00:00",
                "link": "https://huggingface.co/ibm-granite/granite-3.1-2b-base",
                "reference": null,
                "organization": "IBM",
                "parameters": 2500000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Granite 3.1 2B - 1.80e+23 FLOPs"
            },
            {
                "model": "Granite 3.1 8B",
                "training_compute_(flop)": 5.832e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 12000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-12-18T00:00:00",
                "link": "https://huggingface.co/ibm-granite/granite-3.1-8b-base",
                "reference": null,
                "organization": "IBM",
                "parameters": 8100000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Granite 3.1 8B - 5.83e+23 FLOPs"
            },
            {
                "model": "Falcon3-7B",
                "training_compute_(flop)": 5.88e+23,
                "training_power_draw_(w)": 1409287.2610737197,
                "training_dataset_size_(gradients)": 14000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Government",
                "publication_date": "2024-12-17T00:00:00",
                "link": "https://huggingface.co/blog/falcon3",
                "reference": "Welcome to the Falcon 3 Family of Open Models!",
                "organization": "Technology Innovation Institute",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "United Arab Emirates",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Government",
                "access_group": "Open",
                "model_and_compute": "Falcon3-7B - 5.88e+23 FLOPs"
            },
            {
                "model": "F5-TTS",
                "training_compute_(flop)": 4.5287424e+20,
                "training_power_draw_(w)": 6291.741206937689,
                "training_dataset_size_(gradients)": 27325800000.0,
                "training_time_(hours)": 168.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Audio",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-12-15T00:00:00",
                "link": "https://arxiv.org/abs/2410.06885",
                "reference": "F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching",
                "organization": "Shanghai Jiao Tong University,University of Cambridge,Geely Automobile Research Institute (Ningbo) Company",
                "parameters": 335800000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "F5-TTS - 4.53e+20 FLOPs"
            },
            {
                "model": "GigaChat Lite (GigaChat-20B-A3B)",
                "training_compute_(flop)": 9.9e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry, Government",
                "publication_date": "2024-12-13T00:00:00",
                "link": "https://habr.com/ru/companies/sberdevices/articles/865996/",
                "reference": "the first open MoE model in Russia",
                "organization": "Sber",
                "parameters": 20000000000.0,
                "notable_model": false,
                "country": "Russia",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Other",
                "access_group": "Open",
                "model_and_compute": "GigaChat Lite (GigaChat-20B-A3B) - 9.90e+22 FLOPs"
            },
            {
                "model": "Phi-4",
                "training_compute_(flop)": 9.3202015e+23,
                "training_power_draw_(w)": 2642707.855343539,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 504.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-12-12T00:00:00",
                "link": "https://arxiv.org/abs/2412.08905",
                "reference": "Phi-4 Technical Report",
                "organization": "Microsoft Research",
                "parameters": 14000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Phi-4 - 9.32e+23 FLOPs"
            },
            {
                "model": "EXAONE 3.5 2.4B",
                "training_compute_(flop)": 9.36e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 6500000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-12-09T00:00:00",
                "link": "https://arxiv.org/abs/2412.04862",
                "reference": "EXAONE 3.5: Series of Large Language Models for Real-world Use Cases",
                "organization": "LG AI Research",
                "parameters": 2400000000.0,
                "notable_model": false,
                "country": "South Korea",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "South Korea",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "EXAONE 3.5 2.4B - 9.36e+22 FLOPs"
            },
            {
                "model": "EXAONE 3.5 7.8B",
                "training_compute_(flop)": 4.21e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 9000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-12-09T00:00:00",
                "link": "https://arxiv.org/abs/2412.04862",
                "reference": "EXAONE 3.5: Series of Large Language Models for Real-world Use Cases",
                "organization": "LG AI Research",
                "parameters": 7800000000.0,
                "notable_model": false,
                "country": "South Korea",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "South Korea",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "EXAONE 3.5 7.8B - 4.21e+23 FLOPs"
            },
            {
                "model": "EXAONE 3.5 32B",
                "training_compute_(flop)": 1.25e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 6500000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-12-09T00:00:00",
                "link": "https://arxiv.org/abs/2412.04862",
                "reference": "EXAONE 3.5: Series of Large Language Models for Real-world Use Cases",
                "organization": "LG AI Research",
                "parameters": 32000000000.0,
                "notable_model": true,
                "country": "South Korea",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "South Korea",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "EXAONE 3.5 32B - 1.25e+24 FLOPs"
            },
            {
                "model": "Llama 3.3 70B",
                "training_compute_(flop)": 6.8649768e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 15000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-12-06T00:00:00",
                "link": "https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_3/",
                "reference": "Meta Llama 3.3 multilingual large language model (LLM) ",
                "organization": "Meta AI",
                "parameters": 70000000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Llama 3.3 70B - 6.86e+24 FLOPs"
            },
            {
                "model": "NVILA 8B",
                "training_compute_(flop)": 2.2794518e+21,
                "training_power_draw_(w)": 176207.9898367568,
                "training_dataset_size_(gradients)": 47488579166.0,
                "training_time_(hours)": 16.7,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-12-05T00:00:00",
                "link": "https://arxiv.org/abs/2412.04468",
                "reference": "NVILA: Efficient Frontier Visual Language Models",
                "organization": "NVIDIA,Massachusetts Institute of Technology (MIT),University of California (UC) Berkeley,University of California San Diego,University of Washington,Tsinghua University",
                "parameters": 8000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "NVILA 8B - 2.28e+21 FLOPs"
            },
            {
                "model": "Pleias 1.0 350m",
                "training_compute_(flop)": 2.6788982e+21,
                "training_power_draw_(w)": 88103.9949183784,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 46.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-12-05T00:00:00",
                "link": "https://huggingface.co/PleIAs/Pleias-350m-Preview",
                "reference": "Pleias-pico-350m-Preview",
                "organization": "PleIAs",
                "parameters": 350000000.0,
                "notable_model": false,
                "country": "France",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Pleias 1.0 350m - 2.68e+21 FLOPs"
            },
            {
                "model": "Pleias 1.0 1.2B",
                "training_compute_(flop)": 2.9770787e+22,
                "training_power_draw_(w)": 264311.9847551352,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 120.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-12-05T00:00:00",
                "link": "https://huggingface.co/PleIAs/Pleias-1.2b-Preview",
                "reference": "Pleias-nano-1.2b-Preview ",
                "organization": "PleIAs",
                "parameters": 1200000000.0,
                "notable_model": false,
                "country": "France",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Pleias 1.0 1.2B - 2.98e+22 FLOPs"
            },
            {
                "model": "Amazon Nova Pro",
                "training_compute_(flop)": 6.000010000000001e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2024-12-03T00:00:00",
                "link": "https://aws.amazon.com/es/blogs/aws/introducing-amazon-nova-frontier-intelligence-and-industry-leading-price-performance/\n\nhttps://assets.amazon.science/96/7d/0d3e59514abf8fdcfafcdc574300/nova-tech-report-20250317-0810.pdf",
                "reference": "Introducing Amazon Nova foundation models: Frontier intelligence and industry leading price performance",
                "organization": "Amazon",
                "parameters": null,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "API access",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Amazon Nova Pro - 6.00e+24 FLOPs"
            },
            {
                "model": "Hunyuan Video",
                "training_compute_(flop)": 1.4814815e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2024-12-03T00:00:00",
                "link": "https://www.arxiv.org/abs/2412.03603",
                "reference": "HunyuanVideo: A Systematic Framework For Large Video Generative Models",
                "organization": "Tencent",
                "parameters": 13000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Hunyuan Video - 1.48e+23 FLOPs"
            },
            {
                "model": "INTELLECT-1",
                "training_compute_(flop)": 6.000001e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1000000000000.0,
                "training_time_(hours)": 1008.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-11-29T00:00:00",
                "link": "https://github.com/PrimeIntellect-ai/prime/blob/main/INTELLECT_1_Technical_Report.pdf",
                "reference": "INTELLECT-1 Technical Report",
                "organization": "Prime Intellect,Hugging Face,Arcee AI",
                "parameters": 10000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "INTELLECT-1 - 6.00e+22 FLOPs"
            },
            {
                "model": "Fish-Speech 1.5",
                "training_compute_(flop)": 2.6599104e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 700000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Audio",
                "organization_categorization": "Industry",
                "publication_date": "2024-11-24T00:00:00",
                "link": "https://huggingface.co/fishaudio/fish-speech-1.5",
                "reference": "Fish Speech V1.5 is a leading text-to-speech (TTS) model trained on more than 1 million hours of audio data in multiple languages.",
                "organization": "Fish Audio",
                "parameters": null,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Fish-Speech 1.5 - 2.66e+21 FLOPs"
            },
            {
                "model": "Hymba",
                "training_compute_(flop)": 1.35e+22,
                "training_power_draw_(w)": 100719.43414656924,
                "training_dataset_size_(gradients)": 1500000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-11-22T00:00:00",
                "link": "https://arxiv.org/abs/2411.13676",
                "reference": "Hymba: A Hybrid-head Architecture for Small Language Models",
                "organization": "NVIDIA",
                "parameters": 1500000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Hymba - 1.35e+22 FLOPs"
            },
            {
                "model": "360Zhinao2-7B",
                "training_compute_(flop)": 4.242e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-11-18T00:00:00",
                "link": "https://github.com/Qihoo360/360zhinao2",
                "reference": "360Zhinao2 (360\u667a\u8111)",
                "organization": "360 Security Technology",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "360Zhinao2-7B - 4.24e+23 FLOPs"
            },
            {
                "model": "BiRNA-BERT",
                "training_compute_(flop)": 1.8354513e+19,
                "training_power_draw_(w)": 5508.584723910074,
                "training_dataset_size_(gradients)": 32254000000.0,
                "training_time_(hours)": 48.42,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2024-11-18T00:00:00",
                "link": "https://www.biorxiv.org/content/10.1101/2024.07.02.601703v1",
                "reference": "BiRNA-BERT allows efficient RNA language modeling with\nadaptive tokenization",
                "organization": "Bangladesh University of Engineering and Technology,University of California Riverside,Carnegie Mellon University (CMU)",
                "parameters": 117000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "BiRNA-BERT - 1.84e+19 FLOPs"
            },
            {
                "model": "Qwen2.5-Coder (32B)",
                "training_compute_(flop)": 1.0725e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-11-12T00:00:00",
                "link": "https://arxiv.org/abs/2409.12186",
                "reference": "Qwen2.5-Coder Technical Report",
                "organization": "Alibaba",
                "parameters": 32500000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Qwen2.5-Coder (32B) - 1.07e+24 FLOPs"
            },
            {
                "model": "NatureLM-audio",
                "training_compute_(flop)": 1.4108774e+21,
                "training_power_draw_(w)": 5509.443499872667,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 216.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Audio",
                "organization_categorization": null,
                "publication_date": "2024-11-11T00:00:00",
                "link": "https://arxiv.org/abs/2411.07186",
                "reference": "NatureLM-audio: an Audio-Language Foundation Model for Bioacoustics",
                "organization": "Earth Species Project",
                "parameters": 665000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Other",
                "access_group": "Open",
                "model_and_compute": "NatureLM-audio - 1.41e+21 FLOPs"
            },
            {
                "model": "Fish-Speech 1.4",
                "training_compute_(flop)": 1.9151355e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 500000000000.0,
                "training_time_(hours)": 168.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Audio",
                "organization_categorization": "Industry",
                "publication_date": "2024-11-09T00:00:00",
                "link": "https://arxiv.org/abs/2411.01156",
                "reference": "Fish-Speech: Leveraging Large Language Models for Advanced Multilingual Text-to-Speech Synthesis",
                "organization": "Fish Audio",
                "parameters": null,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Fish-Speech 1.4 - 1.92e+21 FLOPs"
            },
            {
                "model": "Hunyuan-Large",
                "training_compute_(flop)": 3.49237e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 7000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-11-06T00:00:00",
                "link": "https://arxiv.org/abs/2411.02265",
                "reference": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent",
                "organization": "Tencent",
                "parameters": 389000000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Hunyuan-Large - 3.49e+24 FLOPs"
            },
            {
                "model": "OpenPhenom-S/16",
                "training_compute_(flop)": 3.18e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 400.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2024-11-05T00:00:00",
                "link": "https://www.rxrx.ai/phenom#:~:text=We%20call%20this%20model%20Phenom-Beta.%20It%20flexibly%20processes,create%20a%20meaningful%20representation%20of%20the%20input%20image.",
                "reference": "Generative deep computer vision models",
                "organization": "Recursion Pharmaceuticals",
                "parameters": 178045568.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "OpenPhenom-S/16 - 3.18e+19 FLOPs"
            },
            {
                "model": "Uni-Med",
                "training_compute_(flop)": 1.425e+23,
                "training_power_draw_(w)": 432.54870917774826,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 10.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": null,
                "organization_categorization": "Academia",
                "publication_date": "2024-11-01T00:00:00",
                "link": "https://arxiv.org/pdf/2409.17508",
                "reference": "Uni-Med: A Unified Medical Generalist Foundation\nModel For Multi-Task Learning Via Connector-MoE",
                "organization": "Tsinghua University,Beijing University of Posts and Telecommunications",
                "parameters": 8800000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Unreleased",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Other",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Uni-Med - 1.42e+23 FLOPs"
            },
            {
                "model": "VASA-1",
                "training_compute_(flop)": 4.012416e+19,
                "training_power_draw_(w)": 2361.768546625511,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 240.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2024-10-31T00:00:00",
                "link": "https://arxiv.org/abs/2404.10667",
                "reference": "VASA-1: Lifelike Audio-Driven Talking Faces Generated in Real Time",
                "organization": "Microsoft Research Asia",
                "parameters": 229000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Unreleased",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "VASA-1 - 4.01e+19 FLOPs"
            },
            {
                "model": "Pro-PRIME",
                "training_compute_(flop)": 8.18e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2024-10-28T00:00:00",
                "link": "https://arxiv.org/abs/2307.12682",
                "reference": "Pro-PRIME: A general Temperature-Guided Language model to engineer enhanced Stability and Activity in Proteins",
                "organization": "Shanghai Jiao Tong University,Shanghai AI Lab,East China University of Science and Technology,Shanghai Tech University,Guangzhou Inernational Bio Island,Chinese Academy of Sciences,Shanghai Academy of Experimental Medicine",
                "parameters": 650000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Unknown",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Pro-PRIME - 8.18e+20 FLOPs"
            },
            {
                "model": "Doubao-pro",
                "training_compute_(flop)": 2.505e+25,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 8350000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-10-28T00:00:00",
                "link": "https://www.volcengine.com/docs/6360/1264663",
                "reference": "Doubao General Model Pro (Doubao-pro)",
                "organization": "ByteDance",
                "parameters": 500000000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "API access",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Doubao-pro - 2.50e+25 FLOPs"
            },
            {
                "model": "Aya Expanse 32B",
                "training_compute_(flop)": 6.688684e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 34513333333.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-10-24T00:00:00",
                "link": "https://cohere.com/blog/aya-expanse-connecting-our-world\n\nhttps://huggingface.co/CohereForAI/aya-expanse-32b?ref=cohere-ai.ghost.io",
                "reference": "Cohere For AI launches Aya Expanse, a state-of-the-art multilingual family of models to help close the language gap with AI.",
                "organization": "Cohere for AI",
                "parameters": 32300000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Aya Expanse 32B - 6.69e+21 FLOPs"
            },
            {
                "model": "Aya Expanse 8B",
                "training_compute_(flop)": 1.65664e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 34513333333.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-10-24T00:00:00",
                "link": "https://cohere.com/blog/aya-expanse-connecting-our-world\n\nhttps://huggingface.co/CohereForAI/aya-expanse-8b?ref=cohere-ai.ghost.io",
                "reference": "Cohere For AI launches Aya Expanse, a state-of-the-art multilingual family of models to help close the language gap with AI.",
                "organization": "Cohere for AI",
                "parameters": 8000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Aya Expanse 8B - 1.66e+21 FLOPs"
            },
            {
                "model": "NVLM-D 72B",
                "training_compute_(flop)": 3.02e+24,
                "training_power_draw_(w)": 176380.73226445544,
                "training_dataset_size_(gradients)": 57016320000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2024-10-22T00:00:00",
                "link": "https://arxiv.org/abs/2409.11402",
                "reference": "NVLM: Open Frontier-Class Multimodal LLMs",
                "organization": "NVIDIA",
                "parameters": 72000000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "NVLM-D 72B - 3.02e+24 FLOPs"
            },
            {
                "model": "NVLM-H 72B",
                "training_compute_(flop)": 3.02e+24,
                "training_power_draw_(w)": 176380.73226445544,
                "training_dataset_size_(gradients)": 125829120000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2024-10-22T00:00:00",
                "link": "https://arxiv.org/abs/2409.11402",
                "reference": "NVLM: Open Frontier-Class Multimodal LLMs",
                "organization": "NVIDIA",
                "parameters": 72000000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "NVLM-H 72B - 3.02e+24 FLOPs"
            },
            {
                "model": "NVLM-X 72B",
                "training_compute_(flop)": 3.0398181e+24,
                "training_power_draw_(w)": 176380.73226445544,
                "training_dataset_size_(gradients)": 45875200000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2024-10-22T00:00:00",
                "link": "https://arxiv.org/abs/2409.11402",
                "reference": "NVLM: Open Frontier-Class Multimodal LLMs",
                "organization": "NVIDIA",
                "parameters": 72000000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "NVLM-X 72B - 3.04e+24 FLOPs"
            },
            {
                "model": "Granite 3.0 8B",
                "training_compute_(flop)": 5.832e+23,
                "training_power_draw_(w)": 352769.3203924083,
                "training_dataset_size_(gradients)": 12000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-10-21T00:00:00",
                "link": "https://github.com/ibm-granite/granite-3.0-language-models/tree/main",
                "reference": "Granite 3.0 Language Models",
                "organization": "IBM",
                "parameters": 8100000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Granite 3.0 8B - 5.83e+23 FLOPs"
            },
            {
                "model": "Granite 3.0 2B",
                "training_compute_(flop)": 1.8e+23,
                "training_power_draw_(w)": 1058307.961177225,
                "training_dataset_size_(gradients)": 12000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-10-21T00:00:00",
                "link": "https://github.com/ibm-granite/granite-3.0-language-models/tree/main",
                "reference": "Granite 3.0 Language Models",
                "organization": "IBM",
                "parameters": 2500000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Granite 3.0 2B - 1.80e+23 FLOPs"
            },
            {
                "model": "Allegro",
                "training_compute_(flop)": 6.565847e+22,
                "training_power_draw_(w)": 352777.1764308529,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2024-10-20T00:00:00",
                "link": "https://arxiv.org/abs/2410.15458",
                "reference": "Allegro: Open the Black Box of Commercial-Level Video Generation Model",
                "organization": "Rhymes AI",
                "parameters": 2975000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Allegro - 6.57e+22 FLOPs"
            },
            {
                "model": "Yi-Lightning",
                "training_compute_(flop)": 1.5e+24,
                "training_power_draw_(w)": 2756194.445567179,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 720.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-10-18T00:00:00",
                "link": "https://www.lingyiwanwu.com/en https://platform.lingyiwanwu.com/",
                "reference": "Yi-Lightning",
                "organization": "01.AI",
                "parameters": null,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "API access",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Yi-Lightning - 1.50e+24 FLOPs"
            },
            {
                "model": "CHAI-1",
                "training_compute_(flop)": 7.7605724e+21,
                "training_power_draw_(w)": 100804.70264211958,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 720.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Industry",
                "publication_date": "2024-10-15T00:00:00",
                "link": "https://www.chaidiscovery.com/blog/introducing-chai-1\nhttps://www.biorxiv.org/content/10.1101/2024.10.10.615955v2",
                "reference": "Introducing Chai-1: Decoding the molecular interactions of life",
                "organization": "Chai discovery",
                "parameters": null,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Biology",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "CHAI-1 - 7.76e+21 FLOPs"
            },
            {
                "model": "RDT-1B",
                "training_compute_(flop)": 4.06e+22,
                "training_power_draw_(w)": 33080.22622858553,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 720.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Robotics",
                "organization_categorization": "Academia",
                "publication_date": "2024-10-10T00:00:00",
                "link": "https://arxiv.org/abs/2410.07864",
                "reference": "RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation",
                "organization": "Tsinghua University",
                "parameters": 1200000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Other",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "RDT-1B - 4.06e+22 FLOPs"
            },
            {
                "model": "Pyramid Flow",
                "training_compute_(flop)": 7.67e+21,
                "training_power_draw_(w)": 100820.41786842256,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 162.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-10-08T00:00:00",
                "link": "https://arxiv.org/abs/2410.05954",
                "reference": "Pyramidal Flow Matching for Efficient Video Generative Modeling",
                "organization": "Peking University,Kuaishou Technology,Beijing University of Posts and Telecommunications",
                "parameters": 2000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Vision",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "Pyramid Flow - 7.67e+21 FLOPs"
            },
            {
                "model": "Aria",
                "training_compute_(flop)": 1.428e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 6800000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2024-10-08T00:00:00",
                "link": "https://arxiv.org/abs/2410.05993",
                "reference": "ARIA : An Open Multimodal Native\nMixture-of-Experts Model",
                "organization": "Rhymes AI",
                "parameters": 24900000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Aria - 1.43e+23 FLOPs"
            },
            {
                "model": "scHyena",
                "training_compute_(flop)": 8.6e+18,
                "training_power_draw_(w)": 1378.5269407888657,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2024-10-04T00:00:00",
                "link": "https://arxiv.org/abs/2310.02713",
                "reference": "scHyena: Foundation Model for Full-Length Single-Cell RNA-Seq Analysis in Brain",
                "organization": "Korea Advanced Institute of Science and Technology (KAIST)",
                "parameters": null,
                "notable_model": false,
                "country": "South Korea",
                "model_accessibility": "Unknown",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "South Korea",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "scHyena - 8.60e+18 FLOPs"
            },
            {
                "model": "Movie Gen Video",
                "training_compute_(flop)": 1.65e+24,
                "training_power_draw_(w)": 8469669.524206791,
                "training_dataset_size_(gradients)": 3400000000.0,
                "training_time_(hours)": 331.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2024-10-04T00:00:00",
                "link": "https://ai.meta.com/static-resource/movie-gen-research-paper",
                "reference": "Movie Gen: A Cast of Media Foundation Models",
                "organization": "Meta AI",
                "parameters": 30000000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Movie Gen Video - 1.65e+24 FLOPs"
            },
            {
                "model": "Movie Gen Audio",
                "training_compute_(flop)": 1.4e+23,
                "training_power_draw_(w)": 529354.3452629244,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 360.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Audio",
                "organization_categorization": "Industry",
                "publication_date": "2024-10-04T00:00:00",
                "link": "https://ai.meta.com/static-resource/movie-gen-research-paper",
                "reference": "Movie Gen: A Cast of Media Foundation Models",
                "organization": "Meta AI",
                "parameters": 13000000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Movie Gen Audio - 1.40e+23 FLOPs"
            },
            {
                "model": "FlexSBDD",
                "training_compute_(flop)": 1.6000000000000008e+19,
                "training_power_draw_(w)": 432.86670140535114,
                "training_dataset_size_(gradients)": 3675000000.0,
                "training_time_(hours)": 36.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2024-09-29T00:00:00",
                "link": "https://arxiv.org/abs/2409.19645",
                "reference": "FlexSBDD: Structure-Based Drug Design with Flexible Protein Modeling",
                "organization": "University of Science and Technology of China (USTC),State Key Laboratory of Cognitive Intelligence,Princeton University",
                "parameters": null,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "FlexSBDD - 1.60e+19 FLOPs"
            },
            {
                "model": "RWKV-5 (Eagle) 7B",
                "training_compute_(flop)": 4.983e+22,
                "training_power_draw_(w)": 88241.44346194508,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry, Government",
                "publication_date": "2024-09-26T00:00:00",
                "link": "https://arxiv.org/abs/2404.05892",
                "reference": "Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence",
                "organization": "RWKV Foundation,EleutherAI,Ohio State University,University of California Santa Barbara (UCSB),Wroclaw Tech (Wroc\u0142aw University of Science and Technology),Guangdong Laboratory of Artificial Intelligence and Digital Economy (Pazhou Lab),New York University (NYU),Harvard University,Contextual AI,University of Chinese Academy of Sciences,University of California Santa Cruz,Tsinghua University,University of Edinburgh,University of British Columbia (UBC),Pennsylvania State University",
                "parameters": 7520000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry, Government",
                "access_group": "Open",
                "model_and_compute": "RWKV-5 (Eagle) 7B - 4.98e+22 FLOPs"
            },
            {
                "model": "RWKV-6 (Finch) 3B",
                "training_compute_(flop)": 2.0944e+22,
                "training_power_draw_(w)": 37817.761483690745,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry, Government",
                "publication_date": "2024-09-26T00:00:00",
                "link": "https://arxiv.org/abs/2404.05892",
                "reference": "Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence",
                "organization": "RWKV Foundation,EleutherAI,Ohio State University,University of California Santa Barbara (UCSB),Wroclaw Tech (Wroc\u0142aw University of Science and Technology),Guangdong Laboratory of Artificial Intelligence and Digital Economy (Pazhou Lab),New York University (NYU),Harvard University,Contextual AI,University of Chinese Academy of Sciences,University of California Santa Cruz,Tsinghua University,University of Edinburgh,University of British Columbia (UBC),Pennsylvania State University",
                "parameters": 3100000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry, Government",
                "access_group": "Open",
                "model_and_compute": "RWKV-6 (Finch) 3B - 2.09e+22 FLOPs"
            },
            {
                "model": "Mothra",
                "training_compute_(flop)": 3.7e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2024-09-25T00:00:00",
                "link": null,
                "reference": "Mothra: Multiobjective de novo Molecular Generation Using Monte Carlo Tree Search",
                "organization": "Tokyo Institute of Technology",
                "parameters": null,
                "notable_model": false,
                "country": "Japan",
                "model_accessibility": "Unknown",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Japan",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Mothra - 3.70e+19 FLOPs"
            },
            {
                "model": "Molmo 72B",
                "training_compute_(flop)": 1.33583e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Academia",
                "publication_date": "2024-09-25T00:00:00",
                "link": "https://arxiv.org/abs/2409.17146",
                "reference": "Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models",
                "organization": "Allen Institute for AI,University of Washington",
                "parameters": 72000000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Multimodal",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "Molmo 72B - 1.34e+22 FLOPs"
            },
            {
                "model": "dnaGrinder",
                "training_compute_(flop)": 2.7713e+19,
                "training_power_draw_(w)": 11030.67171405016,
                "training_dataset_size_(gradients)": 10425000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2024-09-24T00:00:00",
                "link": "https://arxiv.org/abs/2409.15697",
                "reference": "dnaGrinder: a lightweight and high-capacity genomic foundation model",
                "organization": "Hong Kong Polytechnic University",
                "parameters": 63600000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "dnaGrinder - 2.77e+19 FLOPs"
            },
            {
                "model": "ProtBFN",
                "training_compute_(flop)": 3.900000000000003e+22,
                "training_power_draw_(w)": 85724.0773206184,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 432.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Industry",
                "publication_date": "2024-09-24T00:00:00",
                "link": "https://www.biorxiv.org/content/10.1101/2024.09.24.614734v1.abstract",
                "reference": "Protein Sequence Modelling with Bayesian Flow Networks",
                "organization": "InstaDeep",
                "parameters": 650000000.0,
                "notable_model": false,
                "country": "United Kingdom",
                "model_accessibility": "Unknown",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Biology",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "ProtBFN - 3.90e+22 FLOPs"
            },
            {
                "model": "Llama 3.2 11B",
                "training_compute_(flop)": 5.79e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2024-09-24T00:00:00",
                "link": "https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/",
                "reference": "Llama 3.2: Revolutionizing edge AI and vision with open, customizable models",
                "organization": "Meta AI",
                "parameters": 10600000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Llama 3.2 11B - 5.79e+23 FLOPs"
            },
            {
                "model": "Llama 3.2 1B",
                "training_compute_(flop)": 6.642e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 9000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-09-24T00:00:00",
                "link": "https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/",
                "reference": "Llama 3.2: Revolutionizing edge AI and vision with open, customizable models",
                "organization": "Meta AI",
                "parameters": 1230000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Llama 3.2 1B - 6.64e+22 FLOPs"
            },
            {
                "model": "Llama 3.2 3B",
                "training_compute_(flop)": 1.7334e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 9000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-09-24T00:00:00",
                "link": "https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/",
                "reference": "Llama 3.2: Revolutionizing edge AI and vision with open, customizable models",
                "organization": "Meta AI",
                "parameters": 3210000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Llama 3.2 3B - 1.73e+23 FLOPs"
            },
            {
                "model": "Thermostable protein design",
                "training_compute_(flop)": 1.8397012e+16,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2024-09-24T00:00:00",
                "link": "https://www.biorxiv.org/content/10.1101/2024.09.21.614294v1.abstract",
                "reference": "Designing of thermostable proteins with a desired melting temperature",
                "organization": "Indraprastha Institute of Information Technology\nDelhi",
                "parameters": 738000000.0,
                "notable_model": false,
                "country": "India",
                "model_accessibility": "Unknown",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Thermostable protein design - 1.84e+16 FLOPs"
            },
            {
                "model": "MTDP",
                "training_compute_(flop)": 600000000000000.0,
                "training_power_draw_(w)": 2521.296391782894,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2024-09-24T00:00:00",
                "link": "https://academic.oup.com/bioinformatics/article/40/9/btae567/7772445",
                "reference": "Accurate and efficient protein embedding using multi-teacher distillation learning",
                "organization": "Chinese University of Hong Kong (CUHK),City University of Hong Kong",
                "parameters": 20000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "MTDP - 6.00e+14 FLOPs"
            },
            {
                "model": "ByteDance Seaweed",
                "training_compute_(flop)": 5.75e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2024-09-24T00:00:00",
                "link": "https://kr-asia.com/bytedance-enters-ai-video-race-with-doubaos-pixeldance-and-seaweed",
                "reference": "ByteDance enters AI video race with Doubao\u2019s PixelDance and Seaweed",
                "organization": "ByteDance",
                "parameters": null,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Hosted access (no API)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "ByteDance Seaweed - 5.75e+22 FLOPs"
            },
            {
                "model": "ProteinSetTransformer",
                "training_compute_(flop)": 1.6500000000000016e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2024-09-23T00:00:00",
                "link": "https://www.researchsquare.com/article/rs-4844047/v1",
                "reference": "Protein Set Transformer: A protein-based genome language model to power high diversity viromics",
                "organization": "University of Wisconsin Madison",
                "parameters": null,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "ProteinSetTransformer - 1.65e+19 FLOPs"
            },
            {
                "model": "PocketGen",
                "training_compute_(flop)": 2.1e+19,
                "training_power_draw_(w)": 432.9245432852901,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 48.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2024-09-23T00:00:00",
                "link": "https://www.biorxiv.org/content/10.1101/2024.02.25.581968v4.abstract",
                "reference": "Efficient Generation of Protein Pockets with PocketGen",
                "organization": "University of Science and Technology of China (USTC),Hefei Comprehensive National Science Center,Harvard University,Broad Institute,Harvard Data Science Initiative",
                "parameters": 7900000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "PocketGen - 2.10e+19 FLOPs"
            },
            {
                "model": "TAWFN",
                "training_compute_(flop)": 3.5000000000000287e+18,
                "training_power_draw_(w)": 378.8089753746288,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2024-09-23T00:00:00",
                "link": "https://academic.oup.com/bioinformatics/article/40/10/btae571/7766190",
                "reference": "TAWFN: A Deep Learning Framework for Protein Function Prediction",
                "organization": "Northeastern University (China)",
                "parameters": null,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Unknown",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "TAWFN - 3.50e+18 FLOPs"
            },
            {
                "model": "AMPLIFY",
                "training_compute_(flop)": 1.1000000000000008e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-09-23T00:00:00",
                "link": "https://www.biorxiv.org/content/10.1101/2024.09.23.614603v1",
                "reference": "Protein Language Models: Is Scaling Necessary?",
                "organization": "Chandar Research Lab,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms),Amgen,Polytechnique Montreal,CIFAR AI Research",
                "parameters": 350000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "AMPLIFY - 1.10e+22 FLOPs"
            },
            {
                "model": "IgGM",
                "training_compute_(flop)": 8.599999999999978e+20,
                "training_power_draw_(w)": 6303.521724136415,
                "training_dataset_size_(gradients)": 15506880.0,
                "training_time_(hours)": 240.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-09-22T00:00:00",
                "link": "https://www.biorxiv.org/content/10.1101/2024.09.19.613838v1.abstract",
                "reference": "IgGM: A Generative Model for Functional Antibody and Nanobody Design",
                "organization": "Chinese Academy of Sciences,University of Chinese Academy of Sciences,Tencent",
                "parameters": null,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "IgGM - 8.60e+20 FLOPs"
            },
            {
                "model": "Telechat2-115B",
                "training_compute_(flop)": 6.899999999999999e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 10000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-09-20T00:00:00",
                "link": "https://huggingface.co/Tele-AI/TeleChat2-115B",
                "reference": "TeleChat Technical Report",
                "organization": "China Telecom",
                "parameters": 115000000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Telechat2-115B - 6.90e+24 FLOPs"
            },
            {
                "model": "Prithvi WxC",
                "training_compute_(flop)": 7.15e+19,
                "training_power_draw_(w)": 50430.41985055912,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Earth science",
                "organization_categorization": "Academia, Industry, Government",
                "publication_date": "2024-09-20T00:00:00",
                "link": "https://arxiv.org/abs/2409.13598",
                "reference": "Prithvi WxC: Foundation Model for Weather and Climate",
                "organization": "IBM Research,University of Alabama,Stanford University,Colorado State University,Oak Ridge National Laboratory,NASA",
                "parameters": 2300000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Academia, Industry, Government",
                "access_group": "Open",
                "model_and_compute": "Prithvi WxC - 7.15e+19 FLOPs"
            },
            {
                "model": "Qwen2.5-72B",
                "training_compute_(flop)": 7.8e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 18000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-09-19T00:00:00",
                "link": "https://qwenlm.github.io/blog/qwen2.5/",
                "reference": "Qwen2.5: A Party of Foundation Models!",
                "organization": "Alibaba",
                "parameters": 72700000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Qwen2.5-72B - 7.80e+24 FLOPs"
            },
            {
                "model": "GeoSeqBuilder",
                "training_compute_(flop)": 6.480000000000066e+18,
                "training_power_draw_(w)": 178.59728239339805,
                "training_dataset_size_(gradients)": 35250000.0,
                "training_time_(hours)": 30.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2024-09-19T00:00:00",
                "link": "https://onlinelibrary.wiley.com/doi/abs/10.1002/ange.202411461?casa_token=oCX9uTFcpvQAAAAA%3AiGc_jXpH-dzeUXL0LgRnySqIJorSnOaPwaPGpjf8PbI9etcMdXXoXzwrEOX-wrQdbRsjcAEkQO0C6w",
                "reference": "All-Atom Protein Sequence Design Based on Geometric Deep Learning",
                "organization": "Peking University",
                "parameters": null,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "GeoSeqBuilder - 6.48e+18 FLOPs"
            },
            {
                "model": "Qwen2.5 Instruct (72B)",
                "training_compute_(flop)": 7.8516e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-09-19T00:00:00",
                "link": "https://qwenlm.github.io/blog/qwen2.5/",
                "reference": "Qwen2.5: A Party of Foundation Models!",
                "organization": "Alibaba",
                "parameters": 72700000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Qwen2.5 Instruct (72B) - 7.85e+24 FLOPs"
            },
            {
                "model": "Qwen2.5-3B",
                "training_compute_(flop)": 3.3372e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-09-19T00:00:00",
                "link": "https://qwenlm.github.io/blog/qwen2.5-llm/",
                "reference": "Qwen2.5: A Party of Foundation Models!",
                "organization": "Alibaba",
                "parameters": 3090000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Qwen2.5-3B - 3.34e+23 FLOPs"
            },
            {
                "model": "Qwen2.5-7B",
                "training_compute_(flop)": 8.2188e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-09-19T00:00:00",
                "link": "https://qwenlm.github.io/blog/qwen2.5/",
                "reference": "Qwen2.5: A Party of Foundation Models!",
                "organization": "Alibaba",
                "parameters": 7610000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Qwen2.5-7B - 8.22e+23 FLOPs"
            },
            {
                "model": "Qwen2.5-1.5B",
                "training_compute_(flop)": 1.6632e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-09-19T00:00:00",
                "link": "https://qwenlm.github.io/blog/qwen2.5-llm/",
                "reference": "Qwen2.5-LLM: Extending the boundary of LLMs",
                "organization": "Alibaba",
                "parameters": 1540000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Qwen2.5-1.5B - 1.66e+23 FLOPs"
            },
            {
                "model": "Qwen2.5-14B",
                "training_compute_(flop)": 1.58760000000001e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-09-19T00:00:00",
                "link": "https://qwenlm.github.io/blog/qwen2.5/",
                "reference": "Qwen2.5: A Party of Foundation Models!",
                "organization": "Alibaba",
                "parameters": 14700000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Qwen2.5-14B - 1.59e+24 FLOPs"
            },
            {
                "model": "Qwen2.5-Math-7B-Base",
                "training_compute_(flop)": 8.6388e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-09-19T00:00:00",
                "link": "https://qwenlm.github.io/blog/qwen2.5-math/",
                "reference": "Qwen2.5-Math: The world's leading open-sourced mathematical LLMs",
                "organization": "Alibaba",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Qwen2.5-Math-7B-Base - 8.64e+23 FLOPs"
            },
            {
                "model": "Qwen2.5-Math-1.5B",
                "training_compute_(flop)": 1.7532e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-09-19T00:00:00",
                "link": "https://qwenlm.github.io/blog/qwen2.5-math/",
                "reference": "Qwen2.5-Math: The world's leading open-sourced mathematical LLMs",
                "organization": "Alibaba",
                "parameters": 1500000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Qwen2.5-Math-1.5B - 1.75e+23 FLOPs"
            },
            {
                "model": "Qwen2-VL-72B",
                "training_compute_(flop)": 6.048e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1400000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2024-09-18T00:00:00",
                "link": "https://arxiv.org/abs/2409.12191",
                "reference": "Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution",
                "organization": "Alibaba",
                "parameters": 72000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Qwen2-VL-72B - 6.05e+23 FLOPs"
            },
            {
                "model": "Qwen2-VL-2B",
                "training_compute_(flop)": 1.68e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1400000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2024-09-18T00:00:00",
                "link": "https://arxiv.org/abs/2409.12191",
                "reference": "Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution",
                "organization": "Alibaba",
                "parameters": 2000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Qwen2-VL-2B - 1.68e+22 FLOPs"
            },
            {
                "model": "Qwen2-VL-7B",
                "training_compute_(flop)": 6.72e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2024-09-18T00:00:00",
                "link": "https://arxiv.org/abs/2409.12191",
                "reference": "Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution",
                "organization": "Alibaba",
                "parameters": 8000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Qwen2-VL-7B - 6.72e+22 FLOPs"
            },
            {
                "model": "Qwen2.5-Coder (7B)",
                "training_compute_(flop)": 2.5113e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-09-18T00:00:00",
                "link": "https://arxiv.org/abs/2409.12186",
                "reference": "Qwen2.5-Coder Technical Report",
                "organization": "Alibaba",
                "parameters": 7610000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Qwen2.5-Coder (7B) - 2.51e+23 FLOPs"
            },
            {
                "model": "Qwen2.5-Coder (1.5B)",
                "training_compute_(flop)": 5.082e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-09-18T00:00:00",
                "link": "https://arxiv.org/abs/2409.12186",
                "reference": "Qwen2.5-Coder Technical Report",
                "organization": "Alibaba",
                "parameters": 1540000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Qwen2.5-Coder (1.5B) - 5.08e+22 FLOPs"
            },
            {
                "model": "Qwen2.5-32B",
                "training_compute_(flop)": 3.51e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 18000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-09-17T00:00:00",
                "link": "https://qwenlm.github.io/blog/qwen2.5/ ",
                "reference": "Qwen2.5: A Party of Foundation Models!",
                "organization": "Alibaba",
                "parameters": 32500000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Qwen2.5-32B - 3.51e+24 FLOPs"
            },
            {
                "model": "RNAdiffusion",
                "training_compute_(flop)": 2.481192e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 34.5,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2024-09-15T00:00:00",
                "link": "https://arxiv.org/abs/2409.09828",
                "reference": "Latent Diffusion Models for Controllable RNA Sequence Generation",
                "organization": "Princeton University,Tsinghua University,Stanford University",
                "parameters": null,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "RNAdiffusion - 2.48e+19 FLOPs"
            },
            {
                "model": "IDPFold",
                "training_compute_(flop)": 2.59999999999998e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 30928500.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-09-13T00:00:00",
                "link": "https://www.biorxiv.org/content/10.1101/2024.05.05.592611.abstract",
                "reference": "Precise Generation of Conformational Ensembles for Intrinsically Disordered Proteins via Fine-tuned Diffusion Models",
                "organization": "Shandong University,BioMap Research,Fuzhou University,Shanghai Jiao Tong University",
                "parameters": 17800000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Unknown",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "IDPFold - 2.60e+20 FLOPs"
            },
            {
                "model": "Novae",
                "training_compute_(flop)": 1.1e+19,
                "training_power_draw_(w)": 433.0209635948764,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 24.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2024-09-13T00:00:00",
                "link": "https://www.biorxiv.org/content/10.1101/2024.09.09.612009v1.abstract",
                "reference": "Novae: a graph-based foundation model for spatial transcriptomics data",
                "organization": "CentraleSupelec,Gustave Roussy,Universit\u00e9 Paris Cit\u00e9",
                "parameters": 32000000.0,
                "notable_model": false,
                "country": "France",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "Novae - 1.10e+19 FLOPs"
            },
            {
                "model": "Text2Protein",
                "training_compute_(flop)": 5.500000000000003e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 240.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2024-09-13T00:00:00",
                "link": "https://www.researchsquare.com/article/rs-4868665/v1",
                "reference": "Text2Protein: A Generative Model for Designated Protein Design on Given Description",
                "organization": "University of California San Diego,Brown University",
                "parameters": null,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Text2Protein - 5.50e+19 FLOPs"
            },
            {
                "model": "E2 TTS",
                "training_compute_(flop)": 4.939776e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 245760000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Audio",
                "organization_categorization": "Industry",
                "publication_date": "2024-09-12T00:00:00",
                "link": "https://arxiv.org/abs/2406.18009",
                "reference": "E2 TTS: Embarrassingly Easy Fully Non-Autoregressive Zero-Shot TTS",
                "organization": "Microsoft",
                "parameters": 335000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "E2 TTS - 4.94e+20 FLOPs"
            },
            {
                "model": "MolPhenix",
                "training_compute_(flop)": 4.26816e+18,
                "training_power_draw_(w)": 433.0498938750792,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 9.5,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-09-10T00:00:00",
                "link": "https://arxiv.org/abs/2409.08302",
                "reference": "How Molecules Impact Cells: Unlocking Contrastive PhenoMolecular Retrieval",
                "organization": "Valence Labs,University of British Columbia (UBC),Vector Institute,University of Toronto,University of Montreal / Universit\u00e9 de Montr\u00e9al,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms)",
                "parameters": 38700000.0,
                "notable_model": false,
                "country": "Canada",
                "model_accessibility": "Unknown",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Canada",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "MolPhenix - 4.27e+18 FLOPs"
            },
            {
                "model": "AbGPT",
                "training_compute_(flop)": 4.2506168e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 6840000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2024-09-09T00:00:00",
                "link": "https://arxiv.org/abs/2409.06090",
                "reference": "AbGPT: De Novo Antibody Design via Generative Language Modeling",
                "organization": "Carnegie Mellon University (CMU)",
                "parameters": 734000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "AbGPT - 4.25e+21 FLOPs"
            },
            {
                "model": "ALOHA Unleashed",
                "training_compute_(flop)": 3.6084096e+21,
                "training_power_draw_(w)": 17655.364403717842,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 265.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Robotics",
                "organization_categorization": "Industry",
                "publication_date": "2024-09-08T00:00:00",
                "link": "https://aloha-unleashed.github.io/assets/aloha_unleashed.pdf\nhttps://arxiv.org/abs/2410.13126",
                "reference": "ALOHA Unleashed: A Simple Recipe for Robot Dexterity",
                "organization": "Google DeepMind",
                "parameters": 217000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "ALOHA Unleashed - 3.61e+21 FLOPs"
            },
            {
                "model": "MPDF",
                "training_compute_(flop)": 3.074112e+17,
                "training_power_draw_(w)": 378.94397282710867,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 6.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2024-09-07T00:00:00",
                "link": "https://arxiv.org/abs/2409.05916",
                "reference": "Unlocking Potential Binders: Multimodal Pretraining DEL-Fusion for Denoising DNA-Encoded Libraries",
                "organization": "Chinese University of Hong Kong (CUHK),Lanzhou University,Zhejiang Lab,Zhejiang University (ZJU)",
                "parameters": null,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "MPDF - 3.07e+17 FLOPs"
            },
            {
                "model": "DeepSeek-V2.5",
                "training_compute_(flop)": 1.7892e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-09-06T00:00:00",
                "link": "https://huggingface.co/deepseek-ai/DeepSeek-V2.5",
                "reference": "DeepSeek-V2.5",
                "organization": "DeepSeek",
                "parameters": 236000000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "DeepSeek-V2.5 - 1.79e+24 FLOPs"
            },
            {
                "model": "OLMoE",
                "training_compute_(flop)": 5.1741608015e+22,
                "training_power_draw_(w)": 353146.6076498626,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 287.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-09-03T00:00:00",
                "link": "https://arxiv.org/abs/2409.02060v1",
                "reference": "OLMoE: Open Mixture-of-Experts Language Models",
                "organization": "Allen Institute for AI,Contextual AI,University of Washington,Princeton University",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "OLMoE - 5.17e+22 FLOPs"
            },
            {
                "model": "Alphaflow",
                "training_compute_(flop)": 1.64975616e+21,
                "training_power_draw_(w)": 6306.329858756263,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2024-09-02T00:00:00",
                "link": "https://arxiv.org/abs/2402.04845",
                "reference": "AlphaFold Meets Flow Matching for Generating Protein Ensembles",
                "organization": "Massachusetts Institute of Technology (MIT)",
                "parameters": null,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "Alphaflow - 1.65e+21 FLOPs"
            },
            {
                "model": "ESMFlow",
                "training_compute_(flop)": 7.6197888e+20,
                "training_power_draw_(w)": 6306.329858756263,
                "training_dataset_size_(gradients)": 552960000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2024-09-02T00:00:00",
                "link": "https://arxiv.org/abs/2402.04845",
                "reference": "AlphaFold Meets Flow Matching for Generating Protein Ensembles",
                "organization": "Massachusetts Institute of Technology (MIT)",
                "parameters": null,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "ESMFlow - 7.62e+20 FLOPs"
            },
            {
                "model": "GLM-4-Plus",
                "training_compute_(flop)": 3.5999999999999997e+25,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 32706623.53977531,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-08-29T00:00:00",
                "link": "https://bigmodel.cn/dev/howuse/glm-4",
                "reference": "GLM-4-Plus",
                "organization": "Zhipu AI",
                "parameters": null,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "API access",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "GLM-4-Plus - 3.60e+25 FLOPs"
            },
            {
                "model": "GLA Transformer 1.3B",
                "training_compute_(flop)": 7.8e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 100000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-08-27T00:00:00",
                "link": "https://arxiv.org/abs/2312.06635",
                "reference": "Gated Linear Attention Transformers with Hardware-Efficient Training",
                "organization": "MIT-IBM Watson AI Lab,Massachusetts Institute of Technology (MIT)",
                "parameters": 1300000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "GLA Transformer 1.3B - 7.80e+20 FLOPs"
            },
            {
                "model": "GLA Transformer 340M",
                "training_compute_(flop)": 3.06e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 15000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-08-27T00:00:00",
                "link": "https://arxiv.org/abs/2312.06635",
                "reference": "Gated Linear Attention Transformers with Hardware-Efficient Training",
                "organization": "MIT-IBM Watson AI Lab,Massachusetts Institute of Technology (MIT)",
                "parameters": 340000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "GLA Transformer 340M - 3.06e+19 FLOPs"
            },
            {
                "model": "Pharia-1-LLM-7B",
                "training_compute_(flop)": 4.43e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-08-26T00:00:00",
                "link": "https://huggingface.co/Aleph-Alpha/Pharia-1-LLM-7B-control",
                "reference": "Introducing Pharia-1-LLM: transparent and compliant",
                "organization": "Aleph Alpha",
                "parameters": 7041544704.0,
                "notable_model": false,
                "country": "Germany",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Germany",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Pharia-1-LLM-7B - 4.43e+23 FLOPs"
            },
            {
                "model": "DISTRO",
                "training_compute_(flop)": 7.1497946e+20,
                "training_power_draw_(w)": 44151.1910097316,
                "training_dataset_size_(gradients)": 100000000000.0,
                "training_time_(hours)": 19.8,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-08-26T00:00:00",
                "link": "https://github.com/NousResearch/DisTrO/blob/main/A_Preliminary_Report_on_DisTrO.pdf",
                "reference": "A PRELIMINARY REPORT ON DISTRO",
                "organization": "Nous Research",
                "parameters": 1200000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "DISTRO - 7.15e+20 FLOPs"
            },
            {
                "model": "Kosmos-2.5",
                "training_compute_(flop)": 2.2018015e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 260000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2024-08-21T00:00:00",
                "link": "https://arxiv.org/abs/2309.11419",
                "reference": "KOSMOS-2.5: A Multimodal Literate Model",
                "organization": "Microsoft",
                "parameters": 1300000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Kosmos-2.5 - 2.20e+21 FLOPs"
            },
            {
                "model": "AntiFormer",
                "training_compute_(flop)": 1.7100000000000148e+18,
                "training_power_draw_(w)": 433.2524599633082,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2024-08-20T00:00:00",
                "link": "https://academic.oup.com/bib/article/25/5/bbae403/7736247",
                "reference": "AntiFormer: graph enhanced large language model for binding affinity prediction ",
                "organization": "University of Florida,Sichuan University,Shihezi University,University of Macau,University of Texas Health Science Center",
                "parameters": 24670596.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "AntiFormer - 1.71e+18 FLOPs"
            },
            {
                "model": "Hybrid-Phi-Mamba-1.5B",
                "training_compute_(flop)": 1.215e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 5000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-08-19T00:00:00",
                "link": "https://arxiv.org/abs/2408.10189",
                "reference": "Transformers to SSMs: Distilling Quadratic Knowledge to Subquadratic Models",
                "organization": "Carnegie Mellon University (CMU),Mohamed bin Zayed University of Artificial Intelligence (MBZUAI),Cartesia",
                "parameters": 1500000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "Hybrid-Phi-Mamba-1.5B - 1.22e+21 FLOPs"
            },
            {
                "model": "CLR_ESP",
                "training_compute_(flop)": 2.11e+17,
                "training_power_draw_(w)": 75.82593457631364,
                "training_dataset_size_(gradients)": 815728.0,
                "training_time_(hours)": 3.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2024-08-16T00:00:00",
                "link": "https://www.biorxiv.org/content/10.1101/2024.08.13.607829v1",
                "reference": "CLR_ESP: Improved enzyme-substrate pair prediction using contrastive learning",
                "organization": "Kansas State University",
                "parameters": null,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "CLR_ESP - 2.11e+17 FLOPs"
            },
            {
                "model": "xGen-MM (BLIP-3)",
                "training_compute_(flop)": 2.4e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-08-16T00:00:00",
                "link": "https://arxiv.org/pdf/2408.08872v1",
                "reference": "xGen-MM (BLIP-3): A Family of Open Large Multimodal\nModels",
                "organization": "Salesforce Research,University of Washington",
                "parameters": 4000000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Multimodal",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "xGen-MM (BLIP-3) - 2.40e+21 FLOPs"
            },
            {
                "model": "Grok-2",
                "training_compute_(flop)": 2.9599999999999996e+25,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 31602310.530835517,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2024-08-13T00:00:00",
                "link": "https://x.ai/blog/grok-2",
                "reference": "Grok-2 Beta Release",
                "organization": "xAI",
                "parameters": null,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "API access",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Grok-2 - 2.96e+25 FLOPs"
            },
            {
                "model": "Falcon Mamba",
                "training_compute_(flop)": 3.9391101e+23,
                "training_power_draw_(w)": 353319.6658035087,
                "training_dataset_size_(gradients)": 5500000000000.0,
                "training_time_(hours)": 1440.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Government",
                "publication_date": "2024-08-12T00:00:00",
                "link": "https://huggingface.co/tiiuae/falcon-mamba-7b",
                "reference": "Falcon Mamba: The First Competitive Attention-free 7B Language Model",
                "organization": "Technology Innovation Institute",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "United Arab Emirates",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Government",
                "access_group": "Open",
                "model_and_compute": "Falcon Mamba - 3.94e+23 FLOPs"
            },
            {
                "model": "P-LLama3",
                "training_compute_(flop)": 7.20000786432e+23,
                "training_power_draw_(w)": 2365.97990493421,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2024-08-12T00:00:00",
                "link": "https://arxiv.org/abs/2408.06396",
                "reference": "Design Proteins Using Large Language Models: Enhancements and Comparative Analyses",
                "organization": "University of Siena",
                "parameters": 8000000000.0,
                "notable_model": false,
                "country": "Italy",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "P-LLama3 - 7.20e+23 FLOPs"
            },
            {
                "model": "EXAONE 3.0",
                "training_compute_(flop)": 4e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 8000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-08-07T00:00:00",
                "link": "https://arxiv.org/abs/2408.03541",
                "reference": "EXAONE 3.0 7.8B Instruction Tuned Language Model",
                "organization": "LG AI Research",
                "parameters": 7820000000.0,
                "notable_model": false,
                "country": "South Korea",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "South Korea",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "EXAONE 3.0 - 4.00e+23 FLOPs"
            },
            {
                "model": "LLaVA-OV-72B",
                "training_compute_(flop)": 3.036551985824e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 38314782000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-08-06T00:00:00",
                "link": "https://arxiv.org/abs/2408.03326",
                "reference": "LLaVA-OneVision: Easy Visual Task Transfer\n",
                "organization": "ByteDance,Nanyang Technological University,Chinese University of Hong Kong (CUHK),Hong Kong University of Science and Technology (HKUST)",
                "parameters": 72000000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "LLaVA-OV-72B - 3.04e+24 FLOPs"
            },
            {
                "model": "Jais-70B ",
                "training_compute_(flop)": 9.654e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 370000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-08-05T00:00:00",
                "link": "https://www.g42.ai/resources/news/g42-launches-jais-70b-and-20-other-ai-models-champion-arabic-natural-language-processing",
                "reference": "G42 launches JAIS 70B and 20 other AI Models to Champion Arabic Natural Language Processing",
                "organization": "G42,Inception G42",
                "parameters": 70000000000.0,
                "notable_model": false,
                "country": "United Arab Emirates",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Jais-70B  - 9.65e+23 FLOPs"
            },
            {
                "model": "EPInformer",
                "training_compute_(flop)": 3.3696e+17,
                "training_power_draw_(w)": 433.43581568898264,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 1.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2024-08-01T00:00:00",
                "link": "https://www.biorxiv.org/content/10.1101/2024.08.01.606099v1",
                "reference": "EPInformer: a scalable deep learning framework for gene expression prediction by integrating promoter-enhancer sequences with multimodal epigenomic data",
                "organization": "The University of Hong Kong,Harvard Medical School",
                "parameters": 447149.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "EPInformer - 3.37e+17 FLOPs"
            },
            {
                "model": "Llama SEA-LION V2 8B",
                "training_compute_(flop)": 7.23e+23,
                "training_power_draw_(w)": 88355.49182491149,
                "training_dataset_size_(gradients)": 48000000000.0,
                "training_time_(hours)": 48.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Government",
                "publication_date": "2024-07-30T00:00:00",
                "link": "https://huggingface.co/aisingapore/Llama-SEA-LION-v2-8B",
                "reference": "SEA-LION V2",
                "organization": "AI Singapore",
                "parameters": 8000000000.0,
                "notable_model": false,
                "country": "Singapore",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Government",
                "access_group": "Open",
                "model_and_compute": "Llama SEA-LION V2 8B - 7.23e+23 FLOPs"
            },
            {
                "model": "AFM-on-device",
                "training_compute_(flop)": 4.5126e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 7588000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-07-29T00:00:00",
                "link": "https://machinelearning.apple.com/research/apple-intelligence-foundation-language-models",
                "reference": "Apple Intelligence Foundation Language Models",
                "organization": "Apple",
                "parameters": 2730000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Hosted access (no API)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "AFM-on-device - 4.51e+23 FLOPs"
            },
            {
                "model": "AFM-server",
                "training_compute_(flop)": 4.3e+24,
                "training_power_draw_(w)": 2746654.7400360717,
                "training_dataset_size_(gradients)": 7400000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-07-29T00:00:00",
                "link": "https://machinelearning.apple.com/research/apple-intelligence-foundation-language-models",
                "reference": "Apple Intelligence Foundation Language Models",
                "organization": "Apple",
                "parameters": null,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Hosted access (no API)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "AFM-server - 4.30e+24 FLOPs"
            },
            {
                "model": "Segment Anything Model 2",
                "training_compute_(flop)": 1.24e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2024-07-29T00:00:00",
                "link": "https://ai.meta.com/research/publications/sam-2-segment-anything-in-images-and-videos/",
                "reference": "SAM 2: Segment Anything in Images and Videos",
                "organization": "Meta AI",
                "parameters": 224400000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Segment Anything Model 2 - 1.24e+22 FLOPs"
            },
            {
                "model": "Florence-2-B (base)",
                "training_compute_(flop)": 5.4310727e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 22879290000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2024-07-29T00:00:00",
                "link": "https://arxiv.org/abs/2311.06242",
                "reference": "Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks",
                "organization": "Microsoft",
                "parameters": 232000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Florence-2-B (base) - 5.43e+21 FLOPs"
            },
            {
                "model": "Florence-2-L (large)",
                "training_compute_(flop)": 1.2406517e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 22879290000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2024-07-29T00:00:00",
                "link": "https://arxiv.org/abs/2311.06242",
                "reference": "Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks",
                "organization": "Microsoft",
                "parameters": 771000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Florence-2-L (large) - 1.24e+22 FLOPs"
            },
            {
                "model": "Mistral Large 2",
                "training_compute_(flop)": 2.13e+25,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-07-24T00:00:00",
                "link": "https://mistral.ai/news/mistral-large-2407/",
                "reference": "Top-tier reasoning for high-complexity tasks, for your most sophisticated needs.",
                "organization": "Mistral AI",
                "parameters": 123000000000.0,
                "notable_model": true,
                "country": "France",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Mistral Large 2 - 2.13e+25 FLOPs"
            },
            {
                "model": "Llama 3.1-405B",
                "training_compute_(flop)": 3.8e+25,
                "training_power_draw_(w)": 22622532.159299143,
                "training_dataset_size_(gradients)": 15600000000000.0,
                "training_time_(hours)": 2142.0,
                "training_compute_cost_(2023_usd)": 52885433.95402246,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-07-23T00:00:00",
                "link": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/",
                "reference": "The Llama 3 Herd of Models",
                "organization": "Meta AI",
                "parameters": 405000000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Llama 3.1-405B - 3.80e+25 FLOPs"
            },
            {
                "model": "Llama 3.1-70B",
                "training_compute_(flop)": 7.929e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 15000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-07-23T00:00:00",
                "link": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/",
                "reference": "The Llama 3 Herd of Models",
                "organization": "Meta AI",
                "parameters": 70000000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Llama 3.1-70B - 7.93e+24 FLOPs"
            },
            {
                "model": "Llama 3.1-8B",
                "training_compute_(flop)": 1.224e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-07-23T00:00:00",
                "link": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/",
                "reference": "The Llama 3 Herd of Models",
                "organization": "Meta AI",
                "parameters": 8000000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Llama 3.1-8B - 1.22e+24 FLOPs"
            },
            {
                "model": "PepPrCLIP",
                "training_compute_(flop)": 1e+18,
                "training_power_draw_(w)": 433.5323498683974,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2024-07-22T00:00:00",
                "link": "https://pubmed.ncbi.nlm.nih.gov/39091799/",
                "reference": "De Novo Design of Peptide Binders to Conformationally Diverse Targets with Contrastive Language Modeling",
                "organization": "Duke University,Cornell University,Sanford Burnham Prebys Institute",
                "parameters": null,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "PepPrCLIP - 1.00e+18 FLOPs"
            },
            {
                "model": "DCLM 7B",
                "training_compute_(flop)": 1.05e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2500000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-07-20T00:00:00",
                "link": "https://huggingface.co/apple/DCLM-7B",
                "reference": "Model Card for DCLM-Baseline-7B",
                "organization": "Apple",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "DCLM 7B - 1.05e+23 FLOPs"
            },
            {
                "model": "GPT-4o mini",
                "training_compute_(flop)": 7.360010000000001e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2024-07-18T00:00:00",
                "link": "https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/",
                "reference": "GPT-4o mini: advancing cost-efficient intelligence",
                "organization": "OpenAI",
                "parameters": null,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "API access",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "GPT-4o mini - 7.36e+24 FLOPs"
            },
            {
                "model": "OmniGenome",
                "training_compute_(flop)": 3.3900690258459335e+20,
                "training_power_draw_(w)": 7102.366961281501,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2024-07-15T00:00:00",
                "link": "https://arxiv.org/abs/2407.11242",
                "reference": "OmniGenome: Aligning RNA Sequences with Secondary Structures in Genomic Foundation Models",
                "organization": "University of Exeter",
                "parameters": 186000000.0,
                "notable_model": false,
                "country": "United Kingdom",
                "model_accessibility": "Unknown",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "OmniGenome - 3.39e+20 FLOPs"
            },
            {
                "model": "MP4",
                "training_compute_(flop)": 9.6585178e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Industry",
                "publication_date": "2024-07-15T00:00:00",
                "link": "https://310.ai/docs/design/mp",
                "reference": "MP4 for Molecule Programming",
                "organization": "310.ai",
                "parameters": null,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Biology",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "MP4 - 9.66e+22 FLOPs"
            },
            {
                "model": "Deep learning linking mechanistic models to single-cell transcriptomics data reveals transcriptional bursting in response to DNA damage",
                "training_compute_(flop)": 39168000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2024-07-12T00:00:00",
                "link": "https://www.biorxiv.org/content/10.1101/2024.07.10.602845v1",
                "reference": "Deep learning linking mechanistic models to single-cell transcriptomics data reveals transcriptional bursting in response to DNA damage",
                "organization": "Sun Yat-sen University,University of California Irvine,Guangdong Provincial People's Hospital,Guangdong Academy of Medical Sciences",
                "parameters": 2176.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Unknown",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Deep learning linking mechanistic models to single-cell transcriptomics data reveals transcriptional bursting in response to DNA damage - 3.92e+10 FLOPs"
            },
            {
                "model": "PaliGemma",
                "training_compute_(flop)": 1.0652844e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 162.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2024-07-10T00:00:00",
                "link": "https://arxiv.org/abs/2407.07726v1",
                "reference": "PaliGemma: A versatile 3B VLM for transfer",
                "organization": "Google DeepMind",
                "parameters": 3000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "PaliGemma - 1.07e+22 FLOPs"
            },
            {
                "model": "OpenDiLoCo 150M",
                "training_compute_(flop)": 4.152361e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-07-10T00:00:00",
                "link": "https://arxiv.org/abs/2407.07852",
                "reference": "OpenDiLoCo: An Open-Source Framework for Globally Distributed\nLow-Communication Training",
                "organization": "Prime Intellect",
                "parameters": 150000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "OpenDiLoCo 150M - 4.15e+19 FLOPs"
            },
            {
                "model": "OpenDiLoCo 1.1B",
                "training_compute_(flop)": 6.0901294e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-07-10T00:00:00",
                "link": "https://arxiv.org/abs/2407.07852",
                "reference": "OpenDiLoCo: An Open-Source Framework for Globally Distributed\nLow-Communication Training",
                "organization": "Prime Intellect",
                "parameters": 1100000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "OpenDiLoCo 1.1B - 6.09e+20 FLOPs"
            },
            {
                "model": "MAP-Neo",
                "training_compute_(flop)": 1.89e+23,
                "training_power_draw_(w)": 707158.8241182694,
                "training_dataset_size_(gradients)": 4500000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-07-10T00:00:00",
                "link": "https://arxiv.org/abs/2405.19327",
                "reference": "MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series",
                "organization": "University of Waterloo,01.AI,Wuhan University",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "MAP-Neo - 1.89e+23 FLOPs"
            },
            {
                "model": "ESM3 (98B)",
                "training_compute_(flop)": 1.07e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 771000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-06-25T00:00:00",
                "link": "https://www.evolutionaryscale.ai/blog/esm3-release ",
                "reference": "ESM3: Simulating 500 million years of evolution with a language model",
                "organization": "EvolutionaryScale,University of California (UC) Berkeley",
                "parameters": 98500000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "ESM3 (98B) - 1.07e+24 FLOPs"
            },
            {
                "model": "ESM3-open-small",
                "training_compute_(flop)": 2.7e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 48000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-06-25T00:00:00",
                "link": "https://www.evolutionaryscale.ai/blog/esm3-release ",
                "reference": "ESM3: Simulating 500 million years of evolution with a language model",
                "organization": "EvolutionaryScale,University of California (UC) Berkeley",
                "parameters": 1400000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "ESM3-open-small - 2.70e+21 FLOPs"
            },
            {
                "model": "Flexi-JEST++",
                "training_compute_(flop)": 1.26e+21,
                "training_power_draw_(w)": 70739.50833122246,
                "training_dataset_size_(gradients)": 131072000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2024-06-25T00:00:00",
                "link": "https://arxiv.org/abs/2406.17711v1",
                "reference": "Data curation via joint example selection further accelerates multimodal learning",
                "organization": "Google DeepMind",
                "parameters": null,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Flexi-JEST++ - 1.26e+21 FLOPs"
            },
            {
                "model": "JEST++",
                "training_compute_(flop)": 1.9e+21,
                "training_power_draw_(w)": 70739.50833122246,
                "training_dataset_size_(gradients)": 131072000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2024-06-25T00:00:00",
                "link": "https://arxiv.org/abs/2406.17711v1",
                "reference": "Data curation via joint example selection further accelerates multimodal learning",
                "organization": "Google DeepMind",
                "parameters": null,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "JEST++ - 1.90e+21 FLOPs"
            },
            {
                "model": "JEST-L++",
                "training_compute_(flop)": 2e+21,
                "training_power_draw_(w)": 70739.50833122246,
                "training_dataset_size_(gradients)": 98304000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2024-06-25T00:00:00",
                "link": "https://arxiv.org/abs/2406.17711v1",
                "reference": "Data curation via joint example selection further accelerates multimodal learning",
                "organization": "DeepMind",
                "parameters": null,
                "notable_model": false,
                "country": "United Kingdom",
                "model_accessibility": "Unreleased",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "JEST-L++ - 2.00e+21 FLOPs"
            },
            {
                "model": "Gemma 2 9B",
                "training_compute_(flop)": 4.32e+23,
                "training_power_draw_(w)": 1374398.1970781134,
                "training_dataset_size_(gradients)": 8000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-06-24T00:00:00",
                "link": "https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf",
                "reference": "Gemma 2 offers best-in-class performance, runs at incredible speed across different hardware and easily integrates with other AI tools.",
                "organization": "Google DeepMind",
                "parameters": 9000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Gemma 2 9B - 4.32e+23 FLOPs"
            },
            {
                "model": "Gemma 2 27B",
                "training_compute_(flop)": 2.106e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 13000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-06-24T00:00:00",
                "link": "https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf",
                "reference": "Gemma 2 offers best-in-class performance, runs at incredible speed across different hardware and easily integrates with other AI tools.",
                "organization": "Google DeepMind",
                "parameters": 27000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Gemma 2 27B - 2.11e+24 FLOPs"
            },
            {
                "model": "Gemma 2 2B",
                "training_compute_(flop)": 3.12e+22,
                "training_power_draw_(w)": 141482.16734627637,
                "training_dataset_size_(gradients)": 2000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-06-24T00:00:00",
                "link": "https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf",
                "reference": "Gemma 2 offers best-in-class performance, runs at incredible speed across different hardware and easily integrates with other AI tools.",
                "organization": "Google DeepMind",
                "parameters": 2600000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Gemma 2 2B - 3.12e+22 FLOPs"
            },
            {
                "model": "Claude 3.5 Sonnet",
                "training_compute_(flop)": 2.700000000000001e+25,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 25870993.12024943,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2024-06-20T00:00:00",
                "link": "https://www-cdn.anthropic.com/fed9cc193a14b84131812372d8d5857f8f304c52/Model_Card_Claude_3_Addendum.pdf",
                "reference": "Claude 3.5 Sonnet",
                "organization": "Anthropic",
                "parameters": null,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "API access",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Claude 3.5 Sonnet - 2.70e+25 FLOPs"
            },
            {
                "model": "RNA-FrameFlow",
                "training_compute_(flop)": 3.6889344e+18,
                "training_power_draw_(w)": 2763.6312855065025,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 18.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-06-19T00:00:00",
                "link": "https://arxiv.org/abs/2406.13839",
                "reference": "RNA-FrameFlow: Flow Matching for de novo 3D RNA Backbone Design",
                "organization": "National University of Singapore,Prescient Design,University of Missouri,University of Cambridge",
                "parameters": 16800000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "RNA-FrameFlow - 3.69e+18 FLOPs"
            },
            {
                "model": "DeepSeek-Coder-V2 236B",
                "training_compute_(flop)": 1.2852e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-06-17T00:00:00",
                "link": "https://github.com/deepseek-ai/DeepSeek-Coder-V2",
                "reference": "DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence",
                "organization": "DeepSeek",
                "parameters": 236000000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "DeepSeek-Coder-V2 236B - 1.29e+24 FLOPs"
            },
            {
                "model": "Ovis-7B",
                "training_compute_(flop)": 1.7e+23,
                "training_power_draw_(w)": 176880.2801222388,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 15.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-06-17T00:00:00",
                "link": "https://arxiv.org/abs/2405.20797",
                "reference": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model\n",
                "organization": "Alibaba,Nanjing University",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Multimodal",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "Ovis-7B - 1.70e+23 FLOPs"
            },
            {
                "model": "JIUTIAN-139MoE",
                "training_compute_(flop)": 4.3596131e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 1464.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-06-15T00:00:00",
                "link": "https://gitee.com/CMCC-jiutian/JIUTIAN-139MoE?skip_mobile=true",
                "reference": "JIUTIAN-139MOE: TECHNICAL REPORT",
                "organization": "China Mobile",
                "parameters": 38800000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "JIUTIAN-139MoE - 4.36e+23 FLOPs"
            },
            {
                "model": "Nemotron-4 340B",
                "training_compute_(flop)": 1.7999999999999999e+25,
                "training_power_draw_(w)": 8490820.682633882,
                "training_dataset_size_(gradients)": 9000000000000.0,
                "training_time_(hours)": 2200.0,
                "training_compute_cost_(2023_usd)": 21271017.96222655,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-06-14T00:00:00",
                "link": "https://blogs.nvidia.com/blog/nemotron-4-synthetic-data-generation-llm-training/ ",
                "reference": "NVIDIA Releases Open Synthetic Data Generation Pipeline for Training Large Language Models",
                "organization": "NVIDIA",
                "parameters": 340000000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Nemotron-4 340B - 1.80e+25 FLOPs"
            },
            {
                "model": "PLaMo-100B",
                "training_compute_(flop)": 1.1999999999999999e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-06-14T00:00:00",
                "link": "https://tech.preferred.jp/ja/blog/plamo-100b/",
                "reference": "Pre-training of the proprietary LLM \"PLaMo-100B\" with 100 billion parameters",
                "organization": "Preferred Networks Inc",
                "parameters": 100000000000.0,
                "notable_model": false,
                "country": "Japan",
                "model_accessibility": "API access",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Japan",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "PLaMo-100B - 1.20e+24 FLOPs"
            },
            {
                "model": "OpenVLA",
                "training_compute_(flop)": 1.1e+23,
                "training_power_draw_(w)": 50541.724821294745,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 336.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-06-13T00:00:00",
                "link": "https://openvla.github.io/\nhttps://arxiv.org/abs/2406.09246",
                "reference": "OpenVLA: An Open-Source Vision-Language-Action Mode",
                "organization": "Stanford University,University of California (UC) Berkeley,Toyota Research Institute,Google DeepMind,Massachusetts Institute of Technology (MIT),Physical Intelligence",
                "parameters": 7188100000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "OpenVLA - 1.10e+23 FLOPs"
            },
            {
                "model": "Mamba2-Hybrid",
                "training_compute_(flop)": 1.8186e+23,
                "training_power_draw_(w)": 1415199.8102553426,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-06-12T00:00:00",
                "link": "https://arxiv.org/abs/2406.07887",
                "reference": "An Empirical Study of Mamba-based Language Models",
                "organization": "NVIDIA",
                "parameters": 8660000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Mamba2-Hybrid - 1.82e+23 FLOPs"
            },
            {
                "model": "Llama-3.1-Nemotron-70B-Instruct",
                "training_compute_(flop)": 7.929e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 96.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-06-12T00:00:00",
                "link": "https://catalog.ngc.nvidia.com/orgs/nim/teams/nvidia/containers/llama-3.1-nemotron-70b-instruct",
                "reference": "https://www.semanticscholar.org/paper/HelpSteer2%3A-Open-source-dataset-for-training-reward-Wang-Dong/f590d8926dd12345a3bd22253461850f5ca4b3ed",
                "organization": "NVIDIA,Meta AI",
                "parameters": null,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Llama-3.1-Nemotron-70B-Instruct - 7.93e+24 FLOPs"
            },
            {
                "model": "Megrez-3B-Omni",
                "training_compute_(flop)": 3.6e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-06-12T00:00:00",
                "link": "https://github.com/infinigence/Infini-Megrez-Omni/blob/main/assets/Megrez_Omni_Technical_Report.pdf",
                "reference": "Megrez-3B-Omni: The First Open-Source End-Side Full Modality Understanding Model",
                "organization": "Infinigence AI,Tsinghua University,Shanghai Jiao Tong University",
                "parameters": 3000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Multimodal",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "Megrez-3B-Omni - 3.60e+22 FLOPs"
            },
            {
                "model": "Samba 3.8B",
                "training_compute_(flop)": 7.3e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-06-11T00:00:00",
                "link": "https://arxiv.org/abs/2406.07522",
                "reference": "Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling",
                "organization": "Microsoft,University of Illinois Urbana-Champaign (UIUC)",
                "parameters": 3800000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "Samba 3.8B - 7.30e+22 FLOPs"
            },
            {
                "model": "TiTok-L",
                "training_compute_(flop)": 1.7252352e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-06-11T00:00:00",
                "link": "https://arxiv.org/abs/2406.07550",
                "reference": "An Image is Worth 32 Tokens for Reconstruction and Generation",
                "organization": "ByteDance,Technical University of Munich",
                "parameters": 307000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "TiTok-L - 1.73e+21 FLOPs"
            },
            {
                "model": "Qwen2-72B",
                "training_compute_(flop)": 3.02e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 7000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-06-07T00:00:00",
                "link": "https://qwenlm.github.io/blog/qwen2/ \nhttps://arxiv.org/abs/2407.10671 ",
                "reference": "Hello Qwen2",
                "organization": "Alibaba",
                "parameters": 72710000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Qwen2-72B - 3.02e+24 FLOPs"
            },
            {
                "model": "Qwen2-7B",
                "training_compute_(flop)": 2.9400000000001e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 7000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-06-07T00:00:00",
                "link": "https://qwenlm.github.io/blog/qwen2/ \nhttps://arxiv.org/abs/2407.10671 ",
                "reference": "Hello Qwen2",
                "organization": "Alibaba",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Qwen2-7B - 2.94e+23 FLOPs"
            },
            {
                "model": "Qwen2-57B-A14B",
                "training_compute_(flop)": 3.7800000000001e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-06-07T00:00:00",
                "link": "https://qwenlm.github.io/blog/qwen2/ \nhttps://arxiv.org/abs/2407.10671 ",
                "reference": "Hello Qwen2",
                "organization": "Alibaba",
                "parameters": 57000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Qwen2-57B-A14B - 3.78e+23 FLOPs"
            },
            {
                "model": "Qwen2-1.5B",
                "training_compute_(flop)": 6.3e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 7000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-06-07T00:00:00",
                "link": "https://qwenlm.github.io/blog/qwen2/ \nhttps://arxiv.org/abs/2407.10671 ",
                "reference": "Hello Qwen2",
                "organization": "Alibaba",
                "parameters": 1500000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Qwen2-1.5B - 6.30e+22 FLOPs"
            },
            {
                "model": "Qwen2-0.5B",
                "training_compute_(flop)": 3.6e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 12000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-06-07T00:00:00",
                "link": "https://qwenlm.github.io/blog/qwen2/ \nhttps://arxiv.org/abs/2407.10671 ",
                "reference": "Hello Qwen2",
                "organization": "Alibaba",
                "parameters": 500000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Qwen2-0.5B - 3.60e+22 FLOPs"
            },
            {
                "model": "MiniCPM-2.4B",
                "training_compute_(flop)": 1.584e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-06-03T00:00:00",
                "link": "https://arxiv.org/abs/2404.06395",
                "reference": "MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies",
                "organization": "Tsinghua University,ModelBest",
                "parameters": 2442057984.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "MiniCPM-2.4B - 1.58e+22 FLOPs"
            },
            {
                "model": "MiniCPM-1.2B",
                "training_compute_(flop)": 7.92e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-06-03T00:00:00",
                "link": "https://arxiv.org/abs/2404.06395",
                "reference": "MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies",
                "organization": "Tsinghua University,ModelBest",
                "parameters": 1247442432.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "MiniCPM-1.2B - 7.92e+21 FLOPs"
            },
            {
                "model": "MULAN",
                "training_compute_(flop)": 5.1777408e+20,
                "training_power_draw_(w)": 759.5268508808502,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-06-02T00:00:00",
                "link": "https://www.biorxiv.org/content/10.1101/2024.05.30.596565v1",
                "reference": "MULAN: Multimodal Protein Language Model for Sequence and Structure Encoding",
                "organization": "AIRI Artificial Intelligence Research Institute,Skolkovo Institute of Science and Technology,Belozersky Institute of Physio-Chemical Biology,Ligand Pro",
                "parameters": 35000000.0,
                "notable_model": false,
                "country": "Russia",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "MULAN - 5.18e+20 FLOPs"
            },
            {
                "model": "DRGN-AI",
                "training_compute_(flop)": 6.469e+19,
                "training_power_draw_(w)": 3159.6316996643372,
                "training_dataset_size_(gradients)": 11639799808.0,
                "training_time_(hours)": 48.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2024-06-02T00:00:00",
                "link": "https://www.biorxiv.org/content/10.1101/2024.05.30.596729v1",
                "reference": "Revealing biomolecular structure and motion with neural ab initio cryo-EM reconstruction",
                "organization": "Stanford University,SLAC National Laboratory,Princeton University,Columbia University",
                "parameters": null,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "DRGN-AI - 6.47e+19 FLOPs"
            },
            {
                "model": "Mamba 2, 2.7B",
                "training_compute_(flop)": 4.86e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2024-05-31T00:00:00",
                "link": "https://arxiv.org/abs/2405.21060",
                "reference": "Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality",
                "organization": "Princeton University,Carnegie Mellon University (CMU)",
                "parameters": 2700000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "Mamba 2, 2.7B - 4.86e+21 FLOPs"
            },
            {
                "model": "Granite 20B",
                "training_compute_(flop)": 3.0000000000001e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2500000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-05-31T00:00:00",
                "link": "https://www.ibm.com/downloads/documents/us-en/10a99803c92fdb35",
                "reference": "Granite Foundation Models",
                "organization": "IBM Research",
                "parameters": 20000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Granite 20B - 3.00e+23 FLOPs"
            },
            {
                "model": "FoldFlow2",
                "training_compute_(flop)": 7.646000000001e+21,
                "training_power_draw_(w)": 1579.9213978749815,
                "training_dataset_size_(gradients)": 48000000.0,
                "training_time_(hours)": 96.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-05-30T00:00:00",
                "link": "https://arxiv.org/abs/2405.20313",
                "reference": "Sequence-Augmented SE(3)-Flow Matching For Conditional Protein Backbone Generation",
                "organization": "Dreamfold,University of Montreal / Universit\u00e9 de Montr\u00e9al,McGill University,University of Oxford",
                "parameters": null,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "FoldFlow2 - 7.65e+21 FLOPs"
            },
            {
                "model": "CLAY",
                "training_compute_(flop)": 3.1054234e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 360.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-05-30T00:00:00",
                "link": "https://arxiv.org/abs/2406.13897v1",
                "reference": "CLAY: A Controllable Large-scale Generative Model for Creating High-quality 3D Assets",
                "organization": "Shanghai Tech University,Deemos Technology,Huazhong University of Science and Technology",
                "parameters": 1500000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Unreleased",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Multimodal",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "CLAY - 3.11e+22 FLOPs"
            },
            {
                "model": "Aurora",
                "training_compute_(flop)": 4.5287424e+21,
                "training_power_draw_(w)": 25279.86827447104,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 420.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Earth science",
                "organization_categorization": "Industry",
                "publication_date": "2024-05-28T00:00:00",
                "link": "https://arxiv.org/abs/2405.13063",
                "reference": "Aurora: A Foundation Model of the Atmosphere",
                "organization": "Microsoft Research",
                "parameters": 1300000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Aurora - 4.53e+21 FLOPs"
            },
            {
                "model": "Nanbeige2-16B-Chat",
                "training_compute_(flop)": 4.05e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-05-28T00:00:00",
                "link": "https://huggingface.co/Nanbeige/Nanbeige2-16B-Chat",
                "reference": "Nanbeige2-16B-Chat",
                "organization": "Nanbeige LLM Lab",
                "parameters": 15800000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Nanbeige2-16B-Chat - 4.05e+23 FLOPs"
            },
            {
                "model": "Zamba2-7B",
                "training_compute_(flop)": 8.82e+22,
                "training_power_draw_(w)": 176966.95963163028,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-05-26T00:00:00",
                "link": "https://arxiv.org/abs/2405.16712",
                "reference": "Zamba: A Compact 7B SSM Hybrid Model",
                "organization": "Zyphra",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Zamba2-7B - 8.82e+22 FLOPs"
            },
            {
                "model": "Genie 2 (bio)",
                "training_compute_(flop)": 3.234816e+20,
                "training_power_draw_(w)": 6320.530060464735,
                "training_dataset_size_(gradients)": 150673920.0,
                "training_time_(hours)": 120.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2024-05-24T00:00:00",
                "link": "https://arxiv.org/abs/2405.15489",
                "reference": "Out of Many, One: Designing and Scaffolding Proteins at the Scale of the Structural Universe with Genie 2",
                "organization": "Columbia University,Rutgers University",
                "parameters": 15700000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "Genie 2 (bio) - 3.23e+20 FLOPs"
            },
            {
                "model": "OMNI-EPIC",
                "training_compute_(flop)": 2.3e+17,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2024-05-24T00:00:00",
                "link": "https://arxiv.org/abs/2405.15568",
                "reference": "OMNI-EPIC: Open-endedness via Models of human Notions of Interestingness with Environments Programmed in Code",
                "organization": "Imperial College London,University of British Columbia (UBC)",
                "parameters": null,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "OMNI-EPIC - 2.30e+17 FLOPs"
            },
            {
                "model": "YOLOv10-X",
                "training_compute_(flop)": 1.478888e+17,
                "training_power_draw_(w)": 7110.754668295396,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2024-05-23T00:00:00",
                "link": "https://arxiv.org/abs/2405.14458",
                "reference": "YOLOv10: Real-Time End-to-End Object Detection",
                "organization": "Tsinghua University",
                "parameters": 29500000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "YOLOv10-X - 1.48e+17 FLOPs"
            },
            {
                "model": "360Zhinao-7B",
                "training_compute_(flop)": 1.428e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-05-22T00:00:00",
                "link": "https://arxiv.org/abs/2405.13386",
                "reference": "360Zhinao Technical Report",
                "organization": "360 Security Technology",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "360Zhinao-7B - 1.43e+23 FLOPs"
            },
            {
                "model": "ALLaM adapted13B\n",
                "training_compute_(flop)": 1.716e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 5200000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry, Government",
                "publication_date": "2024-05-21T00:00:00",
                "link": "https://arxiv.org/abs/2407.15390\nhttps://huggingface.co/ALLaM-AI/ALLaM-7B-Instruct-previehttps://www.middleeastainews.com/p/sdaias-allam-arabic-llm-live-on-watsonxw",
                "reference": "ALLaM: Large Language Models for Arabic and English\n",
                "organization": "Saudi Data and Artificial Intelligence Authority",
                "parameters": 13000000000.0,
                "notable_model": false,
                "country": "Saudi Arabia",
                "model_accessibility": "API access",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Other",
                "access_group": "Open",
                "model_and_compute": "ALLaM adapted13B\n - 1.72e+23 FLOPs"
            },
            {
                "model": "ALLaM\u00a0adapted 70B",
                "training_compute_(flop)": 1.062e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 600000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry, Government",
                "publication_date": "2024-05-21T00:00:00",
                "link": "https://arxiv.org/abs/2407.15390",
                "reference": "ALLaM: Large Language Models for Arabic and English\n",
                "organization": "Saudi Data and Artificial Intelligence Authority",
                "parameters": 70000000000.0,
                "notable_model": true,
                "country": "Saudi Arabia",
                "model_accessibility": "Unreleased",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Other",
                "access_group": "Closed",
                "model_and_compute": "ALLaM\u00a0adapted 70B - 1.06e+24 FLOPs"
            },
            {
                "model": "ALLaM 7B",
                "training_compute_(flop)": 9.04e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1200000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry, Government",
                "publication_date": "2024-05-21T00:00:00",
                "link": "https://arxiv.org/abs/2407.15390",
                "reference": "ALLaM: Large Language Models for Arabic and English\n",
                "organization": "Saudi Data and Artificial Intelligence Authority",
                "parameters": 7000000000.0,
                "notable_model": true,
                "country": "Saudi Arabia",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Other",
                "access_group": "Open",
                "model_and_compute": "ALLaM 7B - 9.04e+22 FLOPs"
            },
            {
                "model": "ALLaM 34B",
                "training_compute_(flop)": 1.0608e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 5200000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry, Government",
                "publication_date": "2024-05-21T00:00:00",
                "link": "https://arxiv.org/abs/2407.15390",
                "reference": "AI Models for Arabic and English",
                "organization": "Saudi Data and Artificial Intelligence Authority",
                "parameters": 34000000000.0,
                "notable_model": false,
                "country": "Saudi Arabia",
                "model_accessibility": "Unreleased",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Other",
                "access_group": "Closed",
                "model_and_compute": "ALLaM 34B - 1.06e+24 FLOPs"
            },
            {
                "model": "Diamond",
                "training_compute_(flop)": 8.0490594e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 9022.8,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Games",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-05-20T00:00:00",
                "link": "https://arxiv.org/abs/2405.12399",
                "reference": "Diffusion for World Modeling: Visual Details Matter in Atari",
                "organization": "University of Geneva,University of Edinburgh,Microsoft Research",
                "parameters": null,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "Diamond - 8.05e+20 FLOPs"
            },
            {
                "model": "Octo-Base",
                "training_compute_(flop)": 5.85e+20,
                "training_power_draw_(w)": 42983.43309675412,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 14.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Robotics",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-05-20T00:00:00",
                "link": "https://arxiv.org/abs/2405.12213 ",
                "reference": "Octo: An Open-Source Generalist Robot Policy",
                "organization": "University of California (UC) Berkeley,Stanford University,Carnegie Mellon University (CMU),DeepMind",
                "parameters": 93000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "Octo-Base - 5.85e+20 FLOPs"
            },
            {
                "model": "ProSST",
                "training_compute_(flop)": 6.46714368e+20,
                "training_power_draw_(w)": 3950.9471355488704,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 720.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2024-05-17T00:00:00",
                "link": "https://www.biorxiv.org/content/10.1101/2024.04.15.589672v3.abstract",
                "reference": "ProSST: Protein Language Modeling with Quantized Structure and Disentangled Attention",
                "organization": "Shanghai Jiao Tong University,Shanghai AI Lab,East China University of Science and Technology",
                "parameters": 110000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Unknown",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "ProSST - 6.47e+20 FLOPs"
            },
            {
                "model": "Chameleon-34B",
                "training_compute_(flop)": 1.6453571041e+24,
                "training_power_draw_(w)": 2427515.9787339047,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 1394.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2024-05-16T00:00:00",
                "link": "https://arxiv.org/abs/2405.09818v1",
                "reference": "Chameleon: Mixed-Modal Early-Fusion Foundation Models",
                "organization": "Facebook AI Research",
                "parameters": 34000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Chameleon-34B - 1.65e+24 FLOPs"
            },
            {
                "model": "FragLlama: Next-fragment prediction for molecular design",
                "training_compute_(flop)": 3.3399700602e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 4400000000000.0,
                "training_time_(hours)": 836.4,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2024-05-16T00:00:00",
                "link": "https://arxiv.org/abs/2405.09818v1",
                "reference": "Chameleon: Mixed-Modal Early-Fusion Foundation Models",
                "organization": "Facebook AI Research",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "FragLlama: Next-fragment prediction for molecular design - 3.34e+23 FLOPs"
            },
            {
                "model": "LBSTER",
                "training_compute_(flop)": 1.078272e+19,
                "training_power_draw_(w)": 434.1893527126348,
                "training_dataset_size_(gradients)": 3375000000.0,
                "training_time_(hours)": 24.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Industry",
                "publication_date": "2024-05-15T00:00:00",
                "link": "https://www.biorxiv.org/content/10.1101/2024.05.14.594108v1.abstract",
                "reference": "Cramming Protein Language Model Training in 24 GPU Hours",
                "organization": "Prescient Design,Genentech",
                "parameters": 67000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Biology",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "LBSTER - 1.08e+19 FLOPs"
            },
            {
                "model": "Yi-Large",
                "training_compute_(flop)": 1.8e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 3000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-05-13T00:00:00",
                "link": null,
                "reference": null,
                "organization": "01.AI",
                "parameters": 100000000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "API access",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Yi-Large - 1.80e+24 FLOPs"
            },
            {
                "model": "GPT-4o",
                "training_compute_(flop)": 3.810001e+25,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 37428629.72485871,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2024-05-13T00:00:00",
                "link": "https://openai.com/index/hello-gpt-4o/ \nhttps://openai.com/index/gpt-4o-system-card/",
                "reference": "Hello GPT-4o",
                "organization": "OpenAI",
                "parameters": null,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "API access",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "GPT-4o - 3.81e+25 FLOPs"
            },
            {
                "model": "Yi-1.5-34B",
                "training_compute_(flop)": 7.344e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-05-13T00:00:00",
                "link": "https://huggingface.co/01-ai/Yi-1.5-34B",
                "reference": "Yi-1.5 is an upgraded version of Yi, delivering stronger performance in coding, math, reasoning, and instruction-following capability.",
                "organization": "01.AI",
                "parameters": 34000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Yi-1.5-34B - 7.34e+23 FLOPs"
            },
            {
                "model": "Yi-1.5-9B",
                "training_compute_(flop)": 1.90728e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-05-13T00:00:00",
                "link": "https://huggingface.co/01-ai/Yi-1.5-9B",
                "reference": "Yi-1.5 is an upgraded version of Yi.",
                "organization": "01.AI",
                "parameters": 8830000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Yi-1.5-9B - 1.91e+23 FLOPs"
            },
            {
                "model": "MoLeR",
                "training_compute_(flop)": 2.1062592e+18,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Industry",
                "publication_date": "2024-05-12T00:00:00",
                "link": "https://arxiv.org/abs/2103.03864",
                "reference": "Learning to Extend Molecular Scaffolds with Structural Motifs",
                "organization": "Microsoft Research,Novartis",
                "parameters": null,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "MoLeR - 2.11e+18 FLOPs"
            },
            {
                "model": "Fugaku-LLM",
                "training_compute_(flop)": 2.9640000000001e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 380000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-05-10T00:00:00",
                "link": "https://www.fujitsu.com/global/about/resources/news/press-releases/2024/0510-01.html",
                "reference": "Release of \u201cFugaku-LLM\u201d \u2013 a large language model trained on the supercomputer \u201cFugaku\u201d",
                "organization": "Tohoku University,CyberAgent,Tokyo Institute of Technology,Fujitsu,RIKEN,Nagoya University,Kotoba Technologies",
                "parameters": 13000000000.0,
                "notable_model": false,
                "country": "Japan",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Japan",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "Fugaku-LLM - 2.96e+22 FLOPs"
            },
            {
                "model": "MatterSim (M3GNet - MatterSim-v1.0.0-5M)",
                "training_compute_(flop)": 1.62e+16,
                "training_power_draw_(w)": 6322.500926906498,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Materials science",
                "organization_categorization": "Industry",
                "publication_date": "2024-05-10T00:00:00",
                "link": "https://arxiv.org/abs/2405.04967",
                "reference": "MatterSim: A Deep Learning Atomistic Model Across Elements, Temperatures and Pressures",
                "organization": "Microsoft Research AI for Science",
                "parameters": 4500000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "MatterSim (M3GNet - MatterSim-v1.0.0-5M) - 1.62e+16 FLOPs"
            },
            {
                "model": "MatterSim (Grpaphomer)",
                "training_compute_(flop)": 1.118208e+20,
                "training_power_draw_(w)": 50580.00741525198,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Materials science",
                "organization_categorization": "Industry",
                "publication_date": "2024-05-10T00:00:00",
                "link": "https://arxiv.org/abs/2405.04967",
                "reference": "MatterSim: A Deep Learning Atomistic Model Across Elements, Temperatures and Pressures",
                "organization": "Microsoft Research AI for Science",
                "parameters": 182000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "MatterSim (Grpaphomer) - 1.12e+20 FLOPs"
            },
            {
                "model": "Falcon 2 11B",
                "training_compute_(flop)": 3.6e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 5500000000000.0,
                "training_time_(hours)": 1400.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Government",
                "publication_date": "2024-05-09T00:00:00",
                "link": "https://huggingface.co/tiiuae/falcon-11B ",
                "reference": "Falcon2-11B",
                "organization": "Technology Innovation Institute",
                "parameters": 11000000000.0,
                "notable_model": false,
                "country": "United Arab Emirates",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Government",
                "access_group": "Open",
                "model_and_compute": "Falcon 2 11B - 3.60e+23 FLOPs"
            },
            {
                "model": "AlphaFold 3",
                "training_compute_(flop)": 4.1405645e+22,
                "training_power_draw_(w)": 202329.0409413959,
                "training_dataset_size_(gradients)": 29491200000.0,
                "training_time_(hours)": 480.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Industry",
                "publication_date": "2024-05-08T00:00:00",
                "link": "https://www.nature.com/articles/s41586-024-07487-w",
                "reference": "Accurate structure prediction of biomolecular interactions with AlphaFold 3",
                "organization": "Google DeepMind,Isomorphic Labs",
                "parameters": null,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "AlphaFold 3 - 4.14e+22 FLOPs"
            },
            {
                "model": "DeepSeek-V2 (MoE-236B)",
                "training_compute_(flop)": 1.02e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 8100000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-05-07T00:00:00",
                "link": "https://arxiv.org/abs/2405.04434 \nhttps://github.com/deepseek-ai/DeepSeek-V2 ",
                "reference": "DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model",
                "organization": "DeepSeek",
                "parameters": 236000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "DeepSeek-V2 (MoE-236B) - 1.02e+24 FLOPs"
            },
            {
                "model": "xLSTM 1.4B",
                "training_compute_(flop)": 2.56e+18,
                "training_power_draw_(w)": 809334.1869283952,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2024-05-07T00:00:00",
                "link": "https://arxiv.org/abs/2405.04517",
                "reference": "xLSTM: Extended Long Short-Term Memory",
                "organization": "Johannes Kepler University Linz",
                "parameters": 1422600000.0,
                "notable_model": false,
                "country": "Austria",
                "model_accessibility": "Unknown",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "xLSTM 1.4B - 2.56e+18 FLOPs"
            },
            {
                "model": "Microsoft MAI-1",
                "training_compute_(flop)": 1.602828e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-05-06T00:00:00",
                "link": "https://www.theinformation.com/articles/meet-mai-1-microsoft-readies-new-ai-model-to-compete-with-google-openai\n\nhttps://em360tech.com/tech-articles/what-mai-1-deep-dive-microsofts-gpt-4-rival",
                "reference": "Meet MAI-1: Microsoft Readies New AI Model to Compete With Google, OpenAI",
                "organization": "Microsoft",
                "parameters": 500000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Hosted access (no API)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Microsoft MAI-1 - 1.60e+22 FLOPs"
            },
            {
                "model": "VILA1.5-13B",
                "training_compute_(flop)": 2.3003136e+21,
                "training_power_draw_(w)": 101175.78544917757,
                "training_dataset_size_(gradients)": 32430000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-05-03T00:00:00",
                "link": "https://huggingface.co/Efficient-Large-Model/VILA1.5-13b\nhttps://github.com/NVlabs/VILA/tree/bbc609baf326b1b49b93450b48edc516db3737fc/scripts/v1_5/release/13b\nhttps://developer.nvidia.com/blog/visual-language-models-on-nvidia-hardware-with-vila/\n",
                "reference": "VILA: On Pre-training for Visual Language Models",
                "organization": "NVIDIA,Massachusetts Institute of Technology (MIT)",
                "parameters": 13493916736.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "VILA1.5-13B - 2.30e+21 FLOPs"
            },
            {
                "model": "OpenELM-1.1B",
                "training_compute_(flop)": 1.0520327e+22,
                "training_power_draw_(w)": 101178.0385953958,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 240.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-05-02T00:00:00",
                "link": "https://arxiv.org/abs/2404.14619",
                "reference": "OpenELM: An Efficient Language Model Family with Open Training and Inference Framework",
                "organization": "Apple",
                "parameters": 1080000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "OpenELM-1.1B - 1.05e+22 FLOPs"
            },
            {
                "model": "OpenELM-3B",
                "training_compute_(flop)": 3.417119e+22,
                "training_power_draw_(w)": 177061.56754194264,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 288.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-05-02T00:00:00",
                "link": "https://arxiv.org/abs/2404.14619",
                "reference": "OpenELM: An Efficient Language Model Family with Open Training and Inference Framework",
                "organization": "Apple",
                "parameters": 3040000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "OpenELM-3B - 3.42e+22 FLOPs"
            },
            {
                "model": "OpenELM-450M",
                "training_compute_(flop)": 6.3156568e+21,
                "training_power_draw_(w)": 177061.56754194264,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 72.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-05-02T00:00:00",
                "link": "https://arxiv.org/abs/2404.14619",
                "reference": "OpenELM: An Efficient Language Model Family with Open Training and Inference Framework",
                "organization": "Apple",
                "parameters": 450000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "OpenELM-450M - 6.32e+21 FLOPs"
            },
            {
                "model": "OpenELM-270M",
                "training_compute_(flop)": 2.7470309e+21,
                "training_power_draw_(w)": 101178.0385953958,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 72.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-05-02T00:00:00",
                "link": "https://arxiv.org/abs/2404.14619",
                "reference": "OpenELM: An Efficient Language Model Family with Open Training and Inference Framework",
                "organization": "Apple",
                "parameters": 270000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "OpenELM-270M - 2.75e+21 FLOPs"
            },
            {
                "model": "GenCast",
                "training_compute_(flop)": 8.169984e+20,
                "training_power_draw_(w)": 8853.275531781688,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Earth science",
                "organization_categorization": "Industry",
                "publication_date": "2024-05-01T00:00:00",
                "link": "https://arxiv.org/abs/2312.15796",
                "reference": "GenCast: Diffusion-based ensemble forecasting for medium-range weather",
                "organization": "Google DeepMind",
                "parameters": null,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "GenCast - 8.17e+20 FLOPs"
            },
            {
                "model": "Multi-Token Prediction 7B",
                "training_compute_(flop)": 3.841092e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-04-30T00:00:00",
                "link": "https://arxiv.org/abs/2404.19737",
                "reference": "Better & Faster Large Language Models via Multi-token Prediction",
                "organization": "Facebook AI Research",
                "parameters": 6700000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Multi-Token Prediction 7B - 3.84e+23 FLOPs"
            },
            {
                "model": "Multi-Token Prediction 13B",
                "training_compute_(flop)": 1.5364368e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-04-30T00:00:00",
                "link": "https://arxiv.org/abs/2404.19737",
                "reference": "Better & Faster Large Language Models via Multi-token Prediction",
                "organization": "Facebook AI Research",
                "parameters": 13000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Multi-Token Prediction 13B - 1.54e+23 FLOPs"
            },
            {
                "model": "DiffPepBuilder",
                "training_compute_(flop)": 7.667785728001e+21,
                "training_power_draw_(w)": 3952.4431655610742,
                "training_dataset_size_(gradients)": 1489700.0,
                "training_time_(hours)": 120.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2024-04-30T00:00:00",
                "link": "https://arxiv.org/abs/2405.00128",
                "reference": "Target-Specific De Novo Peptide Binder Design with DiffPepBuilder",
                "organization": "Peking University",
                "parameters": 104000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Unknown",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "DiffPepBuilder - 7.67e+21 FLOPs"
            },
            {
                "model": "Arctic",
                "training_compute_(flop)": 3.8347175e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 2000000.0,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-04-24T00:00:00",
                "link": "https://www.snowflake.com/en/blog/arctic-open-efficient-foundation-language-models-snowflake/",
                "reference": "Snowflake Arctic: The Best LLM for Enterprise AI \u2014 Efficiently Intelligent, Truly Open",
                "organization": "Snowflake",
                "parameters": 480000000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Arctic - 3.83e+23 FLOPs"
            },
            {
                "model": "phi-3-mini 3.8B",
                "training_compute_(flop)": 7.524e+22,
                "training_power_draw_(w)": 708388.2341861207,
                "training_dataset_size_(gradients)": 3300000000000.0,
                "training_time_(hours)": 168.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-04-23T00:00:00",
                "link": "https://arxiv.org/abs/2404.14219",
                "reference": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone",
                "organization": "Microsoft",
                "parameters": 3800000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "phi-3-mini 3.8B - 7.52e+22 FLOPs"
            },
            {
                "model": "phi-3-medium 14B",
                "training_compute_(flop)": 4.032e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 4800000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-04-23T00:00:00",
                "link": "https://arxiv.org/abs/2404.14219",
                "reference": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone",
                "organization": "Microsoft",
                "parameters": 14000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "phi-3-medium 14B - 4.03e+23 FLOPs"
            },
            {
                "model": "phi-3-small 7.4B",
                "training_compute_(flop)": 2.1312e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 4800000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-04-23T00:00:00",
                "link": "https://arxiv.org/abs/2404.14219",
                "reference": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone",
                "organization": "Microsoft",
                "parameters": 7400000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "phi-3-small 7.4B - 2.13e+23 FLOPs"
            },
            {
                "model": "phi-3.5-mini",
                "training_compute_(flop)": 3.7101154e+22,
                "training_power_draw_(w)": 708388.2341861207,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 240.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-04-23T00:00:00",
                "link": "https://arxiv.org/abs/2404.14219",
                "reference": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone",
                "organization": "Microsoft",
                "parameters": 3800000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "phi-3.5-mini - 3.71e+22 FLOPs"
            },
            {
                "model": "phi-3.5-Vision",
                "training_compute_(flop)": 8.784e+22,
                "training_power_draw_(w)": 354194.11709306034,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 144.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2024-04-23T00:00:00",
                "link": "https://arxiv.org/abs/2404.14219",
                "reference": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone",
                "organization": "Microsoft",
                "parameters": 4200000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "phi-3.5-Vision - 8.78e+22 FLOPs"
            },
            {
                "model": "Phi-3.5-MoE",
                "training_compute_(flop)": 3.0202896e+23,
                "training_power_draw_(w)": 708388.2341861207,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 552.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-04-23T00:00:00",
                "link": "https://arxiv.org/abs/2404.14219",
                "reference": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone",
                "organization": "Microsoft",
                "parameters": 60800000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Phi-3.5-MoE - 3.02e+23 FLOPs"
            },
            {
                "model": "SaProt",
                "training_compute_(flop)": 6.21084672e+22,
                "training_power_draw_(w)": 50603.66703135999,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 2160.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2024-04-19T00:00:00",
                "link": "https://www.biorxiv.org/content/10.1101/2023.10.01.560349v5.abstract",
                "reference": "SaProt: Protein Language Modeling with Structure-aware Vocabulary",
                "organization": "Zhejiang University (ZJU),Westlake University",
                "parameters": 650000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Unknown",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "SaProt - 6.21e+22 FLOPs"
            },
            {
                "model": "Llama 3-70B",
                "training_compute_(flop)": 7.861e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 15000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-04-18T00:00:00",
                "link": "https://ai.meta.com/blog/meta-llama-3/",
                "reference": "Introducing Meta Llama 3: The most capable openly available LLM to date",
                "organization": "Meta AI",
                "parameters": 70000000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Llama 3-70B - 7.86e+24 FLOPs"
            },
            {
                "model": "Llama 3-8B",
                "training_compute_(flop)": 7.2e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 15000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-04-18T00:00:00",
                "link": "https://ai.meta.com/blog/meta-llama-3/",
                "reference": "Introducing Meta Llama 3: The most capable openly available LLM to date",
                "organization": "Meta AI",
                "parameters": 8000000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Llama 3-8B - 7.20e+23 FLOPs"
            },
            {
                "model": "FRED-T5-XL",
                "training_compute_(flop)": 2.5264222803e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 80000000000.0,
                "training_time_(hours)": 1920.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry, Government",
                "publication_date": "2024-04-18T00:00:00",
                "link": "https://arxiv.org/abs/2309.10931",
                "reference": "A Family of Pretrained Transformer Language Models for Russian",
                "organization": "Sber",
                "parameters": 1740000000.0,
                "notable_model": false,
                "country": "Russia",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Other",
                "access_group": "Open",
                "model_and_compute": "FRED-T5-XL - 2.53e+22 FLOPs"
            },
            {
                "model": "Reka Edge",
                "training_compute_(flop)": 1.89e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2024-04-18T00:00:00",
                "link": "https://arxiv.org/abs/2404.12387",
                "reference": "Reka Core, Flash, and Edge: A Series of Powerful Multimodal Language Models",
                "organization": "Reka AI",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "API access",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Reka Edge - 1.89e+23 FLOPs"
            },
            {
                "model": "Mixtral 8x22B",
                "training_compute_(flop)": 2.34e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-04-17T00:00:00",
                "link": "https://mistral.ai/news/mixtral-8x22b/",
                "reference": "Mixtral 8x22B",
                "organization": "Mistral AI",
                "parameters": 141000000000.0,
                "notable_model": false,
                "country": "France",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Mixtral 8x22B - 2.34e+24 FLOPs"
            },
            {
                "model": "Reka Core",
                "training_compute_(flop)": 8.400010000000001e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2024-04-15T00:00:00",
                "link": "https://arxiv.org/abs/2404.12387",
                "reference": "Reka Core, Flash, and Edge: A Series of Powerful\nMultimodal Language Models",
                "organization": "Reka AI",
                "parameters": 67000000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "API access",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Reka Core - 8.40e+24 FLOPs"
            },
            {
                "model": "Reka Flash",
                "training_compute_(flop)": 6.3e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2024-04-15T00:00:00",
                "link": "https://arxiv.org/abs/2404.12387",
                "reference": "Reka Core, Flash, and Edge: A Series of Powerful\nMultimodal Language Models",
                "organization": "Reka AI",
                "parameters": 21000000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "API access",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Reka Flash - 6.30e+23 FLOPs"
            },
            {
                "model": "DDPM",
                "training_compute_(flop)": 9.000000000000006e+17,
                "training_power_draw_(w)": 325.87415621262454,
                "training_dataset_size_(gradients)": 9541304.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2024-04-13T00:00:00",
                "link": "https://www.biorxiv.org/content/10.1101/2024.04.10.588825v1.abstract",
                "reference": "In Silico Generation of Gene Expression profiles using Diffusion Models",
                "organization": "University Paris-Saclay,Radboud University Medical Center",
                "parameters": null,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "DDPM - 9.00e+17 FLOPs"
            },
            {
                "model": "DDIM",
                "training_compute_(flop)": 9.000000000000006e+17,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 9541304.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2024-04-13T00:00:00",
                "link": "https://www.biorxiv.org/content/10.1101/2024.04.10.588825v1.abstract",
                "reference": "In Silico Generation of Gene Expression profiles using Diffusion Models",
                "organization": "University Paris-Saclay,Radboud University Medical Center",
                "parameters": null,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "DDIM - 9.00e+17 FLOPs"
            },
            {
                "model": "tsuzumi 7B upgrade 2024",
                "training_compute_(flop)": 4.2e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2024-04-11T00:00:00",
                "link": "https://ntt-research.com/2024-upgrade-reality-tsuzumi/",
                "reference": "NTT's Large Language Model \"tsuzumi\" is Here!",
                "organization": "NTT Communication Science Laboratories",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "Japan",
                "model_accessibility": "Unknown",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Japan",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "tsuzumi 7B upgrade 2024 - 4.20e+22 FLOPs"
            },
            {
                "model": "HGRN2 3B",
                "training_compute_(flop)": 1.74e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 100000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-04-11T00:00:00",
                "link": "https://arxiv.org/abs/2404.07904v1",
                "reference": "HGRN2: Gated Linear RNNs with State Expansion",
                "organization": "Shanghai AI Lab,Massachusetts Institute of Technology (MIT),Taptap",
                "parameters": 2900000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "HGRN2 3B - 1.74e+21 FLOPs"
            },
            {
                "model": "HGRN2 1B",
                "training_compute_(flop)": 6.000000001e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 100000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-04-11T00:00:00",
                "link": "https://arxiv.org/abs/2404.07904v1",
                "reference": "HGRN2: Gated Linear RNNs with State Expansion",
                "organization": "Shanghai AI Lab,Massachusetts Institute of Technology (MIT),Taptap",
                "parameters": 1000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "HGRN2 1B - 6.00e+21 FLOPs"
            },
            {
                "model": "DiffBindFR",
                "training_compute_(flop)": 4.000000000000001e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2024-04-09T00:00:00",
                "link": "https://pubs.rsc.org/en/content/articlehtml/2024/sc/d3sc06803j",
                "reference": "DiffBindFR: an SE(3) equivariant network for flexible protein\u2013ligand docking",
                "organization": "Peking University,Tsinghua-Peiking Center for Life Sciences",
                "parameters": null,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "DiffBindFR - 4.00e+20 FLOPs"
            },
            {
                "model": "Stable LM 2 12B",
                "training_compute_(flop)": 2.91e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-04-08T00:00:00",
                "link": "https://stability.ai/news/introducing-stable-lm-2-12b\nhttps://huggingface.co/stabilityai/stablelm-2-12b",
                "reference": "Introducing Stable LM 2 12B",
                "organization": "Stability AI",
                "parameters": 12143605760.0,
                "notable_model": false,
                "country": "United Kingdom",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Stable LM 2 12B - 2.91e+23 FLOPs"
            },
            {
                "model": "YaART",
                "training_compute_(flop)": 8.21376e+19,
                "training_power_draw_(w)": 126540.16145027014,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2024-04-08T00:00:00",
                "link": "https://arxiv.org/abs/2404.05666",
                "reference": "YaART: Yet Another ART Rendering Technology",
                "organization": "Yandex",
                "parameters": 2300000000.0,
                "notable_model": false,
                "country": "Russia",
                "model_accessibility": "API access",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "YaART - 8.21e+19 FLOPs"
            },
            {
                "model": "OpenThaiGPT v1.0.0 (13B)",
                "training_compute_(flop)": 5.11e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2024-04-08T00:00:00",
                "link": "https://openthaigpt.aieat.or.th/openthaigpt-1.0.0-less-than-8-apr-2024-greater-than",
                "reference": "OpenThaiGPT 13b 1.0.0",
                "organization": "Mahidol University,AI Entrepreneurs Association of Thailand",
                "parameters": 13100000000.0,
                "notable_model": false,
                "country": "Thailand",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "OpenThaiGPT v1.0.0 (13B) - 5.11e+21 FLOPs"
            },
            {
                "model": "OpenThaiGPT v1.0.0 (7B)",
                "training_compute_(flop)": 2.66e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2024-04-08T00:00:00",
                "link": "https://openthaigpt.aieat.or.th/openthaigpt-1.0.0-less-than-8-apr-2024-greater-than",
                "reference": "OpenThaiGPT 7b 1.0.0",
                "organization": "Mahidol University,AI Entrepreneurs Association of Thailand",
                "parameters": 6810000000.0,
                "notable_model": false,
                "country": "Thailand",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "OpenThaiGPT v1.0.0 (7B) - 2.66e+21 FLOPs"
            },
            {
                "model": "SambaLingo-Thai-Chat (7B)",
                "training_compute_(flop)": 8.569999999999999e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-04-08T00:00:00",
                "link": "https://arxiv.org/abs/2404.05829",
                "reference": "SambaLingo: Teaching Large Language Models New Languages",
                "organization": "SambaNova Systems, Inc",
                "parameters": 6950000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "SambaLingo-Thai-Chat (7B) - 8.57e+22 FLOPs"
            },
            {
                "model": "SambaLingo-Thai-Chat-70B",
                "training_compute_(flop)": 8.1168e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-04-08T00:00:00",
                "link": "https://arxiv.org/abs/2404.05829",
                "reference": "SambaLingo: Teaching Large Language Models New Languages",
                "organization": "SambaNova Systems, Inc",
                "parameters": 70000000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "SambaLingo-Thai-Chat-70B - 8.12e+23 FLOPs"
            },
            {
                "model": "ESM-AA",
                "training_compute_(flop)": 7.28e+20,
                "training_power_draw_(w)": 12654.861564218108,
                "training_dataset_size_(gradients)": 1143750000.0,
                "training_time_(hours)": 72.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-04-05T00:00:00",
                "link": "https://arxiv.org/abs/2403.12995",
                "reference": "ESM All-Atom: Multi-scale Protein Language Model for Unified Molecular Modeling",
                "organization": "Peking University,Nanjing University,Tsinghua University,PharMolix",
                "parameters": 35000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Unknown",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "ESM-AA - 7.28e+20 FLOPs"
            },
            {
                "model": "Viking",
                "training_compute_(flop)": 2.574e+23,
                "training_power_draw_(w)": 1012411.470653334,
                "training_dataset_size_(gradients)": 2000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-04-04T00:00:00",
                "link": "https://huggingface.co/LumiOpen/Viking-33B",
                "reference": "Viking 33B is a 33B parameter decoder-only transformer pretrained on Finnish, English, Swedish, Danish, Norwegian, Icelandic and code. It is being trained on 2 trillion tokens (700B billion as of this release). Viking 33B is a fully open source model and is made available under the Apache 2.0 License.",
                "organization": "Silo AI,University of Turku",
                "parameters": 33000000000.0,
                "notable_model": false,
                "country": "Finland",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "Viking - 2.57e+23 FLOPs"
            },
            {
                "model": "Sailor-7B-Chat",
                "training_compute_(flop)": 1.7726e+23,
                "training_power_draw_(w)": 50620.5735326667,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2024-04-04T00:00:00",
                "link": "https://arxiv.org/abs/2404.03608",
                "reference": "Sailor: Open Language Models for South-East Asia",
                "organization": "Sea AI Lab,Singapore University of Technology & Design",
                "parameters": 7720000000.0,
                "notable_model": false,
                "country": "Singapore",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "Sailor-7B-Chat - 1.77e+23 FLOPs"
            },
            {
                "model": "Mixture-of-Depths",
                "training_compute_(flop)": 1e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-04-02T00:00:00",
                "link": "https://arxiv.org/abs/2404.02258",
                "reference": "Mixture-of-Depths: Dynamically allocating compute in transformer-based language models",
                "organization": "Google DeepMind,McGill University,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms)",
                "parameters": 3000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "Mixture-of-Depths - 1.00e+20 FLOPs"
            },
            {
                "model": "MobileCLIP-B (LT)",
                "training_compute_(flop)": 1.3423599e+22,
                "training_power_draw_(w)": 202495.82204270296,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2024-04-01T00:00:00",
                "link": "https://arxiv.org/abs/2311.17049",
                "reference": "MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training",
                "organization": "Apple",
                "parameters": 149700000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "MobileCLIP-B (LT) - 1.34e+22 FLOPs"
            },
            {
                "model": "TeleChat-7B",
                "training_compute_(flop)": 4.2e+22,
                "training_power_draw_(w)": 506239.5551067574,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-04-01T00:00:00",
                "link": "https://arxiv.org/abs/2401.03804",
                "reference": "TELECHAT TECHNICAL REPORT",
                "organization": "China Telecom",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "TeleChat-7B - 4.20e+22 FLOPs"
            },
            {
                "model": "TeleChat-3B",
                "training_compute_(flop)": 1.44e+22,
                "training_power_draw_(w)": 506239.5551067574,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-04-01T00:00:00",
                "link": "https://arxiv.org/abs/2401.03804",
                "reference": "TELECHAT TECHNICAL REPORT",
                "organization": "China Telecom",
                "parameters": 3000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Unreleased",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "TeleChat-3B - 1.44e+22 FLOPs"
            },
            {
                "model": "TeleChat-12B",
                "training_compute_(flop)": 8.64e+22,
                "training_power_draw_(w)": 506239.5551067574,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-04-01T00:00:00",
                "link": "https://arxiv.org/abs/2401.03804",
                "reference": "TELECHAT TECHNICAL REPORT",
                "organization": "China Telecom",
                "parameters": 12000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "TeleChat-12B - 8.64e+22 FLOPs"
            },
            {
                "model": "Grok-1.5",
                "training_compute_(flop)": 9.26e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-03-28T00:00:00",
                "link": "https://x.ai/blog/grok-1.5",
                "reference": "Introducing Grok-1.5, our latest model capable of long context understanding and advanced reasoning. Grok-1.5 will be available to our early testers and existing Grok users on the \ud835\udd4f platform in the coming days.",
                "organization": "xAI",
                "parameters": null,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Hosted access (no API)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Grok-1.5 - 9.26e+24 FLOPs"
            },
            {
                "model": "DBRX",
                "training_compute_(flop)": 2.6e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 12000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-03-27T00:00:00",
                "link": "https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm",
                "reference": "Introducing DBRX: A New State-of-the-Art Open LLM",
                "organization": "Databricks",
                "parameters": 132000000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "DBRX - 2.60e+24 FLOPs"
            },
            {
                "model": "ProstT5",
                "training_compute_(flop)": 3.09999999999999e+21,
                "training_power_draw_(w)": 6329.121902923846,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 864.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2024-03-24T00:00:00",
                "link": "https://www.biorxiv.org/content/10.1101/2023.07.23.550085v2.abstract",
                "reference": "Bilingual Language Model for Protein Sequence and Structure",
                "organization": "Technical University of Munich,Seoul National University,Institute for Advanced Study,TUM School of Life Sciences Weihenstephan",
                "parameters": 3000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "ProstT5 - 3.10e+21 FLOPs"
            },
            {
                "model": "Stable Video 3D (SV3D)",
                "training_compute_(flop)": 5.1757056e+20,
                "training_power_draw_(w)": 25319.870531268905,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 144.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2024-03-18T00:00:00",
                "link": "https://arxiv.org/abs/2403.12008",
                "reference": "SV3D: Novel Multi-view Synthesis and 3D Generation from a Single Image using Latent Video Diffusion",
                "organization": "Stability AI",
                "parameters": null,
                "notable_model": false,
                "country": "United Kingdom",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Stable Video 3D (SV3D) - 5.18e+20 FLOPs"
            },
            {
                "model": "ERNIE-RNA",
                "training_compute_(flop)": 2.1000000000000013e+21,
                "training_power_draw_(w)": 11868.95362272414,
                "training_dataset_size_(gradients)": 918000000.0,
                "training_time_(hours)": 480.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-03-17T00:00:00",
                "link": "https://www.biorxiv.org/content/10.1101/2024.03.17.585376v1.abstract",
                "reference": "ERNIE-RNA: An RNA Language Model with Structure-enhanced Representations",
                "organization": "Microsoft Research,Syngentech,Tsinghua University",
                "parameters": 86000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "ERNIE-RNA - 2.10e+21 FLOPs"
            },
            {
                "model": "MM1-30B",
                "training_compute_(flop)": 4.86e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 186800000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2024-03-14T00:00:00",
                "link": "https://arxiv.org/abs/2403.09611",
                "reference": "MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training",
                "organization": "Apple",
                "parameters": 30000000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "MM1-30B - 4.86e+23 FLOPs"
            },
            {
                "model": "RFM-1",
                "training_compute_(flop)": 2.4e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2024-03-11T00:00:00",
                "link": "https://covariant.ai/insights/introducing-rfm-1-giving-robots-human-like-reasoning-capabilities/",
                "reference": "Introducing RFM-1: Giving robots human-like reasoning capabilities",
                "organization": "Covariant",
                "parameters": 8000000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "RFM-1 - 2.40e+20 FLOPs"
            },
            {
                "model": "DeepSeek-VL-7B",
                "training_compute_(flop)": 1.0264959e+23,
                "training_power_draw_(w)": 405208.1557620524,
                "training_dataset_size_(gradients)": 400000000.0,
                "training_time_(hours)": 120.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2024-03-08T00:00:00",
                "link": "https://arxiv.org/abs/2403.05525",
                "reference": "DeepSeek-VL: Towards Real-World Vision-Language Understanding",
                "organization": "DeepSeek",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "DeepSeek-VL-7B - 1.03e+23 FLOPs"
            },
            {
                "model": "DeepSeek-VL-1.3B",
                "training_compute_(flop)": 8.6547326e+21,
                "training_power_draw_(w)": 101302.0389405131,
                "training_dataset_size_(gradients)": 400000000.0,
                "training_time_(hours)": 168.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2024-03-08T00:00:00",
                "link": "https://arxiv.org/abs/2403.05525",
                "reference": "DeepSeek-VL: Towards Real-World Vision-Language Understanding",
                "organization": "DeepSeek",
                "parameters": 1300000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "DeepSeek-VL-1.3B - 8.65e+21 FLOPs"
            },
            {
                "model": "Inflection-2.5",
                "training_compute_(flop)": 8.000001e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 11804593.33,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-03-07T00:00:00",
                "link": "https://inflection.ai/inflection-2-5",
                "reference": "Inflection-2.5: meet the world's best personal AI",
                "organization": "Inflection AI",
                "parameters": null,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Hosted access (no API)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Inflection-2.5 - 8.00e+24 FLOPs"
            },
            {
                "model": "Claude 3 Opus",
                "training_compute_(flop)": 1.640001e+25,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 17106058.0886546,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2024-03-04T00:00:00",
                "link": "https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf",
                "reference": "The Claude 3 Model Family: Opus, Sonnet, Haiku",
                "organization": "Anthropic",
                "parameters": null,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "API access",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Claude 3 Opus - 1.64e+25 FLOPs"
            },
            {
                "model": "Aramco Metabrain AI",
                "training_compute_(flop)": 1.05e+25,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 7000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry, Government",
                "publication_date": "2024-03-04T00:00:00",
                "link": "https://www.offshore-technology.com/news/saudi-aramco-unveils-industry-first-generative-ai-model/",
                "reference": "Saudi Aramco unveils industry\u2019s first generative AI model",
                "organization": "Saudi Aramco",
                "parameters": 250000000000.0,
                "notable_model": true,
                "country": "Saudi Arabia",
                "model_accessibility": "Unreleased",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Other",
                "access_group": "Closed",
                "model_and_compute": "Aramco Metabrain AI - 1.05e+25 FLOPs"
            },
            {
                "model": "MACE-MP-0",
                "training_compute_(flop)": 8.76096e+20,
                "training_power_draw_(w)": 63323.64481276617,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Materials science",
                "organization_categorization": "Academia, Industry, Government",
                "publication_date": "2024-03-01T00:00:00",
                "link": "https://arxiv.org/abs/2401.00096",
                "reference": "A foundation model for atomistic materials chemistry",
                "organization": "University of Cambridge,Federal Institute of Materials Research and Testing (BAM),NERSC, Lawrence Berkeley National Laboratory,University of British Columbia (UBC),Friedrich Schiller University Jena,University of Bayreuth,Fritz Haber Institute of the Max Planck Society,U. S. Naval Research Laboratory,Chemix,Daresbury Laboratory,BASF,University of South Carolina,University of Stuttgart,Uppsala University,Newcastle University,Technical University of Denmark,Aix-Marseille Universit\u00e9,University of Warwick,University of California Los Angeles (UCLA),InstaDeep,University of California (UC) Berkeley",
                "parameters": null,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Academia, Industry, Government",
                "access_group": "Open",
                "model_and_compute": "MACE-MP-0 - 8.76e+20 FLOPs"
            },
            {
                "model": "StarCoder 2 15B",
                "training_compute_(flop)": 3.87e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 913230000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-02-29T00:00:00",
                "link": "https://arxiv.org/abs/2402.19173",
                "reference": "StarCoder 2 and The Stack v2: The Next Generation",
                "organization": "Hugging Face,ServiceNow,NVIDIA,BigCode",
                "parameters": 15000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "StarCoder 2 15B - 3.87e+23 FLOPs"
            },
            {
                "model": "StarCoder 2 7B",
                "training_compute_(flop)": 1.55e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 658580000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-02-29T00:00:00",
                "link": "https://arxiv.org/abs/2402.19173",
                "reference": "StarCoder 2 and The Stack v2: The Next Generation",
                "organization": "Hugging Face,ServiceNow,NVIDIA,BigCode",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "StarCoder 2 7B - 1.55e+23 FLOPs"
            },
            {
                "model": "StarCoder 2 3B",
                "training_compute_(flop)": 5.94e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 622090000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-02-29T00:00:00",
                "link": "https://arxiv.org/abs/2402.19173",
                "reference": "StarCoder 2 and The Stack v2: The Next Generation",
                "organization": "Hugging Face,ServiceNow,NVIDIA,BigCode",
                "parameters": 3000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "StarCoder 2 3B - 5.94e+22 FLOPs"
            },
            {
                "model": "Griffin",
                "training_compute_(flop)": 1.5848931924611137e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 300000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-02-29T00:00:00",
                "link": "https://arxiv.org/abs/2402.19427",
                "reference": "Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models",
                "organization": "Google DeepMind",
                "parameters": 14000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Griffin - 1.58e+22 FLOPs"
            },
            {
                "model": "Hawk",
                "training_compute_(flop)": 3.95e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 300000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-02-29T00:00:00",
                "link": "https://arxiv.org/abs/2402.19427",
                "reference": "Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models",
                "organization": "Google DeepMind",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Hawk - 3.95e+21 FLOPs"
            },
            {
                "model": "YOLOv9-E",
                "training_compute_(flop)": 1.74258e+17,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2024-02-29T00:00:00",
                "link": "https://arxiv.org/abs/2402.13616",
                "reference": "YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information",
                "organization": "Academia Sinica,National Taipei University of Technology,Chung Yuan Christian University",
                "parameters": 57300000.0,
                "notable_model": false,
                "country": "Taiwan",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "YOLOv9-E - 1.74e+17 FLOPs"
            },
            {
                "model": "RiNALMo",
                "training_compute_(flop)": 1.0500000000000004e+21,
                "training_power_draw_(w)": 5540.942313044689,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 336.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Government",
                "publication_date": "2024-02-29T00:00:00",
                "link": "https://arxiv.org/abs/2403.00043",
                "reference": "RiNALMo: General-Purpose RNA Language Models Can Generalize Well on Structure Prediction Tasks",
                "organization": "University of Zagreb,Genome Institute of Singapore,Bioinformatics Institute",
                "parameters": 650000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Other",
                "access_group": "Closed",
                "model_and_compute": "RiNALMo - 1.05e+21 FLOPs"
            },
            {
                "model": "Evo",
                "training_compute_(flop)": 2.0000000001e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 300000000000.0,
                "training_time_(hours)": 2968.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-02-27T00:00:00",
                "link": "https://arcinstitute.org/news/blog/evo\nhttps://www.biorxiv.org/content/10.1101/2024.02.27.582234v1",
                "reference": "Sequence modeling and design from molecular to genome scale with Evo",
                "organization": "Stanford University,University of California (UC) Berkeley,Together",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "Evo - 2.00e+22 FLOPs"
            },
            {
                "model": "BitNet b1.58",
                "training_compute_(flop)": 2.8735486e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 100000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-02-27T00:00:00",
                "link": "https://arxiv.org/abs/2402.17764",
                "reference": "The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits",
                "organization": "University of Chinese Academy of Sciences,Microsoft Research",
                "parameters": 70000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "BitNet b1.58 - 2.87e+22 FLOPs"
            },
            {
                "model": "Nemotron-4 15B",
                "training_compute_(flop)": 7.5005116e+23,
                "training_power_draw_(w)": 4255633.2327503795,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 312.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-02-27T00:00:00",
                "link": "https://arxiv.org/abs/2402.16819",
                "reference": "Nemotron-4 15B Technical Report",
                "organization": "NVIDIA",
                "parameters": 15000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Nemotron-4 15B - 7.50e+23 FLOPs"
            },
            {
                "model": "Mistral Large",
                "training_compute_(flop)": 1.1199999999999999e+25,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 2500.0,
                "training_compute_cost_(2023_usd)": 14110111.93,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-02-26T00:00:00",
                "link": "https://mistral.ai/news/mistral-large/",
                "reference": "Mistral Large, our new flagship model",
                "organization": "Mistral AI",
                "parameters": null,
                "notable_model": true,
                "country": "France",
                "model_accessibility": "API access",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Mistral Large - 1.12e+25 FLOPs"
            },
            {
                "model": "ProLLaMA",
                "training_compute_(flop)": 8.412e+22,
                "training_power_draw_(w)": 4749.696433127339,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 264.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2024-02-26T00:00:00",
                "link": "https://arxiv.org/abs/2402.16445",
                "reference": "ProLLaMA: A Protein Language Model for Multi-Task Protein Language Processing",
                "organization": "Peking University,Peng Cheng Laboratory",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Unknown",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "ProLLaMA - 8.41e+22 FLOPs"
            },
            {
                "model": "DecompDiff",
                "training_compute_(flop)": 1.8999999999999996e+19,
                "training_power_draw_(w)": 434.9538858175219,
                "training_dataset_size_(gradients)": 12500000.0,
                "training_time_(hours)": 41.7,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-02-26T00:00:00",
                "link": "https://arxiv.org/abs/2403.07902",
                "reference": "DecompDiff: Diffusion Models with Decomposed Priors for Structure-Based Drug Design",
                "organization": "University of Illinois Urbana-Champaign (UIUC),ByteDance,University of Chinese Academy of Sciences,Chinese Academy of Sciences,Tsinghua University",
                "parameters": null,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "DecompDiff - 1.90e+19 FLOPs"
            },
            {
                "model": "Gemma 1.1 7B Instruct",
                "training_compute_(flop)": 3.0744e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 6000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-02-24T00:00:00",
                "link": "https://huggingface.co/google/gemma-1.1-7b-it",
                "reference": null,
                "organization": "Google",
                "parameters": 8540000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Gemma 1.1 7B Instruct - 3.07e+23 FLOPs"
            },
            {
                "model": "MegaScale (175B)",
                "training_compute_(flop)": 2.7385671436e+23,
                "training_power_draw_(w)": 9728028.18454986,
                "training_dataset_size_(gradients)": 300000000000.0,
                "training_time_(hours)": 42.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-02-23T00:00:00",
                "link": "https://arxiv.org/abs/2402.15627",
                "reference": "MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs",
                "organization": "ByteDance,Peking University",
                "parameters": 175000000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Unreleased",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "MegaScale (175B) - 2.74e+23 FLOPs"
            },
            {
                "model": "MegaScale (530B)",
                "training_compute_(flop)": 9.6910000000001e+23,
                "training_power_draw_(w)": 8866692.355709508,
                "training_dataset_size_(gradients)": 300000000000.0,
                "training_time_(hours)": 117.9,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-02-23T00:00:00",
                "link": "https://arxiv.org/abs/2402.15627",
                "reference": "MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs",
                "organization": "ByteDance,Peking University",
                "parameters": 530000000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Unreleased",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "MegaScale (530B) - 9.69e+23 FLOPs"
            },
            {
                "model": "MegaScale (Production)",
                "training_compute_(flop)": 3.9e+24,
                "training_power_draw_(w)": 9728028.18454986,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 504.0,
                "training_compute_cost_(2023_usd)": 2614019.245,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-02-23T00:00:00",
                "link": "https://arxiv.org/abs/2402.15627",
                "reference": "MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs",
                "organization": "ByteDance,Peking University",
                "parameters": 530000000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Unreleased",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "MegaScale (Production) - 3.90e+24 FLOPs"
            },
            {
                "model": "Genie",
                "training_compute_(flop)": 6.6e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 942000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2024-02-23T00:00:00",
                "link": "https://arxiv.org/abs/2402.15391",
                "reference": "Genie: Generative Interactive Environments",
                "organization": "Google DeepMind",
                "parameters": 10700000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Genie - 6.60e+22 FLOPs"
            },
            {
                "model": "Stable Diffusion 3",
                "training_compute_(flop)": 5e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2024-02-22T00:00:00",
                "link": "https://arxiv.org/abs/2403.03206\nhttps://stability.ai/news/stable-diffusion-3",
                "reference": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
                "organization": "Stability AI",
                "parameters": 8000000000.0,
                "notable_model": true,
                "country": "United Kingdom",
                "model_accessibility": "API access",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Stable Diffusion 3 - 5.00e+22 FLOPs"
            },
            {
                "model": "Gemma 7B",
                "training_compute_(flop)": 3.07e+23,
                "training_power_draw_(w)": 1134987.171306518,
                "training_dataset_size_(gradients)": 6000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-02-21T00:00:00",
                "link": "https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf",
                "reference": "Gemma: Open Models Based on Gemini Research and Technology",
                "organization": "Google DeepMind",
                "parameters": 8538074112.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Gemma 7B - 3.07e+23 FLOPs"
            },
            {
                "model": "Re-Dock",
                "training_compute_(flop)": 7.500000000000002e+19,
                "training_power_draw_(w)": 435.0023192617824,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 168.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2024-02-21T00:00:00",
                "link": "https://arxiv.org/abs/2402.11459",
                "reference": "Re-Dock: Towards Flexible and Realistic Molecular Docking with Diffusion Bridge",
                "organization": "Zhejiang University (ZJU),Westlake University,University of Washington",
                "parameters": null,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Re-Dock - 7.50e+19 FLOPs"
            },
            {
                "model": "Gemma 2B",
                "training_compute_(flop)": 4.5115822e+22,
                "training_power_draw_(w)": 141873.39641331477,
                "training_dataset_size_(gradients)": 3000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-02-21T00:00:00",
                "link": "https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf",
                "reference": "Gemma: Open Models Based on Gemini Research and Technology",
                "organization": "Google DeepMind",
                "parameters": 2506434560.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Gemma 2B - 4.51e+22 FLOPs"
            },
            {
                "model": "Gemini 1.5 Pro",
                "training_compute_(flop)": 1.580001e+25,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 8034059.174468736,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2024-02-15T00:00:00",
                "link": "https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf",
                "reference": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
                "organization": "Google DeepMind",
                "parameters": null,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "API access",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Gemini 1.5 Pro - 1.58e+25 FLOPs"
            },
            {
                "model": "ProtChatGPT",
                "training_compute_(flop)": 7.200353201209069e+23,
                "training_power_draw_(w)": 2771.3350442941023,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 24.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2024-02-15T00:00:00",
                "link": "https://arxiv.org/abs/2402.09649",
                "reference": "ProtChatGPT: Towards Understanding Proteins with Large Language Models",
                "organization": "University of Technology Sydney,Zhejiang University (ZJU)",
                "parameters": 8000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "ProtChatGPT - 7.20e+23 FLOPs"
            },
            {
                "model": "V-JEPA",
                "training_compute_(flop)": 1.6387080192e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 50.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2024-02-15T00:00:00",
                "link": "https://ai.meta.com/research/publications/revisiting-feature-prediction-for-learning-visual-representations-from-video/",
                "reference": "Revisiting Feature Prediction for Learning Visual Representations from Video",
                "organization": "Meta AI",
                "parameters": 630000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "V-JEPA - 1.64e+21 FLOPs"
            },
            {
                "model": "PLAPT",
                "training_compute_(flop)": 3.900846548437782e+22,
                "training_power_draw_(w)": 174.03580522109968,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-02-12T00:00:00",
                "link": "https://www.biorxiv.org/content/10.1101/2024.02.08.575577v3.abstract",
                "reference": "PLAPT: Protein-Ligand Binding Affinity Prediction Using Pretrained Transformers",
                "organization": "Wolfram Research,ASC27,Newport High School,Sanskriti School",
                "parameters": 1474624.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "PLAPT - 3.90e+22 FLOPs"
            },
            {
                "model": "DiscDiff",
                "training_compute_(flop)": 3.399999999999988e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 983040000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2024-02-08T00:00:00",
                "link": "https://arxiv.org/abs/2402.06079",
                "reference": "DiscDiff: Latent Diffusion Model for DNA Sequence Generation",
                "organization": "Imperial College London",
                "parameters": null,
                "notable_model": false,
                "country": "United Kingdom",
                "model_accessibility": "Unknown",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "DiscDiff - 3.40e+19 FLOPs"
            },
            {
                "model": "Distilled Grandmaster",
                "training_compute_(flop)": 1.035671832e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1194960000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Games",
                "organization_categorization": "Industry",
                "publication_date": "2024-02-07T00:00:00",
                "link": "https://arxiv.org/abs/2402.04494",
                "reference": "Grandmaster-Level Chess Without Search",
                "organization": "DeepMind",
                "parameters": 270000000.0,
                "notable_model": false,
                "country": "United Kingdom",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Distilled Grandmaster - 1.04e+22 FLOPs"
            },
            {
                "model": "CARP",
                "training_compute_(flop)": 1.0193977e+22,
                "training_power_draw_(w)": 76028.99774979397,
                "training_dataset_size_(gradients)": 1867500000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Industry",
                "publication_date": "2024-02-06T00:00:00",
                "link": "https://www.cell.com/cell-systems/abstract/S2405-4712(24)00029-2",
                "reference": "Convolutions are competitive with transformers for protein sequence pretraining",
                "organization": "Microsoft Research",
                "parameters": 643000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "CARP - 1.02e+22 FLOPs"
            },
            {
                "model": "DeepSeekMath 7B",
                "training_compute_(flop)": 1.014e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 500000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-02-05T00:00:00",
                "link": "https://arxiv.org/abs/2402.03300",
                "reference": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models",
                "organization": "DeepSeek,Tsinghua University,Peking University",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "DeepSeekMath 7B - 1.01e+23 FLOPs"
            },
            {
                "model": "Qwen1.5-72B",
                "training_compute_(flop)": 1.3e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-02-04T00:00:00",
                "link": "https://qwenlm.github.io/blog/qwen1.5/",
                "reference": "Introducing Qwen1.5",
                "organization": "Alibaba",
                "parameters": 72000000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Qwen1.5-72B - 1.30e+24 FLOPs"
            },
            {
                "model": "Qwen1.5-7B",
                "training_compute_(flop)": 1.68e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-02-04T00:00:00",
                "link": "https://huggingface.co/Qwen/Qwen1.5-7B",
                "reference": "Introducing Qwen1.5",
                "organization": "Alibaba",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Qwen1.5-7B - 1.68e+23 FLOPs"
            },
            {
                "model": "Qwen1.5-14B",
                "training_compute_(flop)": 3.36e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-02-04T00:00:00",
                "link": "https://huggingface.co/Qwen/Qwen1.5-14B",
                "reference": "Introducing Qwen1.5",
                "organization": "Alibaba",
                "parameters": 14000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Qwen1.5-14B - 3.36e+23 FLOPs"
            },
            {
                "model": "OLMo-7B",
                "training_compute_(flop)": 1.0332e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2024-02-01T00:00:00",
                "link": "https://arxiv.org/abs/2402.00838v1",
                "reference": "OLMo: Accelerating the Science of Language Models",
                "organization": "Allen Institute for AI,University of Washington",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "OLMo-7B - 1.03e+23 FLOPs"
            },
            {
                "model": "OLMo-1B",
                "training_compute_(flop)": 1.2e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2024-02-01T00:00:00",
                "link": "https://arxiv.org/abs/2402.00838v1",
                "reference": "OLMo: Accelerating the Science of Language Models",
                "organization": "Allen Institute for AI,University of Washington",
                "parameters": 1000000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "OLMo-1B - 1.20e+22 FLOPs"
            },
            {
                "model": "LLaVA-NeXT-34B (LLaVA-1.6)",
                "training_compute_(flop)": 2.5878528e+20,
                "training_power_draw_(w)": 25346.95016648552,
                "training_dataset_size_(gradients)": 89338000.0,
                "training_time_(hours)": 24.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-01-30T00:00:00",
                "link": "https://llava-vl.github.io/blog/2024-01-30-llava-next/, https://huggingface.co/liuhaotian/llava-v1.6-34b",
                "reference": "LLaVA-NeXT: Improved reasoning, OCR, and world knowledge",
                "organization": "University of Wisconsin Madison,ByteDance,Nanyang Technological University,University of California (UC) Berkeley",
                "parameters": 34750000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "LLaVA-NeXT-34B (LLaVA-1.6) - 2.59e+20 FLOPs"
            },
            {
                "model": "Code Llama-70B",
                "training_compute_(flop)": 1.26e+24,
                "training_power_draw_(w)": 316843.9329176827,
                "training_dataset_size_(gradients)": 3000000000000.0,
                "training_time_(hours)": 6480.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-01-29T00:00:00",
                "link": "https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/\nhttps://arxiv.org/abs/2308.12950",
                "reference": "Code Llama: Open Foundation Models for Code",
                "organization": "Meta AI",
                "parameters": 70000000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Code Llama-70B - 1.26e+24 FLOPs"
            },
            {
                "model": "ProteinStructureTransformer",
                "training_compute_(flop)": 7.616995200001001e+21,
                "training_power_draw_(w)": 5545.139273980643,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 10.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2024-01-26T00:00:00",
                "link": "https://arxiv.org/abs/2401.14819",
                "reference": "ENDOWING PROTEIN LANGUAGE MODELS WITH STRUCTURAL KNOWLEDGE",
                "organization": "Max Planck Institute of Biochemistry",
                "parameters": 1137000000.0,
                "notable_model": false,
                "country": "Germany",
                "model_accessibility": "Unknown",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Germany",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "ProteinStructureTransformer - 7.62e+21 FLOPs"
            },
            {
                "model": "DeepSeek Coder 33B",
                "training_compute_(flop)": 3.96e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-01-25T00:00:00",
                "link": "https://arxiv.org/abs/2401.14196",
                "reference": "DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence",
                "organization": "DeepSeek,Peking University",
                "parameters": 33000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "DeepSeek Coder 33B - 3.96e+23 FLOPs"
            },
            {
                "model": "DeepSeek Coder 1.3B",
                "training_compute_(flop)": 1.56e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-01-25T00:00:00",
                "link": "https://arxiv.org/abs/2401.14196",
                "reference": "DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence",
                "organization": "DeepSeek,Peking University",
                "parameters": 1300000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "DeepSeek Coder 1.3B - 1.56e+22 FLOPs"
            },
            {
                "model": "DeepSeek Coder 6.7B",
                "training_compute_(flop)": 8.04e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-01-25T00:00:00",
                "link": "https://arxiv.org/abs/2401.14196",
                "reference": "DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence",
                "organization": "DeepSeek,Peking University",
                "parameters": 6700000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "DeepSeek Coder 6.7B - 8.04e+22 FLOPs"
            },
            {
                "model": "Yi-VL-34B",
                "training_compute_(flop)": 1.85174e+22,
                "training_power_draw_(w)": 177456.311892851,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 240.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2024-01-23T00:00:00",
                "link": "https://huggingface.co/01-ai/Yi-VL-34B",
                "reference": "Yi Vision Language Model\nBetter Bilingual Multimodal Model",
                "organization": "01.AI",
                "parameters": 34000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Yi-VL-34B - 1.85e+22 FLOPs"
            },
            {
                "model": "StableLM-2-1.6B",
                "training_compute_(flop)": 1.92e+22,
                "training_power_draw_(w)": 405659.59358954936,
                "training_dataset_size_(gradients)": 2000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-01-18T00:00:00",
                "link": "https://huggingface.co/stabilityai/stablelm-2-1_6b",
                "reference": "Stable LM 2 1.6B",
                "organization": "Stability AI",
                "parameters": 1644417024.0,
                "notable_model": false,
                "country": "United Kingdom",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "StableLM-2-1.6B - 1.92e+22 FLOPs"
            },
            {
                "model": "GLM-4 (0116)",
                "training_compute_(flop)": 1.2e+25,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 12956414.177861895,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-01-17T00:00:00",
                "link": "https://arxiv.org/abs/2406.12793\nhttps://zhipuai.cn/en/devday",
                "reference": "ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools",
                "organization": "Zhipu AI",
                "parameters": null,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "API access",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "GLM-4 (0116) - 1.20e+25 FLOPs"
            },
            {
                "model": "DeciCoder-6B",
                "training_compute_(flop)": 7.56e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-01-15T00:00:00",
                "link": "https://huggingface.co/Deci/DeciCoder-6B",
                "reference": " Model Card for DeciCoder-6B",
                "organization": "Deci AI",
                "parameters": 6000000000.0,
                "notable_model": false,
                "country": "Israel",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "DeciCoder-6B - 7.56e+21 FLOPs"
            },
            {
                "model": "OmniNA",
                "training_compute_(flop)": 2.51092992e+21,
                "training_power_draw_(w)": 6338.854622611998,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2024-01-15T00:00:00",
                "link": "https://www.biorxiv.org/content/10.1101/2024.01.14.575543v1.abstract",
                "reference": "OmniNA: A foundation model for nucleotide sequences",
                "organization": "Tianjin Medical University",
                "parameters": 1700000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Unknown",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "OmniNA - 2.51e+21 FLOPs"
            },
            {
                "model": " InternVL",
                "training_compute_(flop)": 1.744956e+23,
                "training_power_draw_(w)": 507108.36980895983,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 800.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-01-15T00:00:00",
                "link": "https://arxiv.org/abs/2312.14238",
                "reference": "InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks",
                "organization": "Shanghai AI Lab,Nanjing University,The University of Hong Kong,Tsinghua University,SenseTime,University of Science and Technology of China (USTC)",
                "parameters": 14000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": " InternVL - 1.74e+23 FLOPs"
            },
            {
                "model": "InternLM2-20B",
                "training_compute_(flop)": 3.12e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2600000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-01-12T00:00:00",
                "link": "https://arxiv.org/abs/2403.17297",
                "reference": "InternLM2 Technical Report",
                "organization": "Shanghai AI Lab,SenseTime,Chinese University of Hong Kong (CUHK),Fudan University",
                "parameters": 20000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "InternLM2-20B - 3.12e+23 FLOPs"
            },
            {
                "model": "DeepSeekMoE-16B",
                "training_compute_(flop)": 3.4e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-01-11T00:00:00",
                "link": "https://arxiv.org/abs/2401.06066",
                "reference": "DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models",
                "organization": "DeepSeek",
                "parameters": 16000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "DeepSeekMoE-16B - 3.40e+22 FLOPs"
            },
            {
                "model": "Stable Code 3B",
                "training_compute_(flop)": 2.106e+22,
                "training_power_draw_(w)": 202870.4528973297,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-01-09T00:00:00",
                "link": "https://huggingface.co/stabilityai/stable-code-3b",
                "reference": "Stable Code 3B",
                "organization": "Stability AI",
                "parameters": 2796431360.0,
                "notable_model": false,
                "country": "United Kingdom",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Stable Code 3B - 2.11e+22 FLOPs"
            },
            {
                "model": "Improved motif-scaffolding with SE(3) flow matching",
                "training_compute_(flop)": 1.6000000000000008e+19,
                "training_power_draw_(w)": 1188.7205317093983,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 144.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2024-01-08T00:00:00",
                "link": "https://arxiv.org/abs/2401.04082",
                "reference": "Improved motif-scaffolding with SE(3) flow matching",
                "organization": "University of Oxford,Massachusetts Institute of Technology (MIT),Microsoft Research AI for Science",
                "parameters": 16800000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "Improved motif-scaffolding with SE(3) flow matching - 1.60e+19 FLOPs"
            },
            {
                "model": "DeepSeek LLM 67B",
                "training_compute_(flop)": 8.04e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-01-05T00:00:00",
                "link": "https://arxiv.org/abs/2401.02954, https://github.com/deepseek-ai/DeepSeek-LLM",
                "reference": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
                "organization": "DeepSeek",
                "parameters": 67000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "DeepSeek LLM 67B - 8.04e+23 FLOPs"
            },
            {
                "model": "DeepSeek-LLM-1.3b-base",
                "training_compute_(flop)": 3.9e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 500000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-01-05T00:00:00",
                "link": "it is only mentioned in Janus 1.3B release https://huggingface.co/deepseek-ai/Janus-1.3B\n\nupd + supposedly in Janus-Pro-1B paper \"In our experiments, we utilize DeepSeek-LLM (1.5B and 7B) [3]\" while there is no 1.5B model mentioned in the linked paper",
                "reference": null,
                "organization": "DeepSeek",
                "parameters": 1300000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Unreleased",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "DeepSeek-LLM-1.3b-base - 3.90e+21 FLOPs"
            },
            {
                "model": "DeepSeek LLM 7B",
                "training_compute_(flop)": 8.4e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2024-01-05T00:00:00",
                "link": "https://arxiv.org/abs/2401.02954, https://github.com/deepseek-ai/DeepSeek-LLM",
                "reference": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
                "organization": "DeepSeek",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "DeepSeek LLM 7B - 8.40e+22 FLOPs"
            },
            {
                "model": "PLLaMa",
                "training_compute_(flop)": 1.60209723904e+23,
                "training_power_draw_(w)": 6340.548796655679,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 59.7,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2024-01-03T00:00:00",
                "link": "https://arxiv.org/abs/2401.01600",
                "reference": "PLLaMa: An Open-source Large Language Model for Plant Science",
                "organization": "University of California Santa Barbara (UCSB),University of Lincoln,Chinese Academy of Agricultural Sciences,Swedish University of Agricultural Sciences",
                "parameters": 13000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2024,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "PLLaMa - 1.60e+23 FLOPs"
            },
            {
                "model": "YaYi 2.0",
                "training_compute_(flop)": 4.77e+23,
                "training_power_draw_(w)": 495487.7674607929,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-12-22T00:00:00",
                "link": "https://arxiv.org/abs/2312.14862v1",
                "reference": "YAYI 2: Multilingual Open-Source Large Language Models\n",
                "organization": "Yayi (Wenge)",
                "parameters": 30000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "YaYi 2.0 - 4.77e+23 FLOPs"
            },
            {
                "model": "nekomata-14b",
                "training_compute_(flop)": 2.5562e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 66000000000.0,
                "training_time_(hours)": 168.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-12-21T00:00:00",
                "link": "https://arxiv.org/abs/2404.01657",
                "reference": "rinna/nekomata-14b",
                "organization": "rinna",
                "parameters": 14200000000.0,
                "notable_model": true,
                "country": "Japan",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Japan",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "nekomata-14b - 2.56e+23 FLOPs"
            },
            {
                "model": "Konan LLM 41B",
                "training_compute_(flop)": 1.722e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 7000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2023-12-15T00:00:00",
                "link": "https://en.konantech.com/en/llm/konanllm\nhttps://techfinch.kr/ai/konan-technology-unveils-konan-llm--its-own-ai-language-model\nhttps://www.dbpia.co.kr/journal/articleDetail?nodeId=NODE11610127 ",
                "reference": "Konan LLM: A Korean Large Language Model",
                "organization": "Konan Technology",
                "parameters": 41000000000.0,
                "notable_model": false,
                "country": "South Korea",
                "model_accessibility": "Hosted access (no API)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "South Korea",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Konan LLM 41B - 1.72e+23 FLOPs"
            },
            {
                "model": "Poro 34B",
                "training_compute_(flop)": 2.052e+23,
                "training_power_draw_(w)": 507469.8741072884,
                "training_dataset_size_(gradients)": 1000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2023-12-14T00:00:00",
                "link": "https://arxiv.org/abs/2404.01856",
                "reference": "Poro 34B and the Blessing of Multilinguality",
                "organization": "High-Performance Language Technologies (HPLT),University of Turku",
                "parameters": 34200000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "Poro 34B - 2.05e+23 FLOPs"
            },
            {
                "model": "CogAgent",
                "training_compute_(flop)": 6.707e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2023-12-14T00:00:00",
                "link": "https://arxiv.org/abs/2312.08914",
                "reference": "CogAgent: A Visual Language Model for GUI Agents",
                "organization": "Tsinghua University,Zhipu AI",
                "parameters": 18000000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Multimodal",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "CogAgent - 6.71e+22 FLOPs"
            },
            {
                "model": "FunSearch",
                "training_compute_(flop)": 3.87e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 0.0,
                "training_time_(hours)": 48.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2023-12-14T00:00:00",
                "link": "https://www.nature.com/articles/s41586-023-06924-6\nhttps://deepmind.google/discover/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/",
                "reference": "Mathematical discoveries from program search with large language models",
                "organization": "Google DeepMind",
                "parameters": 15000000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "FunSearch - 3.87e+23 FLOPs"
            },
            {
                "model": "Phi-2",
                "training_compute_(flop)": 2.27e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 250000000000.0,
                "training_time_(hours)": 336.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-12-12T00:00:00",
                "link": "https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/\nhttps://huggingface.co/microsoft/phi-2 ",
                "reference": "Phi-2: The surprising power of small language models",
                "organization": "Microsoft",
                "parameters": 2700000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Phi-2 - 2.27e+22 FLOPs"
            },
            {
                "model": "VILA-13B",
                "training_compute_(flop)": 2.3003136e+21,
                "training_power_draw_(w)": 101498.495336141,
                "training_dataset_size_(gradients)": 32430000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2023-12-12T00:00:00",
                "link": "https://arxiv.org/abs/2312.07533\nhttps://huggingface.co/Efficient-Large-Model/VILA-13b",
                "reference": "VILA: On Pre-training for Visual Language Models",
                "organization": "NVIDIA,Massachusetts Institute of Technology (MIT)",
                "parameters": 13350839296.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "VILA-13B - 2.30e+21 FLOPs"
            },
            {
                "model": "Mixtral 8x7B",
                "training_compute_(flop)": 7.74e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-12-11T00:00:00",
                "link": "https://mistral.ai/news/mixtral-of-experts/, https://arxiv.org/abs/2401.04088",
                "reference": "Mixtral of experts: A high quality Sparse Mixture-of-Experts.",
                "organization": "Mistral AI",
                "parameters": 46700000000.0,
                "notable_model": true,
                "country": "France",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Mixtral 8x7B - 7.74e+23 FLOPs"
            },
            {
                "model": "ruDalle: Kandinsky 3.0",
                "training_compute_(flop)": 2.014908e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry, Government",
                "publication_date": "2023-12-11T00:00:00",
                "link": "https://arxiv.org/abs/2312.03511",
                "reference": "KANDINSKY 3.0 TECHNICAL REPORT",
                "organization": "Sber",
                "parameters": 11900000000.0,
                "notable_model": false,
                "country": "Russia",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Vision",
                "org_top5": "Other",
                "access_group": "Open",
                "model_and_compute": "ruDalle: Kandinsky 3.0 - 2.01e+20 FLOPs"
            },
            {
                "model": "MBP",
                "training_compute_(flop)": 1.8e+18,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2023-12-11T00:00:00",
                "link": "https://academic.oup.com/bib/article/25/1/bbad451/7469349",
                "reference": "Multi-task bioassay pre-training for protein-ligand binding affinity prediction",
                "organization": "University of Science and Technology of China (USTC),Tencent,Zhejiang University (ZJU)",
                "parameters": null,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Unknown",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "MBP - 1.80e+18 FLOPs"
            },
            {
                "model": "CRYSTALCODER",
                "training_compute_(flop)": 5.55564e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1382000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2023-12-11T00:00:00",
                "link": "https://arxiv.org/abs/2312.06550",
                "reference": "LLM360: Towards Fully Transparent Open-Source LLMs",
                "organization": "Mohamed bin Zayed University of Artificial Intelligence (MBZUAI),Petuum,University of Southern California,Carnegie Mellon University (CMU),University of Illinois Urbana-Champaign (UIUC),University of California San Diego,LLM360",
                "parameters": 6700000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "CRYSTALCODER - 5.56e+22 FLOPs"
            },
            {
                "model": "Amber",
                "training_compute_(flop)": 4.7898069e+22,
                "training_power_draw_(w)": 177626.32242072464,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 600.5,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2023-12-11T00:00:00",
                "link": "https://arxiv.org/abs/2312.06550",
                "reference": "LLM360: Towards Fully Transparent Open-Source LLMs",
                "organization": "Mohamed bin Zayed University of Artificial Intelligence (MBZUAI),Petuum,University of Southern California,Carnegie Mellon University (CMU),University of Illinois Urbana-Champaign (UIUC),University of California San Diego,LLM360",
                "parameters": 6700000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "Amber - 4.79e+22 FLOPs"
            },
            {
                "model": "XVERSE-65B-2",
                "training_compute_(flop)": 1.24800000000001e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2600000000000.0,
                "training_time_(hours)": 4096.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-12-08T00:00:00",
                "link": "https://github.com/xverse-ai/XVERSE-65B/blob/main/README_EN.md",
                "reference": null,
                "organization": "XVERSE Technology,Shenzhen Yuanxiang Technology",
                "parameters": 65000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "XVERSE-65B-2 - 1.25e+24 FLOPs"
            },
            {
                "model": "Llama Guard",
                "training_compute_(flop)": 1.6e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 4096000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-12-07T00:00:00",
                "link": "https://arxiv.org/abs/2312.06674",
                "reference": "Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations",
                "organization": "Meta AI",
                "parameters": 7000000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Llama Guard - 1.60e+23 FLOPs"
            },
            {
                "model": "Gemini 1.0 Ultra",
                "training_compute_(flop)": 5.0000000001e+25,
                "training_power_draw_(w)": 19211950.056161165,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 2400.0,
                "training_compute_cost_(2023_usd)": 29332885.606729873,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2023-12-06T00:00:00",
                "link": "https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf",
                "reference": "Gemini: A Family of Highly Capable Multimodal Models",
                "organization": "Google DeepMind",
                "parameters": null,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "API access",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Gemini 1.0 Ultra - 5.00e+25 FLOPs"
            },
            {
                "model": "Gemini 1.0 Pro",
                "training_compute_(flop)": 1.830001e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2023-12-06T00:00:00",
                "link": "https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf",
                "reference": "Gemini: A Family of Highly Capable Multimodal Models",
                "organization": "Google DeepMind",
                "parameters": null,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "API access",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Gemini 1.0 Pro - 1.83e+24 FLOPs"
            },
            {
                "model": "Mamba-2.8B",
                "training_compute_(flop)": 5.400000000000001e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 300000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2023-12-01T00:00:00",
                "link": "https://arxiv.org/abs/2312.00752",
                "reference": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
                "organization": "Carnegie Mellon University (CMU),Princeton University",
                "parameters": 2800000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "Mamba-2.8B - 5.40e+21 FLOPs"
            },
            {
                "model": "SEA-LION V1 3B",
                "training_compute_(flop)": 2.1893426e+22,
                "training_power_draw_(w)": 190356.3033115072,
                "training_dataset_size_(gradients)": 980000000000.0,
                "training_time_(hours)": 336.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Government",
                "publication_date": "2023-12-01T00:00:00",
                "link": "https://huggingface.co/aisingapore/sea-lion-3b",
                "reference": "SEA-LION V1",
                "organization": "AI Singapore",
                "parameters": 3000000000.0,
                "notable_model": false,
                "country": "Singapore",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Government",
                "access_group": "Open",
                "model_and_compute": "SEA-LION V1 3B - 2.19e+22 FLOPs"
            },
            {
                "model": "SEA-LION V1 7B",
                "training_compute_(flop)": 4.3297598e+22,
                "training_power_draw_(w)": 203046.72353227437,
                "training_dataset_size_(gradients)": 980000000000.0,
                "training_time_(hours)": 528.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Government",
                "publication_date": "2023-12-01T00:00:00",
                "link": "https://huggingface.co/aisingapore/sea-lion-7b",
                "reference": "SEA-LION V1",
                "organization": "AI Singapore",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "Singapore",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Government",
                "access_group": "Open",
                "model_and_compute": "SEA-LION V1 7B - 4.33e+22 FLOPs"
            },
            {
                "model": "Qwen-72B",
                "training_compute_(flop)": 1.3e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 3000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-11-30T00:00:00",
                "link": "https://huggingface.co/Qwen/Qwen-72B",
                "reference": null,
                "organization": "Alibaba",
                "parameters": 72000000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Qwen-72B - 1.30e+24 FLOPs"
            },
            {
                "model": "Granite 13B",
                "training_compute_(flop)": 2.44e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2500000000000.0,
                "training_time_(hours)": 2208.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-11-30T00:00:00",
                "link": "https://www.ibm.com/downloads/cas/X9W4O6BM",
                "reference": "Granite Foundation Models",
                "organization": "IBM",
                "parameters": 13000000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "API access",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Granite 13B - 2.44e+23 FLOPs"
            },
            {
                "model": "Cohere Command Light",
                "training_compute_(flop)": 1.001e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-11-30T00:00:00",
                "link": "https://txt.cohere.com/embed-command-light-fine-tuning-on-amazon-bedrock/",
                "reference": "Cohere\u2019s Embed and Command Light Models with Fine-tuning Now Available on Amazon Bedrock",
                "organization": "Cohere",
                "parameters": 6000000000.0,
                "notable_model": false,
                "country": "Canada",
                "model_accessibility": "API access",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Canada",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Cohere Command Light - 1.00e+22 FLOPs"
            },
            {
                "model": "Yuan 2.0",
                "training_compute_(flop)": 1.78e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-11-27T00:00:00",
                "link": "https://arxiv.org/abs/2311.15786v1",
                "reference": "YUAN 2.0: A Large Language Model with Localized Filtering-based Attention",
                "organization": "Inspur",
                "parameters": 102600000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Yuan 2.0 - 1.78e+23 FLOPs"
            },
            {
                "model": "StripedHyena-Hessian-7B",
                "training_compute_(flop)": 8e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-11-27T00:00:00",
                "link": "https://huggingface.co/togethercomputer/StripedHyena-Hessian-7B\nhttps://github.com/togethercomputer/stripedhyena\nhttps://www.together.ai/blog/stripedhyena-7b",
                "reference": "StripedHyena-Hessian-7B (SH 7B) ",
                "organization": "Together,Nous Research",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "StripedHyena-Hessian-7B - 8.00e+19 FLOPs"
            },
            {
                "model": "Stable Video Diffusion",
                "training_compute_(flop)": 6.7392e+22,
                "training_power_draw_(w)": 304610.7835231759,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2023-11-25T00:00:00",
                "link": "https://arxiv.org/abs/2311.15127",
                "reference": "Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets",
                "organization": "Stability AI",
                "parameters": null,
                "notable_model": false,
                "country": "United Kingdom",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Stable Video Diffusion - 6.74e+22 FLOPs"
            },
            {
                "model": "Inflection-2",
                "training_compute_(flop)": 1.001e+25,
                "training_power_draw_(w)": 6941464.657305156,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 13461144.182562498,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-11-22T00:00:00",
                "link": "https://inflection.ai/inflection-2",
                "reference": "Inflection-2: The Next Step Up",
                "organization": "Inflection AI",
                "parameters": null,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Hosted access (no API)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Inflection-2 - 1.00e+25 FLOPs"
            },
            {
                "model": "Orca 2-13B",
                "training_compute_(flop)": 4.6e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 80.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-11-21T00:00:00",
                "link": "https://arxiv.org/abs/2311.11045, https://huggingface.co/microsoft/Orca-2-13b",
                "reference": "Orca 2: Teaching Small Language Models How to Reason",
                "organization": "Microsoft Research",
                "parameters": 13000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Orca 2-13B - 4.60e+22 FLOPs"
            },
            {
                "model": "Nemotron-3-8B",
                "training_compute_(flop)": 1.8e+23,
                "training_power_draw_(w)": 812476.3359553809,
                "training_dataset_size_(gradients)": 3800000000000.0,
                "training_time_(hours)": 456.0,
                "training_compute_cost_(2023_usd)": 214467.02013524104,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-11-15T00:00:00",
                "link": "https://developer.nvidia.com/blog/nvidia-ai-foundation-models-build-custom-enterprise-chatbots-and-co-pilots-with-production-ready-llms/\n\nhttps://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/nemotron-3-8b-base-4k",
                "reference": "NVIDIA AI Foundation Models: Build Custom Enterprise Chatbots and Co-Pilots with Production-Ready LLMs",
                "organization": "NVIDIA",
                "parameters": 8000000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Nemotron-3-8B - 1.80e+23 FLOPs"
            },
            {
                "model": "GraphCast",
                "training_compute_(flop)": 2.1e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Earth science",
                "organization_categorization": "Industry",
                "publication_date": "2023-11-14T00:00:00",
                "link": "https://www.science.org/doi/epdf/10.1126/science.adi2336\n\nhttps://arxiv.org/abs/2212.12794",
                "reference": "Learning skillful medium-range globalweather forecasting",
                "organization": "Google DeepMind",
                "parameters": null,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "GraphCast - 2.10e+22 FLOPs"
            },
            {
                "model": "SPHINX (Llama 2 13B)",
                "training_compute_(flop)": 3.04e+22,
                "training_power_draw_(w)": 25391.01635736259,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 290.0,
                "training_compute_cost_(2023_usd)": 239188.6875340231,
                "domain_group": "Multimodal",
                "organization_categorization": "Academia",
                "publication_date": "2023-11-13T00:00:00",
                "link": "https://arxiv.org/abs/2311.07575",
                "reference": "SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models",
                "organization": "Shanghai AI Lab,Chinese University of Hong Kong (CUHK),ShanghaiTech University",
                "parameters": 19900000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "SPHINX (Llama 2 13B) - 3.04e+22 FLOPs"
            },
            {
                "model": "Volcano 13B",
                "training_compute_(flop)": 4.56e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 30.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2023-11-13T00:00:00",
                "link": "https://arxiv.org/abs/2311.07362",
                "reference": "Volcano: Mitigating Multimodal Hallucination through Self-Feedback Guided Revision",
                "organization": "Korea University,Korea Advanced Institute of Science and Technology (KAIST),LG",
                "parameters": 13000000000.0,
                "notable_model": true,
                "country": "South Korea",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "South Korea",
                "domain_top4": "Multimodal",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "Volcano 13B - 4.56e+22 FLOPs"
            },
            {
                "model": "MultiBand Diffusion",
                "training_compute_(flop)": 2.6e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 48.0,
                "training_compute_cost_(2023_usd)": 22.81032329809286,
                "domain_group": "Audio",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2023-11-08T00:00:00",
                "link": "https://arxiv.org/abs/2308.02560",
                "reference": "From Discrete Tokens to High-Fidelity Audio Using Multi-Band Diffusion",
                "organization": "Meta AI,Hebrew University of Jerusalem,LORIA",
                "parameters": null,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "MultiBand Diffusion - 2.60e+19 FLOPs"
            },
            {
                "model": "Prithvi-100M",
                "training_compute_(flop)": 2.299133952e+21,
                "training_power_draw_(w)": 50787.68744924102,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Earth science",
                "organization_categorization": "Industry, Government",
                "publication_date": "2023-11-08T00:00:00",
                "link": "https://arxiv.org/abs/2310.18660",
                "reference": "Foundation Models for Generalist Geospatial Artificial Intelligence",
                "organization": "IBM,NASA",
                "parameters": 100000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Other",
                "access_group": "Open",
                "model_and_compute": "Prithvi-100M - 2.30e+21 FLOPs"
            },
            {
                "model": "HGRN 1B (WT 103)",
                "training_compute_(flop)": 1.96608e+19,
                "training_power_draw_(w)": 6348.460931155128,
                "training_dataset_size_(gradients)": 100000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2023-11-08T00:00:00",
                "link": "https://arxiv.org/abs/2311.04823",
                "reference": "Hierarchically Gated Recurrent Neural Network for Sequence Modeling",
                "organization": "Shanghai AI Lab,Massachusetts Institute of Technology (MIT)",
                "parameters": 1000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "HGRN 1B (WT 103) - 1.97e+19 FLOPs"
            },
            {
                "model": "Jais-30b (phase 1)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t ",
                "training_compute_(flop)": 1.0372049e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 427000000000.0,
                "training_time_(hours)": 1080.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2023-11-08T00:00:00",
                "link": "https://www.g42.ai/resources/publications/Jais-30B",
                "reference": "Jais-30B: Expanding the Horizon in Open-Source Arabic NLP",
                "organization": "Cerebras Systems,Mohamed bin Zayed University of Artificial Intelligence (MBZUAI),Inception G42,G42",
                "parameters": 30000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "Jais-30b (phase 1)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  - 1.04e+23 FLOPs"
            },
            {
                "model": "Whisper v3",
                "training_compute_(flop)": 2.7e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 80000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Audio",
                "organization_categorization": "Industry",
                "publication_date": "2023-11-06T00:00:00",
                "link": "https://huggingface.co/openai/whisper-large-v3",
                "reference": null,
                "organization": "OpenAI",
                "parameters": 1550000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Whisper v3 - 2.70e+23 FLOPs"
            },
            {
                "model": "GPT-4 Turbo",
                "training_compute_(flop)": 2.2e+25,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 25386038.36986032,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2023-11-06T00:00:00",
                "link": "https://openai.com/blog/new-models-and-developer-products-announced-at-devday",
                "reference": "New models and developer products announced at DevDay",
                "organization": "OpenAI",
                "parameters": null,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "API access",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "GPT-4 Turbo - 2.20e+25 FLOPs"
            },
            {
                "model": "CogVLM-17B",
                "training_compute_(flop)": 6.331e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2023-11-06T00:00:00",
                "link": "https://arxiv.org/abs/2311.03079\nhttps://huggingface.co/THUDM/cogvlm-chat-hf\nhttps://github.com/THUDM/CogVLM\n",
                "reference": "CogVLM: Visual Expert for Pretrained Language Models",
                "organization": "Tsinghua University,Zhipu AI,Beihang University",
                "parameters": 17000000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Multimodal",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "CogVLM-17B - 6.33e+22 FLOPs"
            },
            {
                "model": "LLaVA 1.5",
                "training_compute_(flop)": 7.807e+22,
                "training_power_draw_(w)": 6348.88507402357,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 24.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2023-11-05T00:00:00",
                "link": "https://arxiv.org/abs/2310.03744,\n",
                "reference": "Improved Baselines with Visual Instruction Tuning",
                "organization": "University of Wisconsin Madison,Microsoft Research",
                "parameters": 13000000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "LLaVA 1.5 - 7.81e+22 FLOPs"
            },
            {
                "model": "Grok-1",
                "training_compute_(flop)": 2.90000000001e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 6200000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-11-04T00:00:00",
                "link": "https://x.ai/model-card/, https://x.ai/blog/grok-os",
                "reference": "Announcing Grok",
                "organization": "xAI",
                "parameters": 314000000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Grok-1 - 2.90e+24 FLOPs"
            },
            {
                "model": "Yi-34B",
                "training_compute_(flop)": 6.1e+23,
                "training_power_draw_(w)": 101588.94792366636,
                "training_dataset_size_(gradients)": 3100000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-11-02T00:00:00",
                "link": "https://arxiv.org/abs/2403.04652",
                "reference": "Yi: Open Foundation Models by 01.AI",
                "organization": "01.AI",
                "parameters": 34000000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Yi-34B - 6.10e+23 FLOPs"
            },
            {
                "model": "DeepSA",
                "training_compute_(flop)": 1e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2023-11-02T00:00:00",
                "link": "https://jcheminf.biomedcentral.com/articles/10.1186/s13321-023-00771-3",
                "reference": "DeepSA: a deep-learning driven predictor of compound synthesis accessibility",
                "organization": "ShanghaiTech University",
                "parameters": null,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Hosted access (no API)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "DeepSA - 1.00e+19 FLOPs"
            },
            {
                "model": "BlueLM 70B",
                "training_compute_(flop)": 4.2e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-11-02T00:00:00",
                "link": "https://baijiahao.baidu.com/s?id=1781445143383237948&wfr=spider&for=pc",
                "reference": null,
                "organization": "vivo AI lab",
                "parameters": 70000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Unreleased",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "BlueLM 70B - 4.20e+23 FLOPs"
            },
            {
                "model": "BlueLM 130B",
                "training_compute_(flop)": 7.8e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-11-02T00:00:00",
                "link": "https://baijiahao.baidu.com/s?id=1781445143383237948&wfr=spider&for=pc",
                "reference": null,
                "organization": "vivo AI lab",
                "parameters": 130000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Unreleased",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "BlueLM 130B - 7.80e+23 FLOPs"
            },
            {
                "model": "BlueLM 175B",
                "training_compute_(flop)": 1.05e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-11-02T00:00:00",
                "link": "https://baijiahao.baidu.com/s?id=1781445143383237948&wfr=spider&for=pc",
                "reference": null,
                "organization": "vivo AI lab",
                "parameters": 175000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Unreleased",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "BlueLM 175B - 1.05e+24 FLOPs"
            },
            {
                "model": "Yi 6B",
                "training_compute_(flop)": 1.26e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-11-02T00:00:00",
                "link": "https://arxiv.org/abs/2403.04652",
                "reference": "Yi: Open Foundation Models by 01.AI",
                "organization": "01.AI",
                "parameters": 6000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Yi 6B - 1.26e+23 FLOPs"
            },
            {
                "model": "Nanbeige-16B",
                "training_compute_(flop)": 2.4e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2500000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-11-01T00:00:00",
                "link": "https://github.com/Nanbeige/Nanbeige/blob/main/README_EN.md",
                "reference": null,
                "organization": "Nanbeige LLM Lab",
                "parameters": 16000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Nanbeige-16B - 2.40e+23 FLOPs"
            },
            {
                "model": "Calm2-7B",
                "training_compute_(flop)": 5.46e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1300000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-11-01T00:00:00",
                "link": "https://huggingface.co/cyberagent/calm2-7b",
                "reference": " CyberAgentLM2-7B (CALM2-7B)   https://huggingface.co/cyberagent/calm2-7b",
                "organization": "CyberAgent",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "Japan",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Japan",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Calm2-7B - 5.46e+22 FLOPs"
            },
            {
                "model": "BlueLM 7B",
                "training_compute_(flop)": 1.0920000000001e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-10-31T00:00:00",
                "link": "https://github.com/vivo-ai-lab/BlueLM/blob/main/BlueLM_technical_report.pdf",
                "reference": "BlueLM: An Open Multilingual 7B Language Model",
                "organization": "vivo AI lab",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "BlueLM 7B - 1.09e+23 FLOPs"
            },
            {
                "model": "Mi:dm 200B",
                "training_compute_(flop)": 1.1999999999999999e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-10-31T00:00:00",
                "link": "https://genielabs.ai/midm/about",
                "reference": null,
                "organization": "KT",
                "parameters": 200000000000.0,
                "notable_model": false,
                "country": "South Korea",
                "model_accessibility": "API access",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "South Korea",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Mi:dm 200B - 1.20e+24 FLOPs"
            },
            {
                "model": "Skywork-13B",
                "training_compute_(flop)": 2.5e+23,
                "training_power_draw_(w)": 253989.3377909499,
                "training_dataset_size_(gradients)": 3200000000000.0,
                "training_time_(hours)": 940.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-10-30T00:00:00",
                "link": "https://arxiv.org/abs/2310.19341",
                "reference": "Skywork: A More Open Bilingual Foundation Model",
                "organization": "Kunlun Inc.",
                "parameters": 13000000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Skywork-13B - 2.50e+23 FLOPs"
            },
            {
                "model": "ChatGLM3-6B",
                "training_compute_(flop)": 5.04e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2023-10-27T00:00:00",
                "link": "https://www.zhipuai.cn/en/news/76\n\nhttps://huggingface.co/zai-org/chatglm3-6b",
                "reference": "Zhipu AI launches third-generation base model",
                "organization": "Zhipu AI",
                "parameters": 6000000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "ChatGLM3-6B - 5.04e+22 FLOPs"
            },
            {
                "model": "CODEFUSION (Python)",
                "training_compute_(flop)": 7.92e+18,
                "training_power_draw_(w)": 2381.36215809478,
                "training_dataset_size_(gradients)": 4390400.0,
                "training_time_(hours)": 11.0,
                "training_compute_cost_(2023_usd)": 8.542235671062665,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-10-26T00:00:00",
                "link": "https://arxiv.org/abs/2310.17680 (was withdrawn)\n\nhttps://www.microsoft.com/en-us/research/wp-content/uploads/2023/11/CodeFusion-Revised-CameraReady.pdf",
                "reference": "CODEFUSION: A Pre-trained Diffusion Model for Code Generation",
                "organization": "Microsoft,Microsoft Research",
                "parameters": 75000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "CODEFUSION (Python) - 7.92e+18 FLOPs"
            },
            {
                "model": "Stockmark-13B",
                "training_compute_(flop)": 1.716e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 220000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-10-23T00:00:00",
                "link": "https://huggingface.co/stockmark/stockmark-13b",
                "reference": " stockmark/stockmark-13b",
                "organization": "Stockmark",
                "parameters": 13200000000.0,
                "notable_model": false,
                "country": "Japan",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Japan",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Stockmark-13B - 1.72e+22 FLOPs"
            },
            {
                "model": "SILC-S* (86M)",
                "training_compute_(flop)": 1.004e+22,
                "training_power_draw_(w)": 86375.60801213082,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 120.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2023-10-20T00:00:00",
                "link": "https://arxiv.org/abs/2310.13355",
                "reference": "SILC: Improving Vision Language Pretraining with Self-Distillation",
                "organization": "ETH Zurich,DeepMind,Google,Technical University of Munich",
                "parameters": 86000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "SILC-S* (86M) - 1.00e+22 FLOPs"
            },
            {
                "model": "Llemma 34B",
                "training_compute_(flop)": 5.4270979e+23,
                "training_power_draw_(w)": 203254.8293570902,
                "training_dataset_size_(gradients)": 55000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Academia",
                "publication_date": "2023-10-16T00:00:00",
                "link": "https://arxiv.org/abs/2310.10631",
                "reference": "Llemma: An Open Language Model For Mathematics",
                "organization": "Princeton University,University of Toronto,Vector Institute,University of Cambridge,Carnegie Mellon University (CMU),University of Washington,EleutherAI",
                "parameters": 34000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "Llemma 34B - 5.43e+23 FLOPs"
            },
            {
                "model": "Llemma 7B",
                "training_compute_(flop)": 1.180685e+23,
                "training_power_draw_(w)": 203254.8293570902,
                "training_dataset_size_(gradients)": 55000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Academia",
                "publication_date": "2023-10-16T00:00:00",
                "link": "https://arxiv.org/abs/2310.10631",
                "reference": "Llemma: An Open Language Model For Mathematics",
                "organization": "Princeton University,EleutherAI,University of Toronto,Vector Institute,University of Cambridge,Carnegie Mellon University (CMU),University of Washington",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "Llemma 7B - 1.18e+23 FLOPs"
            },
            {
                "model": "Jiutian",
                "training_compute_(flop)": 1.668e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-10-12T00:00:00",
                "link": "https://www.globaltimes.cn/page/202310/1299716.shtml",
                "reference": null,
                "organization": "China Mobile",
                "parameters": 13900000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Jiutian - 1.67e+23 FLOPs"
            },
            {
                "model": "CodeFuse-13B",
                "training_compute_(flop)": 3.09e+23,
                "training_power_draw_(w)": 406563.97863021225,
                "training_dataset_size_(gradients)": 1000000000000.0,
                "training_time_(hours)": 960.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-10-10T00:00:00",
                "link": "https://arxiv.org/abs/2310.06266",
                "reference": "CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model",
                "organization": "Ant Group",
                "parameters": 13000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "CodeFuse-13B - 3.09e+23 FLOPs"
            },
            {
                "model": "RoseTTAFold All-Atom (RFAA)",
                "training_compute_(flop)": 2.14e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 63240960.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2023-10-09T00:00:00",
                "link": "https://www.biorxiv.org/content/10.1101/2023.10.09.561603v1",
                "reference": "Generalized Biomolecular Modeling and Design with RoseTTAFold All-Atom",
                "organization": "University of Washington,Seoul National University,University of Sheffield",
                "parameters": null,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "RoseTTAFold All-Atom (RFAA) - 2.14e+20 FLOPs"
            },
            {
                "model": "FinGPT-13B",
                "training_compute_(flop)": 1.6e+23,
                "training_power_draw_(w)": 381.7900590727425,
                "training_dataset_size_(gradients)": 76800.0,
                "training_time_(hours)": 17.25,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2023-10-07T00:00:00",
                "link": "https://arxiv.org/abs/2310.04793; https://github.com/AI4Finance-Foundation/FinGPT",
                "reference": "FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets",
                "organization": "University of California Los Angeles (UCLA),Columbia University,New York University (NYU)",
                "parameters": 13000000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "FinGPT-13B - 1.60e+23 FLOPs"
            },
            {
                "model": "FoldFlow",
                "training_compute_(flop)": 1.1000000000000008e+20,
                "training_power_draw_(w)": 3176.776258121856,
                "training_dataset_size_(gradients)": 40046400.0,
                "training_time_(hours)": 60.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2023-10-03T00:00:00",
                "link": "https://arxiv.org/abs/2310.02391",
                "reference": "SE(3) Stochastic Flow Matching for Protein Backbone Generation",
                "organization": "McGill University,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms),Dreamfold,University of Montreal / Universit\u00e9 de Montr\u00e9al,University of Oxford",
                "parameters": null,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "FoldFlow - 1.10e+20 FLOPs"
            },
            {
                "model": "LLaMA-7B (protein-oriented instruction-tuned)",
                "training_compute_(flop)": 2.78e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 5068266667.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Academia",
                "publication_date": "2023-10-02T00:00:00",
                "link": "https://arxiv.org/abs/2306.08018",
                "reference": "Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models",
                "organization": "Zhejiang University (ZJU)",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Multimodal",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "LLaMA-7B (protein-oriented instruction-tuned) - 2.78e+22 FLOPs"
            },
            {
                "model": "Phi-1",
                "training_compute_(flop)": 3.3234195e+20,
                "training_power_draw_(w)": 6353.694007439068,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 103.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-10-02T00:00:00",
                "link": "https://arxiv.org/abs/2306.11644",
                "reference": "Textbooks Are All You Need",
                "organization": "Microsoft Research",
                "parameters": 1300000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Phi-1 - 3.32e+20 FLOPs"
            },
            {
                "model": "TinyLlama-1.1B (1T token checkpoint)",
                "training_compute_(flop)": 7.24598784e+21,
                "training_power_draw_(w)": 12707.671003570757,
                "training_dataset_size_(gradients)": 1000000000000.0,
                "training_time_(hours)": 720.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2023-10-01T00:00:00",
                "link": "https://arxiv.org/abs/2401.02385",
                "reference": "TinyLlama: An Open-Source Small Language Model",
                "organization": "Singapore University of Technology & Design",
                "parameters": 1100000000.0,
                "notable_model": false,
                "country": "Singapore",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "TinyLlama-1.1B (1T token checkpoint) - 7.25e+21 FLOPs"
            },
            {
                "model": "TinyLlama-1.1B (3T token checkpoint)",
                "training_compute_(flop)": 2.173796352e+22,
                "training_power_draw_(w)": 12707.671003570757,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 2160.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2023-10-01T00:00:00",
                "link": "https://arxiv.org/abs/2401.02385",
                "reference": "TinyLlama: An Open-Source Small Language Model",
                "organization": "Singapore University of Technology & Design",
                "parameters": 1100000000.0,
                "notable_model": false,
                "country": "Singapore",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "TinyLlama-1.1B (3T token checkpoint) - 2.17e+22 FLOPs"
            },
            {
                "model": "BITTERS",
                "training_compute_(flop)": 7.8015e+17,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 108800000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2023-10-01T00:00:00",
                "link": "https://arxiv.org/abs/2211.06774",
                "reference": "Large-Scale Bidirectional Training for Zero-Shot Image Captioning",
                "organization": "LG,Shutterstock",
                "parameters": 650000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "BITTERS - 7.80e+17 FLOPs"
            },
            {
                "model": "PIXART-\u03b1",
                "training_compute_(flop)": 1.541475e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 626000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2023-09-30T00:00:00",
                "link": "https://arxiv.org/abs/2310.00426\nhttps://openreview.net/pdf?id=eAKmQPe3m1\nhttps://github.com/PixArt-alpha/PixArt-alpha",
                "reference": "PIXART-\u03b1: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis",
                "organization": "Huawei Noah's Ark Lab,The University of Hong Kong,Hong Kong University of Science and Technology (HKUST)",
                "parameters": 600000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "PIXART-\u03b1 - 1.54e+21 FLOPs"
            },
            {
                "model": "StableLM-3B-4E1T",
                "training_compute_(flop)": 6.21e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1000000000000.0,
                "training_time_(hours)": 720.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-09-29T00:00:00",
                "link": "https://stability.wandb.io/stability-llm/stable-lm/reports/StableLM-3B-4E1T--VmlldzoyMjU4?accessToken=u3zujipenkx5g7rtcj9qojjgxpconyjktjkli2po09nffrffdhhchq045vp0wyfo",
                "reference": null,
                "organization": "Stability AI",
                "parameters": 2795443200.0,
                "notable_model": false,
                "country": "United Kingdom",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "StableLM-3B-4E1T - 6.21e+22 FLOPs"
            },
            {
                "model": "Wuerstchen",
                "training_compute_(flop)": 8.2898899e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2023-09-29T00:00:00",
                "link": "https://arxiv.org/abs/2306.00637",
                "reference": "Wuerstchen: An Efficient Architecture for Large-Scale Text-to-Image Diffusion Models",
                "organization": "Technische Hochschule Ingolstadt,University of Montreal / Universit\u00e9 de Montr\u00e9al,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms),Polytechnique Montreal,Wand Technologies",
                "parameters": 1000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "Wuerstchen - 8.29e+21 FLOPs"
            },
            {
                "model": "GAIA-1",
                "training_compute_(flop)": 1.1645338e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2023-09-29T00:00:00",
                "link": "https://arxiv.org/abs/2309.17080",
                "reference": "GAIA-1: A Generative World Model for Autonomous Driving",
                "organization": "Wayve",
                "parameters": 9000000000.0,
                "notable_model": false,
                "country": "United Kingdom",
                "model_accessibility": "Unreleased",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "GAIA-1 - 1.16e+22 FLOPs"
            },
            {
                "model": "Qwen-14B",
                "training_compute_(flop)": 2.5e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 3000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-09-28T00:00:00",
                "link": "https://arxiv.org/abs/2309.16609",
                "reference": "Qwen Technical Report",
                "organization": "Alibaba",
                "parameters": 14000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Qwen-14B - 2.50e+23 FLOPs"
            },
            {
                "model": "Amazon Titan",
                "training_compute_(flop)": 4.7999999999999996e+24,
                "training_power_draw_(w)": 10929327.206416875,
                "training_dataset_size_(gradients)": 4000000000000.0,
                "training_time_(hours)": 1152.0,
                "training_compute_cost_(2023_usd)": 7933464.673729055,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2023-09-28T00:00:00",
                "link": "https://aws.amazon.com/bedrock/titan/",
                "reference": null,
                "organization": "Amazon",
                "parameters": 200000000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "API access",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Amazon Titan - 4.80e+24 FLOPs"
            },
            {
                "model": "Qwen-7B",
                "training_compute_(flop)": 1.01e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2400000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-09-28T00:00:00",
                "link": "https://arxiv.org/abs/2309.16609, https://huggingface.co/Qwen/Qwen-7B",
                "reference": "Qwen Technical Report",
                "organization": "Alibaba",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Qwen-7B - 1.01e+23 FLOPs"
            },
            {
                "model": "PLaMo-13B",
                "training_compute_(flop)": 1.17e+23,
                "training_power_draw_(w)": 381255.6002238446,
                "training_dataset_size_(gradients)": 1500000000000.0,
                "training_time_(hours)": 720.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-09-28T00:00:00",
                "link": "https://huggingface.co/pfnet/plamo-13b",
                "reference": " PLaMo-13B",
                "organization": "Preferred Networks Inc",
                "parameters": 13000000000.0,
                "notable_model": false,
                "country": "Japan",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Japan",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "PLaMo-13B - 1.17e+23 FLOPs"
            },
            {
                "model": "BigRNA",
                "training_compute_(flop)": 1.2e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Industry",
                "publication_date": "2023-09-27T00:00:00",
                "link": "https://www.biorxiv.org/content/10.1101/2023.09.20.558508v1",
                "reference": "An RNA foundation model enables discovery of disease mechanisms and candidate therapeutics",
                "organization": "DeepGenomics",
                "parameters": 2000000000.0,
                "notable_model": false,
                "country": "Canada",
                "model_accessibility": "Unreleased",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Canada",
                "domain_top4": "Biology",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "BigRNA - 1.20e+22 FLOPs"
            },
            {
                "model": "InternLM-XComposer",
                "training_compute_(flop)": 5.28e+21,
                "training_power_draw_(w)": 101672.68833253944,
                "training_dataset_size_(gradients)": 125604608000.0,
                "training_time_(hours)": 80.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Academia",
                "publication_date": "2023-09-26T00:00:00",
                "link": "https://arxiv.org/abs/2309.15112",
                "reference": "InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition",
                "organization": "Shanghai AI Lab",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Multimodal",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "InternLM-XComposer - 5.28e+21 FLOPs"
            },
            {
                "model": "BTLM-3B",
                "training_compute_(flop)": 9.8e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 627000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-09-20T00:00:00",
                "link": "https://arxiv.org/abs/2309.11568",
                "reference": "BTLM-3B-8K: 7B Parameter Performance in a 3B Parameter Model",
                "organization": "Cerebras Systems",
                "parameters": 2600000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "BTLM-3B - 9.80e+21 FLOPs"
            },
            {
                "model": "DreamLLM",
                "training_compute_(flop)": 7.547904e+20,
                "training_power_draw_(w)": 63553.921475779025,
                "training_dataset_size_(gradients)": 70369280000.0,
                "training_time_(hours)": 17.5,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2023-09-20T00:00:00",
                "link": "https://arxiv.org/abs/2309.11499",
                "reference": "DreamLLM: Synergistic Multimodal Comprehension and Creation",
                "organization": "Xi\u2019an Jiaotong University,Megvii Inc,Tsinghua University,Huazhong University of Science and Technology",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Unknown",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Multimodal",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "DreamLLM - 7.55e+20 FLOPs"
            },
            {
                "model": "Baichuan 2-7B",
                "training_compute_(flop)": 1.092e+23,
                "training_power_draw_(w)": 508431.3718062322,
                "training_dataset_size_(gradients)": 2600000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-09-20T00:00:00",
                "link": "https://arxiv.org/pdf/2309.10305",
                "reference": "Baichuan 2: Open Large-scale Language Models\n",
                "organization": "Baichuan",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Baichuan 2-7B - 1.09e+23 FLOPs"
            },
            {
                "model": "OpenChat-13b",
                "training_compute_(flop)": 7.805217030000001e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2023-09-20T00:00:00",
                "link": "https://arxiv.org/pdf/2309.11235",
                "reference": "OpenChat: Advancing Open-source Language Models with Mixed-Quality Data",
                "organization": "Tsinghua University,Shanghai AI Lab,01.AI",
                "parameters": 13000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "OpenChat-13b - 7.81e+22 FLOPs"
            },
            {
                "model": "DeciLM 6B",
                "training_compute_(flop)": 1.026e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-09-13T00:00:00",
                "link": "https://huggingface.co/Deci/DeciLM-6b",
                "reference": "DeciLM 6B",
                "organization": "Deci AI",
                "parameters": 5700000000.0,
                "notable_model": false,
                "country": "Israel",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "DeciLM 6B - 1.03e+22 FLOPs"
            },
            {
                "model": "Phi-1.5",
                "training_compute_(flop)": 1.17e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 30000000000.0,
                "training_time_(hours)": 192.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-09-11T00:00:00",
                "link": "https://arxiv.org/abs/2309.05463, https://huggingface.co/microsoft/phi-1_5",
                "reference": "Textbooks Are All You Need II: phi-1.5 technical report",
                "organization": "Microsoft",
                "parameters": 1300000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Phi-1.5 - 1.17e+21 FLOPs"
            },
            {
                "model": "MADLAD-400 10B",
                "training_compute_(flop)": 1.605e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 250000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-09-09T00:00:00",
                "link": "https://arxiv.org/abs/2309.04662",
                "reference": "MADLAD-400: A Multilingual And Document-Level Large Audited Dataset",
                "organization": "Google DeepMind,Google Research",
                "parameters": 10700000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "MADLAD-400 10B - 1.60e+22 FLOPs"
            },
            {
                "model": "Persimmon-8B",
                "training_compute_(flop)": 4.11246e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 737000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-09-07T00:00:00",
                "link": "https://www.adept.ai/blog/persimmon-8b",
                "reference": "Releasing Persimmon-8B",
                "organization": "Adept",
                "parameters": 9300000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Persimmon-8B - 4.11e+22 FLOPs"
            },
            {
                "model": "FLM-101B",
                "training_compute_(flop)": 5.72e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 311540000000.0,
                "training_time_(hours)": 517.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2023-09-07T00:00:00",
                "link": "https://arxiv.org/abs/2309.03852",
                "reference": "FLM-101B: An Open LLM and How to Train It with $100K Budget",
                "organization": "Chinese Academy of Sciences,Harbin Institute of Technology,Nanyang Technological University,Beijing Academy of Artificial Intelligence / BAAI",
                "parameters": 101000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "FLM-101B - 5.72e+22 FLOPs"
            },
            {
                "model": "XGen-7B",
                "training_compute_(flop)": 8.02e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1429520000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-09-07T00:00:00",
                "link": "https://arxiv.org/abs/2309.03450",
                "reference": "XGen-7B Technical Report",
                "organization": "Salesforce",
                "parameters": 6700000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "XGen-7B - 8.02e+22 FLOPs"
            },
            {
                "model": "Hunyuan",
                "training_compute_(flop)": 1.1999999999999999e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2023-09-07T00:00:00",
                "link": "https://www.tencent.com/en-us/articles/2201685.html",
                "reference": "Tencent Unveils Hunyuan, its Proprietary Large Foundation Model on Tencent Cloud",
                "organization": "Tencent",
                "parameters": 100000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "API access",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Hunyuan - 1.20e+24 FLOPs"
            },
            {
                "model": "Falcon-180B",
                "training_compute_(flop)": 3.76e+24,
                "training_power_draw_(w)": 3254975.4289709153,
                "training_dataset_size_(gradients)": 3500000000000.0,
                "training_time_(hours)": 4320.0,
                "training_compute_cost_(2023_usd)": 10743500.868805485,
                "domain_group": "Language",
                "organization_categorization": "Government",
                "publication_date": "2023-09-06T00:00:00",
                "link": "https://falconllm.tii.ae/falcon-180b.html; https://arxiv.org/abs/2311.16867",
                "reference": "The Falcon Series of Open Language Models",
                "organization": "Technology Innovation Institute",
                "parameters": 180000000000.0,
                "notable_model": true,
                "country": "United Arab Emirates",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Government",
                "access_group": "Open",
                "model_and_compute": "Falcon-180B - 3.76e+24 FLOPs"
            },
            {
                "model": "Baichuan2-13B",
                "training_compute_(flop)": 2.03e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2600000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-09-06T00:00:00",
                "link": "https://huggingface.co/baichuan-inc/Baichuan2-13B-Base, https://arxiv.org/abs/2309.10305",
                "reference": "Baichuan 2: Open Large-scale Language Models",
                "organization": "Baichuan",
                "parameters": 13000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Baichuan2-13B - 2.03e+23 FLOPs"
            },
            {
                "model": "TigerBot-70B",
                "training_compute_(flop)": 1.02e+24,
                "training_power_draw_(w)": 406871.9286213644,
                "training_dataset_size_(gradients)": 300000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-09-06T00:00:00",
                "link": "https://github.com/TigerResearch/TigerBot/blob/main/README_en.md, https://arxiv.org/abs/2312.08688",
                "reference": "TigerBot: An Open Multilingual Multitask LLM",
                "organization": "Tigerobo",
                "parameters": 70000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "TigerBot-70B - 1.02e+24 FLOPs"
            },
            {
                "model": "Swift",
                "training_compute_(flop)": 5.337e+16,
                "training_power_draw_(w)": 382.113280348051,
                "training_dataset_size_(gradients)": 120000000.0,
                "training_time_(hours)": 0.833,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Robotics",
                "organization_categorization": "Industry",
                "publication_date": "2023-08-30T00:00:00",
                "link": "https://www.nature.com/articles/s41586-023-06419-4",
                "reference": "Champion-level drone racing using deep reinforcement learning",
                "organization": "Intel Labs",
                "parameters": 56804.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Swift - 5.34e+16 FLOPs"
            },
            {
                "model": "Jais",
                "training_compute_(flop)": 4.8946763e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 395000000000.0,
                "training_time_(hours)": 600.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2023-08-29T00:00:00",
                "link": "https://arxiv.org/abs/2308.16149",
                "reference": "Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models",
                "organization": "Cerebras Systems,Mohamed bin Zayed University of Artificial Intelligence (MBZUAI),Inception G42",
                "parameters": 13000000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "Jais - 4.89e+22 FLOPs"
            },
            {
                "model": "Refact-1.6B",
                "training_compute_(flop)": 1.152e+22,
                "training_power_draw_(w)": 2742.1059640734707,
                "training_dataset_size_(gradients)": 1200000000000.0,
                "training_time_(hours)": 672.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-08-29T00:00:00",
                "link": "https://huggingface.co/smallcloudai/Refact-1_6B-fim",
                "reference": " Refact-1.6B ",
                "organization": "Refact AI",
                "parameters": 1600000000.0,
                "notable_model": false,
                "country": "United Kingdom",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Refact-1.6B - 1.15e+22 FLOPs"
            },
            {
                "model": "Luca 2.0",
                "training_compute_(flop)": 1.1999999999999999e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": null,
                "organization_categorization": "Industry",
                "publication_date": "2023-08-29T00:00:00",
                "link": "https://www.163.com/dy/article/IDBGA8840511FQO9.html",
                "reference": null,
                "organization": "Mianbi Intelligence",
                "parameters": 100000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Hosted access (no API)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Luca 2.0 - 1.20e+24 FLOPs"
            },
            {
                "model": "PeptideBERT",
                "training_compute_(flop)": 4.9e+16,
                "training_power_draw_(w)": 272.95021398005304,
                "training_dataset_size_(gradients)": 4160566.0,
                "training_time_(hours)": 4.067,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2023-08-28T00:00:00",
                "link": "https://arxiv.org/abs/2309.03099",
                "reference": "PeptideBERT: A language Model based on Transformers for Peptide Property Prediction",
                "organization": "Carnegie Mellon University (CMU)",
                "parameters": null,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "PeptideBERT - 4.90e+16 FLOPs"
            },
            {
                "model": "PULI GPTrio",
                "training_compute_(flop)": 5.8e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 230590476190.0,
                "training_time_(hours)": 2200.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2023-08-23T00:00:00",
                "link": "https://link.springer.com/chapter/10.1007/978-3-031-40498-6_9; https://huggingface.co/NYTK/PULI-GPTrio",
                "reference": "Mono- and Multilingual GPT-3 Models for Hungarian",
                "organization": "Hungarian Research Centre for Linguistics",
                "parameters": 6700000000.0,
                "notable_model": false,
                "country": "Hungary",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "PULI GPTrio - 5.80e+21 FLOPs"
            },
            {
                "model": "ShapeMol",
                "training_compute_(flop)": 2.59999999999998e+19,
                "training_power_draw_(w)": 327.57672938451,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 140.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": null,
                "publication_date": "2023-08-23T00:00:00",
                "link": "https://arxiv.org/abs/2308.11890",
                "reference": "Shape-conditioned 3D Molecule Generation via Equivariant Diffusion Models",
                "organization": "Ohio State University",
                "parameters": 2700000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Biology",
                "org_top5": "Other",
                "access_group": "Closed",
                "model_and_compute": "ShapeMol - 2.60e+19 FLOPs"
            },
            {
                "model": "IDEFICS-80B",
                "training_compute_(flop)": 1.1593580544e+23,
                "training_power_draw_(w)": 407007.86305450817,
                "training_dataset_size_(gradients)": 149600000000.0,
                "training_time_(hours)": 672.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2023-08-22T00:00:00",
                "link": "https://huggingface.co/blog/idefics",
                "reference": "Introducing IDEFICS: An Open Reproduction of State-of-the-Art Visual Language Model",
                "organization": "Hugging Face",
                "parameters": 80000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "IDEFICS-80B - 1.16e+23 FLOPs"
            },
            {
                "model": "VARCO LLM 2.0 base",
                "training_compute_(flop)": 1.248e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1600000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-08-16T00:00:00",
                "link": "https://ncsoft.github.io/ncresearch/varco-llm-details/\nhttps://aws.amazon.com/marketplace/pp/prodview-d7amr4yxpibew?sr=0-3&ref_=beagle&applicationId=AWSMPContessa",
                "reference": "VARCO LLM 2.0 is NCSOFT's large language model that can be applied to the development of natural language processing-based AI services.",
                "organization": "NCSOFT",
                "parameters": 13000000000.0,
                "notable_model": false,
                "country": "South Korea",
                "model_accessibility": "API access",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "South Korea",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "VARCO LLM 2.0 base - 1.25e+23 FLOPs"
            },
            {
                "model": "VARCO LLM 2.0 small Finetuning",
                "training_compute_(flop)": 6.72e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1600000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-08-16T00:00:00",
                "link": "https://ncsoft.github.io/ncresearch/varco-llm-details/\nhttps://aws.amazon.com/marketplace/pp/prodview-or3w6j66on53s?sr=0-2&ref_=beagle&applicationId=AWSMPContessa",
                "reference": "VARCO LLM 2.0 is NCSOFT's large language model that can be applied to the development of natural language processing-based AI services.",
                "organization": "NCSOFT",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "South Korea",
                "model_accessibility": "API access",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "South Korea",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "VARCO LLM 2.0 small Finetuning - 6.72e+22 FLOPs"
            },
            {
                "model": "VARCO LLM KO/EN-13B-IST ver.1",
                "training_compute_(flop)": 2.73e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 350000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-08-16T00:00:00",
                "link": "https://ncsoft.github.io/ncresearch/varco-llm-details/\nhttps://aws.amazon.com/marketplace/pp/prodview-usyosolf3an3u?sr=0-1&ref_=beagle&applicationId=AWSMPContessa",
                "reference": null,
                "organization": "NCSOFT",
                "parameters": 13000000000.0,
                "notable_model": false,
                "country": "South Korea",
                "model_accessibility": "API access",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "South Korea",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "VARCO LLM KO/EN-13B-IST ver.1 - 2.73e+22 FLOPs"
            },
            {
                "model": "DeciCoder-1B",
                "training_compute_(flop)": 2.9e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 446000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-08-15T00:00:00",
                "link": "https://huggingface.co/Deci/DeciCoder-1b",
                "reference": " Model Card for DeciCoder-1b",
                "organization": "Deci AI",
                "parameters": 1100000000.0,
                "notable_model": false,
                "country": "Israel",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "DeciCoder-1B - 2.90e+21 FLOPs"
            },
            {
                "model": "Konan LLM 13B",
                "training_compute_(flop)": 3.86712e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 492000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2023-08-15T00:00:00",
                "link": "https://en.konantech.com/en/llm/konanllm\nhttps://techfinch.kr/ai/konan-technology-unveils-konan-llm--its-own-ai-language-model\nhttps://www.dbpia.co.kr/journal/articleDetail?nodeId=NODE11610127 ",
                "reference": "Konan LLM: A Korean Large Language Model",
                "organization": "Konan Technology",
                "parameters": 13100000000.0,
                "notable_model": false,
                "country": "South Korea",
                "model_accessibility": "Hosted access (no API)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "South Korea",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Konan LLM 13B - 3.87e+22 FLOPs"
            },
            {
                "model": "Code Llama-34B",
                "training_compute_(flop)": 5.3e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 600000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-08-14T00:00:00",
                "link": "https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/\nhttps://arxiv.org/abs/2308.12950",
                "reference": "Code Llama: Open Foundation Models for Code",
                "organization": "Meta AI",
                "parameters": 34000000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Code Llama-34B - 5.30e+23 FLOPs"
            },
            {
                "model": "Code Llama-7B",
                "training_compute_(flop)": 1.1e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 600000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-08-14T00:00:00",
                "link": "https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/\nhttps://arxiv.org/abs/2308.12950",
                "reference": "Code Llama: Open Foundation Models for Code",
                "organization": "Meta AI",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Code Llama-7B - 1.10e+23 FLOPs"
            },
            {
                "model": "Japanese StableLM Base Alpha 7B",
                "training_compute_(flop)": 3.15e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 750000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-08-10T00:00:00",
                "link": "https://stability.ai/news/stability-ai-new-jplm-japanese-language-model-stablelm",
                "reference": "Japanese StableLM, Marking Entry into International Language Model  Market",
                "organization": "Stability AI",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "United Kingdom",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Japanese StableLM Base Alpha 7B - 3.15e+22 FLOPs"
            },
            {
                "model": "Baichuan2-53B",
                "training_compute_(flop)": 8.268e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-08-09T00:00:00",
                "link": "https://technode.com/2023/08/09/chinese-ai-startup-baichuan-rolls-out-third-llm-in-four-months/",
                "reference": "Chinese AI startup Baichuan rolls out third LLM in four months",
                "organization": "Baichuan",
                "parameters": 53000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Unreleased",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Baichuan2-53B - 8.27e+23 FLOPs"
            },
            {
                "model": "SS-pLM",
                "training_compute_(flop)": 2.28096e+19,
                "training_power_draw_(w)": 1312.1138696054595,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 24.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Government",
                "publication_date": "2023-08-06T00:00:00",
                "link": "https://www.biorxiv.org/content/10.1101/2023.08.04.551626.abstract",
                "reference": "Efficient and accurate sequence generation with small-scale protein language models",
                "organization": "Nostrum Biodiscovery,Barcelona Supercomputing Center,Institucio Catalana de Recerca i Estudis Avanc\u00e7ats",
                "parameters": 14800000.0,
                "notable_model": false,
                "country": "Spain",
                "model_accessibility": "Unknown",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Biology",
                "org_top5": "Government",
                "access_group": "Closed",
                "model_and_compute": "SS-pLM - 2.28e+19 FLOPs"
            },
            {
                "model": "StableLM-Base-Alpha-7B",
                "training_compute_(flop)": 4.5e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1100000000000.0,
                "training_time_(hours)": 392.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-08-05T00:00:00",
                "link": "https://huggingface.co/stabilityai/stablelm-base-alpha-7b-v2",
                "reference": null,
                "organization": "Stability AI",
                "parameters": 6890209280.0,
                "notable_model": false,
                "country": "United Kingdom",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "StableLM-Base-Alpha-7B - 4.50e+22 FLOPs"
            },
            {
                "model": "GGNN",
                "training_compute_(flop)": 7.56e+21,
                "training_power_draw_(w)": 1590.4764725839211,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2023-08-05T00:00:00",
                "link": "https://www.nature.com/articles/s42003-023-05133-1",
                "reference": "Integration of pre-trained protein language models into geometric deep learning networks",
                "organization": "Westlake University,Tsinghua University,Toyota Technological Institute at Chicago",
                "parameters": null,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "GGNN - 7.56e+21 FLOPs"
            },
            {
                "model": "Weblab-10B",
                "training_compute_(flop)": 3.6e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 256131000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": null,
                "publication_date": "2023-08-04T00:00:00",
                "link": "https://huggingface.co/matsuo-lab/weblab-10b",
                "reference": " weblab-10b",
                "organization": "Matsuo Lab",
                "parameters": 10000000000.0,
                "notable_model": false,
                "country": "Japan",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Japan",
                "domain_top4": "Language",
                "org_top5": "Other",
                "access_group": "Open",
                "model_and_compute": "Weblab-10B - 3.60e+22 FLOPs"
            },
            {
                "model": "JIANG",
                "training_compute_(flop)": 4.03e+22,
                "training_power_draw_(w)": 76349.67141784582,
                "training_dataset_size_(gradients)": 466720000000.0,
                "training_time_(hours)": 1200.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-08-01T00:00:00",
                "link": "https://arxiv.org/abs/2308.00624",
                "reference": "JIANG: Chinese Open Foundation Language Model",
                "organization": "K.D. Feddersen (KDF)",
                "parameters": null,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "JIANG - 4.03e+22 FLOPs"
            },
            {
                "model": "bilingual-gpt-neox-4b",
                "training_compute_(flop)": 1.2e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-07-31T00:00:00",
                "link": "https://huggingface.co/rinna/bilingual-gpt-neox-4b\n\nhttps://arxiv.org/abs/2404.01657",
                "reference": "Release of Pre-Trained Models for the Japanese Language",
                "organization": "rinna",
                "parameters": 3800000000.0,
                "notable_model": false,
                "country": "Japan",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Japan",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "bilingual-gpt-neox-4b - 1.20e+22 FLOPs"
            },
            {
                "model": "AudioLM",
                "training_compute_(flop)": 3.9e+18,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 135000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Audio",
                "organization_categorization": "Industry",
                "publication_date": "2023-07-26T00:00:00",
                "link": "https://arxiv.org/abs/2209.03143",
                "reference": "AudioLM: a Language Modeling Approach to Audio Generation",
                "organization": "Google Research",
                "parameters": 1500000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "AudioLM - 3.90e+18 FLOPs"
            },
            {
                "model": "LM-Design",
                "training_compute_(flop)": 1.400000000000012e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 811080.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2023-07-23T00:00:00",
                "link": "https://proceedings.mlr.press/v202/zheng23a.html",
                "reference": "Structure-informed Language Models Are Protein Designers",
                "organization": "ByteDance,University of Wisconsin Madison",
                "parameters": 6900000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "LM-Design - 1.40e+20 FLOPs"
            },
            {
                "model": "Llama 2-70B",
                "training_compute_(flop)": 8.1e+23,
                "training_power_draw_(w)": 795557.0703894598,
                "training_dataset_size_(gradients)": 2000000000000.0,
                "training_time_(hours)": 1728.0,
                "training_compute_cost_(2023_usd)": 1102561.194,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-07-18T00:00:00",
                "link": "https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/\nhttps://arxiv.org/abs/2307.09288",
                "reference": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
                "organization": "Meta AI",
                "parameters": 70000000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Llama 2-70B - 8.10e+23 FLOPs"
            },
            {
                "model": "Llama 2-34B",
                "training_compute_(flop)": 4.08e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 600469.8720490151,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-07-18T00:00:00",
                "link": "https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/\nhttps://arxiv.org/abs/2307.09288",
                "reference": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
                "organization": "Meta AI",
                "parameters": 34000000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Llama 2-34B - 4.08e+23 FLOPs"
            },
            {
                "model": "Llama 2-7B",
                "training_compute_(flop)": 8.4e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 114259.38527188863,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-07-18T00:00:00",
                "link": "https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/",
                "reference": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
                "organization": "Meta AI",
                "parameters": 7000000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Llama 2-7B - 8.40e+22 FLOPs"
            },
            {
                "model": "Llama 2-13B",
                "training_compute_(flop)": 1.6e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 235478.3811956922,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-07-18T00:00:00",
                "link": "https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/\nhttps://arxiv.org/abs/2307.09288",
                "reference": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
                "organization": "Meta AI",
                "parameters": 13000000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Llama 2-13B - 1.60e+23 FLOPs"
            },
            {
                "model": "RetNet",
                "training_compute_(flop)": 4.02e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 100000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2023-07-17T00:00:00",
                "link": "https://arxiv.org/abs/2307.08621",
                "reference": "Retentive Network: A Successor to Transformer for Large Language Models",
                "organization": "Microsoft Research,Tsinghua University",
                "parameters": 6700000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "RetNet - 4.02e+21 FLOPs"
            },
            {
                "model": "Claude 2",
                "training_compute_(flop)": 3.866e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 4902644.123383027,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-07-11T00:00:00",
                "link": "https://www.anthropic.com/index/claude-2, https://www-files.anthropic.com/production/images/Model-Card-Claude-2.pdf",
                "reference": null,
                "organization": "Anthropic",
                "parameters": null,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "API access",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Claude 2 - 3.87e+24 FLOPs"
            },
            {
                "model": "Emu1 (BAAI)",
                "training_compute_(flop)": 2.70000000001e+21,
                "training_power_draw_(w)": 101847.18028115285,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 48.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Academia",
                "publication_date": "2023-07-11T00:00:00",
                "link": "https://arxiv.org/abs/2307.05222",
                "reference": "Generative Pretraining in Multimodality",
                "organization": "Beijing Academy of Artificial Intelligence / BAAI,Tsinghua University,Peking University",
                "parameters": 14000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Multimodal",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "Emu1 (BAAI) - 2.70e+21 FLOPs"
            },
            {
                "model": "Baichuan 1-13B",
                "training_compute_(flop)": 9.36e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1400000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-07-11T00:00:00",
                "link": "https://github.com/baichuan-inc/Baichuan-13B/blob/main/README_EN.md",
                "reference": "Baichuan-13B\n",
                "organization": "Baichuan",
                "parameters": 13264901120.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Baichuan 1-13B - 9.36e+22 FLOPs"
            },
            {
                "model": "InternLM",
                "training_compute_(flop)": 9.984e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 1505257.378,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2023-07-06T00:00:00",
                "link": "https://github.com/InternLM/InternLM-techreport/blob/main/InternLM.pdf",
                "reference": null,
                "organization": "Shanghai AI Lab,SenseTime",
                "parameters": 104000000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "InternLM - 9.98e+23 FLOPs"
            },
            {
                "model": "CodeGen2.5",
                "training_compute_(flop)": 5.9e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 300000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-07-06T00:00:00",
                "link": "https://blog.salesforceairesearch.com/codegen25/",
                "reference": "CodeGen2.5: Small, but mighty",
                "organization": "Salesforce",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "CodeGen2.5 - 5.90e+22 FLOPs"
            },
            {
                "model": "xTrimoPGLM -100B",
                "training_compute_(flop)": 6.2e+23,
                "training_power_draw_(w)": 611151.12765533,
                "training_dataset_size_(gradients)": 275000000000.0,
                "training_time_(hours)": 3912.0,
                "training_compute_cost_(2023_usd)": 1823415.258,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2023-07-06T00:00:00",
                "link": "https://www.biorxiv.org/content/10.1101/2023.07.05.547496v4",
                "reference": "xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein",
                "organization": "Tsinghua University,BioMap Research",
                "parameters": 100000000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Unreleased",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "xTrimoPGLM -100B - 6.20e+23 FLOPs"
            },
            {
                "model": "Pangu-Weather",
                "training_compute_(flop)": 3.98e+22,
                "training_power_draw_(w)": 114593.38832967015,
                "training_dataset_size_(gradients)": 24457821696000.0,
                "training_time_(hours)": 1536.0,
                "training_compute_cost_(2023_usd)": 51279.01751905432,
                "domain_group": "Earth science",
                "organization_categorization": "Industry",
                "publication_date": "2023-07-05T00:00:00",
                "link": "https://www.nature.com/articles/s41586-023-06185-3, https://www.huaweicloud.com/intl/en-us/news/20230707180809498.html,\nhttps://www.huawei.com/en/news/2023/7/pangu-ai-model-nature-publish",
                "reference": "Accurate medium-range global weather forecasting with 3D neural networks",
                "organization": "Huawei",
                "parameters": 256000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Pangu-Weather - 3.98e+22 FLOPs"
            },
            {
                "model": "LongNet",
                "training_compute_(flop)": 4.86e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 300000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2023-07-05T00:00:00",
                "link": "https://arxiv.org/abs/2307.02486",
                "reference": "LongNet: Scaling Transformers to 1,000,000,000 Tokens",
                "organization": "Microsoft,Xi\u2019an Jiaotong University",
                "parameters": 2700000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "LongNet - 4.86e+21 FLOPs"
            },
            {
                "model": "Multilingual-E5-large",
                "training_compute_(flop)": 3.370752e+18,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1000160000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-06-30T00:00:00",
                "link": "https://arxiv.org/abs/2402.05672\nhttps://huggingface.co/intfloat/multilingual-e5-large",
                "reference": "Multilingual E5 Text Embeddings: A Technical Report",
                "organization": "Microsoft",
                "parameters": 560000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Multilingual-E5-large - 3.37e+18 FLOPs"
            },
            {
                "model": "HyenaDNA",
                "training_compute_(flop)": 1.811e+21,
                "training_power_draw_(w)": 6367.433640556241,
                "training_dataset_size_(gradients)": 2945000000.0,
                "training_time_(hours)": 672.0,
                "training_compute_cost_(2023_usd)": 5000.0,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2023-06-27T00:00:00",
                "link": "https://arxiv.org/abs/2306.15794",
                "reference": "HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution",
                "organization": "Stanford University,Harvard University,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms),University of Montreal / Universit\u00e9 de Montr\u00e9al",
                "parameters": 6600000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "HyenaDNA - 1.81e+21 FLOPs"
            },
            {
                "model": "Kosmos-2",
                "training_compute_(flop)": 4.5500354284e+20,
                "training_power_draw_(w)": 152821.8105810948,
                "training_dataset_size_(gradients)": 25000000000.0,
                "training_time_(hours)": 24.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2023-06-26T00:00:00",
                "link": "https://arxiv.org/abs/2306.14824",
                "reference": "Kosmos-2: Grounding Multimodal Large Language Models to the World",
                "organization": "Microsoft",
                "parameters": 1600000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Kosmos-2 - 4.55e+20 FLOPs"
            },
            {
                "model": "Inflection-1",
                "training_compute_(flop)": 1.0001e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-06-23T00:00:00",
                "link": "https://inflection.ai/assets/Inflection-1.pdf",
                "reference": "Inflection-1 technical memo",
                "organization": "Inflection AI",
                "parameters": null,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Hosted access (no API)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Inflection-1 - 1.00e+24 FLOPs"
            },
            {
                "model": "MPT-30B",
                "training_compute_(flop)": 1.8900000000001e+23,
                "training_power_draw_(w)": 713231.9794598748,
                "training_dataset_size_(gradients)": 1050000000000.0,
                "training_time_(hours)": 278.4,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-06-22T00:00:00",
                "link": "https://huggingface.co/mosaicml/mpt-30b",
                "reference": null,
                "organization": "MosaicML",
                "parameters": 30000000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "MPT-30B - 1.89e+23 FLOPs"
            },
            {
                "model": "GigaGAN",
                "training_compute_(flop)": 3.8680312e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1960000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2023-06-19T00:00:00",
                "link": "https://arxiv.org/abs/2303.05511",
                "reference": "Scaling up GANs for Text-to-Image Synthesis",
                "organization": "POSTECH,Carnegie Mellon University (CMU),Adobe",
                "parameters": 1000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "GigaGAN - 3.87e+22 FLOPs"
            },
            {
                "model": "Pix2Struct-Large",
                "training_compute_(flop)": 1.7380147e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2023-06-15T00:00:00",
                "link": "https://arxiv.org/abs/2210.03347",
                "reference": "Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding",
                "organization": "Google Research,University of Cambridge",
                "parameters": 1300000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "Pix2Struct-Large - 1.74e+20 FLOPs"
            },
            {
                "model": "WizardCoder-15.5B",
                "training_compute_(flop)": 1.12e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 209715200.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-06-14T00:00:00",
                "link": "https://arxiv.org/abs/2306.08568",
                "reference": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct",
                "organization": "Microsoft",
                "parameters": 15500000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "WizardCoder-15.5B - 1.12e+23 FLOPs"
            },
            {
                "model": "PoET",
                "training_compute_(flop)": 2.3000000000000197e+20,
                "training_power_draw_(w)": 5573.738213375315,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 72.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Industry",
                "publication_date": "2023-06-09T00:00:00",
                "link": "https://arxiv.org/abs/2306.06156",
                "reference": "PoET: A generative model of protein families as sequences-of-sequences",
                "organization": "OpenProtein.ai",
                "parameters": 57000000.0,
                "notable_model": false,
                "country": "Singapore",
                "model_accessibility": "Unknown",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Biology",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "PoET - 2.30e+20 FLOPs"
            },
            {
                "model": "RedPajama-INCITE-7B-Base",
                "training_compute_(flop)": 4.1e+22,
                "training_power_draw_(w)": 1834678.6878449952,
                "training_dataset_size_(gradients)": 1001000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-06-06T00:00:00",
                "link": "https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Base",
                "reference": null,
                "organization": "Together",
                "parameters": 6900000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "RedPajama-INCITE-7B-Base - 4.10e+22 FLOPs"
            },
            {
                "model": "GELU for CIFAR-10",
                "training_compute_(flop)": 741600000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 50000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2023-06-06T00:00:00",
                "link": "https://arxiv.org/abs/1606.08415",
                "reference": "Gaussian Error Linear Units (GELUs)",
                "organization": "University of California (UC) Berkeley,Toyota Technological Institute at Chicago",
                "parameters": 9888.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "GELU for CIFAR-10 - 7.42e+11 FLOPs"
            },
            {
                "model": "life2vec",
                "training_compute_(flop)": 163905134400000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 3252086.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Medicine",
                "organization_categorization": "Academia",
                "publication_date": "2023-06-05T00:00:00",
                "link": "https://arxiv.org/abs/2306.03009",
                "reference": "Using Sequences of Life-events to Predict Human Lives",
                "organization": "Technical University of Denmark,University of Copenhagen",
                "parameters": 8400000.0,
                "notable_model": false,
                "country": "Denmark",
                "model_accessibility": "Unreleased",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Other",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "life2vec - 1.64e+14 FLOPs"
            },
            {
                "model": "Polyglot-Ko-12.8B",
                "training_compute_(flop)": 1.28e+22,
                "training_power_draw_(w)": 203862.2671051655,
                "training_dataset_size_(gradients)": 95793000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": null,
                "publication_date": "2023-06-04T00:00:00",
                "link": "https://arxiv.org/abs/2306.02254; https://huggingface.co/EleutherAI/polyglot-ko-12.8b",
                "reference": "A Technical Report for Polyglot-Ko: Open-Source Large-Scale Korean Language Models",
                "organization": "EleutherAI",
                "parameters": 12898631680.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Other",
                "access_group": "Open",
                "model_and_compute": "Polyglot-Ko-12.8B - 1.28e+22 FLOPs"
            },
            {
                "model": "Baichuan1-7B",
                "training_compute_(flop)": 5.04e+22,
                "training_power_draw_(w)": 509689.7180341823,
                "training_dataset_size_(gradients)": 1200000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-06-01T00:00:00",
                "link": "https://huggingface.co/baichuan-inc/Baichuan-7B",
                "reference": "Baichuan-7B",
                "organization": "Baichuan",
                "parameters": 7000559616.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Baichuan1-7B - 5.04e+22 FLOPs"
            },
            {
                "model": "TransAct",
                "training_compute_(flop)": 1.656e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Recommendation",
                "organization_categorization": "Industry",
                "publication_date": "2023-05-31T00:00:00",
                "link": "https://arxiv.org/abs/2306.00248",
                "reference": "TransAct: Transformer-based Realtime User Action Model for Recommendation at Pinterest",
                "organization": "Pinterest",
                "parameters": 92000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "TransAct - 1.66e+20 FLOPs"
            },
            {
                "model": "UniDiffuser (\u591a\u6a21\u6001\u5927\u6a21\u578b)",
                "training_compute_(flop)": 1.992646656e+22,
                "training_power_draw_(w)": 70085.45767834295,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 672.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2023-05-30T00:00:00",
                "link": "https://arxiv.org/abs/2303.06555\n\n",
                "reference": "One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale",
                "organization": "ShengShu,Tsinghua University,Beijing Academy of Artificial Intelligence / BAAI",
                "parameters": null,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Vision",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "UniDiffuser (\u591a\u6a21\u6001\u5927\u6a21\u578b) - 1.99e+22 FLOPs"
            },
            {
                "model": "PaLI-X",
                "training_compute_(flop)": 5.6e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2023-05-29T00:00:00",
                "link": "https://arxiv.org/abs/2305.18565",
                "reference": "PaLI-X: On Scaling up a Multilingual Vision and Language Model",
                "organization": "Google Research",
                "parameters": 55000000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "PaLI-X - 5.60e+23 FLOPs"
            },
            {
                "model": "Guanaco-65B",
                "training_compute_(flop)": 5.5e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 24.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2023-05-23T00:00:00",
                "link": "https://arxiv.org/abs/2305.14314; https://github.com/artidoro/qlora",
                "reference": "QLoRA: Efficient Finetuning of Quantized LLMs",
                "organization": "University of Washington",
                "parameters": 65000000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "Guanaco-65B - 5.50e+23 FLOPs"
            },
            {
                "model": "RWKV-4 14B",
                "training_compute_(flop)": 2.78e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 330000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": null,
                "publication_date": "2023-05-22T00:00:00",
                "link": "https://arxiv.org/abs/2305.13048",
                "reference": "RWKV: Reinventing RNNs for the Transformer Era",
                "organization": "RWKV Foundation",
                "parameters": 14000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Other",
                "access_group": "Closed",
                "model_and_compute": "RWKV-4 14B - 2.78e+22 FLOPs"
            },
            {
                "model": "ONE-PEACE",
                "training_compute_(flop)": 1.8e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 490617000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2023-05-18T00:00:00",
                "link": "https://arxiv.org/abs/2305.11172v1",
                "reference": "ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities",
                "organization": "Alibaba,Huazhong University of Science and Technology",
                "parameters": 4000000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Multimodal",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "ONE-PEACE - 1.80e+20 FLOPs"
            },
            {
                "model": "LIMA",
                "training_compute_(flop)": 5.5000439e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 685300.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2023-05-18T00:00:00",
                "link": "https://arxiv.org/abs/2305.11206",
                "reference": "LIMA: Less Is More for Alignment",
                "organization": "Meta AI,Carnegie Mellon University (CMU),University of Southern California,Tel Aviv University",
                "parameters": 65000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "LIMA - 5.50e+23 FLOPs"
            },
            {
                "model": "WeLM",
                "training_compute_(flop)": 2.484338688e+22,
                "training_power_draw_(w)": 101974.27158107296,
                "training_dataset_size_(gradients)": 262000000000.0,
                "training_time_(hours)": 576.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-05-16T00:00:00",
                "link": "https://arxiv.org/abs/2209.10372 ",
                "reference": "WeLM: A Well-Read Pre-trained Language Model for Chinese",
                "organization": "WeChat AI",
                "parameters": 10000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Unknown",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "WeLM - 2.48e+22 FLOPs"
            },
            {
                "model": "OpenCALM",
                "training_compute_(flop)": 8.95104e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2131200000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-05-15T00:00:00",
                "link": "https://huggingface.co/cyberagent/open-calm-7b",
                "reference": "OpenCALM-7B",
                "organization": "CyberAgent",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "Japan",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Japan",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "OpenCALM - 8.95e+19 FLOPs"
            },
            {
                "model": "LMRec",
                "training_compute_(flop)": 1.9782e+18,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 12998864057.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2023-05-13T00:00:00",
                "link": "https://arxiv.org/abs/2212.03760",
                "reference": "Pivotal Role of Language Modeling in Recommender Systems: Enriching Task-specific and Task-agnostic Representation Learning",
                "organization": "NAVER,Naver AI Lab",
                "parameters": 210000000.0,
                "notable_model": false,
                "country": "South Korea",
                "model_accessibility": "Unknown",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "South Korea",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "LMRec - 1.98e+18 FLOPs"
            },
            {
                "model": "InstructBLIP",
                "training_compute_(flop)": 1.94e+20,
                "training_power_draw_(w)": 12748.20334097663,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 36.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2023-05-11T00:00:00",
                "link": "https://arxiv.org/abs/2305.06500",
                "reference": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning",
                "organization": "Salesforce Research,Hong Kong University of Science and Technology (HKUST),Nanyang Technological University",
                "parameters": 13000000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "InstructBLIP - 1.94e+20 FLOPs"
            },
            {
                "model": "ESM-GearNet",
                "training_compute_(flop)": 2.145e+19,
                "training_power_draw_(w)": 3187.0508352441575,
                "training_dataset_size_(gradients)": 109500000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2023-05-11T00:00:00",
                "link": "https://arxiv.org/abs/2303.06275",
                "reference": "A Systematic Study of Joint Representation Learning on Protein Sequences and Structures",
                "organization": "Mila - Quebec AI (originally Montreal Institute for Learning Algorithms),University of Montreal / Universit\u00e9 de Montr\u00e9al,IBM Research,HEC Montreal,CIFAR AI Research",
                "parameters": 650000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "ESM-GearNet - 2.14e+19 FLOPs"
            },
            {
                "model": "PaLM 2",
                "training_compute_(flop)": 7.34e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 3600000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 4804456.380612964,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-05-10T00:00:00",
                "link": "https://arxiv.org/abs/2305.10403",
                "reference": "PaLM 2 Technical Report",
                "organization": "Google",
                "parameters": 340000000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "API access",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "PaLM 2 - 7.34e+24 FLOPs"
            },
            {
                "model": "StarCoder",
                "training_compute_(flop)": 8.46e+22,
                "training_power_draw_(w)": 407960.6765621668,
                "training_dataset_size_(gradients)": 203750000000.0,
                "training_time_(hours)": 625.5,
                "training_compute_cost_(2023_usd)": 212217.65075330864,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry, Government",
                "publication_date": "2023-05-09T00:00:00",
                "link": "https://arxiv.org/abs/2305.06161",
                "reference": "StarCoder: may the source be with you!",
                "organization": "Hugging Face,ServiceNow,Northeastern University,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms),Carnegie Mellon University (CMU),Johns Hopkins University,Leipzig University,ScaDS.AI,Queen Mary University of London,Roblox,Sea AI Lab,Technion - Israel Institute of Technology,Monash University,CSIRO,Data61,McGill University,Saama,University of British Columbia (UBC),Massachusetts Institute of Technology (MIT),Technical University of Munich,IBM,University of Vermont,UnfoldML,SAP,University of Notre Dame,Columbia University,New York University (NYU),University of Allahabad,Discover Dollar,Toloka,Telefonica,Stanford University,Weizmann Institute of Science,Alan Turing Institute,Wellesley College,EleutherAI,Forschungszentrum Julich",
                "parameters": 15500000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry, Government",
                "access_group": "Open",
                "model_and_compute": "StarCoder - 8.46e+22 FLOPs"
            },
            {
                "model": "MPT-7B",
                "training_compute_(flop)": 4.2e+22,
                "training_power_draw_(w)": 350622.43759455526,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 228.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-05-05T00:00:00",
                "link": "https://www.mosaicml.com/blog/mpt-7b",
                "reference": "\"Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs\"",
                "organization": "MosaicML",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "MPT-7B - 4.20e+22 FLOPs"
            },
            {
                "model": "OpenLLaMA-13B",
                "training_compute_(flop)": 7.8e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": null,
                "publication_date": "2023-05-01T00:00:00",
                "link": "https://github.com/openlm-research/open_llama",
                "reference": "OpenLLaMA: An Open Reproduction of LLaMA",
                "organization": "OpenLM Research",
                "parameters": 13000000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Other",
                "access_group": "Open",
                "model_and_compute": "OpenLLaMA-13B - 7.80e+22 FLOPs"
            },
            {
                "model": "MosaicML Diffusion",
                "training_compute_(flop)": 1.07085888e+22,
                "training_power_draw_(w)": 102015.15602722247,
                "training_dataset_size_(gradients)": 790000000.0,
                "training_time_(hours)": 186.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2023-04-28T00:00:00",
                "link": "https://www.databricks.com/blog/stable-diffusion-2\nhttps://www.databricks.com/blog/diffusion\nhttps://github.com/mosaicml/diffusion",
                "reference": "Training Stable Diffusion from Scratch for <$50k with MosaicML",
                "organization": "Databricks",
                "parameters": 1289952427.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "MosaicML Diffusion - 1.07e+22 FLOPs"
            },
            {
                "model": "WizardLM-7B",
                "training_compute_(flop)": 4.02e+22,
                "training_power_draw_(w)": 4782.3864227583035,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 70.0,
                "training_compute_cost_(2023_usd)": 46907.42757520694,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2023-04-24T00:00:00",
                "link": "https://arxiv.org/abs/2304.12244",
                "reference": "WizardLM: Empowering Large Language Models to Follow Complex Instructions",
                "organization": "Microsoft,Peking University",
                "parameters": 6700000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "WizardLM-7B - 4.02e+22 FLOPs"
            },
            {
                "model": "Falcon-7B",
                "training_compute_(flop)": 6.3e+22,
                "training_power_draw_(w)": 306072.7310565314,
                "training_dataset_size_(gradients)": 1500000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Government",
                "publication_date": "2023-04-24T00:00:00",
                "link": "https://huggingface.co/tiiuae/falcon-7b",
                "reference": "Falcon-7B ",
                "organization": "Technology Innovation Institute",
                "parameters": 7000000000.0,
                "notable_model": false,
                "country": "United Arab Emirates",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Government",
                "access_group": "Open",
                "model_and_compute": "Falcon-7B - 6.30e+22 FLOPs"
            },
            {
                "model": "ruGPT-3.5 13B",
                "training_compute_(flop)": 1.0699776e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 300000000000.0,
                "training_time_(hours)": 1080.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry, Government",
                "publication_date": "2023-04-24T00:00:00",
                "link": "https://huggingface.co/ai-forever/ruGPT-3.5-13B",
                "reference": "ruGPT-3.5 13B",
                "organization": "Sber",
                "parameters": 13000000000.0,
                "notable_model": false,
                "country": "Russia",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Other",
                "access_group": "Open",
                "model_and_compute": "ruGPT-3.5 13B - 1.07e+23 FLOPs"
            },
            {
                "model": "MOSS-Moon-003",
                "training_compute_(flop)": 6.67e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 700000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2023-04-19T00:00:00",
                "link": "https://huggingface.co/fnlp/moss-moon-003-base",
                "reference": null,
                "organization": "Fudan University",
                "parameters": 16000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "MOSS-Moon-003 - 6.67e+22 FLOPs"
            },
            {
                "model": "LLaVA",
                "training_compute_(flop)": 7.8049e+22,
                "training_power_draw_(w)": 6377.509314719864,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 10.0,
                "training_compute_cost_(2023_usd)": 42.46267260692187,
                "domain_group": "Multimodal",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2023-04-17T00:00:00",
                "link": "https://arxiv.org/abs/2304.08485",
                "reference": "Visual Instruction Tuning",
                "organization": "University of Wisconsin Madison,Microsoft Research,Columbia University",
                "parameters": 13000000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "LLaVA - 7.80e+22 FLOPs"
            },
            {
                "model": "DINOv2",
                "training_compute_(flop)": 7.41851136e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 36380002816.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 10203.60518105836,
                "domain_group": "Vision",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2023-04-14T00:00:00",
                "link": "https://arxiv.org/abs/2304.07193",
                "reference": "DINOv2: Learning Robust Visual Features without Supervision",
                "organization": "Facebook AI Research,INRIA",
                "parameters": 1140000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "DINOv2 - 7.42e+21 FLOPs"
            },
            {
                "model": "SenseChat",
                "training_compute_(flop)": 3.89e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 5328550.47393551,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-04-10T00:00:00",
                "link": "https://www.sensetime.com/en/news-detail/51166397?categoryId=1072",
                "reference": "SenseTime Launches \u201cSenseNova\u201d Foundation Model Sets and AI Computing Systems, Advancing AGI Development",
                "organization": "SenseTime",
                "parameters": 180000000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "API access",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "SenseChat - 3.89e+24 FLOPs"
            },
            {
                "model": "Incoder-6.7B",
                "training_compute_(flop)": 3.00001e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 52000000000.0,
                "training_time_(hours)": 576.0,
                "training_compute_cost_(2023_usd)": 3129.0771365574788,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2023-04-09T00:00:00",
                "link": "https://arxiv.org/abs/2204.05999",
                "reference": "InCoder: A Generative Model for Code Infilling and Synthesis",
                "organization": "Facebook AI Research,University of Washington,University of California (UC) Berkeley,Carnegie Mellon University (CMU),Toyota Technological Institute at Chicago",
                "parameters": 6700000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "Incoder-6.7B - 3.00e+21 FLOPs"
            },
            {
                "model": "gLM",
                "training_compute_(flop)": 2.26437e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2023-04-08T00:00:00",
                "link": "https://www.biorxiv.org/content/10.1101/2023.04.07.536042v1.full\nhttps://github.com/y-hwang/gLM",
                "reference": "Deep learning of genomic contexts predicts protein co-regulation and function",
                "organization": "Harvard University",
                "parameters": 1000000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "gLM - 2.26e+20 FLOPs"
            },
            {
                "model": "DiffDock-PP",
                "training_compute_(flop)": 3.5382841e+16,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 42826.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2023-04-08T00:00:00",
                "link": "https://arxiv.org/abs/2304.03889",
                "reference": "DiffDock-PP: Rigid Protein-Protein Docking with Diffusion Models",
                "organization": "Technical University of Munich,Massachusetts Institute of Technology (MIT)",
                "parameters": 1620000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "DiffDock-PP - 3.54e+16 FLOPs"
            },
            {
                "model": "Cerebras-GPT-13B",
                "training_compute_(flop)": 2.3e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 257100000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-04-06T00:00:00",
                "link": "https://arxiv.org/abs/2304.03208",
                "reference": "Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster",
                "organization": "Cerebras Systems",
                "parameters": 13000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Cerebras-GPT-13B - 2.30e+22 FLOPs"
            },
            {
                "model": "Segment Anything Model",
                "training_compute_(flop)": 7.8e+21,
                "training_power_draw_(w)": 204134.84223782964,
                "training_dataset_size_(gradients)": 1100000000.0,
                "training_time_(hours)": 68.0,
                "training_compute_cost_(2023_usd)": 15888.411228475235,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2023-04-05T00:00:00",
                "link": "https://arxiv.org/abs/2304.02643",
                "reference": "Segment Anything",
                "organization": "Meta AI",
                "parameters": 636000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Segment Anything Model - 7.80e+21 FLOPs"
            },
            {
                "model": "Pythia-12b",
                "training_compute_(flop)": 2.1590000000001e+22,
                "training_power_draw_(w)": 204143.93434948783,
                "training_dataset_size_(gradients)": 299892736000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2023-04-03T00:00:00",
                "link": "https://arxiv.org/abs/2304.01373",
                "reference": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling",
                "organization": "EleutherAI,Booz Allen Hamilton, McLean,University of Cambridge,Indraprastha Institute of Information Technology\nDelhi,Stability AI,datasaur.ai,University of Amsterdam",
                "parameters": 12000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "Pythia-12b - 2.16e+22 FLOPs"
            },
            {
                "model": "Pythia-2.8b",
                "training_compute_(flop)": 5.038000000001e+21,
                "training_power_draw_(w)": 51035.98358737197,
                "training_dataset_size_(gradients)": 299892736000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2023-04-03T00:00:00",
                "link": "https://arxiv.org/abs/2304.01373",
                "reference": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling",
                "organization": "EleutherAI,Booz Allen Hamilton, McLean,University of Cambridge,Indraprastha Institute of Information Technology\nDelhi,Stability AI,datasaur.ai,University of Amsterdam",
                "parameters": 2800000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "Pythia-2.8b - 5.04e+21 FLOPs"
            },
            {
                "model": "Pythia-6.9b",
                "training_compute_(flop)": 1.2420000000001e+22,
                "training_power_draw_(w)": 102071.96717474391,
                "training_dataset_size_(gradients)": 299892736000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2023-04-03T00:00:00",
                "link": "https://arxiv.org/abs/2304.01373",
                "reference": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling",
                "organization": "EleutherAI,Booz Allen Hamilton, McLean,University of Cambridge,Indraprastha Institute of Information Technology\nDelhi,Stability AI,datasaur.ai,University of Amsterdam",
                "parameters": 6900000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "Pythia-6.9b - 1.24e+22 FLOPs"
            },
            {
                "model": "Pythia-160m",
                "training_compute_(flop)": 2.87900000001e+20,
                "training_power_draw_(w)": 25517.991793685986,
                "training_dataset_size_(gradients)": 299892736000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2023-04-03T00:00:00",
                "link": "https://arxiv.org/abs/2304.01373",
                "reference": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling",
                "organization": "EleutherAI,Booz Allen Hamilton, McLean,University of Cambridge,Indraprastha Institute of Information Technology\nDelhi,Stability AI,datasaur.ai,University of Amsterdam",
                "parameters": 160000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "Pythia-160m - 2.88e+20 FLOPs"
            },
            {
                "model": "Pythia-1b",
                "training_compute_(flop)": 1.799000000001e+21,
                "training_power_draw_(w)": 51035.98358737197,
                "training_dataset_size_(gradients)": 299892736000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2023-04-03T00:00:00",
                "link": "https://arxiv.org/abs/2304.01373",
                "reference": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling",
                "organization": "EleutherAI,Booz Allen Hamilton, McLean,University of Cambridge,Indraprastha Institute of Information Technology\nDelhi,Stability AI,datasaur.ai,University of Amsterdam",
                "parameters": 1000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "Pythia-1b - 1.80e+21 FLOPs"
            },
            {
                "model": "Pythia-1.4b",
                "training_compute_(flop)": 2.5190000000001e+21,
                "training_power_draw_(w)": 51035.98358737197,
                "training_dataset_size_(gradients)": 299892736000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2023-04-03T00:00:00",
                "link": "https://arxiv.org/abs/2304.01373",
                "reference": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling",
                "organization": "EleutherAI,Booz Allen Hamilton, McLean,University of Cambridge,Indraprastha Institute of Information Technology\nDelhi,Stability AI,datasaur.ai,University of Amsterdam",
                "parameters": 1400000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "Pythia-1.4b - 2.52e+21 FLOPs"
            },
            {
                "model": "Pythia-70m",
                "training_compute_(flop)": 1.26000000001e+20,
                "training_power_draw_(w)": 25517.991793685986,
                "training_dataset_size_(gradients)": 299892736000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2023-04-03T00:00:00",
                "link": "https://arxiv.org/abs/2304.01373",
                "reference": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling",
                "organization": "EleutherAI,Booz Allen Hamilton, McLean,University of Cambridge,Indraprastha Institute of Information Technology\nDelhi,Stability AI,datasaur.ai,University of Amsterdam",
                "parameters": 70000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "Pythia-70m - 1.26e+20 FLOPs"
            },
            {
                "model": "Pythia-410m",
                "training_compute_(flop)": 7.37700000001e+20,
                "training_power_draw_(w)": 25517.991793685986,
                "training_dataset_size_(gradients)": 299892736000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2023-04-03T00:00:00",
                "link": "https://arxiv.org/abs/2304.01373",
                "reference": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling",
                "organization": "EleutherAI,Booz Allen Hamilton, McLean,University of Cambridge,Indraprastha Institute of Information Technology\nDelhi,Stability AI,datasaur.ai,University of Amsterdam",
                "parameters": 410000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "Pythia-410m - 7.38e+20 FLOPs"
            },
            {
                "model": "BloombergGPT",
                "training_compute_(flop)": 2.36e+23,
                "training_power_draw_(w)": 408324.2395754059,
                "training_dataset_size_(gradients)": 569000000000.0,
                "training_time_(hours)": 1270.0,
                "training_compute_cost_(2023_usd)": 369586.1352802876,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2023-03-30T00:00:00",
                "link": "https://arxiv.org/abs/2303.17564",
                "reference": "BloombergGPT: A Large Language Model for Finance",
                "organization": "Bloomberg,Johns Hopkins University",
                "parameters": 50558868480.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "BloombergGPT - 2.36e+23 FLOPs"
            },
            {
                "model": "VideoMAE V2",
                "training_compute_(flop)": 9.7e+21,
                "training_power_draw_(w)": 51041.66660009344,
                "training_dataset_size_(gradients)": 1243350000.0,
                "training_time_(hours)": 336.0,
                "training_compute_cost_(2023_usd)": 18339.96928068276,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2023-03-29T00:00:00",
                "link": "https://arxiv.org/abs/2303.16727v2",
                "reference": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
                "organization": "Nanjing University,Shenzhen Institute of Advanced Technology,Shanghai AI Lab",
                "parameters": 1000000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "VideoMAE V2 - 9.70e+21 FLOPs"
            },
            {
                "model": "ERNIE-ViLG 2.0",
                "training_compute_(flop)": 4.658135e+22,
                "training_power_draw_(w)": 255214.01639286993,
                "training_dataset_size_(gradients)": 11141120000000.0,
                "training_time_(hours)": 432.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2023-03-28T00:00:00",
                "link": "https://arxiv.org/abs/2210.15257",
                "reference": "ERNIE-ViLG 2.0: Improving Text-to-Image Diffusion Model with Knowledge-Enhanced Mixture-of-Denoising-Experts",
                "organization": "Baidu,Wuhan University of Science and Technology",
                "parameters": 24000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "API access",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Vision",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "ERNIE-ViLG 2.0 - 4.66e+22 FLOPs"
            },
            {
                "model": "SigLIP 400M",
                "training_compute_(flop)": 4.9467301e+21,
                "training_power_draw_(w)": 10846.837246253186,
                "training_dataset_size_(gradients)": 6705000000000.0,
                "training_time_(hours)": 120.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2023-03-27T00:00:00",
                "link": "https://arxiv.org/abs/2303.15343",
                "reference": "Sigmoid Loss for Language Image Pre-Training",
                "organization": "Google DeepMind",
                "parameters": 400000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "SigLIP 400M - 4.95e+21 FLOPs"
            },
            {
                "model": "SigLiT",
                "training_compute_(flop)": 7.6032e+19,
                "training_power_draw_(w)": 1355.8546557816485,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 48.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2023-03-27T00:00:00",
                "link": "https://arxiv.org/abs/2303.15343",
                "reference": "Sigmoid Loss for Language Image Pre-Training",
                "organization": "Google DeepMind",
                "parameters": null,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "SigLiT - 7.60e+19 FLOPs"
            },
            {
                "model": "EVA-CLIP (EVA-02-CLIP-E/14+)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t",
                "training_compute_(flop)": 3.456e+22,
                "training_power_draw_(w)": 17227.329744049177,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2023-03-27T00:00:00",
                "link": "https://arxiv.org/abs/2303.15389\t",
                "reference": "EVA-CLIP: Improved Training Techniques for CLIP at Scale",
                "organization": "Beijing Academy of Artificial Intelligence / BAAI,Huazhong University of Science and Technology",
                "parameters": 5000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "EVA-CLIP (EVA-02-CLIP-E/14+)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t - 3.46e+22 FLOPs"
            },
            {
                "model": "CPM-Bee",
                "training_compute_(flop)": 6.012e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 140066666667.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2023-03-24T00:00:00",
                "link": "https://github.com/OpenBMB/CPM-Bee",
                "reference": "Large model of open source Chinese and English bilingual base with tens of billions of parameters",
                "organization": "Tsinghua University,Beijing Academy of Artificial Intelligence / BAAI,ModelBest,OpenBMB (Open Lab for Big Model Base)",
                "parameters": 10000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "CPM-Bee - 6.01e+22 FLOPs"
            },
            {
                "model": "Lightweight Fine-tuning a Pretrained Protein Language Model for Protein Secondary",
                "training_compute_(flop)": 1.8710548688e+22,
                "training_power_draw_(w)": 493.0421354054347,
                "training_dataset_size_(gradients)": 10000001.0,
                "training_time_(hours)": 27.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2023-03-23T00:00:00",
                "link": "https://www.biorxiv.org/content/10.1101/2023.03.22.530066v1.abstract",
                "reference": "Lightweight Fine-tuning a Pretrained Protein Language Model for Protein Secondary Structure Prediction",
                "organization": "Henan University",
                "parameters": null,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Unknown",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Lightweight Fine-tuning a Pretrained Protein Language Model for Protein Secondary - 1.87e+22 FLOPs"
            },
            {
                "model": "Sparse Wide GPT-3 Small",
                "training_compute_(flop)": 1.875e+18,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2500000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-03-21T00:00:00",
                "link": "https://arxiv.org/abs/2303.11525",
                "reference": "Sparse Iso-FLOP Transformations for Maximizing Training Efficiency",
                "organization": "Cerebras Systems",
                "parameters": 1300000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Sparse Wide GPT-3 Small - 1.88e+18 FLOPs"
            },
            {
                "model": "LightOn Mini",
                "training_compute_(flop)": 2.4e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-03-21T00:00:00",
                "link": "https://www.lighton.ai/blog/lighton-s-blog-4/lighton-s-large-language-model-of-40-billion-parameters-mini-19",
                "reference": "LightOn's Large Language Model of 40 billion parameters: MINI",
                "organization": "LightOn",
                "parameters": 40000000000.0,
                "notable_model": false,
                "country": "France",
                "model_accessibility": "Hosted access (no API)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "LightOn Mini - 2.40e+23 FLOPs"
            },
            {
                "model": "PanGu-\u03a3",
                "training_compute_(flop)": 4.67e+23,
                "training_power_draw_(w)": 316521.76523003745,
                "training_dataset_size_(gradients)": 329000000000.0,
                "training_time_(hours)": 2400.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-03-20T00:00:00",
                "link": "https://arxiv.org/abs/2303.10845",
                "reference": "PanGu-\u03a3: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing",
                "organization": "Huawei Noah's Ark Lab",
                "parameters": 1085000000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Unreleased",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "PanGu-\u03a3 - 4.67e+23 FLOPs"
            },
            {
                "model": "GPT-4",
                "training_compute_(flop)": 2.1e+25,
                "training_power_draw_(w)": 19944368.12599428,
                "training_dataset_size_(gradients)": 5416666666666.667,
                "training_time_(hours)": 2280.0,
                "training_compute_cost_(2023_usd)": 42166697.17422247,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2023-03-15T00:00:00",
                "link": "https://arxiv.org/abs/2303.08774",
                "reference": "GPT-4 Technical Report",
                "organization": "OpenAI",
                "parameters": 1800000000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "API access",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "GPT-4 - 2.10e+25 FLOPs"
            },
            {
                "model": "Falcon-40B",
                "training_compute_(flop)": 2.4e+23,
                "training_power_draw_(w)": 306345.49441527214,
                "training_dataset_size_(gradients)": 1000000000000.0,
                "training_time_(hours)": 1440.0,
                "training_compute_cost_(2023_usd)": 319783.157242365,
                "domain_group": "Language",
                "organization_categorization": "Government",
                "publication_date": "2023-03-15T00:00:00",
                "link": "https://arxiv.org/abs/2311.16867; https://www.tii.ae/news/abu-dhabi-based-technology-innovation-institute-introduces-falcon-llm-foundational-large",
                "reference": "Abu Dhabi-based Technology Innovation Institute Introduces Falcon LLM: Foundational Large Language Model (LLM) outperforms GPT-3 with 40 Billion Parameters",
                "organization": "Technology Innovation Institute",
                "parameters": 40000000000.0,
                "notable_model": true,
                "country": "United Arab Emirates",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Government",
                "access_group": "Open",
                "model_and_compute": "Falcon-40B - 2.40e+23 FLOPs"
            },
            {
                "model": "VALL-E X",
                "training_compute_(flop)": 1.2e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 151200000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Audio",
                "organization_categorization": "Industry",
                "publication_date": "2023-03-07T00:00:00",
                "link": "https://arxiv.org/abs/2303.03926",
                "reference": "Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling",
                "organization": "Microsoft",
                "parameters": 700000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "VALL-E X - 1.20e+21 FLOPs"
            },
            {
                "model": "Uni-Mol Molecular Model",
                "training_compute_(flop)": 5.413824e+18,
                "training_power_draw_(w)": 3989.67317269014,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 20.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry, Government",
                "publication_date": "2023-03-06T00:00:00",
                "link": "https://chemrxiv.org/engage/chemrxiv/article-details/6402990d37e01856dc1d1581",
                "reference": "Uni-Mol: A Universal 3D Molecular Representation Learning Framework",
                "organization": "Renmin University of China,DP Technology,AI for Science Institute, Beijing (AISI)",
                "parameters": null,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry, Government",
                "access_group": "Open",
                "model_and_compute": "Uni-Mol Molecular Model - 5.41e+18 FLOPs"
            },
            {
                "model": "AudioGen",
                "training_compute_(flop)": 9.5e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 230400000000.0,
                "training_time_(hours)": 168.0,
                "training_compute_cost_(2023_usd)": 9429.74091062958,
                "domain_group": "Audio",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2023-03-05T00:00:00",
                "link": "https://arxiv.org/abs/2209.15352",
                "reference": "AudioGen: Textually Guided Audio Generation",
                "organization": "Meta AI,Hebrew University of Jerusalem",
                "parameters": 1000000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "AudioGen - 9.50e+21 FLOPs"
            },
            {
                "model": "DiT-XL/2",
                "training_compute_(flop)": 6e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 111048.19613085664,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2023-03-02T00:00:00",
                "link": "https://arxiv.org/abs/2212.09748",
                "reference": "Scalable Diffusion Models with Transformers",
                "organization": "New York University (NYU),University of California (UC) Berkeley",
                "parameters": 675000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "DiT-XL/2 - 6.00e+20 FLOPs"
            },
            {
                "model": "Palmyra Large 20B",
                "training_compute_(flop)": 9.6e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 800000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-03-01T00:00:00",
                "link": "https://huggingface.co/Writer/palmyra-large",
                "reference": null,
                "organization": "Writer",
                "parameters": 20000000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Palmyra Large 20B - 9.60e+22 FLOPs"
            },
            {
                "model": "gpt-sw3-40b",
                "training_compute_(flop)": 7.68e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": null,
                "publication_date": "2023-03-01T00:00:00",
                "link": "https://huggingface.co/AI-Sweden-Models/gpt-sw3-40b",
                "reference": "gpt-sw3-40b",
                "organization": "AI Sweden",
                "parameters": 40000000000.0,
                "notable_model": false,
                "country": "Sweden",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Other",
                "access_group": "Open",
                "model_and_compute": "gpt-sw3-40b - 7.68e+22 FLOPs"
            },
            {
                "model": "Kosmos-1",
                "training_compute_(flop)": 3.456e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 360000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2023-03-01T00:00:00",
                "link": "https://arxiv.org/abs/2302.14045",
                "reference": "Language Is Not All You Need: Aligning Perception with Language Models",
                "organization": "Microsoft",
                "parameters": 1600000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Kosmos-1 - 3.46e+21 FLOPs"
            },
            {
                "model": "LLaMA-13B",
                "training_compute_(flop)": 7.8e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 61300.86591335317,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-02-27T00:00:00",
                "link": "https://arxiv.org/abs/2302.13971",
                "reference": "LLaMA: Open and Efficient Foundation Language Models",
                "organization": "Meta AI",
                "parameters": 13000000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "LLaMA-13B - 7.80e+22 FLOPs"
            },
            {
                "model": "LLaMA-33B",
                "training_compute_(flop)": 2.7300000000001e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1400000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-02-27T00:00:00",
                "link": "https://arxiv.org/abs/2302.13971",
                "reference": "LLaMA: Open and Efficient Foundation Language Models",
                "organization": "Meta AI",
                "parameters": 32500000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "LLaMA-33B - 2.73e+23 FLOPs"
            },
            {
                "model": "LLaMA-65B",
                "training_compute_(flop)": 5.5e+23,
                "training_power_draw_(w)": 1634534.091472035,
                "training_dataset_size_(gradients)": 1400000000000.0,
                "training_time_(hours)": 500.0,
                "training_compute_cost_(2023_usd)": 578026.3043,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-02-24T00:00:00",
                "link": "https://arxiv.org/abs/2302.13971",
                "reference": "LLaMA: Open and Efficient Foundation Language Models",
                "organization": "Meta AI",
                "parameters": 65200000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "LLaMA-65B - 5.50e+23 FLOPs"
            },
            {
                "model": "LLaMA-7B",
                "training_compute_(flop)": 4.00000001e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 46889.820096866126,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2023-02-24T00:00:00",
                "link": "https://arxiv.org/abs/2302.13971",
                "reference": "LLaMA: Open and Efficient Foundation Language Models",
                "organization": "Meta AI",
                "parameters": 6700000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "LLaMA-7B - 4.00e+22 FLOPs"
            },
            {
                "model": "Hyena 1.3B",
                "training_compute_(flop)": 4.76e+19,
                "training_power_draw_(w)": 6385.325372107343,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2023-02-21T00:00:00",
                "link": "https://arxiv.org/abs/2302.10866",
                "reference": "Hyena Hierarchy: Towards Larger Convolutional Language Models",
                "organization": "Stanford University,University of Montreal / Universit\u00e9 de Montr\u00e9al,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms)",
                "parameters": 1300000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Hyena 1.3B - 4.76e+19 FLOPs"
            },
            {
                "model": "Hyena-2 355M",
                "training_compute_(flop)": 3.93e+19,
                "training_power_draw_(w)": 6385.325372107343,
                "training_dataset_size_(gradients)": 15000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2023-02-21T00:00:00",
                "link": "https://arxiv.org/abs/2302.10866",
                "reference": "Hyena Hierarchy: Towards Larger Convolutional Language Models",
                "organization": "Stanford University,University of Montreal / Universit\u00e9 de Montr\u00e9al,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms)",
                "parameters": 355000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Hyena-2 355M - 3.93e+19 FLOPs"
            },
            {
                "model": "Hyena-2 153M",
                "training_compute_(flop)": 1.87e+19,
                "training_power_draw_(w)": 6385.325372107343,
                "training_dataset_size_(gradients)": 15000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2023-02-21T00:00:00",
                "link": "https://arxiv.org/abs/2302.10866",
                "reference": "Hyena Hierarchy: Towards Larger Convolutional Language Models",
                "organization": "Stanford University,University of Montreal / Universit\u00e9 de Montr\u00e9al,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms)",
                "parameters": 153000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "Hyena-2 153M - 1.87e+19 FLOPs"
            },
            {
                "model": "ViT-22B",
                "training_compute_(flop)": 1.93248e+23,
                "training_power_draw_(w)": 347446.80145890714,
                "training_dataset_size_(gradients)": 4000000000.0,
                "training_time_(hours)": 347.4,
                "training_compute_cost_(2023_usd)": 285555.57016183576,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2023-02-10T00:00:00",
                "link": "https://arxiv.org/abs/2302.05442v1",
                "reference": "Scaling Vision Transformers to 22 Billion Parameters",
                "organization": "Google",
                "parameters": 21743000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "ViT-22B - 1.93e+23 FLOPs"
            },
            {
                "model": "ProteinSGM",
                "training_compute_(flop)": 3.024e+19,
                "training_power_draw_(w)": 329.03896884421727,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 168.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2023-02-04T00:00:00",
                "link": "https://www.biorxiv.org/content/10.1101/2022.07.13.499967v2.abstract",
                "reference": "ProteinSGM: Score-based generative modeling for de novo protein design",
                "organization": "University of Toronto",
                "parameters": null,
                "notable_model": false,
                "country": "Canada",
                "model_accessibility": "Unknown",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Canada",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "ProteinSGM - 3.02e+19 FLOPs"
            },
            {
                "model": "BLIP-2 (Q-Former)",
                "training_compute_(flop)": 1.20000000001e+21,
                "training_power_draw_(w)": 12776.908953102382,
                "training_dataset_size_(gradients)": 2322000000.0,
                "training_time_(hours)": 200.0,
                "training_compute_cost_(2023_usd)": 1960.8225382725811,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2023-01-30T00:00:00",
                "link": "https://arxiv.org/abs/2301.12597",
                "reference": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
                "organization": "Salesforce Research",
                "parameters": 1480000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "BLIP-2 (Q-Former) - 1.20e+21 FLOPs"
            },
            {
                "model": "Genie-SCOPe (bio)",
                "training_compute_(flop)": 1.81149696e+21,
                "training_power_draw_(w)": 9582.89511749987,
                "training_dataset_size_(gradients)": 1753200.0,
                "training_time_(hours)": 336.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2023-01-29T00:00:00",
                "link": "https://arxiv.org/abs/2301.12485",
                "reference": "Generating Novel, Designable, and Diverse Protein Structures by Equivariantly Diffusing Oriented Residue Clouds",
                "organization": "Columbia University",
                "parameters": 4100000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "Genie-SCOPe (bio) - 1.81e+21 FLOPs"
            },
            {
                "model": "Protst",
                "training_compute_(flop)": 1.400000000000012e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2023-01-28T00:00:00",
                "link": "https://proceedings.mlr.press/v202/xu23t/xu23t.pdf",
                "reference": "ProtST: Multi-Modality Learning of Protein Sequences and Biomedical Texts",
                "organization": "Mila - Quebec AI (originally Montreal Institute for Learning Algorithms),University of Montreal / Universit\u00e9 de Montr\u00e9al,Intel Labs,HEC Montreal,CIFAR AI Research",
                "parameters": null,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "Protst - 1.40e+19 FLOPs"
            },
            {
                "model": "DDPM-IP (CelebA)",
                "training_compute_(flop)": 3.5e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 831488000.0,
                "training_time_(hours)": 120.0,
                "training_compute_cost_(2023_usd)": 390.4861317667304,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2023-01-27T00:00:00",
                "link": "https://arxiv.org/abs/2301.11706v3",
                "reference": "Input Perturbation Reduces Exposure Bias in Diffusion Models",
                "organization": "Utrecht University",
                "parameters": 295000000.0,
                "notable_model": true,
                "country": "Netherlands",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "DDPM-IP (CelebA) - 3.50e+20 FLOPs"
            },
            {
                "model": "MoLFormer-XL",
                "training_compute_(flop)": 4.509e+20,
                "training_power_draw_(w)": 9583.748775717228,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 208.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Industry",
                "publication_date": "2023-01-25T00:00:00",
                "link": "https://research.ibm.com/blog/molecular-transformer-discovery\nhttps://www.nature.com/articles/s42256-022-00580-7",
                "reference": "An AI foundation model that learns the grammar of molecules",
                "organization": "IBM",
                "parameters": null,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Biology",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "MoLFormer-XL - 4.51e+20 FLOPs"
            },
            {
                "model": "GPT-2+Active-SGD (WT2)",
                "training_compute_(flop)": 2.976e+17,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2023-01-24T00:00:00",
                "link": "https://arxiv.org/abs/2301.10133",
                "reference": "Read the Signs Towards Invariance to Gradient Descent\u2019s Hyperparameter Initialization",
                "organization": "University of Montreal / Universit\u00e9 de Montr\u00e9al",
                "parameters": 124000000.0,
                "notable_model": false,
                "country": "Canada",
                "model_accessibility": "Unreleased",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Canada",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "GPT-2+Active-SGD (WT2) - 2.98e+17 FLOPs"
            },
            {
                "model": "Adaptive Agent",
                "training_compute_(flop)": 2.8e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 840.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Games",
                "organization_categorization": "Industry",
                "publication_date": "2023-01-18T00:00:00",
                "link": "https://arxiv.org/abs/2301.07608",
                "reference": "Human-Timescale Adaptation in an Open-Ended Task Space",
                "organization": "DeepMind",
                "parameters": 533000000.0,
                "notable_model": false,
                "country": "United Kingdom",
                "model_accessibility": "Unreleased",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Adaptive Agent - 2.80e+21 FLOPs"
            },
            {
                "model": "Ankh_large",
                "training_compute_(flop)": 6.5e+21,
                "training_power_draw_(w)": 21727.518178781513,
                "training_dataset_size_(gradients)": 14000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 4802.398249072418,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2023-01-16T00:00:00",
                "link": "https://arxiv.org/abs/2301.06568",
                "reference": "Ankh: Optimized Protein Language Model Unlocks General-Purpose Modelling",
                "organization": "Technical University of Munich,Columbia University",
                "parameters": 1900000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "Ankh_large - 6.50e+21 FLOPs"
            },
            {
                "model": "Ankh_base",
                "training_compute_(flop)": 2.6e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 14000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 1920.959299628968,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2023-01-16T00:00:00",
                "link": "https://arxiv.org/abs/2301.06568",
                "reference": "Ankh: Optimized Protein Language Model Unlocks General-Purpose Modelling",
                "organization": "Technical University of Munich,Columbia University",
                "parameters": 740000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "Ankh_base - 2.60e+21 FLOPs"
            },
            {
                "model": "Nucleotide Transformer",
                "training_compute_(flop)": 8.08e+21,
                "training_power_draw_(w)": 102249.42137571,
                "training_dataset_size_(gradients)": 300000000000.0,
                "training_time_(hours)": 672.0,
                "training_compute_cost_(2023_usd)": 51064.70621394041,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2023-01-15T00:00:00",
                "link": "https://www.biorxiv.org/content/10.1101/2023.01.11.523679v1.full.pdf",
                "reference": "The Nucleotide Transformer: Building and Evaluating Robust\nFoundation Models for Human Genomics",
                "organization": "NVIDIA,Technical University of Munich,InstaDeep",
                "parameters": 2500000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "Nucleotide Transformer - 8.08e+21 FLOPs"
            },
            {
                "model": "DreamerV3",
                "training_compute_(flop)": 2.2032e+20,
                "training_power_draw_(w)": 9586.950671364422,
                "training_dataset_size_(gradients)": 1600000000.0,
                "training_time_(hours)": 6528.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Games",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2023-01-10T00:00:00",
                "link": "https://arxiv.org/abs/2301.04104v1",
                "reference": "Mastering Diverse Domains through World Models",
                "organization": "DeepMind,University of Toronto",
                "parameters": 200000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "DreamerV3 - 2.20e+20 FLOPs"
            },
            {
                "model": "SantaCoder",
                "training_compute_(flop)": 2.1e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 150.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2023-01-09T00:00:00",
                "link": "https://arxiv.org/abs/2301.03988",
                "reference": "SantaCoder: don't reach for the stars!",
                "organization": "Hugging Face,ServiceNow,Massachusetts Institute of Technology (MIT),Wellesley College,Saama,EleutherAI,Huawei Noah's Ark Lab,Carnegie Mellon University (CMU)",
                "parameters": 1100000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "SantaCoder - 2.10e+21 FLOPs"
            },
            {
                "model": "VALL-E",
                "training_compute_(flop)": 1.01e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 76800000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 11.40575196486363,
                "domain_group": "Audio",
                "organization_categorization": "Industry",
                "publication_date": "2023-01-05T00:00:00",
                "link": "https://arxiv.org/abs/2301.02111",
                "reference": "Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers",
                "organization": "Microsoft",
                "parameters": 353000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "VALL-E - 1.01e+19 FLOPs"
            },
            {
                "model": "SparseOPT-175B",
                "training_compute_(flop)": 1.58e+23,
                "training_power_draw_(w)": 439.0411532246195,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 4.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2023-01-02T00:00:00",
                "link": "https://arxiv.org/abs/2301.00774",
                "reference": "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot",
                "organization": "Institute of Science and Technology Austria (ISTA),Neural Magic",
                "parameters": 87500000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "SparseOPT-175B - 1.58e+23 FLOPs"
            },
            {
                "model": "DNA Fine-Tuned Language Model (DFLM)",
                "training_compute_(flop)": 3.5e+18,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2023-01-02T00:00:00",
                "link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9751352",
                "reference": "Predicting the Sequence Specificities of DNA-Binding Proteins by DNA Fine-Tuned Language Model With Decaying Learning Rates",
                "organization": "Tongji University",
                "parameters": null,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Unreleased",
                "year": 2023,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "DNA Fine-Tuned Language Model (DFLM) - 3.50e+18 FLOPs"
            },
            {
                "model": "Hybrid H3-2.7B",
                "training_compute_(flop)": 6.48e+21,
                "training_power_draw_(w)": 6393.151008587547,
                "training_dataset_size_(gradients)": 400000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2022-12-28T00:00:00",
                "link": "https://arxiv.org/abs/2212.14052",
                "reference": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
                "organization": "Stanford University,University at Buffalo",
                "parameters": 2700000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "Hybrid H3-2.7B - 6.48e+21 FLOPs"
            },
            {
                "model": "OPT-IML (175B)",
                "training_compute_(flop)": 4.3e+23,
                "training_power_draw_(w)": 102304.08471008124,
                "training_dataset_size_(gradients)": 2000000000.0,
                "training_time_(hours)": 72.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2022-12-22T00:00:00",
                "link": "https://arxiv.org/abs/2212.12017",
                "reference": "OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization",
                "organization": "Meta AI",
                "parameters": 175000000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "OPT-IML (175B) - 4.30e+23 FLOPs"
            },
            {
                "model": "CaLM",
                "training_compute_(flop)": 2.9e+19,
                "training_power_draw_(w)": 1278.886496016633,
                "training_dataset_size_(gradients)": 2523746560.0,
                "training_time_(hours)": 960.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2022-12-19T00:00:00",
                "link": "https://www.biorxiv.org/content/10.1101/2022.12.15.519894v1.full.pdf",
                "reference": "Codon language embeddings provide strong signals for protein engineering",
                "organization": "University of Oxford",
                "parameters": 86000000.0,
                "notable_model": true,
                "country": "United Kingdom",
                "model_accessibility": "Unreleased",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "CaLM - 2.90e+19 FLOPs"
            },
            {
                "model": "Stable Diffusion 2.1",
                "training_compute_(flop)": 6.7392e+22,
                "training_power_draw_(w)": 204676.52826620263,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2022-12-07T00:00:00",
                "link": "https://stability.ai/news/stablediffusion2-1-release7-dec-2022",
                "reference": "Stable Diffusion v2.1 and DreamStudio Updates 7-Dec 22",
                "organization": "Stability AI",
                "parameters": null,
                "notable_model": false,
                "country": "United Kingdom",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Stable Diffusion 2.1 - 6.74e+22 FLOPs"
            },
            {
                "model": "Whisper v2",
                "training_compute_(flop)": 1.1e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 12403200000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Audio",
                "organization_categorization": "Industry",
                "publication_date": "2022-12-05T00:00:00",
                "link": "https://huggingface.co/openai/whisper-large-v2\n\nhttps://arxiv.org/abs/2212.04356",
                "reference": "Robust Speech Recognition via Large-Scale Weak Supervision",
                "organization": "OpenAI",
                "parameters": 1550000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Whisper v2 - 1.10e+23 FLOPs"
            },
            {
                "model": "Vega v2",
                "training_compute_(flop)": 7.76e+22,
                "training_power_draw_(w)": 255862.7534697245,
                "training_dataset_size_(gradients)": 6439999999.0,
                "training_time_(hours)": 720.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2022-12-04T00:00:00",
                "link": "https://arxiv.org/abs/2212.01853",
                "reference": "Toward Efficient Language Model Pretraining and Downstream Adaptation via Self-Evolution: A Case Study on SuperGLUE",
                "organization": "Wuhan University,JD Explore Academy,Shanghai AI Lab,Nanyang Technological University,Washington University in St Louis,Chongqing University of Posts and Telecommunications,University of Sydney",
                "parameters": 6000000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Vega v2 - 7.76e+22 FLOPs"
            },
            {
                "model": "Transformer + GFM",
                "training_compute_(flop)": 7.7369416e+18,
                "training_power_draw_(w)": 5597.371669502683,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2022-12-01T00:00:00",
                "link": "https://cs.nju.edu.cn/wujx/paper/AAAI2023_AFM.pdf",
                "reference": "\"Compressing Transformers: Features Are Low-Rank, but Weights Are Not\"",
                "organization": "Nanjing University",
                "parameters": 185200000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Unreleased",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Transformer + GFM - 7.74e+18 FLOPs"
            },
            {
                "model": "ZymCTRL",
                "training_compute_(flop)": 5.05e+21,
                "training_power_draw_(w)": 38381.97716230411,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2022-12-01T00:00:00",
                "link": "https://www.mlsb.io/papers_2022/ZymCTRL_a_conditional_language_model_for_the_controllable_generation_of_artificial_enzymes.pdf",
                "reference": "ZymCTRL: a conditional language model for the controllable generation of artificial enzymes",
                "organization": "Basecamp Research,Friedrich-Alexander-Universit\u00e4t,University of Girona",
                "parameters": 738000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "ZymCTRL - 5.05e+21 FLOPs"
            },
            {
                "model": "GPT-3.5",
                "training_compute_(flop)": 2.578e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 4805631.250163697,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2022-11-28T00:00:00",
                "link": "https://platform.openai.com/docs/models/gpt-3-5",
                "reference": null,
                "organization": "OpenAI",
                "parameters": null,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "API access",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "GPT-3.5 - 2.58e+24 FLOPs"
            },
            {
                "model": "Discriminator Guidance",
                "training_compute_(flop)": 2.1570000001e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 327978752.0,
                "training_time_(hours)": 481.0,
                "training_compute_cost_(2023_usd)": 337.8811363173893,
                "domain_group": "Vision",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2022-11-28T00:00:00",
                "link": "https://arxiv.org/abs/2211.17091v4",
                "reference": "Refining Generative Process with Discriminator Guidance in Score-based Diffusion Models",
                "organization": "Korea Advanced Institute of Science and Technology (KAIST),NAVER",
                "parameters": null,
                "notable_model": true,
                "country": "South Korea",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "South Korea",
                "domain_top4": "Vision",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "Discriminator Guidance - 2.16e+20 FLOPs"
            },
            {
                "model": "CLUE",
                "training_compute_(flop)": 3.456e+18,
                "training_power_draw_(w)": 38389.67061575342,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 168.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Recommendation",
                "organization_categorization": "Industry",
                "publication_date": "2022-11-22T00:00:00",
                "link": "https://arxiv.org/abs/2111.11294",
                "reference": "Scaling Law for Recommendation Models: Towards General-purpose User Representations",
                "organization": "Naver Clova,Naver AI Lab",
                "parameters": 160000000.0,
                "notable_model": false,
                "country": "South Korea",
                "model_accessibility": "Unknown",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "South Korea",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "CLUE - 3.46e+18 FLOPs"
            },
            {
                "model": "AR-LDM",
                "training_compute_(flop)": 5.1e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 194.0,
                "training_compute_cost_(2023_usd)": 745.8360575556148,
                "domain_group": "Vision",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2022-11-20T00:00:00",
                "link": "https://arxiv.org/abs/2211.10950",
                "reference": "Synthesizing Coherent Story with Auto-Regressive Latent Diffusion Models",
                "organization": "Alibaba,University of Waterloo,Vector Institute",
                "parameters": 1500000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "AR-LDM - 5.10e+20 FLOPs"
            },
            {
                "model": "Fusion in Encoder",
                "training_compute_(flop)": 1.3e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 960000.0,
                "training_time_(hours)": 48.0,
                "training_compute_cost_(2023_usd)": 233.0630322095263,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2022-11-18T00:00:00",
                "link": "https://arxiv.org/abs/2211.10147",
                "reference": "FiE: Building a Global Probability Space by Leveraging Early Fusion in Encoder for Open-Domain Question Answering",
                "organization": "Samsung",
                "parameters": 330000000.0,
                "notable_model": true,
                "country": "South Korea",
                "model_accessibility": "Unreleased",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "South Korea",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Fusion in Encoder - 1.30e+20 FLOPs"
            },
            {
                "model": "Galactica",
                "training_compute_(flop)": 3.24e+23,
                "training_power_draw_(w)": 102386.13451047534,
                "training_dataset_size_(gradients)": 106000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 591076.8943544837,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2022-11-16T00:00:00",
                "link": "https://arxiv.org/abs/2211.09085",
                "reference": "Galactica: A Large Language Model for Science",
                "organization": "Meta AI",
                "parameters": 120000000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Galactica - 3.24e+23 FLOPs"
            },
            {
                "model": "EVA-01",
                "training_compute_(flop)": 1.501e+22,
                "training_power_draw_(w)": 102390.69476171424,
                "training_dataset_size_(gradients)": 7577600000.0,
                "training_time_(hours)": 348.0,
                "training_compute_cost_(2023_usd)": 29374.46909439904,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2022-11-14T00:00:00",
                "link": "https://arxiv.org/abs/2211.07636",
                "reference": "EVA: Exploring the Limits of Masked Visual Representation Learning at Scale",
                "organization": "Beijing Academy of Artificial Intelligence / BAAI,Huazhong University of Science and Technology,Zhejiang University (ZJU),Beijing Institute of Technology",
                "parameters": 1011000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "EVA-01 - 1.50e+22 FLOPs"
            },
            {
                "model": "InternImage",
                "training_compute_(flop)": 2.408e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 83692000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2022-11-10T00:00:00",
                "link": "https://arxiv.org/abs/2211.05778",
                "reference": "InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions",
                "organization": "Shanghai AI Lab,Tsinghua University,Nanjing University,SenseTime,Chinese University of Hong Kong (CUHK)",
                "parameters": 1080000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "InternImage - 2.41e+21 FLOPs"
            },
            {
                "model": "Mogrifier RLSTM (WT2)",
                "training_compute_(flop)": 1.4e+17,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2666667.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2022-11-03T00:00:00",
                "link": "https://arxiv.org/abs/2211.01848",
                "reference": "Circling Back to Recurrent Models of Language",
                "organization": "DeepMind",
                "parameters": 35000000.0,
                "notable_model": true,
                "country": "United Kingdom",
                "model_accessibility": "Unreleased",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Mogrifier RLSTM (WT2) - 1.40e+17 FLOPs"
            },
            {
                "model": "Mogrifier RLSTM (PTB)",
                "training_compute_(flop)": 7.1347219e+16,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1238667.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2022-11-03T00:00:00",
                "link": "https://arxiv.org/abs/2211.01848",
                "reference": "Circling Back to Recurrent Models of Language",
                "organization": "DeepMind",
                "parameters": 24000000.0,
                "notable_model": false,
                "country": "United Kingdom",
                "model_accessibility": "Unreleased",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Mogrifier RLSTM (PTB) - 7.13e+16 FLOPs"
            },
            {
                "model": "eDiff-I",
                "training_compute_(flop)": 5.46e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1556275200000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2022-11-02T00:00:00",
                "link": "https://arxiv.org/abs/2211.01324",
                "reference": "eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers",
                "organization": "NVIDIA",
                "parameters": 9100000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "eDiff-I - 5.46e+19 FLOPs"
            },
            {
                "model": "Taiyi-Stable Diffusion",
                "training_compute_(flop)": 5.1e+22,
                "training_power_draw_(w)": 25605.655552009957,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 100.0,
                "training_compute_cost_(2023_usd)": 113638.30314103366,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2022-10-31T00:00:00",
                "link": "https://arxiv.org/abs/2209.02970",
                "reference": "Fengshenbang 1.0: Being the Foundation of Chinese Cognitive Intelligence",
                "organization": "IDEA CCNL",
                "parameters": 1000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "Taiyi-Stable Diffusion - 5.10e+22 FLOPs"
            },
            {
                "model": "Transformer-XL + PowerSGD + L-Greco",
                "training_compute_(flop)": 1.3150368e+18,
                "training_power_draw_(w)": 5601.237152002178,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": 4.28,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2022-10-31T00:00:00",
                "link": "https://arxiv.org/abs/2210.17357",
                "reference": "L-GreCo: An Efficient and General Framework for Layerwise-Adaptive Gradient Compression",
                "organization": "Institute of Science and Technology Austria (ISTA),Neural Magic",
                "parameters": null,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "Transformer-XL + PowerSGD + L-Greco - 1.32e+18 FLOPs"
            },
            {
                "model": "MIF-ST",
                "training_compute_(flop)": 2.50000000000002e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2022-10-26T00:00:00",
                "link": "https://academic.oup.com/peds/article-abstract/doi/10.1093/protein/gzad015/7330543?redirectedFrom=fulltext#no-access-message",
                "reference": "Masked inverse folding with sequence transfer for protein representation learning",
                "organization": "Microsoft Research,OpenBioML,University of Chicago",
                "parameters": null,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "MIF-ST - 2.50e+22 FLOPs"
            },
            {
                "model": "XY-LENTXL",
                "training_compute_(flop)": 7.2e+21,
                "training_power_draw_(w)": 717038.1909599457,
                "training_dataset_size_(gradients)": 600000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2022-10-26T00:00:00",
                "link": "https://arxiv.org/abs/2210.14867",
                "reference": "Beyond English-Centric Bitexts for Better Multilingual Language Representation Learning",
                "organization": "Microsoft",
                "parameters": 2000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "XY-LENTXL - 7.20e+21 FLOPs"
            },
            {
                "model": "Verbatim Memory Transformer (108M)",
                "training_compute_(flop)": 9.864288e+17,
                "training_power_draw_(w)": 285.82195776886823,
                "training_dataset_size_(gradients)": 40000000.0,
                "training_time_(hours)": 12.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2022-10-24T00:00:00",
                "link": "https://arxiv.org/abs/2210.13569",
                "reference": "Characterizing Verbatim Short-Term Memory in Neural Language Models",
                "organization": "Johns Hopkins University,New York University (NYU)",
                "parameters": 107700000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Verbatim Memory Transformer (108M) - 9.86e+17 FLOPs"
            },
            {
                "model": "DiffSBDD (CrossDocked)",
                "training_compute_(flop)": 2.69568e+20,
                "training_power_draw_(w)": 439.7260888751819,
                "training_dataset_size_(gradients)": 2900000.0,
                "training_time_(hours)": 600.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2022-10-24T00:00:00",
                "link": "https://arxiv.org/abs/2210.13695",
                "reference": "Structure-based Drug Design with Equivariant Diffusion Models",
                "organization": "Ecole Polytechnique F\u00b4ed\u00b4erale de Lausanne (EPFL),University of Cambridge,Cornell University,Chinese Academy of Mathematics and System Science,University of Rome,Microsoft Research,University of Oxford,AITHYRA Institute",
                "parameters": null,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "DiffSBDD (CrossDocked) - 2.70e+20 FLOPs"
            },
            {
                "model": "U-PaLM (540B)",
                "training_compute_(flop)": 2.53e+24,
                "training_power_draw_(w)": 174161.11557264757,
                "training_dataset_size_(gradients)": 1300000000.0,
                "training_time_(hours)": 120.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2022-10-20T00:00:00",
                "link": "https://arxiv.org/abs/2210.11399",
                "reference": "Transcending Scaling Laws with 0.1% Extra Compute",
                "organization": "Google",
                "parameters": 540000000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "U-PaLM (540B) - 2.53e+24 FLOPs"
            },
            {
                "model": "Flan-PaLM 540B",
                "training_compute_(flop)": 2.540000000001e+24,
                "training_power_draw_(w)": 174161.11557264757,
                "training_dataset_size_(gradients)": 1400000000.0,
                "training_time_(hours)": 37.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2022-10-20T00:00:00",
                "link": "https://arxiv.org/abs/2210.11416",
                "reference": "Scaling Instruction-Finetuned Language Models",
                "organization": "Google",
                "parameters": 540000000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Flan-PaLM 540B - 2.54e+24 FLOPs"
            },
            {
                "model": "Flan-T5 11B",
                "training_compute_(flop)": 3.3e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 100000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 98374.29476250126,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2022-10-20T00:00:00",
                "link": "https://arxiv.org/abs/2210.11416, https://huggingface.co/google/flan-t5-xxl",
                "reference": "Scaling Instruction-Finetuned Language Models",
                "organization": "Google",
                "parameters": 11000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Flan-T5 11B - 3.30e+22 FLOPs"
            },
            {
                "model": "GPT-2 + Progressive LRD",
                "training_compute_(flop)": 7.936e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2022-10-12T00:00:00",
                "link": "https://neurips2022-enlsp.github.io/papers/paper_33.pdf",
                "reference": "Strategies for Applying Low Rank Decomposition to Transformer-Based Models",
                "organization": "Huawei,Huawei Noah's Ark Lab",
                "parameters": 31000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Unreleased",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "GPT-2 + Progressive LRD - 7.94e+20 FLOPs"
            },
            {
                "model": "GenSLM",
                "training_compute_(flop)": 1.42e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 225280000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2022-10-11T00:00:00",
                "link": "https://www.biorxiv.org/content/biorxiv/early/2022/10/11/2022.10.10.511571.full.pdf",
                "reference": "GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics",
                "organization": "University of Chicago,NVIDIA,Harvard University,Cerebras Systems,Technical University of Munich,California Institute of Technology",
                "parameters": 25000000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "GenSLM - 1.42e+21 FLOPs"
            },
            {
                "model": "Decaying Fast Weights Transformer (WT-103)",
                "training_compute_(flop)": 7.9e+19,
                "training_power_draw_(w)": 329.9047498229277,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": 35.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2022-10-09T00:00:00",
                "link": "https://arxiv.org/abs/2210.04243",
                "reference": "Fine-Tuning Pre-trained Transformers into Decaying Fast Weights",
                "organization": "Jenni",
                "parameters": 242000000.0,
                "notable_model": false,
                "country": "Singapore",
                "model_accessibility": "Unreleased",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Decaying Fast Weights Transformer (WT-103) - 7.90e+19 FLOPs"
            },
            {
                "model": "AlphaTensor",
                "training_compute_(flop)": 7.1414784e+20,
                "training_power_draw_(w)": 28182.534174334724,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 168.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2022-10-05T00:00:00",
                "link": "https://www.nature.com/articles/s41586-022-05172-4",
                "reference": "Discovering faster matrix multiplication algorithms with reinforcement learning",
                "organization": "DeepMind",
                "parameters": null,
                "notable_model": true,
                "country": "United Kingdom",
                "model_accessibility": "Unreleased",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "AlphaTensor - 7.14e+20 FLOPs"
            },
            {
                "model": "DiffDock",
                "training_compute_(flop)": 7.2e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 4352000.0,
                "training_time_(hours)": 432.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2022-10-04T00:00:00",
                "link": "https://arxiv.org/abs/2210.01776, https://docs.nvidia.com/bionemo-framework/latest/models/diffdock.html",
                "reference": "DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking",
                "organization": "Massachusetts Institute of Technology (MIT)",
                "parameters": 20240000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "DiffDock - 7.20e+19 FLOPs"
            },
            {
                "model": "NMST+GPT-2",
                "training_compute_(flop)": 1.20380928e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2022-10-03T00:00:00",
                "link": "https://arxiv.org/abs/2210.00660",
                "reference": "A Non-monotonic Self-terminating Language Model",
                "organization": "New York University (NYU)",
                "parameters": 124000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "NMST+GPT-2 - 1.20e+20 FLOPs"
            },
            {
                "model": "GemNet-OC ",
                "training_compute_(flop)": 5.4e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Materials science",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2022-09-30T00:00:00",
                "link": "https://arxiv.org/abs/2204.02782",
                "reference": "GemNet-OC: Developing Graph Neural Networks for Large and Diverse Molecular Simulation Datasets",
                "organization": "Technical University of Munich,Carnegie Mellon University (CMU),Facebook AI Research",
                "parameters": null,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "GemNet-OC  - 5.40e+20 FLOPs"
            },
            {
                "model": "CPM-Ant\n",
                "training_compute_(flop)": 2.004e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 33400000000.0,
                "training_time_(hours)": 1632.0,
                "training_compute_cost_(2023_usd)": 66779.34,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2022-09-22T00:00:00",
                "link": "https://github.com/OpenBMB/CPM-Live/tree/cpm-ant/cpm-live#model-checkpoints",
                "reference": "Open-Source 10B-Parameter Chinese pretrained language model",
                "organization": "Tsinghua University,Beijing Academy of Artificial Intelligence / BAAI,ModelBest,OpenBMB (Open Lab for Big Model Base)",
                "parameters": 10000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "CPM-Ant\n - 2.00e+22 FLOPs"
            },
            {
                "model": "Whisper",
                "training_compute_(flop)": 4.2072663e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 12403200000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Audio",
                "organization_categorization": "Industry",
                "publication_date": "2022-09-21T00:00:00",
                "link": "https://cdn.openai.com/papers/whisper.pdf\n\nhttps://arxiv.org/abs/2212.04356",
                "reference": "Robust Speech Recognition via Large-Scale Weak Supervision",
                "organization": "OpenAI",
                "parameters": 1550000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Whisper - 4.21e+21 FLOPs"
            },
            {
                "model": "DistilProtBert",
                "training_compute_(flop)": 1.9e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 11008000000.0,
                "training_time_(hours)": 288.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2022-09-18T00:00:00",
                "link": "https://academic.oup.com/bioinformatics/article/38/Supplement_2/ii95/6701995",
                "reference": "DistilProtBert: a distilled protein language model used to distinguish between real proteins and their randomly shuffled counterparts",
                "organization": "Bar-Ilan University",
                "parameters": 230000000.0,
                "notable_model": false,
                "country": "Israel",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "DistilProtBert - 1.90e+20 FLOPs"
            },
            {
                "model": "CLIP ViT-H/14 - LAION-2B",
                "training_compute_(flop)": 7.8410000000001e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 11555735510.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": null,
                "publication_date": "2022-09-15T00:00:00",
                "link": "https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K",
                "reference": " Model Card for CLIP ViT-H/14 - LAION-2B ",
                "organization": "LAION",
                "parameters": 986000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Other",
                "access_group": "Open",
                "model_and_compute": "CLIP ViT-H/14 - LAION-2B - 7.84e+22 FLOPs"
            },
            {
                "model": "PaLI",
                "training_compute_(flop)": 1.69e+23,
                "training_power_draw_(w)": 348601.5921270125,
                "training_dataset_size_(gradients)": 143507000000.0,
                "training_time_(hours)": 240.0,
                "training_compute_cost_(2023_usd)": 50878.10777366616,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2022-09-14T00:00:00",
                "link": "https://arxiv.org/abs/2209.06794v4",
                "reference": "PaLI: A Jointly-Scaled Multilingual Language-Image Model",
                "organization": "Google",
                "parameters": 16900000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "PaLI - 1.69e+23 FLOPs"
            },
            {
                "model": "BEIT-3",
                "training_compute_(flop)": 7e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2022-08-22T00:00:00",
                "link": "https://arxiv.org/abs/2208.10442",
                "reference": "Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks",
                "organization": "Microsoft",
                "parameters": 1900000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "BEIT-3 - 7.00e+19 FLOPs"
            },
            {
                "model": "Stable Diffusion 1.4",
                "training_compute_(flop)": 5e+22,
                "training_power_draw_(w)": 153873.61325387988,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2022-08-22T00:00:00",
                "link": "https://huggingface.co/CompVis/stable-diffusion-v1-4",
                "reference": "Stable Diffusion v1-4 Model Card",
                "organization": "Ludwig Maximilian University of Munich",
                "parameters": null,
                "notable_model": false,
                "country": "Germany",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Germany",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "Stable Diffusion 1.4 - 5.00e+22 FLOPs"
            },
            {
                "model": "Luminous-supreme",
                "training_compute_(flop)": 3.5461e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1069300000000.0,
                "training_time_(hours)": 2016.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2022-08-15T00:00:00",
                "link": "https://docs.aleph-alpha.com/docs/introduction/model-card/",
                "reference": "Model Card Luminous",
                "organization": "Aleph Alpha",
                "parameters": 70000000000.0,
                "notable_model": false,
                "country": "Germany",
                "model_accessibility": "API access",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Germany",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Luminous-supreme - 3.55e+23 FLOPs"
            },
            {
                "model": "Luminous-extended",
                "training_compute_(flop)": 1.0019457e+23,
                "training_power_draw_(w)": 410393.6048104728,
                "training_dataset_size_(gradients)": 460000000000.0,
                "training_time_(hours)": 1344.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2022-08-15T00:00:00",
                "link": "https://docs.aleph-alpha.com/docs/Deprecated%20Luminous/Deprecated-Luminous/model-card/",
                "reference": null,
                "organization": "Aleph Alpha",
                "parameters": 30000000000.0,
                "notable_model": false,
                "country": "Germany",
                "model_accessibility": "API access",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Germany",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Luminous-extended - 1.00e+23 FLOPs"
            },
            {
                "model": "Luminous-base",
                "training_compute_(flop)": 3.1673782e+22,
                "training_power_draw_(w)": 102598.4012026182,
                "training_dataset_size_(gradients)": 402000000000.0,
                "training_time_(hours)": 1344.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2022-08-15T00:00:00",
                "link": "https://docs.aleph-alpha.com/docs/Deprecated%20Luminous/Deprecated-Luminous/model-card/",
                "reference": null,
                "organization": "Aleph Alpha",
                "parameters": 13000000000.0,
                "notable_model": false,
                "country": "Germany",
                "model_accessibility": "API access",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Germany",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Luminous-base - 3.17e+22 FLOPs"
            },
            {
                "model": "PeTriBERT",
                "training_compute_(flop)": 1e+20,
                "training_power_draw_(w)": 4007.928551099969,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 70.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Industry",
                "publication_date": "2022-08-13T00:00:00",
                "link": "https://www.biorxiv.org/content/10.1101/2022.08.10.503344v1.abstract",
                "reference": "PeTriBERT : Augmenting BERT with tridimensional encoding for inverse protein folding and design",
                "organization": "University of Montpellier,BionomeeX",
                "parameters": 40000000.0,
                "notable_model": false,
                "country": "France",
                "model_accessibility": "Unknown",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Biology",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "PeTriBERT - 1.00e+20 FLOPs"
            },
            {
                "model": "BlenderBot 3",
                "training_compute_(flop)": 4.3e+23,
                "training_power_draw_(w)": 102609.82584809876,
                "training_dataset_size_(gradients)": 1300000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2022-08-10T00:00:00",
                "link": "https://arxiv.org/abs/2208.03188, https://github.com/facebookresearch/ParlAI/blob/main/parlai/zoo/bb3/model_card.md\n\ntraining code: https://parl.ai/projects/bb3/ ",
                "reference": "BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage",
                "organization": "McGill University,Meta AI,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms)",
                "parameters": 175000000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "BlenderBot 3 - 4.30e+23 FLOPs"
            },
            {
                "model": "RNA-FM",
                "training_compute_(flop)": 2.59999999999998e+21,
                "training_power_draw_(w)": 6413.399753905654,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 720.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2022-08-08T00:00:00",
                "link": "https://arxiv.org/abs/2204.00300",
                "reference": "Interpretable RNA Foundation Model from Unannotated Data for Highly Accurate RNA Structure and Function Predictions",
                "organization": "Chinese University of Hong Kong (CUHK),Fudan University,Shanghai AI Lab,Harbin Institute of Technology,University of Electronic Science and Technology of China,Massachusetts Institute of Technology (MIT),Harvard University,Shanghai Zelixir Biotech,CUHK Shenzhen Research Institute",
                "parameters": null,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "RNA-FM - 2.60e+21 FLOPs"
            },
            {
                "model": "FastSpeech 2",
                "training_compute_(flop)": 2.2977e+18,
                "training_power_draw_(w)": 330.3605642465138,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Audio",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2022-08-08T00:00:00",
                "link": "https://arxiv.org/abs/2006.04558",
                "reference": "FastSpeech 2: Fast and High-Quality End-to-End Text to Speech",
                "organization": "Zhejiang University (ZJU),Microsoft Research Asia",
                "parameters": 27000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Unreleased",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Other",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "FastSpeech 2 - 2.30e+18 FLOPs"
            },
            {
                "model": "GLM-130B",
                "training_compute_(flop)": 3.5490054945e+23,
                "training_power_draw_(w)": 615741.2226117075,
                "training_dataset_size_(gradients)": 152000000000.0,
                "training_time_(hours)": 1440.0,
                "training_compute_cost_(2023_usd)": 820296.6313095269,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2022-08-04T00:00:00",
                "link": "https://arxiv.org/abs/2210.02414",
                "reference": "GLM-130B: An Open Bilingual Pre-trained Model",
                "organization": "Tsinghua University",
                "parameters": 130000000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "GLM-130B - 3.55e+23 FLOPs"
            },
            {
                "model": "AlexaTM 20B",
                "training_compute_(flop)": 2.04374016e+23,
                "training_power_draw_(w)": 102628.10792703854,
                "training_dataset_size_(gradients)": 1319000000000.0,
                "training_time_(hours)": 2880.0,
                "training_compute_cost_(2023_usd)": 267943.21130997164,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2022-08-02T00:00:00",
                "link": "https://arxiv.org/abs/2208.01448",
                "reference": "AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model",
                "organization": "Amazon",
                "parameters": 19750000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "API access",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "AlexaTM 20B - 2.04e+23 FLOPs"
            },
            {
                "model": "ProtGPT2",
                "training_compute_(flop)": 4.1e+21,
                "training_power_draw_(w)": 102641.82162383666,
                "training_dataset_size_(gradients)": 25535777280.0,
                "training_time_(hours)": 96.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2022-07-27T00:00:00",
                "link": "https://www.nature.com/articles/s41467-022-32007-7",
                "reference": "ProtGPT2 is a deep unsupervised language model for protein design",
                "organization": "University of Bayreuth",
                "parameters": 738000000.0,
                "notable_model": false,
                "country": "Germany",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Germany",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "ProtGPT2 - 4.10e+21 FLOPs"
            },
            {
                "model": "OmegaPLM",
                "training_compute_(flop)": 1.03514112e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1258291200000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 52400.32118528479,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2022-07-22T00:00:00",
                "link": "https://www.biorxiv.org/content/10.1101/2022.07.21.500999v1",
                "reference": "High-resolution de novo structure prediction from primary sequence",
                "organization": "Massachusetts Institute of Technology (MIT),Westlake University",
                "parameters": 670000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "OmegaPLM - 1.04e+22 FLOPs"
            },
            {
                "model": "ESM2-15B",
                "training_compute_(flop)": 7.35000000001e+22,
                "training_power_draw_(w)": 307966.61145938886,
                "training_dataset_size_(gradients)": 15360000000.0,
                "training_time_(hours)": 1440.0,
                "training_compute_cost_(2023_usd)": 163467.82019979745,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2022-07-21T00:00:00",
                "link": "https://www.science.org/doi/abs/10.1126/science.ade2574\nhttps://www.biorxiv.org/content/10.1101/2022.07.20.500902v2",
                "reference": "Evolutionary-scale prediction of atomic-level protein structure with a language model",
                "organization": "Meta AI,New York University (NYU),Stanford University,Massachusetts Institute of Technology (MIT)",
                "parameters": 15000000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "ESM2-15B - 7.35e+22 FLOPs"
            },
            {
                "model": "ESM2-3B",
                "training_compute_(flop)": 3.000000001e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 15360000000.0,
                "training_time_(hours)": 720.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2022-07-21T00:00:00",
                "link": "https://www.science.org/doi/abs/10.1126/science.ade2574",
                "reference": "Evolutionary-scale prediction of atomic-level protein structure with a language model",
                "organization": "Meta AI,New York University (NYU),Stanford University,Massachusetts Institute of Technology (MIT)",
                "parameters": 3000000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "ESM2-3B - 3.00e+22 FLOPs"
            },
            {
                "model": "ESM2-650M",
                "training_compute_(flop)": 7.560000000001e+21,
                "training_power_draw_(w)": 307966.61145938886,
                "training_dataset_size_(gradients)": 15360000000.0,
                "training_time_(hours)": 192.0,
                "training_compute_cost_(2023_usd)": 20972.27663102349,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2022-07-21T00:00:00",
                "link": "https://www.science.org/doi/abs/10.1126/science.ade2574",
                "reference": "Evolutionary-scale prediction of atomic-level protein structure with a language model",
                "organization": "Meta AI,New York University (NYU),Stanford University,Massachusetts Institute of Technology (MIT)",
                "parameters": 650000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "ESM2-650M - 7.56e+21 FLOPs"
            },
            {
                "model": "ESM2-150M",
                "training_compute_(flop)": 1.1e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 15360000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2022-07-21T00:00:00",
                "link": "https://www.science.org/doi/abs/10.1126/science.ade2574",
                "reference": "Evolutionary-scale prediction of atomic-level protein structure with a language model",
                "organization": "Meta AI,New York University (NYU),Stanford University,Massachusetts Institute of Technology (MIT)",
                "parameters": 150000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "ESM2-150M - 1.10e+21 FLOPs"
            },
            {
                "model": "ESM2-35M",
                "training_compute_(flop)": 2.1e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 15360000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2022-07-21T00:00:00",
                "link": "https://www.science.org/doi/abs/10.1126/science.ade2574",
                "reference": "Evolutionary-scale prediction of atomic-level protein structure with a language model",
                "organization": "Meta AI,New York University (NYU),Stanford University,Massachusetts Institute of Technology (MIT)",
                "parameters": 35000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "ESM2-35M - 2.10e+20 FLOPs"
            },
            {
                "model": "ESM2-8M",
                "training_compute_(flop)": 4.8e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 15360000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2022-07-21T00:00:00",
                "link": "https://www.science.org/doi/abs/10.1126/science.ade2574",
                "reference": "Evolutionary-scale prediction of atomic-level protein structure with a language model",
                "organization": "Meta AI,New York University (NYU),Stanford University,Massachusetts Institute of Technology (MIT)",
                "parameters": 8000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "ESM2-8M - 4.80e+19 FLOPs"
            },
            {
                "model": "Transformer-XL + RMT",
                "training_compute_(flop)": 6.7839792e+18,
                "training_power_draw_(w)": 1604.2428268807498,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": 30.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2022-07-14T00:00:00",
                "link": "https://arxiv.org/abs/2207.06881",
                "reference": "Recurrent Memory Transformer",
                "organization": "Moscow Institute of Physics and Technology,AIRI Artificial Intelligence Research Institute",
                "parameters": 247000000.00000003,
                "notable_model": false,
                "country": "Russia",
                "model_accessibility": "Unreleased",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Transformer-XL + RMT - 6.78e+18 FLOPs"
            },
            {
                "model": "Rita-XLarge",
                "training_compute_(flop)": 8.64e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 150000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2022-07-14T00:00:00",
                "link": "https://arxiv.org/abs/2205.05789",
                "reference": "RITA: a Study on Scaling Up Generative Protein Sequence Models",
                "organization": "LightOn,Harvard University,University of Oxford",
                "parameters": 1200000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "Rita-XLarge - 8.64e+20 FLOPs"
            },
            {
                "model": "BLOOM-176B",
                "training_compute_(flop)": 3.65664e+23,
                "training_power_draw_(w)": 308035.2013244814,
                "training_dataset_size_(gradients)": 379000000000.0,
                "training_time_(hours)": 2808.0,
                "training_compute_cost_(2023_usd)": 995819.1171,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2022-07-11T00:00:00",
                "link": "https://arxiv.org/abs/2211.05100",
                "reference": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model",
                "organization": "Hugging Face,BigScience",
                "parameters": 176247271424.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "BLOOM-176B - 3.66e+23 FLOPs"
            },
            {
                "model": "NLLB",
                "training_compute_(flop)": 1.751113728e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 300000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 50667.25034038439,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2022-07-06T00:00:00",
                "link": "https://research.facebook.com/publications/no-language-left-behind/",
                "reference": "No Language Left Behind: Scaling Human-Centered Machine Translation",
                "organization": "Meta AI",
                "parameters": 54500000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "NLLB - 1.75e+22 FLOPs"
            },
            {
                "model": "BLOOM-7.1B",
                "training_compute_(flop)": 1.5014556e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 354000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2022-07-05T00:00:00",
                "link": "https://arxiv.org/abs/2211.05100",
                "reference": "BigScience Language Open-science Open-access Multilingual (BLOOM) Language Model",
                "organization": "Hugging Face,BigScience",
                "parameters": 7070000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "BLOOM-7.1B - 1.50e+22 FLOPs"
            },
            {
                "model": "CodeT5-large",
                "training_compute_(flop)": 2.72e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 10500000000.0,
                "training_time_(hours)": 504.0,
                "training_compute_cost_(2023_usd)": 4478.145684414144,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2022-07-05T00:00:00",
                "link": "https://arxiv.org/abs/2207.01780",
                "reference": "CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning",
                "organization": "Salesforce",
                "parameters": 770000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "CodeT5-large - 2.72e+21 FLOPs"
            },
            {
                "model": "Minerva (540B)",
                "training_compute_(flop)": 2.7415e+24,
                "training_power_draw_(w)": 349199.866571188,
                "training_dataset_size_(gradients)": 26000000000.0,
                "training_time_(hours)": 696.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2022-06-29T00:00:00",
                "link": "https://arxiv.org/abs/2206.14858",
                "reference": "Solving Quantitative Reasoning Problems with Language Models",
                "organization": "Google",
                "parameters": 540350000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Minerva (540B) - 2.74e+24 FLOPs"
            },
            {
                "model": "DALL-E mega",
                "training_compute_(flop)": 2.285273088e+22,
                "training_power_draw_(w)": 56489.47168105598,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 1344.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2022-06-28T00:00:00",
                "link": "https://huggingface.co/dalle-mini/dalle-mega\nhttps://github.com/borisdayma/dalle-mini",
                "reference": "DALL\u00b7E Mega Model Card",
                "organization": "Craiyon",
                "parameters": null,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "DALL-E mega - 2.29e+22 FLOPs"
            },
            {
                "model": "ProGen2-xlarge",
                "training_compute_(flop)": 1.35e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 350000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 11850.178410269727,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2022-06-27T00:00:00",
                "link": "https://arxiv.org/abs/2206.13517",
                "reference": "ProGen2: Exploring the Boundaries of Protein Language Models",
                "organization": "Salesforce Research,Columbia University,Johns Hopkins University",
                "parameters": 6400000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "ProGen2-xlarge - 1.35e+22 FLOPs"
            },
            {
                "model": "ProGen2-base",
                "training_compute_(flop)": 1.1e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2022-06-27T00:00:00",
                "link": "https://arxiv.org/abs/2206.13517",
                "reference": "ProGen2: Exploring the Boundaries of Protein Language Models",
                "organization": "Salesforce Research,Columbia University,Johns Hopkins University",
                "parameters": 764000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "ProGen2-base - 1.10e+21 FLOPs"
            },
            {
                "model": "GPT-SW3",
                "training_compute_(flop)": 1.6663535e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 101711872000.0,
                "training_time_(hours)": 60.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2022-06-25T00:00:00",
                "link": "http://www.lrec-conf.org/proceedings/lrec2022/pdf/2022.lrec-1.376.pdf\n",
                "reference": "Lessons Learned from GPT-SW3: Building the First Large-Scale Generative Language Model for Swedish",
                "organization": "AI Sweden,RISE",
                "parameters": 3500000000.0,
                "notable_model": false,
                "country": "Sweden",
                "model_accessibility": "Unreleased",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "GPT-SW3 - 1.67e+21 FLOPs"
            },
            {
                "model": "YaLM",
                "training_compute_(flop)": 2.2e+23,
                "training_power_draw_(w)": 641997.2949584174,
                "training_dataset_size_(gradients)": 300000000000.0,
                "training_time_(hours)": 1560.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2022-06-23T00:00:00",
                "link": "https://medium.com/yandex/yandex-publishes-yalm-100b-its-the-largest-gpt-like-neural-network-in-open-source-d1df53d0e9a6",
                "reference": "Yandex Publishes YaLM 100B. It\u2019s the Largest GPT-Like Neural Network in Open Source",
                "organization": "Yandex",
                "parameters": 100000000000.0,
                "notable_model": false,
                "country": "Russia",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "YaLM - 2.20e+23 FLOPs"
            },
            {
                "model": "Parti",
                "training_compute_(flop)": 5.09607936e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 4718592000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 427178.712,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2022-06-22T00:00:00",
                "link": "https://arxiv.org/abs/2206.10789v1",
                "reference": "Scaling Autoregressive Models for Content-Rich Text-to-Image Generation",
                "organization": "Google Research",
                "parameters": 20000000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Parti - 5.10e+23 FLOPs"
            },
            {
                "model": "CodeGeeX",
                "training_compute_(flop)": 6.630000000001e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 850000000000.0,
                "training_time_(hours)": 156.1,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2022-06-22T00:00:00",
                "link": "https://github.com/THUDM/CodeGeeX",
                "reference": null,
                "organization": "Zhipu AI,Tsinghua University",
                "parameters": 13000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "CodeGeeX - 6.63e+22 FLOPs"
            },
            {
                "model": "OPT-6.7B",
                "training_compute_(flop)": 1.2060000000001e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 180000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2022-06-21T00:00:00",
                "link": "https://arxiv.org/abs/2205.01068",
                "reference": "OPT: Open Pre-trained Transformer Language Models",
                "organization": "Meta AI",
                "parameters": 6700000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "OPT-6.7B - 1.21e+22 FLOPs"
            },
            {
                "model": "OPT-66B",
                "training_compute_(flop)": 1.100000000001e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 180000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2022-06-21T00:00:00",
                "link": "https://arxiv.org/abs/2205.01068",
                "reference": "OPT: Open Pre-trained Transformer Language Models",
                "organization": "Meta AI",
                "parameters": 66000000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "OPT-66B - 1.10e+23 FLOPs"
            },
            {
                "model": "OPT-30B",
                "training_compute_(flop)": 5.4000000000001e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 180000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2022-06-21T00:00:00",
                "link": "https://arxiv.org/abs/2205.01068",
                "reference": "OPT: Open Pre-trained Transformer Language Models",
                "organization": "Meta AI",
                "parameters": 30000000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "OPT-30B - 5.40e+22 FLOPs"
            },
            {
                "model": "Unified-IO (XL)",
                "training_compute_(flop)": 3.5e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 74880000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Academia",
                "publication_date": "2022-06-17T00:00:00",
                "link": "https://arxiv.org/abs/2206.08916",
                "reference": "Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks",
                "organization": "Allen Institute for AI,University of Washington",
                "parameters": 2925000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Multimodal",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "Unified-IO (XL) - 3.50e+21 FLOPs"
            },
            {
                "model": "CoCa",
                "training_compute_(flop)": 7.3e+22,
                "training_power_draw_(w)": 698633.0659558588,
                "training_dataset_size_(gradients)": 1351680000000.0,
                "training_time_(hours)": 120.0,
                "training_compute_cost_(2023_usd)": 78043.3756911775,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2022-06-14T00:00:00",
                "link": "https://arxiv.org/abs/2205.01917v2",
                "reference": "CoCa: Contrastive Captioners are Image-Text Foundation Models",
                "organization": "Google Research",
                "parameters": 2100000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "CoCa - 7.30e+22 FLOPs"
            },
            {
                "model": "EGRU (WT2)",
                "training_compute_(flop)": 2.22e+18,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2022-06-13T00:00:00",
                "link": "https://arxiv.org/abs/2206.06178v3",
                "reference": "Efficient recurrent architectures through activity sparsity and sparse back-propagation through time",
                "organization": "Ruhr University Bochum,Technische Universit\u00e4t Dresden,University of London",
                "parameters": 74000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "EGRU (WT2) - 2.22e+18 FLOPs"
            },
            {
                "model": "BIG-G 137B",
                "training_compute_(flop)": 5.6e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 681200000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2022-06-09T00:00:00",
                "link": "https://arxiv.org/abs/2206.04615",
                "reference": "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models",
                "organization": "Google",
                "parameters": 137000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "BIG-G 137B - 5.60e+23 FLOPs"
            },
            {
                "model": "LIMoE-H/14",
                "training_compute_(flop)": 1.8e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 4575625612000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2022-06-06T00:00:00",
                "link": "https://arxiv.org/abs/2206.02770",
                "reference": "Multimodal Contrastive Learning with LIMoE: the Language-Image Mixture of Experts",
                "organization": "Google",
                "parameters": 5600000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "LIMoE-H/14 - 1.80e+22 FLOPs"
            },
            {
                "model": "DITTO",
                "training_compute_(flop)": 3.31776e+18,
                "training_power_draw_(w)": 4816.802908983849,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2022-06-06T00:00:00",
                "link": "https://arxiv.org/abs/2206.02369",
                "reference": "Learning to Break the Loop: Analyzing and Mitigating Repetitions for Neural Text Generation",
                "organization": "Tsinghua University,Apple,Westlake University,Chinese University of Hong Kong (CUHK)",
                "parameters": 750000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "DITTO - 3.32e+18 FLOPs"
            },
            {
                "model": "B2T connection (16L)",
                "training_compute_(flop)": 2.8e+19,
                "training_power_draw_(w)": 96346.78549412632,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": 7.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2022-06-01T00:00:00",
                "link": "https://arxiv.org/abs/2206.00330",
                "reference": "On Layer Normalizations and Residual Connections in Transformers",
                "organization": "LINE Corporation,Tohoku University",
                "parameters": null,
                "notable_model": false,
                "country": "Japan",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Japan",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "B2T connection (16L) - 2.80e+19 FLOPs"
            },
            {
                "model": "Tranception",
                "training_compute_(flop)": 7.24e+21,
                "training_power_draw_(w)": 51390.67413498394,
                "training_dataset_size_(gradients)": 48230400000.0,
                "training_time_(hours)": 336.0,
                "training_compute_cost_(2023_usd)": 15247.43608848737,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2022-05-27T00:00:00",
                "link": "https://arxiv.org/abs/2205.13760",
                "reference": "Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval",
                "organization": "University of Oxford,Harvard Medical School,Cohere",
                "parameters": 700000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "Tranception - 7.24e+21 FLOPs"
            },
            {
                "model": "GPT-2 Medium (FlashAttention)",
                "training_compute_(flop)": 8.9280922e+20,
                "training_power_draw_(w)": 6423.834266872993,
                "training_dataset_size_(gradients)": 10130000000.0,
                "training_time_(hours)": 165.6,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2022-05-27T00:00:00",
                "link": "https://arxiv.org/abs/2205.14135",
                "reference": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
                "organization": "Stanford University,University at Buffalo",
                "parameters": 355000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "GPT-2 Medium (FlashAttention) - 8.93e+20 FLOPs"
            },
            {
                "model": "TRIMELMext (247M)",
                "training_compute_(flop)": 3.12e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 21086208000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2022-05-25T00:00:00",
                "link": "https://arxiv.org/abs/2205.12674",
                "reference": "Training Language Models with Memory Augmentation",
                "organization": "Princeton University",
                "parameters": 247000000.00000003,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "TRIMELMext (247M) - 3.12e+19 FLOPs"
            },
            {
                "model": "TRIMELMext (7M)",
                "training_compute_(flop)": 2.06e+17,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 4915200000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2022-05-25T00:00:00",
                "link": "https://arxiv.org/abs/2205.12674",
                "reference": "Training Language Models with Memory Augmentation",
                "organization": "Princeton University",
                "parameters": 7000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "TRIMELMext (7M) - 2.06e+17 FLOPs"
            },
            {
                "model": "TRIMELMlong (150M)",
                "training_compute_(flop)": 6.48e+18,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 7200000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2022-05-25T00:00:00",
                "link": "https://arxiv.org/abs/2205.12674",
                "reference": "Training Language Models with Memory Augmentation",
                "organization": "Princeton University",
                "parameters": 150000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "TRIMELMlong (150M) - 6.48e+18 FLOPs"
            },
            {
                "model": "Imagen",
                "training_compute_(flop)": 1.46e+22,
                "training_power_draw_(w)": 87371.9285545078,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 96.0,
                "training_compute_cost_(2023_usd)": 7915.823806150154,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2022-05-23T00:00:00",
                "link": "https://arxiv.org/abs/2205.11487",
                "reference": "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding",
                "organization": "Google Brain",
                "parameters": 7762000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "API access",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Imagen - 1.46e+22 FLOPs"
            },
            {
                "model": "Gato",
                "training_compute_(flop)": 4.02e+21,
                "training_power_draw_(w)": 113097.25585951032,
                "training_dataset_size_(gradients)": 524288000000.0,
                "training_time_(hours)": 96.0,
                "training_compute_cost_(2023_usd)": 3523.0649800752253,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2022-05-12T00:00:00",
                "link": "https://arxiv.org/abs/2205.06175",
                "reference": "A Generalist Agent",
                "organization": "DeepMind",
                "parameters": 1180000000.0,
                "notable_model": true,
                "country": "United Kingdom",
                "model_accessibility": "Unreleased",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Gato - 4.02e+21 FLOPs"
            },
            {
                "model": "UL2",
                "training_compute_(flop)": 1.2e+23,
                "training_power_draw_(w)": 174794.45309829456,
                "training_dataset_size_(gradients)": 1000000000000.0,
                "training_time_(hours)": 744.0,
                "training_compute_cost_(2023_usd)": 126785.76203549476,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2022-05-10T00:00:00",
                "link": "https://arxiv.org/abs/2205.05131v1",
                "reference": "Unifying Language Learning Paradigms",
                "organization": "Google Research,Google Brain",
                "parameters": 20000000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "UL2 - 1.20e+23 FLOPs"
            },
            {
                "model": "ASE",
                "training_compute_(flop)": 4.4928e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 240.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Robotics",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2022-05-05T00:00:00",
                "link": "https://arxiv.org/abs/2205.01906",
                "reference": "ASE: Large-Scale Reusable Adversarial Skill Embeddings for Physically Simulated Characters",
                "organization": "NVIDIA,University of California (UC) Berkeley",
                "parameters": null,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "ASE - 4.49e+19 FLOPs"
            },
            {
                "model": "StyleGAN-XL",
                "training_compute_(flop)": 1.296e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2022-05-05T00:00:00",
                "link": "https://arxiv.org/abs/2202.00273",
                "reference": "StyleGAN-XL: Scaling StyleGAN to Large Diverse Datasets",
                "organization": "Max Planck Institute for Intelligent Systems,University of T\u00fcbingen",
                "parameters": null,
                "notable_model": false,
                "country": "Germany",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Germany",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "StyleGAN-XL - 1.30e+21 FLOPs"
            },
            {
                "model": "OPT-175B",
                "training_compute_(flop)": 4.3e+23,
                "training_power_draw_(w)": 822708.6888139298,
                "training_dataset_size_(gradients)": 180000000000.0,
                "training_time_(hours)": 793.5,
                "training_compute_cost_(2023_usd)": 733634.637,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2022-05-02T00:00:00",
                "link": "https://arxiv.org/abs/2205.01068",
                "reference": "OPT: Open Pre-trained Transformer Language Models",
                "organization": "Meta AI",
                "parameters": 175000000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "OPT-175B - 4.30e+23 FLOPs"
            },
            {
                "model": "OPT-13B",
                "training_compute_(flop)": 3.53e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 180000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": null,
                "organization_categorization": "Industry",
                "publication_date": "2022-05-02T00:00:00",
                "link": "https://arxiv.org/abs/2205.01068",
                "reference": null,
                "organization": "Meta AI",
                "parameters": 13000000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "OPT-13B - 3.53e+23 FLOPs"
            },
            {
                "model": "Flamingo",
                "training_compute_(flop)": 2.18972000000001e+23,
                "training_power_draw_(w)": 524511.829594388,
                "training_dataset_size_(gradients)": 458333333333.0,
                "training_time_(hours)": 360.0,
                "training_compute_cost_(2023_usd)": 183423.16330597224,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2022-04-29T00:00:00",
                "link": "https://arxiv.org/abs/2204.14198",
                "reference": "Flamingo: a Visual Language Model for Few-Shot Learning",
                "organization": "DeepMind",
                "parameters": 80000000000.0,
                "notable_model": true,
                "country": "United Kingdom",
                "model_accessibility": "Unreleased",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Flamingo - 2.19e+23 FLOPs"
            },
            {
                "model": "CogView2",
                "training_compute_(flop)": 2.265e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 629145600000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2022-04-28T00:00:00",
                "link": "https://arxiv.org/abs/2204.14217",
                "reference": "CogView2: Faster and Better Text-to-Image Generation via Hierarchical Transformers",
                "organization": "Tsinghua University,Beijing Academy of Artificial Intelligence / BAAI",
                "parameters": 6000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Unknown",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "CogView2 - 2.27e+22 FLOPs"
            },
            {
                "model": "Sparse all-MLP",
                "training_compute_(flop)": 5.32224e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 100000000000.0,
                "training_time_(hours)": 112.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2022-04-14T00:00:00",
                "link": "https://arxiv.org/abs/2203.06850",
                "reference": "Efficient Language Modeling with Sparse all-MLP",
                "organization": "Meta AI",
                "parameters": 9410000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Sparse all-MLP - 5.32e+20 FLOPs"
            },
            {
                "model": "Stable Diffusion (LDM-KL-8-G)",
                "training_compute_(flop)": 5e+22,
                "training_power_draw_(w)": 205764.21634205984,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 585.9375,
                "training_compute_cost_(2023_usd)": 111248.21698072633,
                "domain_group": "Vision",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2022-04-13T00:00:00",
                "link": "https://arxiv.org/abs/2112.10752",
                "reference": "High-Resolution Image Synthesis with Latent Diffusion Models",
                "organization": "Runway,Ludwig Maximilian University of Munich,Heidelberg University",
                "parameters": 1450000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "Stable Diffusion (LDM-KL-8-G) - 5.00e+22 FLOPs"
            },
            {
                "model": "BERT-RBP",
                "training_compute_(flop)": 1.4e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2022-04-07T00:00:00",
                "link": "https://academic.oup.com/bioinformaticsadvances/article/2/1/vbac023/6564689",
                "reference": "Prediction of RNA\u2013protein interactions using a nucleotide language model",
                "organization": "Waseda University",
                "parameters": 110000000.0,
                "notable_model": true,
                "country": "Japan",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Japan",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "BERT-RBP - 1.40e+20 FLOPs"
            },
            {
                "model": "DALL\u00b7E 2",
                "training_compute_(flop)": 3.3695784e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 167050000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2022-04-06T00:00:00",
                "link": "https://cdn.openai.com/papers/dall-e-2.pdf",
                "reference": "Hierarchical Text-Conditional Image Generation with CLIP Latents",
                "organization": "OpenAI",
                "parameters": 3500000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "API access",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "DALL\u00b7E 2 - 3.37e+23 FLOPs"
            },
            {
                "model": "PaLM (540B)",
                "training_compute_(flop)": 2.5272e+24,
                "training_power_draw_(w)": 2099215.6984531507,
                "training_dataset_size_(gradients)": 780000000000.0,
                "training_time_(hours)": 1536.0,
                "training_compute_cost_(2023_usd)": 2967721.8222349114,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2022-04-04T00:00:00",
                "link": "https://arxiv.org/abs/2204.02311",
                "reference": "PaLM: Scaling Language Modeling with Pathways",
                "organization": "Google Research",
                "parameters": 540350000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "PaLM (540B) - 2.53e+24 FLOPs"
            },
            {
                "model": "Monarch-GPT-2-Medium",
                "training_compute_(flop)": 4.36e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2022-04-01T00:00:00",
                "link": "https://arxiv.org/abs/2204.00595",
                "reference": "Monarch: Expressive Structured Matrices for Efficient and Accurate Training",
                "organization": "Stanford University,University at Buffalo,University of Michigan",
                "parameters": 165000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Monarch-GPT-2-Medium - 4.36e+20 FLOPs"
            },
            {
                "model": "Monarch-GPT-2-Small",
                "training_compute_(flop)": 9e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2022-04-01T00:00:00",
                "link": "https://arxiv.org/abs/2204.00595",
                "reference": "Monarch: Expressive Structured Matrices for Efficient and Accurate Training",
                "organization": "Stanford University,University at Buffalo,University of Michigan",
                "parameters": 72000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Monarch-GPT-2-Small - 9.00e+19 FLOPs"
            },
            {
                "model": "NoPos",
                "training_compute_(flop)": 2.09664e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 21000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2022-03-30T00:00:00",
                "link": "https://arxiv.org/abs/2203.16634",
                "reference": "Transformer Language Models without Positional Encodings Still Learn Positional Information",
                "organization": "Tel Aviv University,University of Washington,Intel Labs,Meta AI",
                "parameters": 1300000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "NoPos - 2.10e+19 FLOPs"
            },
            {
                "model": "Chinchilla",
                "training_compute_(flop)": 5.76e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1400000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2022-03-29T00:00:00",
                "link": "https://arxiv.org/abs/2203.15556",
                "reference": "Training Compute-Optimal Large Language Models",
                "organization": "DeepMind",
                "parameters": 70000000000.0,
                "notable_model": true,
                "country": "United Kingdom",
                "model_accessibility": "Unreleased",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Chinchilla - 5.76e+23 FLOPs"
            },
            {
                "model": "GraSR",
                "training_compute_(flop)": 3.799999999999999e+18,
                "training_power_draw_(w)": 1005.1556716229876,
                "training_dataset_size_(gradients)": 13265.0,
                "training_time_(hours)": 120.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Government",
                "publication_date": "2022-03-24T00:00:00",
                "link": "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009986",
                "reference": "Fast protein structure comparison through effective representation learning with contrastive graph neural networks",
                "organization": "Shanghai Jiao Tong University,Ministry of Education of China",
                "parameters": null,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Biology",
                "org_top5": "Other",
                "access_group": "Open",
                "model_and_compute": "GraSR - 3.80e+18 FLOPs"
            },
            {
                "model": "Make-A-Scene",
                "training_compute_(flop)": 6.4172851e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 267386880000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2022-03-24T00:00:00",
                "link": "https://arxiv.org/abs/2203.13131",
                "reference": "Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors",
                "organization": "Meta AI",
                "parameters": 4000000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Make-A-Scene - 6.42e+21 FLOPs"
            },
            {
                "model": "MemSizer (language modeling)",
                "training_compute_(flop)": 7.3e+18,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2022-03-23T00:00:00",
                "link": "https://arxiv.org/abs/2203.12644",
                "reference": "Linearizing Transformer with Key-Value Memory",
                "organization": "Meta AI,Chinese University of Hong Kong (CUHK)",
                "parameters": 357000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "MemSizer (language modeling) - 7.30e+18 FLOPs"
            },
            {
                "model": "Segatron-XL large, M=384 + HCP",
                "training_compute_(flop)": 2.65e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2022-03-21T00:00:00",
                "link": "https://arxiv.org/abs/2203.10692",
                "reference": "Better Language Model with Hypernym Class Prediction",
                "organization": "Microsoft Research,University of Waterloo",
                "parameters": 256999999.99999997,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "Segatron-XL large, M=384 + HCP - 2.65e+19 FLOPs"
            },
            {
                "model": "Transformer Large + HCP",
                "training_compute_(flop)": 6.06e+18,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2022-03-21T00:00:00",
                "link": "https://arxiv.org/abs/2203.10692",
                "reference": "Better Language Model with Hypernym Class Prediction",
                "organization": "University of Waterloo,Microsoft Research",
                "parameters": 256999999.99999997,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "Transformer Large + HCP - 6.06e+18 FLOPs"
            },
            {
                "model": "Segatron -XL base, M=150 + HCP",
                "training_compute_(flop)": 1.74e+18,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2022-03-21T00:00:00",
                "link": "https://arxiv.org/abs/2203.10692",
                "reference": "Better Language Model with Hypernym Class Prediction",
                "organization": "Microsoft Research,University of Waterloo",
                "parameters": 151000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "Segatron -XL base, M=150 + HCP - 1.74e+18 FLOPs"
            },
            {
                "model": "ViT-G (model soup)",
                "training_compute_(flop)": 3.4e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1752320000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2022-03-10T00:00:00",
                "link": "https://arxiv.org/abs/2203.05482v3",
                "reference": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time",
                "organization": "University of Washington,Columbia University,Google,Meta AI,Tel Aviv University",
                "parameters": 1843000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "ViT-G (model soup) - 3.40e+21 FLOPs"
            },
            {
                "model": "GPT3-6.7B + muP",
                "training_compute_(flop)": 1.28e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 300000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2022-03-07T00:00:00",
                "link": "https://arxiv.org/abs/2203.03466",
                "reference": "Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer",
                "organization": "Microsoft,OpenAI",
                "parameters": 6700000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "GPT3-6.7B + muP - 1.28e+22 FLOPs"
            },
            {
                "model": "RQ-Transformer (LSUN-cat dataset)",
                "training_compute_(flop)": 2.9113344e+18,
                "training_power_draw_(w)": 3218.0027181984096,
                "training_dataset_size_(gradients)": 106065024.0,
                "training_time_(hours)": 216.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2022-03-03T00:00:00",
                "link": "https://arxiv.org/abs/2203.01941",
                "reference": "Autoregressive Image Generation using Residual Quantization",
                "organization": "Kakao,POSTECH",
                "parameters": 612000000.0,
                "notable_model": false,
                "country": "South Korea",
                "model_accessibility": "Unknown",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "South Korea",
                "domain_top4": "Vision",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "RQ-Transformer (LSUN-cat dataset) - 2.91e+18 FLOPs"
            },
            {
                "model": "RQ-Transformer (1.4B params ImageNet dataset)",
                "training_compute_(flop)": 1.05670656e+20,
                "training_power_draw_(w)": 6436.005436396819,
                "training_dataset_size_(gradients)": 327680000.0,
                "training_time_(hours)": 78.442700157,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2022-03-03T00:00:00",
                "link": "https://arxiv.org/abs/2203.01941",
                "reference": "Autoregressive Image Generation using Residual Quantization",
                "organization": "Kakao,POSTECH",
                "parameters": 1388000000.0,
                "notable_model": false,
                "country": "South Korea",
                "model_accessibility": "Unknown",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "South Korea",
                "domain_top4": "Vision",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "RQ-Transformer (1.4B params ImageNet dataset) - 1.06e+20 FLOPs"
            },
            {
                "model": "RQ-Transformer (3.8B params ImageNet dataset)",
                "training_compute_(flop)": 2.9113344e+20,
                "training_power_draw_(w)": 3218.0027181984096,
                "training_dataset_size_(gradients)": 327680000.0,
                "training_time_(hours)": 108.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2022-03-03T00:00:00",
                "link": "https://arxiv.org/abs/2203.01941",
                "reference": "Autoregressive Image Generation using Residual Quantization",
                "organization": "Kakao,POSTECH",
                "parameters": 3822000000.0,
                "notable_model": false,
                "country": "South Korea",
                "model_accessibility": "Unknown",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "South Korea",
                "domain_top4": "Vision",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "RQ-Transformer (3.8B params ImageNet dataset) - 2.91e+20 FLOPs"
            },
            {
                "model": "PolyCoder",
                "training_compute_(flop)": 1.1e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 39300000000.0,
                "training_time_(hours)": 1000.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2022-02-26T00:00:00",
                "link": "https://arxiv.org/abs/2202.13169",
                "reference": "A Systematic Evaluation of Large Language Models of Code",
                "organization": "Carnegie Mellon University (CMU)",
                "parameters": 2700000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "PolyCoder - 1.10e+21 FLOPs"
            },
            {
                "model": "FourCastNet",
                "training_compute_(flop)": 3.4504704e+20,
                "training_power_draw_(w)": 51498.36398258959,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 16.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Earth science",
                "organization_categorization": "Academia, Industry, Government",
                "publication_date": "2022-02-22T00:00:00",
                "link": "https://arxiv.org/abs/2202.11214",
                "reference": "FourCastNet: A Global Data-driven High-resolution Weather Model using Adaptive Fourier Neural Operators",
                "organization": "NVIDIA,NERSC, Lawrence Berkeley National Laboratory,University of Michigan,Rice University,California Institute of Technology,Purdue University",
                "parameters": null,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Academia, Industry, Government",
                "access_group": "Closed",
                "model_and_compute": "FourCastNet - 3.45e+20 FLOPs"
            },
            {
                "model": "ST-MoE",
                "training_compute_(flop)": 2.9e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1500000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2022-02-17T00:00:00",
                "link": "https://arxiv.org/abs/2202.08906v2",
                "reference": "ST-MoE: Designing Stable and Transferable Sparse Expert Models",
                "organization": "Google,Google Brain,Google Research",
                "parameters": 269000000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "ST-MoE - 2.90e+23 FLOPs"
            },
            {
                "model": "LaMDA",
                "training_compute_(flop)": 3.55e+23,
                "training_power_draw_(w)": 453306.7251313544,
                "training_dataset_size_(gradients)": 2080000000000.0,
                "training_time_(hours)": 1385.0,
                "training_compute_cost_(2023_usd)": 229949.98625999544,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2022-02-10T00:00:00",
                "link": "https://arxiv.org/abs/2201.08239",
                "reference": "LaMDA: Language Models for Dialog Applications",
                "organization": "Google",
                "parameters": 137000000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "LaMDA - 3.55e+23 FLOPs"
            },
            {
                "model": "ProteinBERT",
                "training_compute_(flop)": 6.5e+19,
                "training_power_draw_(w)": 254.28806247515055,
                "training_dataset_size_(gradients)": 37598200000.0,
                "training_time_(hours)": 672.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2022-02-10T00:00:00",
                "link": "https://academic.oup.com/bioinformatics/article/38/8/2102/6502274",
                "reference": "ProteinBERT: a universal deep-learning model of protein sequence and function",
                "organization": "Hebrew University of Jerusalem,Ben-Gurion University of the Negev,Deep Trading",
                "parameters": 16000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "ProteinBERT - 6.50e+19 FLOPs"
            },
            {
                "model": "GPT-NeoX-20B",
                "training_compute_(flop)": 9.31627008e+22,
                "training_power_draw_(w)": 77269.91251696396,
                "training_dataset_size_(gradients)": 341173367965.0,
                "training_time_(hours)": 2160.0,
                "training_compute_cost_(2023_usd)": 184272.80736002637,
                "domain_group": "Language",
                "organization_categorization": null,
                "publication_date": "2022-02-09T00:00:00",
                "link": "https://arxiv.org/abs/2204.06745",
                "reference": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model",
                "organization": "EleutherAI",
                "parameters": 20000000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Other",
                "access_group": "Open",
                "model_and_compute": "GPT-NeoX-20B - 9.32e+22 FLOPs"
            },
            {
                "model": "RETRO-7B",
                "training_compute_(flop)": 1.68e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 419430400000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2022-02-07T00:00:00",
                "link": "https://arxiv.org/abs/2112.04426",
                "reference": "Improving language models by retrieving from trillions of tokens",
                "organization": "DeepMind",
                "parameters": 7500000000.0,
                "notable_model": true,
                "country": "United Kingdom",
                "model_accessibility": "Unreleased",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "RETRO-7B - 1.68e+22 FLOPs"
            },
            {
                "model": "AlphaCode",
                "training_compute_(flop)": 2.38010000000001e+23,
                "training_power_draw_(w)": 1283001.2678531131,
                "training_dataset_size_(gradients)": 967000000000.0,
                "training_time_(hours)": 147.2,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2022-02-02T00:00:00",
                "link": "https://arxiv.org/abs/2203.07814",
                "reference": "Competition-Level Code Generation with AlphaCode",
                "organization": "DeepMind",
                "parameters": 41100000000.0,
                "notable_model": true,
                "country": "United Kingdom",
                "model_accessibility": "Unreleased",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "AlphaCode - 2.38e+23 FLOPs"
            },
            {
                "model": "DARK",
                "training_compute_(flop)": 9.7e+18,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 50000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2022-01-28T00:00:00",
                "link": "https://www.biorxiv.org/content/10.1101/2022.01.27.478087v1.full",
                "reference": "Design in the DARK: Learning Deep Generative Models for De Novo Protein Design",
                "organization": "University College London (UCL)",
                "parameters": null,
                "notable_model": false,
                "country": "United Kingdom",
                "model_accessibility": "Unknown",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "DARK - 9.70e+18 FLOPs"
            },
            {
                "model": "InstructGPT 175B",
                "training_compute_(flop)": 3.19181e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 16969897.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2022-01-27T00:00:00",
                "link": "https://arxiv.org/pdf/2203.02155",
                "reference": "Training language models to follow instructions with human feedback",
                "organization": "OpenAI",
                "parameters": 175000000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "API access",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "InstructGPT 175B - 3.19e+23 FLOPs"
            },
            {
                "model": "Primer (GPT-3 XL-like 1.9B)",
                "training_compute_(flop)": 2.2049963e+22,
                "training_power_draw_(w)": 175207.5521103308,
                "training_dataset_size_(gradients)": 2000000000000.0,
                "training_time_(hours)": 140.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2022-01-24T00:00:00",
                "link": "https://arxiv.org/abs/2109.08668",
                "reference": "Primer: Searching for Efficient Transformers for Language Modeling",
                "organization": "Google Brain",
                "parameters": 1900000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Primer (GPT-3 XL-like 1.9B) - 2.20e+22 FLOPs"
            },
            {
                "model": "Detic",
                "training_compute_(flop)": 2.34399744e+19,
                "training_power_draw_(w)": 19331.67955389363,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 24.0,
                "training_compute_cost_(2023_usd)": 191.44581825616137,
                "domain_group": "Vision",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2022-01-07T00:00:00",
                "link": "https://arxiv.org/abs/2201.02605",
                "reference": "Detecting Twenty-thousand Classes using Image-level Supervision",
                "organization": "Meta AI,University of Texas at Austin",
                "parameters": 88000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2022,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "Detic - 2.34e+19 FLOPs"
            },
            {
                "model": "ERNIE 3.0 Titan",
                "training_compute_(flop)": 1.0421e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 668000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2021-12-23T00:00:00",
                "link": "https://arxiv.org/abs/2112.12731",
                "reference": "ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation",
                "organization": "Baidu,Peng Cheng Laboratory",
                "parameters": 260000000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Hosted access (no API)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "ERNIE 3.0 Titan - 1.04e+24 FLOPs"
            },
            {
                "model": "GLIDE",
                "training_compute_(flop)": 4.7e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 7602176000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2021-12-20T00:00:00",
                "link": "https://arxiv.org/abs/2112.10741",
                "reference": "GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models",
                "organization": "OpenAI",
                "parameters": 3500000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "GLIDE - 4.70e+22 FLOPs"
            },
            {
                "model": "MoE-1.1T",
                "training_compute_(flop)": 2.227e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 112000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2021-12-20T00:00:00",
                "link": "https://arxiv.org/abs/2112.10684",
                "reference": "Efficient Large Scale Language Modeling with Mixtures of Experts",
                "organization": "Meta AI",
                "parameters": 1100000000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "MoE-1.1T - 2.23e+22 FLOPs"
            },
            {
                "model": "Fairseq-dense 13B",
                "training_compute_(flop)": 3.267e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 112000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2021-12-20T00:00:00",
                "link": "https://arxiv.org/abs/2112.10684",
                "reference": "Efficient Large Scale Language Modeling with Mixtures of Experts",
                "organization": "Meta AI",
                "parameters": 13000000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Fairseq-dense 13B - 3.27e+22 FLOPs"
            },
            {
                "model": "XGLM-7.5B",
                "training_compute_(flop)": 2.25e+22,
                "training_power_draw_(w)": 206287.25531193183,
                "training_dataset_size_(gradients)": 500000000000.0,
                "training_time_(hours)": 504.0,
                "training_compute_cost_(2023_usd)": 104152.22590136188,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2021-12-20T00:00:00",
                "link": "https://arxiv.org/abs/2112.10668",
                "reference": "Few-shot Learning with Multilingual Language Models",
                "organization": "Meta AI,Facebook AI Research",
                "parameters": 7500000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "XGLM-7.5B - 2.25e+22 FLOPs"
            },
            {
                "model": "HSO",
                "training_compute_(flop)": 2.272000000071e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 0.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2021-12-16T00:00:00",
                "link": "https://arxiv.org/abs/2112.08653",
                "reference": "Reconsidering the Past: Optimizing Hidden States in Language Models",
                "organization": "Toyota Technological Institute at Chicago",
                "parameters": 345000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "HSO - 2.27e+21 FLOPs"
            },
            {
                "model": "Contriever",
                "training_compute_(flop)": 1.57e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 262144000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2021-12-16T00:00:00",
                "link": "https://arxiv.org/abs/2112.09118",
                "reference": "Unsupervised Dense Information Retrieval with Contrastive Learning",
                "organization": "Meta AI,University College London (UCL),PSL University,Universit\u00e9 Grenoble Alpes",
                "parameters": 110000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "Contriever - 1.57e+20 FLOPs"
            },
            {
                "model": "EXAONE 1.0",
                "training_compute_(flop)": 1.6956e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 2889163.9293768858,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2021-12-14T00:00:00",
                "link": "https://www.lgcorp.com/media/release/27387#:~:text=LG%20AI%20Research%20proposes%20EXAONE,performance%20while%20learning%20fewer%20parameters.",
                "reference": null,
                "organization": "LG",
                "parameters": 300000000000.0,
                "notable_model": true,
                "country": "South Korea",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "South Korea",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "EXAONE 1.0 - 1.70e+24 FLOPs"
            },
            {
                "model": "GLaM",
                "training_compute_(flop)": 3.6363112434e+23,
                "training_power_draw_(w)": 350743.0055524121,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 1366.0,
                "training_compute_cost_(2023_usd)": 541437.4162400038,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2021-12-13T00:00:00",
                "link": "https://arxiv.org/abs/2112.06905",
                "reference": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts",
                "organization": "Google",
                "parameters": 1200000000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "GLaM - 3.64e+23 FLOPs"
            },
            {
                "model": "Gopher (280B)",
                "training_compute_(flop)": 6.31e+23,
                "training_power_draw_(w)": 1815813.0260876147,
                "training_dataset_size_(gradients)": 300000000000.0,
                "training_time_(hours)": 920.0,
                "training_compute_cost_(2023_usd)": 594227.771058999,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2021-12-08T00:00:00",
                "link": "https://arxiv.org/abs/2112.11446",
                "reference": "\"Scaling Language Models: Methods, Analysis & Insights from Training Gopher\"",
                "organization": "DeepMind",
                "parameters": 280000000000.0,
                "notable_model": true,
                "country": "United Kingdom",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Gopher (280B) - 6.31e+23 FLOPs"
            },
            {
                "model": "Student of Games",
                "training_compute_(flop)": 3.6679273004682862e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 245760000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Games",
                "organization_categorization": "Industry",
                "publication_date": "2021-12-06T00:00:00",
                "link": "https://arxiv.org/abs/2112.03178",
                "reference": "Player of Games",
                "organization": "DeepMind",
                "parameters": null,
                "notable_model": true,
                "country": "United Kingdom",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Student of Games - 3.67e+22 FLOPs"
            },
            {
                "model": "CTR-BERT",
                "training_compute_(flop)": 6.469632e+19,
                "training_power_draw_(w)": 6448.4868676049155,
                "training_dataset_size_(gradients)": 1200000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Recommendation",
                "organization_categorization": "Industry",
                "publication_date": "2021-12-06T00:00:00",
                "link": "https://neurips2021-nlp.github.io/papers/20/CameraReady/camera_ready_final.pdf",
                "reference": "CTR-BERT: Cost-effective knowledge distillation for billion-parameter teacher models",
                "organization": "Amazon",
                "parameters": 70000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "CTR-BERT - 6.47e+19 FLOPs"
            },
            {
                "model": "GPT-2-Medium+Pixelfly",
                "training_compute_(flop)": 1.25454e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2021-11-30T00:00:00",
                "link": "https://arxiv.org/abs/2112.00029",
                "reference": "Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models",
                "organization": "Stanford University,SambaNova Systems, Inc,Peking University,Adobe,University at Buffalo",
                "parameters": 202999999.99999997,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "GPT-2-Medium+Pixelfly - 1.25e+19 FLOPs"
            },
            {
                "model": "GPT-2-Small+Pixelfly",
                "training_compute_(flop)": 4.2024e+18,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2021-11-30T00:00:00",
                "link": "https://arxiv.org/abs/2112.00029",
                "reference": "Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models",
                "organization": "Stanford University,SambaNova Systems, Inc,Peking University,Adobe,University at Buffalo",
                "parameters": 68000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "GPT-2-Small+Pixelfly - 4.20e+18 FLOPs"
            },
            {
                "model": "N\u00dcWA",
                "training_compute_(flop)": 7.24598784e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 5547780000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2021-11-24T00:00:00",
                "link": "https://arxiv.org/abs/2111.12417",
                "reference": "N\u00dcWA: Visual Synthesis Pre-training for Neural visUal World creAtion",
                "organization": "Microsoft Research,Peking University",
                "parameters": 870000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "N\u00dcWA - 7.25e+21 FLOPs"
            },
            {
                "model": "Florence",
                "training_compute_(flop)": 4.831e+22,
                "training_power_draw_(w)": 412831.8485448412,
                "training_dataset_size_(gradients)": 7500000000.0,
                "training_time_(hours)": 240.0,
                "training_compute_cost_(2023_usd)": 106950.61569328008,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2021-11-22T00:00:00",
                "link": "https://arxiv.org/abs/2111.11432v1",
                "reference": "Florence: A New Foundation Model for Computer Vision",
                "organization": "Microsoft",
                "parameters": 893000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Florence - 4.83e+22 FLOPs"
            },
            {
                "model": "BASIC-L",
                "training_compute_(flop)": 4.12e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 8905031712000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 1684.770712126102,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2021-11-19T00:00:00",
                "link": "https://arxiv.org/abs/2111.10050",
                "reference": "Combined Scaling for Zero-shot Transfer Learning",
                "organization": "Google",
                "parameters": 3070000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "BASIC-L - 4.12e+22 FLOPs"
            },
            {
                "model": "Swin Transformer V2 (SwinV2-G)",
                "training_compute_(flop)": 1.1e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 2326.6636503781665,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2021-11-18T00:00:00",
                "link": "https://arxiv.org/abs/2111.09883v2",
                "reference": "Swin Transformer V2: Scaling Up Capacity and Resolution",
                "organization": "Microsoft Research Asia",
                "parameters": 3000000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Swin Transformer V2 (SwinV2-G) - 1.10e+21 FLOPs"
            },
            {
                "model": "DeBERTaV3large",
                "training_compute_(flop)": 1.07008e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 42666666667.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2021-11-18T00:00:00",
                "link": "https://arxiv.org/abs/2111.09543",
                "reference": "DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing",
                "organization": "Microsoft Research",
                "parameters": 418000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "DeBERTaV3large - 1.07e+20 FLOPs"
            },
            {
                "model": "ESM1v",
                "training_compute_(flop)": 1.3500000000000008e+20,
                "training_power_draw_(w)": 38707.29549681129,
                "training_dataset_size_(gradients)": 22050000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2021-11-17T00:00:00",
                "link": "https://www.biorxiv.org/content/10.1101/2021.07.09.450648v2",
                "reference": "Language models enable zero-shot prediction of the effects of mutations on protein function",
                "organization": "Facebook AI Research,New York University (NYU),University of California (UC) Berkeley",
                "parameters": 650000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "ESM1v - 1.35e+20 FLOPs"
            },
            {
                "model": "EquiDock",
                "training_compute_(flop)": 1.08e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 39938.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2021-11-15T00:00:00",
                "link": "https://arxiv.org/abs/2111.07786",
                "reference": "Independent SE(3)-Equivariant Models for End-to-End Rigid Protein Docking",
                "organization": "Massachusetts Institute of Technology (MIT),ETH Zurich,Tencent",
                "parameters": null,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "EquiDock - 1.08e+19 FLOPs"
            },
            {
                "model": "A.X (Adot) 18B",
                "training_compute_(flop)": 1.35e+18,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2021-11-15T00:00:00",
                "link": "https://aclanthology.org/2023.acl-industry.68.pdf",
                "reference": "WHAT, WHEN, and HOW to Ground: Designing User Persona-Aware Conversational Agents for Engaging Dialogue",
                "organization": "SK Telecom",
                "parameters": 18000000000.0,
                "notable_model": false,
                "country": "South Korea",
                "model_accessibility": "Unknown",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "South Korea",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "A.X (Adot) 18B - 1.35e+18 FLOPs"
            },
            {
                "model": "Masked Autoencoders ViT-H",
                "training_compute_(flop)": 4.6e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 69.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2021-11-11T00:00:00",
                "link": "https://arxiv.org/abs/2111.06377",
                "reference": "Masked Autoencoders Are Scalable Vision Learners",
                "organization": "Facebook AI Research",
                "parameters": 632000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Masked Autoencoders ViT-H - 4.60e+20 FLOPs"
            },
            {
                "model": "GPT2+CoreLM+Fine-Tuning",
                "training_compute_(flop)": 1.1725747e+20,
                "training_power_draw_(w)": 277.0039416764727,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 8.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2021-11-04T00:00:00",
                "link": "https://arxiv.org/abs/2111.02687",
                "reference": "CoreLM: Coreference-aware Language Model Fine-Tuning",
                "organization": "Aristotle University of Thessaloniki",
                "parameters": 132000000.0,
                "notable_model": false,
                "country": "Greece",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "GPT2+CoreLM+Fine-Tuning - 1.17e+20 FLOPs"
            },
            {
                "model": "CodeT5-base",
                "training_compute_(flop)": 1.56e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 288.0,
                "training_compute_cost_(2023_usd)": 3114.869094617447,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2021-11-01T00:00:00",
                "link": "https://aclanthology.org/2021.emnlp-main.685/",
                "reference": "CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation",
                "organization": "Salesforce,Nanyang Technological University",
                "parameters": 220000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "CodeT5-base - 1.56e+21 FLOPs"
            },
            {
                "model": "Projected GAN",
                "training_compute_(flop)": 1.05e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 3000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2021-11-01T00:00:00",
                "link": "https://proceedings.neurips.cc/paper/2021/hash/9219adc5c42107c4911e249155320648-Abstract.html",
                "reference": "Projected GANs Converge Faster",
                "organization": "Heidelberg University",
                "parameters": null,
                "notable_model": true,
                "country": "Germany",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Germany",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "Projected GAN - 1.05e+19 FLOPs"
            },
            {
                "model": "S4",
                "training_compute_(flop)": 7.8328627e+19,
                "training_power_draw_(w)": 6453.658675376135,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2021-10-31T00:00:00",
                "link": "https://arxiv.org/abs/2111.00396",
                "reference": "Efficiently Modeling Long Sequences with Structured State Spaces",
                "organization": "Stanford University",
                "parameters": 249000000.00000003,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "S4 - 7.83e+19 FLOPs"
            },
            {
                "model": "DALL-E mini",
                "training_compute_(flop)": 3.8e+19,
                "training_power_draw_(w)": 1774.9537602472165,
                "training_dataset_size_(gradients)": 4352000000.0,
                "training_time_(hours)": 72.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2021-10-26T00:00:00",
                "link": "https://huggingface.co/dalle-mini/dalle-mini",
                "reference": "DALL\u00b7E Mini Model Card ",
                "organization": "Craiyon",
                "parameters": null,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "DALL-E mini - 3.80e+19 FLOPs"
            },
            {
                "model": "PMLM-large",
                "training_compute_(flop)": 3.81e+21,
                "training_power_draw_(w)": 14523.966055408006,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 1176.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2021-10-21T00:00:00",
                "link": "https://arxiv.org/abs/2110.15527",
                "reference": "Pre-Training Co-Evolutionary Protein Representation via a Pairwise Masked Language Model",
                "organization": "Microsoft Research Asia,Nanyang Technological University,Xi\u2019an Jiaotong University,Sun Yat-sen University",
                "parameters": 250000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "PMLM-large - 3.81e+21 FLOPs"
            },
            {
                "model": "base LM+GNN+kNN",
                "training_compute_(flop)": 5.2587456e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2021-10-17T00:00:00",
                "link": "https://arxiv.org/abs/2110.08743",
                "reference": "GNN-LM: Language Modeling based on Global Contexts via GNN",
                "organization": "Shannon.AI,Nanjing University,Nanyang Technological University,Zhejiang University (ZJU)",
                "parameters": 274000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "base LM+GNN+kNN - 5.26e+19 FLOPs"
            },
            {
                "model": "PAGnol-XL",
                "training_compute_(flop)": 2.592e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2021-10-16T00:00:00",
                "link": "https://arxiv.org/abs/2110.08554",
                "reference": "PAGnol: An Extra-Large French Generative Model",
                "organization": "LightOn,Laboratoire de Physique de l'Ecole Normale (LPENS),INRIA",
                "parameters": 1500000000.0,
                "notable_model": false,
                "country": "France",
                "model_accessibility": "API access",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "PAGnol-XL - 2.59e+20 FLOPs"
            },
            {
                "model": "GPT-2 (fine-tuned with HYDRA)",
                "training_compute_(flop)": 1.92e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2021-10-16T00:00:00",
                "link": "https://arxiv.org/abs/2110.08633",
                "reference": "Hydra: A System for Large Multi-Model Deep Learning",
                "organization": "University of California San Diego",
                "parameters": 1540000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "GPT-2 (fine-tuned with HYDRA) - 1.92e+21 FLOPs"
            },
            {
                "model": "MGK 4 heads (medium)",
                "training_compute_(flop)": 8.8992e+18,
                "training_power_draw_(w)": 1613.9537048231884,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2021-10-16T00:00:00",
                "link": "https://arxiv.org/abs/2110.08678",
                "reference": "Improving Transformers with Probabilistic Attention Keys",
                "organization": "FPT Software AI Center,University of California Los Angeles (UCLA),VinUniversity,Deezer Research,Rice University,University of Texas at Austin",
                "parameters": 90000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "MGK 4 heads (medium) - 8.90e+18 FLOPs"
            },
            {
                "model": "MGK 8 heads (small)",
                "training_compute_(flop)": 2.9664e+18,
                "training_power_draw_(w)": 1613.9537048231884,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2021-10-16T00:00:00",
                "link": "https://arxiv.org/abs/2110.08678",
                "reference": "Improving Transformers with Probabilistic Attention Keys",
                "organization": "FPT Software AI Center,University of California Los Angeles (UCLA),VinUniversity,Deezer Research,Rice University,University of Texas at Austin",
                "parameters": 40000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "MGK 8 heads (small) - 2.97e+18 FLOPs"
            },
            {
                "model": "T0-XXL",
                "training_compute_(flop)": 9.1819e+20,
                "training_power_draw_(w)": 113624.87114581963,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 27.0,
                "training_compute_cost_(2023_usd)": 11671.840843653788,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2021-10-15T00:00:00",
                "link": "https://arxiv.org/abs/2110.08207",
                "reference": "Multitask Prompted Training Enables Zero-Shot Task Generalization",
                "organization": "Hugging Face,Brown University",
                "parameters": 11000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "T0-XXL - 9.18e+20 FLOPs"
            },
            {
                "model": "KnGPT2",
                "training_compute_(flop)": 7.9402496e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 853333333.0,
                "training_time_(hours)": 6.5,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2021-10-15T00:00:00",
                "link": "https://arxiv.org/abs/2110.08152",
                "reference": "Kronecker Decomposition for GPT Compression",
                "organization": "Huawei Noah's Ark Lab,McGill University",
                "parameters": 83000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "KnGPT2 - 7.94e+20 FLOPs"
            },
            {
                "model": "Yuan 1.0",
                "training_compute_(flop)": 3.5380000000001e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 180000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 632797.8938515903,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2021-10-12T00:00:00",
                "link": "https://arxiv.org/abs/2110.04725",
                "reference": "Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning",
                "organization": "Inspur",
                "parameters": 245730000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "API access",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Yuan 1.0 - 3.54e+23 FLOPs"
            },
            {
                "model": "TOME",
                "training_compute_(flop)": 1.03809024e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2021-10-12T00:00:00",
                "link": "https://arxiv.org/abs/2110.06176v2",
                "reference": "Mention Memory: incorporating textual knowledge into Transformers through entity mention attention",
                "organization": "University of Southern California,Google",
                "parameters": 220000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "TOME - 1.04e+21 FLOPs"
            },
            {
                "model": "Megatron-Turing NLG 530B",
                "training_compute_(flop)": 8.586e+23,
                "training_power_draw_(w)": 3615658.868639838,
                "training_dataset_size_(gradients)": 270000000000.0,
                "training_time_(hours)": 770.0,
                "training_compute_cost_(2023_usd)": 3848505.6256471737,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2021-10-11T00:00:00",
                "link": "https://arxiv.org/abs/2201.11990",
                "reference": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",
                "organization": "Microsoft,NVIDIA",
                "parameters": 530000000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Megatron-Turing NLG 530B - 8.59e+23 FLOPs"
            },
            {
                "model": "M6-10T",
                "training_compute_(flop)": 5.53e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2021-10-08T00:00:00",
                "link": "https://arxiv.org/abs/2110.03888",
                "reference": "M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining",
                "organization": "Alibaba",
                "parameters": 10000000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "M6-10T - 5.53e+21 FLOPs"
            },
            {
                "model": "AlphaFold-Multimer",
                "training_compute_(flop)": 4.35e+21,
                "training_power_draw_(w)": 28413.1771135238,
                "training_dataset_size_(gradients)": 56573952.0,
                "training_time_(hours)": 384.0,
                "training_compute_cost_(2023_usd)": 7966.2330131223,
                "domain_group": "Biology",
                "organization_categorization": "Industry",
                "publication_date": "2021-10-04T00:00:00",
                "link": "https://www.biorxiv.org/content/10.1101/2021.10.04.463034v1",
                "reference": "Protein complex prediction with AlphaFold-Multimer",
                "organization": "Google DeepMind,DeepMind",
                "parameters": null,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "AlphaFold-Multimer - 4.35e+21 FLOPs"
            },
            {
                "model": "Turing ULRv5",
                "training_compute_(flop)": 2.8983951e+22,
                "training_power_draw_(w)": 206668.900572444,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 336.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2021-09-28T00:00:00",
                "link": "https://www.microsoft.com/en-us/research/blog/microsoft-turing-universal-language-representation-model-t-ulrv5-tops-xtreme-leaderboard-and-trains-100x-faster/",
                "reference": "Microsoft Turing Universal Language Representation model, T-ULRv5, tops XTREME leaderboard and trains 100x faster",
                "organization": "Microsoft",
                "parameters": 2200000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Hosted access (no API)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Turing ULRv5 - 2.90e+22 FLOPs"
            },
            {
                "model": "PLATO-XL",
                "training_compute_(flop)": 9.9e+21,
                "training_power_draw_(w)": 155029.29220816068,
                "training_dataset_size_(gradients)": 150000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2021-09-20T00:00:00",
                "link": "https://arxiv.org/abs/2109.09519",
                "reference": "PLATO-XL: Exploring the Large-scale Pre-training of Dialogue Generation",
                "organization": "Baidu",
                "parameters": 11000000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "PLATO-XL - 9.90e+21 FLOPs"
            },
            {
                "model": "DLRM-2022",
                "training_compute_(flop)": 1.1e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Recommendation",
                "organization_categorization": "Industry",
                "publication_date": "2021-09-15T00:00:00",
                "link": "https://arxiv.org/abs/2104.05158",
                "reference": "Software-Hardware Co-design for Fast and Scalable Training of Deep Learning Recommendation Models",
                "organization": "Facebook",
                "parameters": 3000000000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "DLRM-2022 - 1.10e+21 FLOPs"
            },
            {
                "model": "MegaMolBART",
                "training_compute_(flop)": 7.2e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 80.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Industry",
                "publication_date": "2021-09-14T00:00:00",
                "link": "https://docs.nvidia.com/bionemo-framework/0.4.0/models/megamolbart.html, https://github.com/NVIDIA/MegaMolBART",
                "reference": "MegaMolBART",
                "organization": "NVIDIA",
                "parameters": 45000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "MegaMolBART - 7.20e+20 FLOPs"
            },
            {
                "model": "HyperCLOVA 82B",
                "training_compute_(flop)": 1.476e+23,
                "training_power_draw_(w)": 827007.0405400413,
                "training_dataset_size_(gradients)": 300000000000.0,
                "training_time_(hours)": 643.2,
                "training_compute_cost_(2023_usd)": 586408.7810520589,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2021-09-10T00:00:00",
                "link": "https://arxiv.org/abs/2109.04650",
                "reference": "What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers",
                "organization": "NAVER,Search Solutions",
                "parameters": 82000000000.0,
                "notable_model": true,
                "country": "South Korea",
                "model_accessibility": "API access",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "South Korea",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "HyperCLOVA 82B - 1.48e+23 FLOPs"
            },
            {
                "model": "HyperCLOVA 204B",
                "training_compute_(flop)": 2.0000000001e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 560000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 441770.0038182093,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2021-09-10T00:00:00",
                "link": "https://aibusiness.com/nlp/south-korea-s-naver-unveils-hyperscale-ai-platform-language-model-with-more-parameters-than-gpt-3",
                "reference": "South Korea's Naver unveils 'hyperscale AI' platform, language model with more parameters than GPT-3",
                "organization": "NAVER",
                "parameters": 204000000000.0,
                "notable_model": true,
                "country": "South Korea",
                "model_accessibility": "Hosted access (no API)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "South Korea",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "HyperCLOVA 204B - 2.00e+23 FLOPs"
            },
            {
                "model": "NLM",
                "training_compute_(flop)": 2.783e+19,
                "training_power_draw_(w)": 388.2894463979599,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2021-09-09T00:00:00",
                "link": "https://arxiv.org/abs/2109.04212",
                "reference": "Efficient Nearest Neighbor Language Models",
                "organization": "Carnegie Mellon University (CMU),University of California San Diego",
                "parameters": 247000512.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "NLM - 2.78e+19 FLOPs"
            },
            {
                "model": "PermuteFormer",
                "training_compute_(flop)": 2.775e+18,
                "training_power_draw_(w)": 4846.17604411253,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2021-09-06T00:00:00",
                "link": "https://arxiv.org/abs/2109.02377",
                "reference": "PermuteFormer: Efficient Relative Position Encoding for Long Sequences",
                "organization": "Peking University",
                "parameters": 149697024.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "PermuteFormer - 2.78e+18 FLOPs"
            },
            {
                "model": "FLAN 137B",
                "training_compute_(flop)": 2.047e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 9150000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2021-09-03T00:00:00",
                "link": "https://arxiv.org/abs/2109.01652",
                "reference": "Finetuned Language Models Are Zero-Shot Learners",
                "organization": "Google Research",
                "parameters": 137000000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "FLAN 137B - 2.05e+24 FLOPs"
            },
            {
                "model": "$\\infty$-former (SM)",
                "training_compute_(flop)": 1.2e+22,
                "training_power_draw_(w)": 277.3990201821971,
                "training_dataset_size_(gradients)": 200000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2021-09-01T00:00:00",
                "link": "https://arxiv.org/abs/2109.00301",
                "reference": "$\\infty$-former: Infinite Memory Transformer",
                "organization": "Universidade de Lisboa (ULisboa),DeepMind",
                "parameters": 124000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "$\\infty$-former (SM) - 1.20e+22 FLOPs"
            },
            {
                "model": "ALiBi (L=3072, Lvalid = 3072)",
                "training_compute_(flop)": 8.1e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2021-08-27T00:00:00",
                "link": "https://arxiv.org/abs/2108.12409",
                "reference": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
                "organization": "University of Washington,Facebook AI Research,Allen Institute for AI",
                "parameters": 1300000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "ALiBi (L=3072, Lvalid = 3072) - 8.10e+20 FLOPs"
            },
            {
                "model": "XLMR-XXL",
                "training_compute_(flop)": 3.366e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 167000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2021-08-17T00:00:00",
                "link": "https://arxiv.org/abs/2105.00572",
                "reference": "Larger-Scale Transformers for Multilingual Masked Language Modeling",
                "organization": "Facebook AI Research",
                "parameters": 10700000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "XLMR-XXL - 3.37e+22 FLOPs"
            },
            {
                "model": "ProteinLM",
                "training_compute_(flop)": 1.6e+22,
                "training_power_draw_(w)": 29090.009704341683,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 252.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2021-08-17T00:00:00",
                "link": "https://arxiv.org/abs/2108.07435c",
                "reference": "Modeling Protein Using Large-scale Pretrain Language Model",
                "organization": "Tsinghua University,Beijing Academy of Artificial Intelligence / BAAI,Tencent",
                "parameters": 3000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "ProteinLM - 1.60e+22 FLOPs"
            },
            {
                "model": "DNABERT",
                "training_compute_(flop)": 1.07e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1444128539.3656242,
                "training_time_(hours)": 600.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2021-08-15T00:00:00",
                "link": "https://academic.oup.com/bioinformatics/article/37/15/2112/6128680",
                "reference": "DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome",
                "organization": "Northeastern University",
                "parameters": 110000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "DNABERT - 1.07e+20 FLOPs"
            },
            {
                "model": "GPT-2 (1.5B, Curriculum Learning 45K)",
                "training_compute_(flop)": 2.3811e+21,
                "training_power_draw_(w)": 77580.26955910088,
                "training_dataset_size_(gradients)": 157000000000.0,
                "training_time_(hours)": 155.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2021-08-13T00:00:00",
                "link": "https://arxiv.org/abs/2108.06084",
                "reference": "Curriculum Learning: A Regularization Method for Efficient and Stable Billion-Scale GPT Model Pre-Training",
                "organization": "Microsoft",
                "parameters": 1500000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "GPT-2 (1.5B, Curriculum Learning 45K) - 2.38e+21 FLOPs"
            },
            {
                "model": "GPT-2 (117M, SLW 110K)",
                "training_compute_(flop)": 1.10214e+20,
                "training_power_draw_(w)": 77580.26955910088,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2021-08-13T00:00:00",
                "link": "https://arxiv.org/abs/2108.06084",
                "reference": "Curriculum Learning: A Regularization Method for Efficient and Stable Billion-Scale GPT Model Pre-Training",
                "organization": "Microsoft",
                "parameters": 117000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "GPT-2 (117M, SLW 110K) - 1.10e+20 FLOPs"
            },
            {
                "model": "Jurassic-1-Jumbo",
                "training_compute_(flop)": 3.7e+23,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 300000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 836700.050829598,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2021-08-11T00:00:00",
                "link": "https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf",
                "reference": "Jurassic-1: Technical Details and Evaluation",
                "organization": "AI21 Labs",
                "parameters": 178000000000.0,
                "notable_model": true,
                "country": "Israel",
                "model_accessibility": "API access",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Jurassic-1-Jumbo - 3.70e+23 FLOPs"
            },
            {
                "model": "Zidong Taichu",
                "training_compute_(flop)": 8.016e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Academia, Government",
                "publication_date": "2021-08-11T00:00:00",
                "link": "https://gitee.com/zidongtaichu/multi-modal-models",
                "reference": "Zidong Ancestral multi-modal large model",
                "organization": "Chinese Academy of Sciences,Wuhan AI Computing Center",
                "parameters": 3200000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Multimodal",
                "org_top5": "Other",
                "access_group": "Open",
                "model_and_compute": "Zidong Taichu - 8.02e+20 FLOPs"
            },
            {
                "model": "YOLOX-X",
                "training_compute_(flop)": 6.34275e+20,
                "training_power_draw_(w)": 4849.522759284234,
                "training_dataset_size_(gradients)": 2500000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2021-08-06T00:00:00",
                "link": "https://arxiv.org/abs/2107.08430",
                "reference": "YOLOX: Exceeding YOLO Series in 2021",
                "organization": "Megvii Inc",
                "parameters": 99100000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "YOLOX-X - 6.34e+20 FLOPs"
            },
            {
                "model": "FMMformer (2-kernel fast weight + Band20)",
                "training_compute_(flop)": 2.472e+16,
                "training_power_draw_(w)": 3637.223067232183,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2021-08-05T00:00:00",
                "link": "https://proceedings.neurips.cc/paper/2021/file/f621585df244e9596dc70a39b579efb1-Paper.pdf",
                "reference": "FMMformer: Efficient and Flexible Transformer via Decomposed Near-field and Far-field Attention",
                "organization": "University of California Los Angeles (UCLA),University of Utah",
                "parameters": 40000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "FMMformer (2-kernel fast weight + Band20) - 2.47e+16 FLOPs"
            },
            {
                "model": "SEER",
                "training_compute_(flop)": 1.8e+22,
                "training_power_draw_(w)": 310424.75538121304,
                "training_dataset_size_(gradients)": 1000000000.0,
                "training_time_(hours)": 195.5,
                "training_compute_cost_(2023_usd)": 34114.252463690456,
                "domain_group": "Vision",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2021-07-29T00:00:00",
                "link": "https://arxiv.org/abs/2103.01988",
                "reference": "Self-supervised Pretraining of Visual Features in the Wild",
                "organization": "Facebook AI Research,INRIA",
                "parameters": 1300000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "SEER - 1.80e+22 FLOPs"
            },
            {
                "model": "GOAT",
                "training_compute_(flop)": 2.412e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 798720000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 84799.78517435073,
                "domain_group": "Games",
                "organization_categorization": "Industry",
                "publication_date": "2021-07-27T00:00:00",
                "link": "https://deepmind.com/blog/article/generally-capable-agents-emerge-from-open-ended-play\n\nhttps://arxiv.org/abs/2107.12808",
                "reference": "Open-Ended Learning Leads to Generally Capable Agents",
                "organization": "DeepMind",
                "parameters": 3472816.0,
                "notable_model": true,
                "country": "United Kingdom",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "GOAT - 2.41e+22 FLOPs"
            },
            {
                "model": "HuBERT",
                "training_compute_(flop)": 5.54e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 864000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Audio",
                "organization_categorization": "Industry",
                "publication_date": "2021-07-27T00:00:00",
                "link": "https://arxiv.org/abs/2106.07447",
                "reference": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units",
                "organization": "Facebook AI Research",
                "parameters": 1000000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "HuBERT - 5.54e+21 FLOPs"
            },
            {
                "model": "Codex",
                "training_compute_(flop)": 7.344e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 52788000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2021-07-07T00:00:00",
                "link": "https://openai.com/blog/openai-codex/\nhttps://arxiv.org/abs/2107.03374",
                "reference": "Evaluating Large Language Models Trained on Code",
                "organization": "OpenAI",
                "parameters": 12000000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "API access",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Codex - 7.34e+22 FLOPs"
            },
            {
                "model": "ERNIE 3.0",
                "training_compute_(flop)": 2.25e+22,
                "training_power_draw_(w)": 232943.03314881725,
                "training_dataset_size_(gradients)": 375000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 39104.26710360624,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2021-07-05T00:00:00",
                "link": "https://arxiv.org/abs/2107.02137",
                "reference": "ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation",
                "organization": "Baidu",
                "parameters": 10000000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "ERNIE 3.0 - 2.25e+22 FLOPs"
            },
            {
                "model": "GemNet-T (OC20)",
                "training_compute_(flop)": 7.34832e+17,
                "training_power_draw_(w)": 277.78228982155673,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 60.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Materials science",
                "organization_categorization": "Academia",
                "publication_date": "2021-07-01T00:00:00",
                "link": "https://arxiv.org/abs/2106.08903",
                "reference": "GemNet: Universal Directional Graph Neural Networks for Molecules",
                "organization": "Technical University of Munich",
                "parameters": 1900000.0,
                "notable_model": false,
                "country": "Germany",
                "model_accessibility": "Open weights (restricted use)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Germany",
                "domain_top4": "Other",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "GemNet-T (OC20) - 7.35e+17 FLOPs"
            },
            {
                "model": "DEQ-Transformer (Post-LN) + Jacobian Regularisation",
                "training_compute_(flop)": 2.9e+19,
                "training_power_draw_(w)": 2022.3901774609187,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 250.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2021-06-28T00:00:00",
                "link": "https://arxiv.org/abs/2106.14342",
                "reference": "Stabilizing Equilibrium Models by Jacobian Regularization",
                "organization": "Carnegie Mellon University (CMU),Intel Labs",
                "parameters": 98000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "DEQ-Transformer (Post-LN) + Jacobian Regularisation - 2.90e+19 FLOPs"
            },
            {
                "model": "Adaptive Input Transformer + RD",
                "training_compute_(flop)": 8.58e+19,
                "training_power_draw_(w)": 4853.736425906205,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": 79.4,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2021-06-28T00:00:00",
                "link": "https://arxiv.org/abs/2106.14448",
                "reference": "R-Drop: Regularized Dropout for Neural Networks",
                "organization": "Microsoft Research Asia,Soochow University",
                "parameters": 247000000.00000003,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "Adaptive Input Transformer + RD - 8.58e+19 FLOPs"
            },
            {
                "model": "CPM-2",
                "training_compute_(flop)": 3.600000000001e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 69615000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2021-06-24T00:00:00",
                "link": "https://arxiv.org/abs/2106.10715",
                "reference": "CPM-2: Large-scale Cost-effective Pre-trained Language Models",
                "organization": "Tsinghua University,Beijing Academy of Artificial Intelligence / BAAI",
                "parameters": 11000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Unknown",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "CPM-2 - 3.60e+22 FLOPs"
            },
            {
                "model": "Fold2Seq",
                "training_compute_(flop)": 1.4e+17,
                "training_power_draw_(w)": 1213.542200949413,
                "training_dataset_size_(gradients)": 45995.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2021-06-24T00:00:00",
                "link": "https://arxiv.org/abs/2106.13058",
                "reference": "Fold2Seq: A Joint Sequence(1D)-Fold(3D) Embedding-based Generative Model for Protein Design",
                "organization": "IBM,Texas A&M",
                "parameters": 12427904.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "Fold2Seq - 1.40e+17 FLOPs"
            },
            {
                "model": "EfficientNetV2-XL",
                "training_compute_(flop)": 9.56e+19,
                "training_power_draw_(w)": 7119.606126290003,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 45.0,
                "training_compute_cost_(2023_usd)": 104.34013401561587,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2021-06-23T00:00:00",
                "link": "https://arxiv.org/abs/2104.00298",
                "reference": "EfficientNetV2: Smaller Models and Faster Training",
                "organization": "Google,Google Brain",
                "parameters": 208000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "EfficientNetV2-XL - 9.56e+19 FLOPs"
            },
            {
                "model": "StyleGAN3-T",
                "training_compute_(flop)": 1.70208e+21,
                "training_power_draw_(w)": 4854.493112492733,
                "training_dataset_size_(gradients)": 50000000.0,
                "training_time_(hours)": 1576.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2021-06-21T00:00:00",
                "link": "https://arxiv.org/abs/2106.12423",
                "reference": "Alias-Free Generative Adversarial Networks",
                "organization": "NVIDIA,Aalto University",
                "parameters": 2230000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "StyleGAN3-T - 1.70e+21 FLOPs"
            },
            {
                "model": "StyleGAN3-R",
                "training_compute_(flop)": 2.42784e+21,
                "training_power_draw_(w)": 4854.493112492733,
                "training_dataset_size_(gradients)": 50000000.0,
                "training_time_(hours)": 2248.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2021-06-21T00:00:00",
                "link": "https://arxiv.org/pdf/2106.12423",
                "reference": "Alias-Free Generative Adversarial Networks",
                "organization": "NVIDIA,Aalto University",
                "parameters": 1580000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "StyleGAN3-R - 2.43e+21 FLOPs"
            },
            {
                "model": "ALIGN",
                "training_compute_(flop)": 2.598670000001e+22,
                "training_power_draw_(w)": 227888.2870513745,
                "training_dataset_size_(gradients)": 1800000000.0,
                "training_time_(hours)": 347.3,
                "training_compute_cost_(2023_usd)": 32852.91660437908,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2021-06-11T00:00:00",
                "link": "https://arxiv.org/abs/2102.05918",
                "reference": "Scaling up visual and vision-language representation learning with noisy text supervision",
                "organization": "Google Research",
                "parameters": 820000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "ALIGN - 2.60e+22 FLOPs"
            },
            {
                "model": "Denoising Diffusion Probabilistic Models (LSUN Bedroom)",
                "training_compute_(flop)": 7.840125000000001e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 596320321536.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 436.308484475363,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2021-06-11T00:00:00",
                "link": "https://arxiv.org/abs/2006.11239",
                "reference": "Denoising Diffusion Probabilistic Models",
                "organization": "University of California (UC) Berkeley",
                "parameters": 256000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "Denoising Diffusion Probabilistic Models (LSUN Bedroom) - 7.84e+19 FLOPs"
            },
            {
                "model": "Delta RNN (+ full context)",
                "training_compute_(flop)": 1.1e+18,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2021-06-11T00:00:00",
                "link": "https://proceedings.neurips.cc/paper/2021/file/3f9e3767ef3b10a0de4c256d7ef9805d-Paper.pdf",
                "reference": "Going Beyond Linear Transformers with Recurrent Fast Weight Programmers",
                "organization": "IDSIA,SUPSI,King Abdullah University of Science and Technology (KAUST)",
                "parameters": 44600000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Delta RNN (+ full context) - 1.10e+18 FLOPs"
            },
            {
                "model": "DeBERTa",
                "training_compute_(flop)": 2.588e+22,
                "training_power_draw_(w)": 155381.83775233384,
                "training_dataset_size_(gradients)": 20800000000.0,
                "training_time_(hours)": 720.0,
                "training_compute_cost_(2023_usd)": 6682.2289986716,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2021-06-10T00:00:00",
                "link": "https://arxiv.org/abs/2006.03654",
                "reference": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention",
                "organization": "Microsoft",
                "parameters": 1500000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "DeBERTa - 2.59e+22 FLOPs"
            },
            {
                "model": "CoAtNet",
                "training_compute_(flop)": 4.27e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 88779000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 1887.1635925610951,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2021-06-09T00:00:00",
                "link": "https://arxiv.org/abs/2106.04803v2",
                "reference": "CoAtNet: Marrying Convolution and Attention\nfor All Data Sizes",
                "organization": "Google,Google Research,Google Brain",
                "parameters": 2440000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "CoAtNet - 4.27e+22 FLOPs"
            },
            {
                "model": "EMDR",
                "training_compute_(flop)": 1.91e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 171600000000.0,
                "training_time_(hours)": 355.0,
                "training_compute_cost_(2023_usd)": 2773.312430781674,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2021-06-09T00:00:00",
                "link": "https://arxiv.org/abs/2106.05346v2",
                "reference": "End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering",
                "organization": "Mila - Quebec AI (originally Montreal Institute for Learning Algorithms),McGill University,DeepMind",
                "parameters": 440000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "EMDR - 1.91e+21 FLOPs"
            },
            {
                "model": "ViT-G/14",
                "training_compute_(flop)": 5.85e+22,
                "training_power_draw_(w)": 911614.0493863056,
                "training_dataset_size_(gradients)": 3000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 3847.8613922131217,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2021-06-08T00:00:00",
                "link": "https://arxiv.org/abs/2106.04560",
                "reference": "Scaling Vision Transformers",
                "organization": "Google Brain,Google Research",
                "parameters": 1843000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "ViT-G/14 - 5.85e+22 FLOPs"
            },
            {
                "model": "AFP+FPI (PTB)",
                "training_compute_(flop)": 223363710000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 912344.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2021-06-04T00:00:00",
                "link": "https://arxiv.org/abs/2106.02417",
                "reference": "Approximate Fixed-Points in Recurrent Neural Networks",
                "organization": "University of Sheffield",
                "parameters": 2040000.0,
                "notable_model": false,
                "country": "United Kingdom",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "AFP+FPI (PTB) - 2.23e+14 FLOPs"
            },
            {
                "model": "AFP+FPI (WT2)",
                "training_compute_(flop)": 6528000000000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2021-06-04T00:00:00",
                "link": "https://arxiv.org/abs/2106.02417",
                "reference": "Approximate Fixed-Points in Recurrent Neural Networks",
                "organization": "University of Sheffield",
                "parameters": 13600000.0,
                "notable_model": false,
                "country": "United Kingdom",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "AFP+FPI (WT2) - 6.53e+15 FLOPs"
            },
            {
                "model": "GPT2-Large+LHOPT",
                "training_compute_(flop)": 4.9540697e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2021-06-02T00:00:00",
                "link": "https://arxiv.org/abs/2106.00958",
                "reference": "A Generalizable Approach to Learning Optimizers",
                "organization": "OpenAI",
                "parameters": 760000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "GPT2-Large+LHOPT - 4.95e+21 FLOPs"
            },
            {
                "model": "Wu Dao 2.0",
                "training_compute_(flop)": 1.54350000000001e+24,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 4900000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 3073311.166618605,
                "domain_group": "Multimodal",
                "organization_categorization": "Academia",
                "publication_date": "2021-05-31T00:00:00",
                "link": "https://www.engadget.com/chinas-gigantic-multi-modal-ai-is-no-one-trick-pony-211414388.html",
                "reference": "China's gigantic multi-modal AI is no one-trick pony",
                "organization": "Beijing Academy of Artificial Intelligence / BAAI",
                "parameters": 1750000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "API access",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Multimodal",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "Wu Dao 2.0 - 1.54e+24 FLOPs"
            },
            {
                "model": "ByT5-XXL",
                "training_compute_(flop)": 8.1e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1048576000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 92453.37635669748,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2021-05-28T00:00:00",
                "link": "https://arxiv.org/abs/2105.13626",
                "reference": "ByT5: Towards a token-free future with pre-trained byte-to-byte models",
                "organization": "Google,Google Research",
                "parameters": 12900000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "ByT5-XXL - 8.10e+22 FLOPs"
            },
            {
                "model": "Transformer local-attention (NesT-B)",
                "training_compute_(flop)": 2.40576e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1280000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2021-05-26T00:00:00",
                "link": "https://arxiv.org/abs/2105.12723v4",
                "reference": "Nested Hierarchical Transformer: Towards Accurate, Data-Efficient and Interpretable Visual Understanding",
                "organization": "Google Cloud,Google Research",
                "parameters": 90100000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Transformer local-attention (NesT-B) - 2.41e+19 FLOPs"
            },
            {
                "model": "CogView",
                "training_compute_(flop)": 2.68e+22,
                "training_power_draw_(w)": 259056.25043301377,
                "training_dataset_size_(gradients)": 964800000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 60071.706664791694,
                "domain_group": "Vision",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2021-05-26T00:00:00",
                "link": "https://arxiv.org/abs/2105.13290",
                "reference": "CogView: Mastering Text-to-Image Generation via Transformers",
                "organization": "Tsinghua University,Alibaba DAMO Academy",
                "parameters": 4000000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Vision",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "CogView - 2.68e+22 FLOPs"
            },
            {
                "model": "ConSERT",
                "training_compute_(flop)": 2.8e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 0.1,
                "training_compute_cost_(2023_usd)": 957.274379475876,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2021-05-25T00:00:00",
                "link": "https://arxiv.org/abs/2105.11741",
                "reference": "ConSERT: A contrastive framework for self-supervised sentence representation transfer",
                "organization": "Meituan University,Beijing University of Posts and Telecommunications",
                "parameters": 340000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "ConSERT - 2.80e+20 FLOPs"
            },
            {
                "model": "MedBERT",
                "training_compute_(flop)": 9.47e+18,
                "training_power_draw_(w)": 278.0422249147422,
                "training_dataset_size_(gradients)": 14587212800.0,
                "training_time_(hours)": 168.0,
                "training_compute_cost_(2023_usd)": 62.48645493610712,
                "domain_group": "Medicine",
                "organization_categorization": "Academia",
                "publication_date": "2021-05-20T00:00:00",
                "link": "https://www.nature.com/articles/s41746-021-00455-y",
                "reference": "Med-BERT: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction",
                "organization": "Peng Cheng Laboratory,University of Texas at Houston",
                "parameters": 17000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "MedBERT - 9.47e+18 FLOPs"
            },
            {
                "model": "ADM",
                "training_compute_(flop)": 6.2e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 130191200000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 11274.484326547095,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2021-05-11T00:00:00",
                "link": "https://arxiv.org/abs/2105.05233",
                "reference": "Diffusion Models Beat GANs on Image Synthesis",
                "organization": "OpenAI",
                "parameters": 559000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "ADM - 6.20e+21 FLOPs"
            },
            {
                "model": "ProtT5-XXL",
                "training_compute_(flop)": 7.37e+22,
                "training_power_draw_(w)": 228081.21596876832,
                "training_dataset_size_(gradients)": 61050000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 85701.26256441118,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry, Government",
                "publication_date": "2021-05-04T00:00:00",
                "link": "https://www.biorxiv.org/content/10.1101/2020.07.12.199554v3 or \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9477085",
                "reference": "ProtTrans: Towards Cracking the Language of Life\u2019s Code Through Self-Supervised Learning",
                "organization": "Technical University of Munich,Med AI Technology,NVIDIA,Oak Ridge National Laboratory,Google,Seoul National University",
                "parameters": 11000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry, Government",
                "access_group": "Open",
                "model_and_compute": "ProtT5-XXL - 7.37e+22 FLOPs"
            },
            {
                "model": "ProtT5-XXL-BFD",
                "training_compute_(flop)": 3.7e+22,
                "training_power_draw_(w)": 228081.21596876832,
                "training_dataset_size_(gradients)": 393000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 43025.05718973151,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry, Government",
                "publication_date": "2021-05-04T00:00:00",
                "link": "https://www.biorxiv.org/content/10.1101/2020.07.12.199554v3 or \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9477085",
                "reference": "ProtTrans:Towards Cracking the Language of Life's Code Through Self-Supervised Learning",
                "organization": "Technical University of Munich,Med AI Technology,NVIDIA,Oak Ridge National Laboratory,Google,Seoul National University",
                "parameters": 11000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry, Government",
                "access_group": "Open",
                "model_and_compute": "ProtT5-XXL-BFD - 3.70e+22 FLOPs"
            },
            {
                "model": "ProtBERT-BFD",
                "training_compute_(flop)": 3.9e+22,
                "training_power_draw_(w)": 456162.43193753663,
                "training_dataset_size_(gradients)": 58950000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 45350.73595674403,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry, Government",
                "publication_date": "2021-05-04T00:00:00",
                "link": "https://www.biorxiv.org/content/10.1101/2020.07.12.199554v3 or \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9477085",
                "reference": "ProtTrans:Towards Cracking the Language of Life's Code Through Self-Supervised Learning",
                "organization": "Technical University of Munich,NVIDIA,Seoul National University,Google,Oak Ridge National Laboratory,Med AI Technology",
                "parameters": 420000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry, Government",
                "access_group": "Open",
                "model_and_compute": "ProtBERT-BFD - 3.90e+22 FLOPs"
            },
            {
                "model": "ProtBERT-UniRef",
                "training_compute_(flop)": 7.27e+21,
                "training_power_draw_(w)": 228081.21596876832,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry, Government",
                "publication_date": "2021-05-04T00:00:00",
                "link": "https://www.biorxiv.org/content/10.1101/2020.07.12.199554v3 or \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9477085",
                "reference": "ProtTrans:Towards Cracking the Language of Life's Code Through Self-Supervised Learning",
                "organization": "Technical University of Munich,NVIDIA,Seoul National University,Google,Oak Ridge National Laboratory,Med AI Technology",
                "parameters": 420000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry, Government",
                "access_group": "Closed",
                "model_and_compute": "ProtBERT-UniRef - 7.27e+21 FLOPs"
            },
            {
                "model": "ProtT5-XL-U50",
                "training_compute_(flop)": 1.8704498688e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 19582371375.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry, Government",
                "publication_date": "2021-05-04T00:00:00",
                "link": "https://www.biorxiv.org/content/10.1101/2020.07.12.199554v3.full.pdf",
                "reference": "ProtTrans: Towards Cracking the Language of Life\u2019s Code Through Self-Supervised Learning",
                "organization": "Technical University of Munich,Med AI Technology,NVIDIA,Oak Ridge National Laboratory,Google,Seoul National University",
                "parameters": 3000000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry, Government",
                "access_group": "Open",
                "model_and_compute": "ProtT5-XL-U50 - 1.87e+22 FLOPs"
            },
            {
                "model": "Transformer-XL + SIS",
                "training_compute_(flop)": 3.7848651e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2021-05-03T00:00:00",
                "link": "http://proceedings.mlr.press/v139/verma21b/verma21b.pdf",
                "reference": "Sparsifying Networks via Subdifferential Inclusion",
                "organization": "INRIA",
                "parameters": 246000000.0,
                "notable_model": false,
                "country": "France",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Transformer-XL + SIS - 3.78e+20 FLOPs"
            },
            {
                "model": "GPT-J-6B",
                "training_compute_(flop)": 1.5e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 400000000000.0,
                "training_time_(hours)": 840.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": null,
                "publication_date": "2021-05-01T00:00:00",
                "link": "https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/",
                "reference": "GPT-J-6B: 6B JAX-Based Transformer",
                "organization": "EleutherAI,LAION",
                "parameters": 6053381344.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Other",
                "access_group": "Open",
                "model_and_compute": "GPT-J-6B - 1.50e+22 FLOPs"
            },
            {
                "model": "ViT + DINO",
                "training_compute_(flop)": 2.1e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1280000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 380.2849049080349,
                "domain_group": "Vision",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2021-04-29T00:00:00",
                "link": "https://arxiv.org/abs/2104.14294",
                "reference": "Emerging Properties in Self-Supervised Vision Transformers",
                "organization": "INRIA,Facebook AI Research",
                "parameters": 85000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "ViT + DINO - 2.10e+20 FLOPs"
            },
            {
                "model": "PanGu-\u03b1",
                "training_compute_(flop)": 5.112e+22,
                "training_power_draw_(w)": 1285806.3528918722,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2021-04-25T00:00:00",
                "link": "https://arxiv.org/abs/2104.12369",
                "reference": "PanGu-\u03b1: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation",
                "organization": "Huawei Noah's Ark Lab",
                "parameters": 207000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "PanGu-\u03b1 - 5.11e+22 FLOPs"
            },
            {
                "model": "DiffQ Transformer (16L)",
                "training_compute_(flop)": 3.02e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2021-04-20T00:00:00",
                "link": "https://arxiv.org/abs/2104.09987",
                "reference": "Differentiable Model Compression via Pseudo Quantization Noise",
                "organization": "Meta AI",
                "parameters": 247000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "DiffQ Transformer (16L) - 3.02e+19 FLOPs"
            },
            {
                "model": "PLUG",
                "training_compute_(flop)": 3.5997696e+22,
                "training_power_draw_(w)": 103707.91685199105,
                "training_dataset_size_(gradients)": 60000000000.0,
                "training_time_(hours)": 840.0,
                "training_compute_cost_(2023_usd)": 108672.69136370042,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2021-04-19T00:00:00",
                "link": "https://mp.weixin.qq.com/s/DAQomIkDa52Sef-ruyH5qg",
                "reference": null,
                "organization": "Alibaba",
                "parameters": 27000000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Hosted access (no API)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "PLUG - 3.60e+22 FLOPs"
            },
            {
                "model": "Transformer-C",
                "training_compute_(flop)": 1.8877734e+18,
                "training_power_draw_(w)": 2026.0414956053005,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": 40.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2021-04-08T00:00:00",
                "link": "https://arxiv.org/abs/2104.03474",
                "reference": "Revisiting Simple Neural Probabilistic Language Models",
                "organization": "University of Massachusetts Amherst",
                "parameters": 148000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Transformer-C - 1.89e+18 FLOPs"
            },
            {
                "model": "T2R + Random Init",
                "training_compute_(flop)": 2.749544e+19,
                "training_power_draw_(w)": 4864.124132899583,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2021-03-24T00:00:00",
                "link": "https://arxiv.org/abs/2103.13076",
                "reference": "Finetuning Pretrained Transformers into RNNs",
                "organization": "University of Washington,Microsoft,DeepMind,Allen Institute for AI",
                "parameters": 450000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "T2R + Random Init - 2.75e+19 FLOPs"
            },
            {
                "model": "T2R 75% + Pretrain (WT-103)",
                "training_compute_(flop)": 1.3517515512289972e+19,
                "training_power_draw_(w)": 4864.124132899583,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 11.8,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2021-03-24T00:00:00",
                "link": "https://arxiv.org/abs/2103.13076",
                "reference": "Finetuning Pretrained Transformers into RNNs",
                "organization": "University of Washington,Microsoft,DeepMind",
                "parameters": 668893184.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "T2R 75% + Pretrain (WT-103) - 1.35e+19 FLOPs"
            },
            {
                "model": "T2R + Pretrain",
                "training_compute_(flop)": 1.372929105052406e+19,
                "training_power_draw_(w)": 4864.124132899583,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 12.25,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2021-03-24T00:00:00",
                "link": "https://arxiv.org/abs/2103.13076",
                "reference": "Finetuning Pretrained Transformers into RNNs",
                "organization": "University of Washington,Microsoft,DeepMind",
                "parameters": 668893184.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "T2R + Pretrain - 1.37e+19 FLOPs"
            },
            {
                "model": "GPT-Neo-2.7B",
                "training_compute_(flop)": 7.9e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 420000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": null,
                "publication_date": "2021-03-21T00:00:00",
                "link": "https://github.com/EleutherAI/gpt-neo",
                "reference": "GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow",
                "organization": "EleutherAI",
                "parameters": 2700000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Other",
                "access_group": "Open",
                "model_and_compute": "GPT-Neo-2.7B - 7.90e+21 FLOPs"
            },
            {
                "model": "GLM-10B",
                "training_compute_(flop)": 4.9397553e+21,
                "training_power_draw_(w)": 38918.19281791449,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 1791.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2021-03-18T00:00:00",
                "link": "https://arxiv.org/abs/2103.10360",
                "reference": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling",
                "organization": "Tsinghua University,Beijing Academy of Artificial Intelligence / BAAI,Massachusetts Institute of Technology (MIT),Shanghai Qi Zhi institute",
                "parameters": 10000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "GLM-10B - 4.94e+21 FLOPs"
            },
            {
                "model": "Very Deep VAEs (ImageNet-64)",
                "training_compute_(flop)": 1.8144e+21,
                "training_power_draw_(w)": 19459.963111943263,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 420.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2021-03-16T00:00:00",
                "link": "https://arxiv.org/abs/2011.10650",
                "reference": "Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them on Images",
                "organization": "OpenAI",
                "parameters": 125000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Very Deep VAEs (ImageNet-64) - 1.81e+21 FLOPs"
            },
            {
                "model": "ResNet-RS",
                "training_compute_(flop)": 1.763328e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2021-03-13T00:00:00",
                "link": "https://arxiv.org/abs/2103.07579",
                "reference": "Revisiting ResNets: Improved Training and Scaling Strategies",
                "organization": "Google Brain,University of California (UC) Berkeley",
                "parameters": 192000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "ResNet-RS - 1.76e+22 FLOPs"
            },
            {
                "model": "AraELECTRA",
                "training_compute_(flop)": 2.5587079e+20,
                "training_power_draw_(w)": 3568.3750214042025,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 576.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2021-03-07T00:00:00",
                "link": "https://arxiv.org/abs/2012.15516",
                "reference": "AraELECTRA: Pre-Training Text Discriminators for Arabic Language Understanding",
                "organization": "American University of Beirut",
                "parameters": 136000000.0,
                "notable_model": false,
                "country": "Lebanon",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "AraELECTRA - 2.56e+20 FLOPs"
            },
            {
                "model": "M6-T",
                "training_compute_(flop)": 5.5e+21,
                "training_power_draw_(w)": 243309.1333554511,
                "training_dataset_size_(gradients)": 111800000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 13156.86123531961,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2021-03-05T00:00:00",
                "link": "https://arxiv.org/abs/2105.15082",
                "reference": "M6-T: Exploring Sparse Expert Models and Beyond",
                "organization": "Alibaba",
                "parameters": 1002700000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "M6-T - 5.50e+21 FLOPs"
            },
            {
                "model": "Generative BST",
                "training_compute_(flop)": 1.449e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 56800000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2021-03-05T00:00:00",
                "link": "https://arxiv.org/abs/2004.13637",
                "reference": "Recipes for building an open-domain chatbot",
                "organization": "Facebook AI Research",
                "parameters": 9431810048.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Generative BST - 1.45e+22 FLOPs"
            },
            {
                "model": "DCTransformer (ImageNet)",
                "training_compute_(flop)": 4.428e+21,
                "training_power_draw_(w)": 57096.5432940792,
                "training_dataset_size_(gradients)": 1000000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2021-03-05T00:00:00",
                "link": "https://arxiv.org/abs/2103.03841",
                "reference": "Generating Images with Sparse Representations",
                "organization": "DeepMind",
                "parameters": 736000000.0,
                "notable_model": false,
                "country": "United Kingdom",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "DCTransformer (ImageNet) - 4.43e+21 FLOPs"
            },
            {
                "model": "ProteinGAN",
                "training_compute_(flop)": 4.3e+18,
                "training_power_draw_(w)": 278.51940448398403,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 210.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2021-03-04T00:00:00",
                "link": "https://www.nature.com/articles/s42256-021-00310-5",
                "reference": "Expanding functional protein sequence spaces using generative adversarial networks",
                "organization": "Vilnius University,Chalmers University of Technology",
                "parameters": 60000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "ProteinGAN - 4.30e+18 FLOPs"
            },
            {
                "model": "RFA-GATE-Gaussian-Stateful Big",
                "training_compute_(flop)": 7.14e+18,
                "training_power_draw_(w)": 7137.385794869192,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": 3.36,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2021-03-03T00:00:00",
                "link": "https://arxiv.org/abs/2103.02143",
                "reference": "Random Feature Attention",
                "organization": "University of Washington,DeepMind,Allen Institute for AI,Hebrew University of Jerusalem,The University of Hong Kong",
                "parameters": 242000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "RFA-GATE-Gaussian-Stateful Big - 7.14e+18 FLOPs"
            },
            {
                "model": "Wu Dao - Wen Hui",
                "training_compute_(flop)": 1.161216e+20,
                "training_power_draw_(w)": 38932.92922983772,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 60.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Academia",
                "publication_date": "2021-03-01T00:00:00",
                "link": "https://medium.com/syncedreview/chinas-gpt-3-baai-introduces-superscale-intelligence-model-wu-dao-1-0-98a573fc4d70",
                "reference": "China's GPT-3? BAAI Introduces Superscale Intelligence Model 'Wu Dao 1.0'",
                "organization": "Beijing Academy of Artificial Intelligence / BAAI",
                "parameters": 11300000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Unknown",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Multimodal",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Wu Dao - Wen Hui - 1.16e+20 FLOPs"
            },
            {
                "model": "Wu Dao - Wen Lan",
                "training_compute_(flop)": 7.1995392e+21,
                "training_power_draw_(w)": 103821.1446129006,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 168.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Academia",
                "publication_date": "2021-03-01T00:00:00",
                "link": "https://medium.com/syncedreview/chinas-gpt-3-baai-introduces-superscale-intelligence-model-wu-dao-1-0-98a573fc4d70",
                "reference": "China's GPT-3? BAAI Introduces Superscale Intelligence Model 'Wu Dao 1.0'",
                "organization": "Beijing Academy of Artificial Intelligence / BAAI",
                "parameters": 1000000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Unknown",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Multimodal",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Wu Dao - Wen Lan - 7.20e+21 FLOPs"
            },
            {
                "model": "Meta Pseudo Labels",
                "training_compute_(flop)": 4.79e+22,
                "training_power_draw_(w)": 456813.03629676264,
                "training_dataset_size_(gradients)": 131280000.0,
                "training_time_(hours)": 264.0,
                "training_compute_cost_(2023_usd)": 53844.28059190435,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2021-03-01T00:00:00",
                "link": "https://arxiv.org/abs/2003.10580",
                "reference": "Meta pseudo labels",
                "organization": "Google Brain,Google AI",
                "parameters": 480000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Meta Pseudo Labels - 4.79e+22 FLOPs"
            },
            {
                "model": "SRU++ Large",
                "training_compute_(flop)": 2.1173704e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2021-02-24T00:00:00",
                "link": "https://arxiv.org/abs/2102.12459",
                "reference": "When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute",
                "organization": "ASAPP",
                "parameters": 234000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "SRU++ Large - 2.12e+19 FLOPs"
            },
            {
                "model": "SRU++ Base",
                "training_compute_(flop)": 2.592e+19,
                "training_power_draw_(w)": 4867.158066309197,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 24.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2021-02-24T00:00:00",
                "link": "https://arxiv.org/abs/2102.12459",
                "reference": "When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute",
                "organization": "ASAPP",
                "parameters": 148000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "SRU++ Base - 2.59e+19 FLOPs"
            },
            {
                "model": "SRU++ Large only 2 attention layers (k=5) (WT103)",
                "training_compute_(flop)": 3.564e+19,
                "training_power_draw_(w)": 4867.158066309197,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 33.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2021-02-24T00:00:00",
                "link": "https://arxiv.org/abs/2102.12459",
                "reference": "When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute",
                "organization": "ASAPP",
                "parameters": 225000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "SRU++ Large only 2 attention layers (k=5) (WT103) - 3.56e+19 FLOPs"
            },
            {
                "model": "Linear Transformer (large)",
                "training_compute_(flop)": 3.89e+18,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2021-02-22T00:00:00",
                "link": "https://arxiv.org/abs/2102.11174",
                "reference": "Linear Transformers Are Secretly Fast Weight Programmers",
                "organization": "IDSIA",
                "parameters": 90000000.0,
                "notable_model": false,
                "country": "Switzerland",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Linear Transformer (large) - 3.89e+18 FLOPs"
            },
            {
                "model": "Linear Transformer (small)",
                "training_compute_(flop)": 1.7304e+18,
                "training_power_draw_(w)": 1216.8437120592066,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2021-02-22T00:00:00",
                "link": "https://arxiv.org/abs/2102.11174",
                "reference": "Linear Transformers Are Secretly Fast Weight Programmers",
                "organization": "IDSIA,SUPSI",
                "parameters": 40000000.0,
                "notable_model": false,
                "country": "Switzerland",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Linear Transformer (small) - 1.73e+18 FLOPs"
            },
            {
                "model": "MSA Transformer",
                "training_compute_(flop)": 5.49e+21,
                "training_power_draw_(w)": 16227.834954676886,
                "training_dataset_size_(gradients)": 1395000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 13256.937301517895,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2021-02-13T00:00:00",
                "link": "https://proceedings.mlr.press/v139/rao21a/rao21a.pdf",
                "reference": "MSA Transformer",
                "organization": "Facebook AI Research,University of California (UC) Berkeley,New York University (NYU)",
                "parameters": 100000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "MSA Transformer - 5.49e+21 FLOPs"
            },
            {
                "model": "DLWP",
                "training_compute_(flop)": 5.6845152e+18,
                "training_power_draw_(w)": 278.6620974208019,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 168.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Earth science",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2021-02-09T00:00:00",
                "link": "https://arxiv.org/abs/2102.05107",
                "reference": "Sub-seasonal forecasting with a large ensemble of deep-learning weather prediction models",
                "organization": "University of Washington,Microsoft Research",
                "parameters": 2676376.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "DLWP - 5.68e+18 FLOPs"
            },
            {
                "model": "Selfish-RNN (SNT-ASGD) Stacked LSTMs",
                "training_compute_(flop)": 1.178e+16,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 912344.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2021-01-22T00:00:00",
                "link": "https://arxiv.org/abs/2101.09048",
                "reference": "Selfish Sparse RNN Training",
                "organization": "Eindhoven University of Technology,University of Twente",
                "parameters": 25200000.0,
                "notable_model": false,
                "country": "Netherlands",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "Selfish-RNN (SNT-ASGD) Stacked LSTMs - 1.18e+16 FLOPs"
            },
            {
                "model": "DeiT-B",
                "training_compute_(flop)": 7.884e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 3840000.0,
                "training_time_(hours)": 53.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2021-01-15T00:00:00",
                "link": "https://arxiv.org/abs/2012.12877",
                "reference": "Training data-efficient image transformers & distillation through attention",
                "organization": "Meta AI,Sorbonne University",
                "parameters": 86000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "DeiT-B - 7.88e+19 FLOPs"
            },
            {
                "model": "Switch",
                "training_compute_(flop)": 8.22e+22,
                "training_power_draw_(w)": 457311.78237926256,
                "training_dataset_size_(gradients)": 86400000000.0,
                "training_time_(hours)": 648.0,
                "training_compute_cost_(2023_usd)": 136886.37981186676,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2021-01-11T00:00:00",
                "link": "https://arxiv.org/abs/2101.03961",
                "reference": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity",
                "organization": "Google",
                "parameters": 1571000000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Switch - 8.22e+22 FLOPs"
            },
            {
                "model": "Wu Dao - Wen Yuan",
                "training_compute_(flop)": 6.5028096e+20,
                "training_power_draw_(w)": 38975.435998232606,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 336.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2021-01-11T00:00:00",
                "link": "https://mp.weixin.qq.com/s/BUQWZ5EdR19i40GuFofpBg",
                "reference": "Tencent: Facing cognition, Zhiyuan Research Institute and several units released a super-large-scale new pre-training model \"Enlightenment\u00b7Wenhui\"",
                "organization": "Beijing Academy of Artificial Intelligence / BAAI",
                "parameters": 2600000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "API access",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "Wu Dao - Wen Yuan - 6.50e+20 FLOPs"
            },
            {
                "model": "NVAE (CIFAR 10)",
                "training_compute_(flop)": 5.94e+19,
                "training_power_draw_(w)": 4060.2124958936606,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 55.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2021-01-08T00:00:00",
                "link": "https://arxiv.org/abs/2007.03898",
                "reference": "NVAE: A Deep Hierarchical Variational Autoencoder",
                "organization": "NVIDIA",
                "parameters": null,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "NVAE (CIFAR 10) - 5.94e+19 FLOPs"
            },
            {
                "model": "NVAE (FFHQ)",
                "training_compute_(flop)": 5.184e+20,
                "training_power_draw_(w)": 12180.637487680982,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 160.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2021-01-08T00:00:00",
                "link": "https://arxiv.org/abs/2007.03898",
                "reference": "NVAE: A Deep Hierarchical Variational Autoencoder",
                "organization": "NVIDIA",
                "parameters": null,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "NVAE (FFHQ) - 5.18e+20 FLOPs"
            },
            {
                "model": "NVAE (Celeba HQ)",
                "training_compute_(flop)": 3.0456e+20,
                "training_power_draw_(w)": 12180.637487680982,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 94.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2021-01-08T00:00:00",
                "link": "https://arxiv.org/abs/2007.03898",
                "reference": "NVAE: A Deep Hierarchical Variational Autoencoder",
                "organization": "NVIDIA",
                "parameters": null,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "NVAE (Celeba HQ) - 3.05e+20 FLOPs"
            },
            {
                "model": "DALL-E",
                "training_compute_(flop)": 4.7e+22,
                "training_power_draw_(w)": 519741.92129196384,
                "training_dataset_size_(gradients)": 320000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 125818.6901,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2021-01-05T00:00:00",
                "link": "https://openai.com/blog/dall-e/\n\nhttps://arxiv.org/abs/2102.12092",
                "reference": "Zero-Shot Text-to-Image Generation",
                "organization": "OpenAI",
                "parameters": 12000000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "API access",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "DALL-E - 4.70e+22 FLOPs"
            },
            {
                "model": "CLIP (ViT L/14@336px)",
                "training_compute_(flop)": 1.05e+22,
                "training_power_draw_(w)": 155922.57638758916,
                "training_dataset_size_(gradients)": 400000000.0,
                "training_time_(hours)": 288.0,
                "training_compute_cost_(2023_usd)": 24638.51841340951,
                "domain_group": "Multimodal",
                "organization_categorization": "Industry",
                "publication_date": "2021-01-05T00:00:00",
                "link": "https://arxiv.org/abs/2103.00020\nhttps://huggingface.co/openai/clip-vit-large-patch14-336",
                "reference": "Learning Transferable Visual Models From Natural Language Supervision",
                "organization": "OpenAI",
                "parameters": 370000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Multimodal",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "CLIP (ViT L/14@336px) - 1.05e+22 FLOPs"
            },
            {
                "model": "Transformer-XL + AutoDropout (PTB)",
                "training_compute_(flop)": 5.7829941034035304e+16,
                "training_power_draw_(w)": 2273.870905652342,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 0.66,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2021-01-05T00:00:00",
                "link": "https://arxiv.org/abs/2101.01761",
                "reference": "AutoDropout: Learning Dropout Patterns to Regularize Deep Networks",
                "organization": "Google Research",
                "parameters": 24000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Transformer-XL + AutoDropout (PTB) - 5.78e+16 FLOPs"
            },
            {
                "model": "Subformer (122M)",
                "training_compute_(flop)": 5.1450348e+18,
                "training_power_draw_(w)": 4873.014568662182,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2021-01-01T00:00:00",
                "link": "https://arxiv.org/abs/2101.00234",
                "reference": "Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers",
                "organization": "National Institute of Advanced Industrial Science and Technology (AIST),The University of Tokyo",
                "parameters": 122000000.0,
                "notable_model": false,
                "country": "Japan",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Japan",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Subformer (122M) - 5.15e+18 FLOPs"
            },
            {
                "model": "Subformer (83M)",
                "training_compute_(flop)": 3.5e+18,
                "training_power_draw_(w)": 4873.014568662182,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2021-01-01T00:00:00",
                "link": "https://arxiv.org/abs/2101.00234",
                "reference": "Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers",
                "organization": "The University of Tokyo,National Institute of Advanced Industrial Science and Technology (AIST)",
                "parameters": 83000000.0,
                "notable_model": false,
                "country": "Japan",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Japan",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Subformer (83M) - 3.50e+18 FLOPs"
            },
            {
                "model": "Subformer (96M)",
                "training_compute_(flop)": 4.05e+18,
                "training_power_draw_(w)": 4873.014568662182,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2021-01-01T00:00:00",
                "link": "https://arxiv.org/abs/2101.00234",
                "reference": "Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers",
                "organization": "The University of Tokyo,National Institute of Advanced Industrial Science and Technology (AIST)",
                "parameters": 96000000.0,
                "notable_model": false,
                "country": "Japan",
                "model_accessibility": "Unreleased",
                "year": 2021,
                "era": "Deep learning era",
                "country_top8": "Japan",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Subformer (96M) - 4.05e+18 FLOPs"
            },
            {
                "model": "AraGPT2-Mega",
                "training_compute_(flop)": 2e+21,
                "training_power_draw_(w)": 57177.97757573645,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 216.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2020-12-31T00:00:00",
                "link": "https://arxiv.org/abs/2012.15520",
                "reference": "AraGPT2: Pre-Trained Transformer for Arabic Language Generation",
                "organization": "American University of Beirut",
                "parameters": 1500000000.0,
                "notable_model": false,
                "country": "Lebanon",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "AraGPT2-Mega - 2.00e+21 FLOPs"
            },
            {
                "model": "Shortformer",
                "training_compute_(flop)": 3.04e+18,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2020-12-31T00:00:00",
                "link": "https://arxiv.org/abs/2012.15832",
                "reference": "Shortformer: Better Language Modeling using Shorter Inputs",
                "organization": "University of Washington,Facebook AI Research,Allen Institute for AI",
                "parameters": 247000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "Shortformer - 3.04e+18 FLOPs"
            },
            {
                "model": "ERNIE-Doc (247M)",
                "training_compute_(flop)": 3.0302798e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2020-12-31T00:00:00",
                "link": "https://arxiv.org/abs/2012.15688",
                "reference": "ERNIE-Doc: A Retrospective Long-Document Modeling Transformer",
                "organization": "Baidu",
                "parameters": 247000000.00000003,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "ERNIE-Doc (247M) - 3.03e+19 FLOPs"
            },
            {
                "model": "ERNIE-Doc Base (151M, WT103)",
                "training_compute_(flop)": 1.7395e+18,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2020-12-31T00:00:00",
                "link": "https://arxiv.org/pdf/2012.15688",
                "reference": "ERNIE-Doc: A Retrospective Long-Document Modeling Transformer",
                "organization": "Baidu",
                "parameters": 151000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "ERNIE-Doc Base (151M, WT103) - 1.74e+18 FLOPs"
            },
            {
                "model": "CT-MoS (WT2)",
                "training_compute_(flop)": 5.4e+17,
                "training_power_draw_(w)": 2030.739275278243,
                "training_dataset_size_(gradients)": 2000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2020-12-25T00:00:00",
                "link": "https://arxiv.org/abs/2012.13575",
                "reference": "Contextual Temperature for Language Modeling",
                "organization": "Google,National Tsing Hua University",
                "parameters": 45000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "CT-MoS (WT2) - 5.40e+17 FLOPs"
            },
            {
                "model": "CT-MoS + DynamicEval (WT2)",
                "training_compute_(flop)": 5.4e+17,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2020-12-25T00:00:00",
                "link": "https://arxiv.org/abs/2012.13575",
                "reference": "Contextual Temperature for Language Modeling",
                "organization": "National Tsing Hua University,Google",
                "parameters": 45000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "CT-MoS + DynamicEval (WT2) - 5.40e+17 FLOPs"
            },
            {
                "model": "DensePhrases",
                "training_compute_(flop)": 2.09952e+18,
                "training_power_draw_(w)": 4061.6594477324606,
                "training_dataset_size_(gradients)": 3223074.0,
                "training_time_(hours)": 20.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2020-12-23T00:00:00",
                "link": "https://arxiv.org/abs/2012.12624v3",
                "reference": "Learning Dense Representations of Phrases at Scale",
                "organization": "Korea University,Princeton University",
                "parameters": null,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "DensePhrases - 2.10e+18 FLOPs"
            },
            {
                "model": "ESM1b",
                "training_compute_(flop)": 5.1e+21,
                "training_power_draw_(w)": 77997.75584661258,
                "training_dataset_size_(gradients)": 27750400000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 924.3248891034536,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2020-12-15T00:00:00",
                "link": "https://www.pnas.org/doi/abs/10.1073/pnas.2016239118",
                "reference": "Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences",
                "organization": "Facebook AI Research,New York University (NYU)",
                "parameters": 652400000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "ESM1b - 5.10e+21 FLOPs"
            },
            {
                "model": "RoBERTa (PFAM)",
                "training_compute_(flop)": 1.2276e+19,
                "training_power_draw_(w)": 2031.643942464676,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2020-12-05T00:00:00",
                "link": "https://arxiv.org/abs/2012.03084",
                "reference": "Pre-training Protein Language Models with Label-Agnostic Binding Pairs Enhances Performance in Downstream Tasks",
                "organization": "IBM Research,ETH Zurich",
                "parameters": null,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "RoBERTa (PFAM) - 1.23e+19 FLOPs"
            },
            {
                "model": "CPM-Large",
                "training_compute_(flop)": 2.6052e+20,
                "training_power_draw_(w)": 39011.038545758856,
                "training_dataset_size_(gradients)": 16700000000.0,
                "training_time_(hours)": 336.0,
                "training_compute_cost_(2023_usd)": 7340.099044719796,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2020-12-01T00:00:00",
                "link": "https://arxiv.org/abs/2012.00413",
                "reference": "CPM: A Large-scale Generative Chinese Pre-trained Language Model",
                "organization": "Tsinghua University,Beijing Academy of Artificial Intelligence / BAAI",
                "parameters": 2600000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "CPM-Large - 2.61e+20 FLOPs"
            },
            {
                "model": "Profile Prediction",
                "training_compute_(flop)": 4.9999999999999993e+20,
                "training_power_draw_(w)": 4876.379818219857,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2020-12-01T00:00:00",
                "link": "https://arxiv.org/abs/2012.00195",
                "reference": "Profile Prediction: An Alignment-Based Pre-Training Task for Protein Sequence Models",
                "organization": "University of Washington,Salesforce Research",
                "parameters": null,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "Profile Prediction - 5.00e+20 FLOPs"
            },
            {
                "model": "AlphaFold 2",
                "training_compute_(flop)": 2.99e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 5724000000.0,
                "training_time_(hours)": 264.0,
                "training_compute_cost_(2023_usd)": 3841.772612266474,
                "domain_group": "Biology",
                "organization_categorization": "Industry",
                "publication_date": "2020-11-30T00:00:00",
                "link": "https://www.nature.com/articles/s41586-021-03819-2",
                "reference": "Highly accurate protein structure prediction with AlphaFold",
                "organization": "DeepMind",
                "parameters": 93000000.0,
                "notable_model": true,
                "country": "United Kingdom",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Biology",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "AlphaFold 2 - 2.99e+21 FLOPs"
            },
            {
                "model": "KEPLER",
                "training_compute_(flop)": 1.66e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 3533640000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2020-11-23T00:00:00",
                "link": "https://arxiv.org/abs/1911.06136",
                "reference": "KEPLER: A Unified Model for Knowledge Embedding and Pre- trained Language Representation.",
                "organization": "Tsinghua University,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms),HEC,CIFAR AI Research,Princeton University,University of Montreal / Universit\u00e9 de Montr\u00e9al",
                "parameters": 125000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "KEPLER - 1.66e+21 FLOPs"
            },
            {
                "model": "AWD-FWM (WT2)",
                "training_compute_(flop)": 7.104e+17,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2020-11-16T00:00:00",
                "link": "https://arxiv.org/abs/2011.07831",
                "reference": "Learning Associative Inference Using Fast Weight Memory",
                "organization": "IDSIA,Microsoft Research",
                "parameters": 37000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "AWD-FWM (WT2) - 7.10e+17 FLOPs"
            },
            {
                "model": "Machine learning a model for RNA structure prediction",
                "training_compute_(flop)": 1.799999999999987e+18,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2020-11-16T00:00:00",
                "link": "https://academic.oup.com/nargab/article/2/4/lqaa090/5983421",
                "reference": "Machine learning a model for RNA structure prediction ",
                "organization": "International School for Advanced Studies,Institute of Structural Biology,Technical University of Munich",
                "parameters": null,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Machine learning a model for RNA structure prediction - 1.80e+18 FLOPs"
            },
            {
                "model": "ChemBERTa",
                "training_compute_(flop)": 8.4654366e+18,
                "training_power_draw_(w)": 335.20719920857016,
                "training_dataset_size_(gradients)": 225000000.0,
                "training_time_(hours)": 48.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2020-10-23T00:00:00",
                "link": "https://arxiv.org/abs/2010.09885",
                "reference": "ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction",
                "organization": "University of Toronto,Reverie Labs,DeepChem",
                "parameters": 125000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "ChemBERTa - 8.47e+18 FLOPs"
            },
            {
                "model": "ViT-Huge/14",
                "training_compute_(flop)": 4.262e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 303000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 6724.023241261178,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2020-10-22T00:00:00",
                "link": "https://arxiv.org/abs/2010.11929",
                "reference": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
                "organization": "Google Brain,Google Research",
                "parameters": 632000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "ViT-Huge/14 - 4.26e+21 FLOPs"
            },
            {
                "model": "wave2vec 2.0 LARGE",
                "training_compute_(flop)": 3.87072e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 4598395200.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 5021.241075362514,
                "domain_group": "Audio",
                "organization_categorization": "Industry",
                "publication_date": "2020-10-22T00:00:00",
                "link": "https://arxiv.org/abs/2006.11477",
                "reference": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations",
                "organization": "Facebook",
                "parameters": 317000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "wave2vec 2.0 LARGE - 3.87e+21 FLOPs"
            },
            {
                "model": "GBERT-Large",
                "training_compute_(flop)": 2.2444646e+21,
                "training_power_draw_(w)": 28634.227317541987,
                "training_dataset_size_(gradients)": 5457559999.0,
                "training_time_(hours)": 264.0,
                "training_compute_cost_(2023_usd)": 3771.13338489269,
                "domain_group": "Language",
                "organization_categorization": "Industry, Government",
                "publication_date": "2020-10-21T00:00:00",
                "link": "https://arxiv.org/abs/2010.10906",
                "reference": "German's Next Language Model",
                "organization": "deepset,Bayerische Staatsbibliothek Muenchen",
                "parameters": 335000000.0,
                "notable_model": false,
                "country": "Germany",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "Germany",
                "domain_top4": "Language",
                "org_top5": "Other",
                "access_group": "Open",
                "model_and_compute": "GBERT-Large - 2.24e+21 FLOPs"
            },
            {
                "model": "German ELECTRA Large",
                "training_compute_(flop)": 1.42829568e+21,
                "training_power_draw_(w)": 28634.227317541987,
                "training_dataset_size_(gradients)": 36383733333.0,
                "training_time_(hours)": 168.0,
                "training_compute_cost_(2023_usd)": 2392.2883349806307,
                "domain_group": "Language",
                "organization_categorization": "Industry, Government",
                "publication_date": "2020-10-21T00:00:00",
                "link": "https://arxiv.org/abs/2010.10906",
                "reference": "German's Next Language Model",
                "organization": "deepset,Bayerische Staatsbibliothek Muenchen",
                "parameters": 335000000.0,
                "notable_model": true,
                "country": "Germany",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "Germany",
                "domain_top4": "Language",
                "org_top5": "Other",
                "access_group": "Open",
                "model_and_compute": "German ELECTRA Large - 1.43e+21 FLOPs"
            },
            {
                "model": "Conformer + Wav2vec 2.0 + Noisy Student",
                "training_compute_(flop)": 7.6e+21,
                "training_power_draw_(w)": 114539.4599635263,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 168.0,
                "training_compute_cost_(2023_usd)": 9449.541661137231,
                "domain_group": "Audio",
                "organization_categorization": "Industry",
                "publication_date": "2020-10-20T00:00:00",
                "link": "https://arxiv.org/abs/2010.10504v2",
                "reference": "Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition",
                "organization": "Google,Google Research,Google Brain",
                "parameters": 1000000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Conformer + Wav2vec 2.0 + Noisy Student - 7.60e+21 FLOPs"
            },
            {
                "model": "mT5-XXL",
                "training_compute_(flop)": 8.2e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 150000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 192595.40371209933,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2020-10-20T00:00:00",
                "link": "https://aclanthology.org/2021.naacl-main.41/",
                "reference": "mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer",
                "organization": "Google,Google Research",
                "parameters": 13000000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "mT5-XXL - 8.20e+22 FLOPs"
            },
            {
                "model": "TinyBert",
                "training_compute_(flop)": 3.9798e+18,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 3300000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2020-10-16T00:00:00",
                "link": "https://arxiv.org/abs/1909.10351",
                "reference": "TinyBERT: Distilling BERT for Natural Language Understanding",
                "organization": "Huazhong University of Science and Technology,Huawei Noah's Ark Lab,Huawei",
                "parameters": 67000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "TinyBert - 3.98e+18 FLOPs"
            },
            {
                "model": "Memformer (4 encoder + 16 decoder)",
                "training_compute_(flop)": 1.2e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": 96.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2020-10-14T00:00:00",
                "link": "https://arxiv.org/abs/2010.06891",
                "reference": "Memformer: A Memory-Augmented Transformer for Sequence Modeling",
                "organization": "UC Davis,Westlake University,Facebook AI",
                "parameters": 76200000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "Memformer (4 encoder + 16 decoder) - 1.20e+19 FLOPs"
            },
            {
                "model": "LUKE",
                "training_compute_(flop)": 1.8144e+22,
                "training_power_draw_(w)": 9765.799615782104,
                "training_dataset_size_(gradients)": 3511000000.0,
                "training_time_(hours)": 720.0,
                "training_compute_cost_(2023_usd)": 4186.383565732907,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2020-10-02T00:00:00",
                "link": "https://arxiv.org/abs/2010.01057v1",
                "reference": "LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention",
                "organization": "University of Washington,National Institute of Informatics",
                "parameters": 483000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "LUKE - 1.81e+22 FLOPs"
            },
            {
                "model": "ProBERTa",
                "training_compute_(flop)": 9.72e+18,
                "training_power_draw_(w)": 2443.135942042391,
                "training_dataset_size_(gradients)": 58320000.0,
                "training_time_(hours)": 18.0,
                "training_compute_cost_(2023_usd)": 26.206992435694065,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2020-09-01T00:00:00",
                "link": "https://dl.acm.org/doi/10.1145/3388440.3412467",
                "reference": "Transforming the Language of Life: Transformer Neural Networks for Protein Prediction Tasks",
                "organization": "University of Illinois Urbana-Champaign (UIUC),Reed College",
                "parameters": 44000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "ProBERTa - 9.72e+18 FLOPs"
            },
            {
                "model": "ESM1-670M (UR50/S)",
                "training_compute_(flop)": 4.4e+20,
                "training_power_draw_(w)": 78182.09119198628,
                "training_dataset_size_(gradients)": 27750400000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 966.9178746226474,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2020-08-31T00:00:00",
                "link": "https://www.pnas.org/doi/abs/10.1073/pnas.2016239118",
                "reference": "Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences",
                "organization": "Facebook AI Research,New York University (NYU)",
                "parameters": 669200000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "ESM1-670M (UR50/S) - 4.40e+20 FLOPs"
            },
            {
                "model": "ESM1-670M (UR50/D)",
                "training_compute_(flop)": 4.8e+20,
                "training_power_draw_(w)": 78182.09119198628,
                "training_dataset_size_(gradients)": 33847900000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 1054.8194995883428,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2020-08-31T00:00:00",
                "link": "https://www.pnas.org/doi/abs/10.1073/pnas.2016239118",
                "reference": "Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences",
                "organization": "Facebook AI Research,New York University (NYU)",
                "parameters": 669200000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "ESM1-670M (UR50/D) - 4.80e+20 FLOPs"
            },
            {
                "model": "ESM1-670M (UR100)",
                "training_compute_(flop)": 1.4e+20,
                "training_power_draw_(w)": 78182.09119198628,
                "training_dataset_size_(gradients)": 127897600000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 307.6556873799333,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2020-08-31T00:00:00",
                "link": "https://www.pnas.org/doi/abs/10.1073/pnas.2016239118",
                "reference": "Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences",
                "organization": "Facebook AI Research,New York University (NYU)",
                "parameters": 669200000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "ESM1-670M (UR100) - 1.40e+20 FLOPs"
            },
            {
                "model": "ESM1-85M",
                "training_compute_(flop)": 5.6e+19,
                "training_power_draw_(w)": 78182.09119198628,
                "training_dataset_size_(gradients)": 27750400000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 123.06227495197334,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2020-08-31T00:00:00",
                "link": "https://www.pnas.org/doi/abs/10.1073/pnas.2016239118",
                "reference": "Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences",
                "organization": "Facebook AI Research,New York University (NYU)",
                "parameters": 85100000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "ESM1-85M - 5.60e+19 FLOPs"
            },
            {
                "model": "ESM1-43M",
                "training_compute_(flop)": 2.8e+19,
                "training_power_draw_(w)": 78182.09119198628,
                "training_dataset_size_(gradients)": 27750400000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 61.53113747598667,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2020-08-31T00:00:00",
                "link": "https://www.pnas.org/doi/abs/10.1073/pnas.2016239118",
                "reference": "Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences",
                "organization": "Facebook AI Research,New York University (NYU)",
                "parameters": 42600000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "ESM1-43M - 2.80e+19 FLOPs"
            },
            {
                "model": "Transformer+Recurrent Windows of Context",
                "training_compute_(flop)": 7.9375326e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2020-08-16T00:00:00",
                "link": "https://arxiv.org/abs/2008.07027",
                "reference": "Adding Recurrence to Pretrained Transformers for Improved Efficiency and Context Size",
                "organization": "Toyota Technological Institute at Chicago,University of Chicago",
                "parameters": 124000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Transformer+Recurrent Windows of Context - 7.94e+20 FLOPs"
            },
            {
                "model": "ERNIE-GEN (large)",
                "training_compute_(flop)": 2e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 114666666666.66669,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2020-08-06T00:00:00",
                "link": "https://arxiv.org/abs/2001.11314",
                "reference": "ERNIE-GEN: An Enhanced Multi-Flow Pre-training and Fine-tuning Framework for Natural Language Generation",
                "organization": "Baidu",
                "parameters": 340000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "ERNIE-GEN (large) - 2.00e+20 FLOPs"
            },
            {
                "model": "DeLighT",
                "training_compute_(flop)": 3.8016e+18,
                "training_power_draw_(w)": 4889.428515149248,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": 30.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2020-08-03T00:00:00",
                "link": "https://arxiv.org/abs/2008.00623",
                "reference": "DeLighT: Deep and Light-weight Transformer",
                "organization": "University of Washington,Allen Institute for AI,Facebook AI Research",
                "parameters": 99000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "DeLighT - 3.80e+18 FLOPs"
            },
            {
                "model": "mBART-50",
                "training_compute_(flop)": 1.45152e+22,
                "training_power_draw_(w)": 156465.19682753857,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 420.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2020-08-02T00:00:00",
                "link": "https://arxiv.org/abs/2008.00401\n\nhttps://huggingface.co/facebook/mbart-large-50",
                "reference": "Multilingual Translation with Extensible Multilingual Pretraining and Finetuning\n",
                "organization": "Facebook AI",
                "parameters": 610000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "mBART-50 - 1.45e+22 FLOPs"
            },
            {
                "model": "DLRM-2021",
                "training_compute_(flop)": 3e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Recommendation",
                "organization_categorization": "Industry",
                "publication_date": "2020-07-01T00:00:00",
                "link": "https://arxiv.org/abs/2104.05158",
                "reference": "High-performance, Distributed Training of Large scale Deep Learning Recommendation Models",
                "organization": "Meta AI",
                "parameters": 1000000000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "DLRM-2021 - 3.00e+20 FLOPs"
            },
            {
                "model": "GShard (dense)",
                "training_compute_(flop)": 4.765e+22,
                "training_power_draw_(w)": 459301.98946084874,
                "training_dataset_size_(gradients)": 346666666667.0,
                "training_time_(hours)": 1008.0,
                "training_compute_cost_(2023_usd)": 254267.11053357512,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2020-06-30T00:00:00",
                "link": "https://arxiv.org/abs/2006.16668",
                "reference": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding",
                "organization": "Google",
                "parameters": 2300000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "GShard (dense) - 4.77e+22 FLOPs"
            },
            {
                "model": "GShard (600B)",
                "training_compute_(flop)": 1.33e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1000000000000.0,
                "training_time_(hours)": 96.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2020-06-30T00:00:00",
                "link": "https://arxiv.org/abs/2006.16668",
                "reference": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding",
                "organization": "Google",
                "parameters": 600000000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "GShard (600B) - 1.33e+22 FLOPs"
            },
            {
                "model": "GPT-3 6.7B",
                "training_compute_(flop)": 1.2e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 300000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2020-06-22T00:00:00",
                "link": "https://arxiv.org/abs/2005.14165",
                "reference": "Language Models are Few-Shot Learners",
                "organization": "OpenAI",
                "parameters": 6660000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "GPT-3 6.7B - 1.20e+22 FLOPs"
            },
            {
                "model": "GPT-3 XL",
                "training_compute_(flop)": 2.38e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 300000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2020-06-22T00:00:00",
                "link": "https://arxiv.org/abs/2005.14165",
                "reference": "Language Models are Few-Shot Learners",
                "organization": "OpenAI",
                "parameters": 1320000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "GPT-3 XL - 2.38e+21 FLOPs"
            },
            {
                "model": "GPT-3 Small",
                "training_compute_(flop)": 2.25e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 300000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2020-06-22T00:00:00",
                "link": "https://arxiv.org/abs/2005.14165",
                "reference": "Language Models are Few-Shot Learners",
                "organization": "OpenAI",
                "parameters": 125000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "GPT-3 Small - 2.25e+20 FLOPs"
            },
            {
                "model": "GPT-3 Medium",
                "training_compute_(flop)": 6.41e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 300000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2020-06-22T00:00:00",
                "link": "https://arxiv.org/abs/2005.14165",
                "reference": "Language Models are Few-Shot Learners",
                "organization": "OpenAI",
                "parameters": 356000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "GPT-3 Medium - 6.41e+20 FLOPs"
            },
            {
                "model": "GPT-3 Large",
                "training_compute_(flop)": 1.37e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 300000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2020-06-22T00:00:00",
                "link": "https://arxiv.org/abs/2005.14165",
                "reference": "Language Models are Few-Shot Learners",
                "organization": "OpenAI",
                "parameters": 760000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "GPT-3 Large - 1.37e+21 FLOPs"
            },
            {
                "model": "GPT-3 2.7B",
                "training_compute_(flop)": 4.77e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 300000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2020-06-22T00:00:00",
                "link": "https://arxiv.org/abs/2005.14165",
                "reference": "Language Models are Few-Shot Learners",
                "organization": "OpenAI",
                "parameters": 2650000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "GPT-3 2.7B - 4.77e+21 FLOPs"
            },
            {
                "model": "GPT-3 13B",
                "training_compute_(flop)": 2.31e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 300000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2020-06-22T00:00:00",
                "link": "https://arxiv.org/abs/2005.14165",
                "reference": "Language Models are Few-Shot Learners",
                "organization": "OpenAI",
                "parameters": 12850000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "GPT-3 13B - 2.31e+22 FLOPs"
            },
            {
                "model": "iGPT-XL",
                "training_compute_(flop)": 3.3e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 4718592000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 108390.73774775192,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2020-06-17T00:00:00",
                "link": "https://openai.com/research/image-gpt",
                "reference": "Generative Pretraining from Pixels",
                "organization": "OpenAI",
                "parameters": 6801000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "iGPT-XL - 3.30e+22 FLOPs"
            },
            {
                "model": "iGPT-L",
                "training_compute_(flop)": 8.91e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2654208000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 30093.444683107013,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2020-06-17T00:00:00",
                "link": "https://openai.com/blog/image-gpt/",
                "reference": "Generative Pretraining from Pixels",
                "organization": "OpenAI",
                "parameters": 1362000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "iGPT-L - 8.91e+21 FLOPs"
            },
            {
                "model": "6-Layer-Tensor-Transformer+AdaHessian",
                "training_compute_(flop)": 1.58e+18,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Government",
                "publication_date": "2020-06-01T00:00:00",
                "link": "https://arxiv.org/abs/2006.00719",
                "reference": "ADAHESSIAN: An Adaptive Second Order Optimizer for Machine Learning",
                "organization": "NERSC, Lawrence Berkeley National Laboratory,University of California (UC) Berkeley",
                "parameters": 85500000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Other",
                "access_group": "Closed",
                "model_and_compute": "6-Layer-Tensor-Transformer+AdaHessian - 1.58e+18 FLOPs"
            },
            {
                "model": "GPT-3 175B (davinci)",
                "training_compute_(flop)": 3.14e+23,
                "training_power_draw_(w)": 5100759.605963441,
                "training_dataset_size_(gradients)": 238000000000.0,
                "training_time_(hours)": 355.2,
                "training_compute_cost_(2023_usd)": 2277557.456144878,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2020-05-28T00:00:00",
                "link": "https://arxiv.org/abs/2005.14165",
                "reference": "Language Models are Few-Shot Learners",
                "organization": "OpenAI",
                "parameters": 174600000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "API access",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "GPT-3 175B (davinci) - 3.14e+23 FLOPs"
            },
            {
                "model": "GPT3-6.7B (rerun of original)",
                "training_compute_(flop)": 1.2e+22,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 300000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2020-05-28T00:00:00",
                "link": "https://arxiv.org/abs/2203.03466",
                "reference": "Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer",
                "organization": "Microsoft,OpenAI",
                "parameters": 6700000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "GPT3-6.7B (rerun of original) - 1.20e+22 FLOPs"
            },
            {
                "model": "DETR",
                "training_compute_(flop)": 4e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 826000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 959.9097627206426,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2020-05-26T00:00:00",
                "link": "https://arxiv.org/abs/2005.12872",
                "reference": "End-to-End Object Detection with Transformers",
                "organization": "Facebook",
                "parameters": 60000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "DETR - 4.00e+20 FLOPs"
            },
            {
                "model": "rTop-k(distributed setting)",
                "training_compute_(flop)": 1.4352996e+16,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 912344.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2020-05-21T00:00:00",
                "link": "https://arxiv.org/abs/2005.10761",
                "reference": "rTop-k: A Statistical Estimation Approach to Distributed SGD",
                "organization": "Stanford University",
                "parameters": 69000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "rTop-k(distributed setting) - 1.44e+16 FLOPs"
            },
            {
                "model": "ONLSTM-SYD",
                "training_compute_(flop)": 1.368516e+17,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 912344.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2020-05-12T00:00:00",
                "link": "https://arxiv.org/abs/2005.05864",
                "reference": "Exploiting Syntactic Structure for Better Language Modeling: A Syntactic Distance Approach",
                "organization": "Westlake University,Institute for Advanced Study,McGill University,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms),CIFAR AI Research,University of Montreal / Universit\u00e9 de Montr\u00e9al",
                "parameters": 25000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "ONLSTM-SYD - 1.37e+17 FLOPs"
            },
            {
                "model": "NAS+ESS (156M)",
                "training_compute_(flop)": 2.89e+18,
                "training_power_draw_(w)": 280.3988579029138,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2020-05-06T00:00:00",
                "link": "https://arxiv.org/abs/2005.02593",
                "reference": "Learning Architectures from an Extended Search Space for Language Modeling",
                "organization": "Northeastern University (China),Chinese Academy of Sciences,NiuTrans Research,Kingsoft",
                "parameters": 156000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Unreleased",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "NAS+ESS (156M) - 2.89e+18 FLOPs"
            },
            {
                "model": "UnifiedQA",
                "training_compute_(flop)": 1.65e+19,
                "training_power_draw_(w)": 3593.01452898213,
                "training_dataset_size_(gradients)": 2619750.0,
                "training_time_(hours)": 36.0,
                "training_compute_cost_(2023_usd)": 59.122086907352475,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2020-05-02T00:00:00",
                "link": "https://arxiv.org/abs/2005.00700v3",
                "reference": "UnifiedQA: Crossing Format Boundaries With a Single QA System",
                "organization": "Allen Institute for AI,University of Washington",
                "parameters": 11000000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "UnifiedQA - 1.65e+19 FLOPs"
            },
            {
                "model": "Segatron XL large, M=384",
                "training_compute_(flop)": 2.6527334e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2020-04-30T00:00:00",
                "link": "https://arxiv.org/abs/2004.14996",
                "reference": "Segatron: Segment-Aware Transformer for Language Modeling and Understanding",
                "organization": "University of Waterloo,Peking University,RSVP.ai",
                "parameters": 256999999.99999997,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "Segatron XL large, M=384 - 2.65e+19 FLOPs"
            },
            {
                "model": "Segatron XL base, M=384",
                "training_compute_(flop)": 2.653e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2020-04-30T00:00:00",
                "link": "https://arxiv.org/abs/2004.14996",
                "reference": "Segatron: Segment-Aware Transformer for Language Modeling and Understanding",
                "organization": "University of Waterloo,RSVP.ai,Peking University",
                "parameters": 257000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "Segatron XL base, M=384 - 2.65e+19 FLOPs"
            },
            {
                "model": "Once for All",
                "training_compute_(flop)": 6.237e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1280000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 1753.9255676777682,
                "domain_group": "Vision",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2020-04-29T00:00:00",
                "link": "https://arxiv.org/abs/1908.09791",
                "reference": "Once for all: Train one network and specialize it for efficient deployment.",
                "organization": "MIT-IBM Watson AI Lab,Massachusetts Institute of Technology (MIT),IBM",
                "parameters": 7700000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "Once for All - 6.24e+20 FLOPs"
            },
            {
                "model": "Cube-Space AutoEncoder",
                "training_compute_(flop)": 1.0660896e+17,
                "training_power_draw_(w)": 336.5460747498431,
                "training_dataset_size_(gradients)": 4243600000.0,
                "training_time_(hours)": 24.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2020-04-27T00:00:00",
                "link": "https://arxiv.org/abs/2004.12850",
                "reference": "Learning Neural-Symbolic Descriptive Planning Models via Cube-Space Priors: The Voyage Home (to STRIPS)",
                "organization": "MIT-IBM Watson AI Lab",
                "parameters": null,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Multimodal",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "Cube-Space AutoEncoder - 1.07e+17 FLOPs"
            },
            {
                "model": "DiffStk-MRNN",
                "training_compute_(flop)": 276440230000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 912344.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2020-04-04T00:00:00",
                "link": "https://arxiv.org/abs/2004.07623",
                "reference": "Recognizing Long Grammatical Sequences Using Recurrent Networks Augmented With An External Differentiable Stack",
                "organization": "Pennsylvania State University,Rochester Institute of Technology",
                "parameters": 1010000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "DiffStk-MRNN - 2.76e+14 FLOPs"
            },
            {
                "model": "AraBERT",
                "training_compute_(flop)": 3.1765134e+19,
                "training_power_draw_(w)": 4576.289412806967,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 96.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2020-03-30T00:00:00",
                "link": "https://arxiv.org/abs/2003.00104v2",
                "reference": "AraBERT: Transformer-based Model for Arabic Language Understanding",
                "organization": "American University of Beirut",
                "parameters": 110000000.0,
                "notable_model": false,
                "country": "Lebanon",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "AraBERT - 3.18e+19 FLOPs"
            },
            {
                "model": "AraBERT LArge v2",
                "training_compute_(flop)": 1.5399499e+21,
                "training_power_draw_(w)": 57530.49547528759,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 168.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2020-03-30T00:00:00",
                "link": "https://huggingface.co/aubmindlab/bert-large-arabertv2",
                "reference": "AraBERT v1 & v2 : Pre-training BERT for Arabic Language Understanding",
                "organization": "American University of Beirut",
                "parameters": 371000000.0,
                "notable_model": false,
                "country": "Lebanon",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "AraBERT LArge v2 - 1.54e+21 FLOPs"
            },
            {
                "model": "MetNet",
                "training_compute_(flop)": 9.510912e+18,
                "training_power_draw_(w)": 115076.36599329064,
                "training_dataset_size_(gradients)": 7045120000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Earth science",
                "organization_categorization": "Industry",
                "publication_date": "2020-03-24T00:00:00",
                "link": "https://arxiv.org/abs/2003.12140",
                "reference": "MetNet: A Neural Weather Model for Precipitation Forecasting",
                "organization": "Google",
                "parameters": 225000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "MetNet - 9.51e+18 FLOPs"
            },
            {
                "model": "ELECTRA",
                "training_compute_(flop)": 3.1e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 33000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2020-03-23T00:00:00",
                "link": "https://arxiv.org/abs/2003.10555v1",
                "reference": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators",
                "organization": "Stanford University,Google,Google Brain",
                "parameters": 335000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "ELECTRA - 3.10e+21 FLOPs"
            },
            {
                "model": "Tensor-Transformer(1core)+PN (WT103)",
                "training_compute_(flop)": 1.58e+18,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2020-03-17T00:00:00",
                "link": "https://arxiv.org/abs/2003.07845",
                "reference": "PowerNorm: Rethinking Batch Normalization in Transformers",
                "organization": "University of California (UC) Berkeley",
                "parameters": 85300000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "Tensor-Transformer(1core)+PN (WT103) - 1.58e+18 FLOPs"
            },
            {
                "model": "WDC20 / DLWP",
                "training_compute_(flop)": 2.4362208e+18,
                "training_power_draw_(w)": 280.72374990639975,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 72.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Earth science",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2020-03-15T00:00:00",
                "link": "https://arxiv.org/abs/2003.11927",
                "reference": "Improving data-driven global weather prediction using deep convolutional neural networks on a cubed sphere",
                "organization": "University of Washington,Microsoft Research",
                "parameters": 672080.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "WDC20 / DLWP - 2.44e+18 FLOPs"
            },
            {
                "model": "ProGen",
                "training_compute_(flop)": 3.7e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2020-03-13T00:00:00",
                "link": "https://www.biorxiv.org/content/10.1101/2020.03.07.982272v2",
                "reference": "ProGen: Language Modeling for Protein Generation",
                "organization": "Salesforce Research,Stanford University",
                "parameters": 1200000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "ProGen - 3.70e+20 FLOPs"
            },
            {
                "model": "TransformerXL + spectrum control",
                "training_compute_(flop)": 2.6289761e+19,
                "training_power_draw_(w)": 2452.6211427753756,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": 219.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2020-03-11T00:00:00",
                "link": "https://openreview.net/forum?id=ByxY8CNtvr",
                "reference": "Improving Neural Language Generation with Spectrum Control",
                "organization": "University of California Los Angeles (UCLA),JD.com",
                "parameters": 151000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "TransformerXL + spectrum control - 2.63e+19 FLOPs"
            },
            {
                "model": "LSTM-3-layer+Gadam",
                "training_compute_(flop)": 2.6275507e+16,
                "training_power_draw_(w)": 280.8050317098096,
                "training_dataset_size_(gradients)": 912344.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2020-03-02T00:00:00",
                "link": "https://arxiv.org/abs/2003.01247",
                "reference": "Iterative Averaging in the Quest for Best Test Error",
                "organization": "University of Oxford,University of Bristol,University of Cambridge",
                "parameters": 24000000.0,
                "notable_model": false,
                "country": "United Kingdom",
                "model_accessibility": "Unreleased",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "LSTM-3-layer+Gadam - 2.63e+16 FLOPs"
            },
            {
                "model": "Feedback Transformer",
                "training_compute_(flop)": 7.690547e+18,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": 84.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2020-02-21T00:00:00",
                "link": "https://arxiv.org/abs/2002.09402",
                "reference": "Addressing Some Limitations of Transformers with Feedback Memory",
                "organization": "LORIA,University of Lorraine,Facebook AI Research",
                "parameters": 126000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "Feedback Transformer - 7.69e+18 FLOPs"
            },
            {
                "model": "Turing-NLG",
                "training_compute_(flop)": 1.57e+22,
                "training_power_draw_(w)": 130885.13499433055,
                "training_dataset_size_(gradients)": 46400000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 51659.713290894986,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2020-02-13T00:00:00",
                "link": "https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/",
                "reference": "Turing-NLG: A 17-billion-parameter language model by Microsoft",
                "organization": "Microsoft",
                "parameters": 17000000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Turing-NLG - 1.57e+22 FLOPs"
            },
            {
                "model": "ALBERT-xxlarge",
                "training_compute_(flop)": 2.39e+21,
                "training_power_draw_(w)": 230378.35820081632,
                "training_dataset_size_(gradients)": 3300000000.0,
                "training_time_(hours)": 32.0,
                "training_compute_cost_(2023_usd)": 4439.921298510215,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2020-02-09T00:00:00",
                "link": "https://arxiv.org/abs/1909.11942",
                "reference": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.",
                "organization": "Toyota Technological Institute at Chicago,Google",
                "parameters": 235000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "ALBERT-xxlarge - 2.39e+21 FLOPs"
            },
            {
                "model": "TaLK Convolution",
                "training_compute_(flop)": 2.6990346e+19,
                "training_power_draw_(w)": 4090.615920439178,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2020-02-08T00:00:00",
                "link": "https://arxiv.org/abs/2002.03184",
                "reference": "Time-aware Large Kernel Convolutions",
                "organization": "Carleton University",
                "parameters": 240000000.0,
                "notable_model": true,
                "country": "Canada",
                "model_accessibility": "Unreleased",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "Canada",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "TaLK Convolution - 2.70e+19 FLOPs"
            },
            {
                "model": "Meena",
                "training_compute_(flop)": 1.12e+23,
                "training_power_draw_(w)": 460879.8620037727,
                "training_dataset_size_(gradients)": 53333333333.333336,
                "training_time_(hours)": 720.0,
                "training_compute_cost_(2023_usd)": 206226.21339944057,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2020-01-28T00:00:00",
                "link": "https://arxiv.org/abs/2001.09977",
                "reference": "Towards a Human-like Open-Domain Chatbot",
                "organization": "Google Brain",
                "parameters": 2600000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Meena - 1.12e+23 FLOPs"
            },
            {
                "model": "ContextNet + Noisy Student",
                "training_compute_(flop)": 8.16e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 1440.0,
                "training_compute_cost_(2023_usd)": 14226.054462994534,
                "domain_group": "Audio",
                "organization_categorization": "Industry",
                "publication_date": "2020-01-19T00:00:00",
                "link": "https://arxiv.org/abs/2005.09629v2",
                "reference": "Improved Noisy Student Training for Automatic Speech Recognition",
                "organization": "Google",
                "parameters": null,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "ContextNet + Noisy Student - 8.16e+21 FLOPs"
            },
            {
                "model": "AlphaFold",
                "training_compute_(flop)": 1e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 6622252080.0,
                "training_time_(hours)": 120.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Industry",
                "publication_date": "2020-01-15T00:00:00",
                "link": "https://www.nature.com/articles/s41586-019-1923-7",
                "reference": "Improved protein structure prediction using potentials from deep learning",
                "organization": "DeepMind",
                "parameters": 16340840.0,
                "notable_model": true,
                "country": "United Kingdom",
                "model_accessibility": "Unreleased",
                "year": 2020,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Biology",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "AlphaFold - 1.00e+20 FLOPs"
            },
            {
                "model": "DD-PPO",
                "training_compute_(flop)": 7.8e+20,
                "training_power_draw_(w)": 39314.53850266432,
                "training_dataset_size_(gradients)": 2500000000.0,
                "training_time_(hours)": 66.0,
                "training_compute_cost_(2023_usd)": 1926.8992900057376,
                "domain_group": "Robotics",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2019-12-19T00:00:00",
                "link": "https://openreview.net/forum?id=H1gX8C4YPr\nhttps://arxiv.org/abs/1911.00357",
                "reference": "DD-PPO: Learning Near-Perfect PointGoal Navigators from 2.5 Billion Frames",
                "organization": "Georgia Institute of Technology,Facebook AI Research,Oregon State University,Simon Fraser University",
                "parameters": null,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "DD-PPO - 7.80e+20 FLOPs"
            },
            {
                "model": "SeqVec",
                "training_compute_(flop)": 4.1e+19,
                "training_power_draw_(w)": 2559.654268344264,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 508.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2019-12-17T00:00:00",
                "link": "https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-3220-8",
                "reference": "Modeling aspects of the language of life through transfer-learning protein sequences",
                "organization": "Technical University of Munich",
                "parameters": 93000000.0,
                "notable_model": false,
                "country": "Germany",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "Germany",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "SeqVec - 4.10e+19 FLOPs"
            },
            {
                "model": "OpenAI Five",
                "training_compute_(flop)": 6.7e+22,
                "training_power_draw_(w)": 786395.8382790724,
                "training_dataset_size_(gradients)": 454164480000.0,
                "training_time_(hours)": 7104.0,
                "training_compute_cost_(2023_usd)": 4328311.470477086,
                "domain_group": "Games",
                "organization_categorization": "Industry",
                "publication_date": "2019-12-13T00:00:00",
                "link": "https://arxiv.org/abs/1912.06680",
                "reference": "Dota 2 with Large Scale Deep Reinforcement Learning",
                "organization": "OpenAI",
                "parameters": 159000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "OpenAI Five - 6.70e+22 FLOPs"
            },
            {
                "model": "OpenAI Five Rerun",
                "training_compute_(flop)": 1.3e+22,
                "training_power_draw_(w)": 262131.94609302413,
                "training_dataset_size_(gradients)": 53084160000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 321105.9055,
                "domain_group": "Games",
                "organization_categorization": "Industry",
                "publication_date": "2019-12-13T00:00:00",
                "link": "https://cdn.openai.com/dota-2.pdf",
                "reference": "Dota 2 with Large Scale Deep Reinforcement Learning",
                "organization": "OpenAI",
                "parameters": 159000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "OpenAI Five Rerun - 1.30e+22 FLOPs"
            },
            {
                "model": "MMLSTM (WT-103)",
                "training_compute_(flop)": 2.3175e+18,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2019-12-05T00:00:00",
                "link": "http://repository.uwl.ac.uk/id/eprint/6490/1/Loo_etal_IEEE_TNNLS_2019_Major-minor_long_short-term_memory_for_word-level_language_model.pdf",
                "reference": "Major\u2013Minor Long Short-Term Memory for Word-Level Language Model",
                "organization": "Beijing University of Posts and Telecommunications,University of West London",
                "parameters": 75000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "MMLSTM (WT-103) - 2.32e+18 FLOPs"
            },
            {
                "model": "MMLSTM (WT-2)",
                "training_compute_(flop)": 1.938e+17,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2019-12-05T00:00:00",
                "link": "http://repository.uwl.ac.uk/id/eprint/6490/1/Loo_etal_IEEE_TNNLS_2019_Major-minor_long_short-term_memory_for_word-level_language_model.pdf",
                "reference": "Major\u2013Minor Long Short-Term Memory for Word-Level Language Model",
                "organization": "Beijing University of Posts and Telecommunications,University of West London",
                "parameters": 32300000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "MMLSTM (WT-2) - 1.94e+17 FLOPs"
            },
            {
                "model": "MMLSTM (PTB)",
                "training_compute_(flop)": 5.8298782e+16,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2019-12-05T00:00:00",
                "link": "http://repository.uwl.ac.uk/id/eprint/6490/1/Loo_etal_IEEE_TNNLS_2019_Major-minor_long_short-term_memory_for_word-level_language_model.pdf",
                "reference": "Major\u2013Minor Long Short-Term Memory for Word-Level Language Model",
                "organization": "Beijing University of Posts and Telecommunications,University of West London",
                "parameters": 21300000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "MMLSTM (PTB) - 5.83e+16 FLOPs"
            },
            {
                "model": "bRSM + cache",
                "training_compute_(flop)": 275400000000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 912344.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2019-12-02T00:00:00",
                "link": "https://arxiv.org/abs/1912.01116",
                "reference": "Long Distance Relationships without Time Travel: Boosting the Performance of a Sparse Predictive Autoencoder in Sequence Modeling",
                "organization": "Numenta,Incubator 491",
                "parameters": 2550000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "bRSM + cache - 2.75e+14 FLOPs"
            },
            {
                "model": "Transformer-XL DeFINE (141M)",
                "training_compute_(flop)": 1.74276e+18,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2019-11-27T00:00:00",
                "link": "https://arxiv.org/abs/1911.12385",
                "reference": "DeFINE: DEep Factorized INput Token Embeddings for Neural Sequence Modeling",
                "organization": "University of Washington,Allen Institute for AI",
                "parameters": 141000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Transformer-XL DeFINE (141M) - 1.74e+18 FLOPs"
            },
            {
                "model": "FastSpeech",
                "training_compute_(flop)": 7.1712e+18,
                "training_power_draw_(w)": 2458.746030907577,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Audio",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2019-11-20T00:00:00",
                "link": "https://arxiv.org/abs/1905.09263",
                "reference": "FastSpeech: Fast, Robust and Controllable Text to Speech",
                "organization": "Zhejiang University (ZJU),Microsoft Research",
                "parameters": 30100000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "FastSpeech - 7.17e+18 FLOPs"
            },
            {
                "model": "MuZero",
                "training_compute_(flop)": 4.8e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 12288000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Games",
                "organization_categorization": "Industry",
                "publication_date": "2019-11-19T00:00:00",
                "link": "https://arxiv.org/abs/1911.08265v2",
                "reference": "Mastering Atari Go Chess and Shogi by Planning with a Learned Model",
                "organization": "DeepMind",
                "parameters": 36864000.0,
                "notable_model": true,
                "country": "United Kingdom",
                "model_accessibility": "Unreleased",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "MuZero - 4.80e+19 FLOPs"
            },
            {
                "model": "Long-range sequence Compressive Transformers",
                "training_compute_(flop)": 1.0202112e+20,
                "training_power_draw_(w)": 28853.7843015362,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 12.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2019-11-13T00:00:00",
                "link": "https://arxiv.org/abs/1911.05507",
                "reference": "Compressive Transformers for Long-Range Sequence Modelling",
                "organization": "DeepMind",
                "parameters": null,
                "notable_model": false,
                "country": "United Kingdom",
                "model_accessibility": "Unreleased",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Long-range sequence Compressive Transformers - 1.02e+20 FLOPs"
            },
            {
                "model": "Noisy Student (L2)",
                "training_compute_(flop)": 2.612e+22,
                "training_power_draw_(w)": 461681.1110628138,
                "training_dataset_size_(gradients)": 81000000.0,
                "training_time_(hours)": 144.0,
                "training_compute_cost_(2023_usd)": 43831.851062553185,
                "domain_group": "Vision",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2019-11-11T00:00:00",
                "link": "https://arxiv.org/abs/1911.04252v4",
                "reference": "Self-training with Noisy Student improves ImageNet classification",
                "organization": "Carnegie Mellon University (CMU),Google",
                "parameters": 480000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "Noisy Student (L2) - 2.61e+22 FLOPs"
            },
            {
                "model": "Sandwich Transformer",
                "training_compute_(flop)": 2.3504093e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 700000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2019-11-10T00:00:00",
                "link": "https://arxiv.org/abs/1911.03864",
                "reference": "Improving Transformer Models by Reordering their Sublayers",
                "organization": "Allen Institute for AI,Facebook AI Research",
                "parameters": 209000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Sandwich Transformer - 2.35e+19 FLOPs"
            },
            {
                "model": "CamemBERT",
                "training_compute_(flop)": 8.3e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 28615771779.58404,
                "training_time_(hours)": 24.0,
                "training_compute_cost_(2023_usd)": 2319.5419478533995,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2019-11-10T00:00:00",
                "link": "https://arxiv.org/abs/1911.03894",
                "reference": "CamemBERT: a Tasty French Language Model",
                "organization": "Facebook,INRIA,Sorbonne University",
                "parameters": 335000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "CamemBERT - 8.30e+20 FLOPs"
            },
            {
                "model": "Self-Attention and Convolutional Layers",
                "training_compute_(flop)": 6.75e+17,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2019-11-08T00:00:00",
                "link": "https://arxiv.org/abs/1911.03584",
                "reference": "On the Relationship between Self-Attention and Convolutional Layers",
                "organization": "Ecole Polytechnique F\u00b4ed\u00b4erale de Lausanne (EPFL)",
                "parameters": 29500000.0,
                "notable_model": false,
                "country": "Switzerland",
                "model_accessibility": "Unreleased",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Self-Attention and Convolutional Layers - 6.75e+17 FLOPs"
            },
            {
                "model": "XLM-RoBERTa",
                "training_compute_(flop)": 2.076e+22,
                "training_power_draw_(w)": 256204.94677329363,
                "training_dataset_size_(gradients)": 167000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 83813.51482274817,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2019-11-05T00:00:00",
                "link": "https://arxiv.org/abs/1911.02116",
                "reference": "Unsupervised Cross-lingual Representation Learning at Scale",
                "organization": "Facebook AI",
                "parameters": 550000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "XLM-RoBERTa - 2.08e+22 FLOPs"
            },
            {
                "model": "Base LM + kNN LM + Continuous Cache",
                "training_compute_(flop)": 3.05292e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2019-11-01T00:00:00",
                "link": "https://arxiv.org/abs/1911.00172",
                "reference": "Generalization through Memorization: Nearest Neighbor Language Models",
                "organization": "Stanford University,Facebook AI Research",
                "parameters": 247000000.00000003,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "Base LM + kNN LM + Continuous Cache - 3.05e+19 FLOPs"
            },
            {
                "model": "AlphaStar",
                "training_compute_(flop)": 1.0773400001e+23,
                "training_power_draw_(w)": 173176.688897332,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 1056.0,
                "training_compute_cost_(2023_usd)": 125764.90293671023,
                "domain_group": "Games",
                "organization_categorization": "Industry",
                "publication_date": "2019-10-30T00:00:00",
                "link": "https://www.deepmind.com/blog/alphastar-grandmaster-level-in-starcraft-ii-using-multi-agent-reinforcement-learning",
                "reference": "Grandmaster level in StarCraft II using multi-agent reinforcement learning",
                "organization": "DeepMind",
                "parameters": 139000000.0,
                "notable_model": true,
                "country": "United Kingdom",
                "model_accessibility": "Unreleased",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "AlphaStar - 1.08e+23 FLOPs"
            },
            {
                "model": "T5-3B",
                "training_compute_(flop)": 9.0000000001e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 5100000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 16767.21230361689,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2019-10-23T00:00:00",
                "link": "https://arxiv.org/abs/1910.10683",
                "reference": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
                "organization": "Google",
                "parameters": 2800000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "T5-3B - 9.00e+21 FLOPs"
            },
            {
                "model": "T5-11B",
                "training_compute_(flop)": 3.3e+22,
                "training_power_draw_(w)": 230938.24900457988,
                "training_dataset_size_(gradients)": 34000000000.0,
                "training_time_(hours)": 481.9,
                "training_compute_cost_(2023_usd)": 75489.81779029286,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2019-10-23T00:00:00",
                "link": "https://arxiv.org/abs/1910.10683",
                "reference": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
                "organization": "Google",
                "parameters": 11000000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "T5-11B - 3.30e+22 FLOPs"
            },
            {
                "model": "LSTM(large)+Sememe+cell",
                "training_compute_(flop)": 2.304e+16,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2019-10-20T00:00:00",
                "link": "https://arxiv.org/abs/1910.08910",
                "reference": "Improving Sequence Modeling Ability of Recurrent Neural Networks via Sememes",
                "organization": "Tsinghua University,Beijing University of Posts and Telecommunications,Huawei Noah's Ark Lab",
                "parameters": 48000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Unreleased",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "LSTM(large)+Sememe+cell - 2.30e+16 FLOPs"
            },
            {
                "model": "Rubik's cube ADR robot",
                "training_compute_(flop)": 8.54e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Robotics",
                "organization_categorization": "Industry",
                "publication_date": "2019-10-15T00:00:00",
                "link": "https://arxiv.org/abs/1910.07113",
                "reference": "Solving Rubik\u2019s Cube with a Robot Hand",
                "organization": "OpenAI",
                "parameters": 27769565.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Rubik's cube ADR robot - 8.54e+20 FLOPs"
            },
            {
                "model": "AlphaX-1",
                "training_compute_(flop)": 8.89344e+17,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 61280000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2019-10-02T00:00:00",
                "link": "https://arxiv.org/abs/1903.11059",
                "reference": "AlphaX: eXploring Neural Architectures with Deep Neural Networks and Monte Carlo Tree Search",
                "organization": "Facebook AI Research,Brown University",
                "parameters": 5400000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "AlphaX-1 - 8.89e+17 FLOPs"
            },
            {
                "model": "DistilBERT",
                "training_compute_(flop)": 1.24416e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 495000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2019-10-02T00:00:00",
                "link": "https://arxiv.org/abs/1910.01108",
                "reference": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
                "organization": "Hugging Face",
                "parameters": 66000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "DistilBERT - 1.24e+19 FLOPs"
            },
            {
                "model": "Megatron-BERT",
                "training_compute_(flop)": 2.2e+22,
                "training_power_draw_(w)": 262640.30207328824,
                "training_dataset_size_(gradients)": 6960000000.0,
                "training_time_(hours)": 374.0,
                "training_compute_cost_(2023_usd)": 185064.40813324932,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2019-09-17T00:00:00",
                "link": "https://arxiv.org/abs/1909.08053",
                "reference": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism",
                "organization": "NVIDIA",
                "parameters": 3900000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Megatron-BERT - 2.20e+22 FLOPs"
            },
            {
                "model": "Megatron-LM (8.3B)",
                "training_compute_(flop)": 9.1e+21,
                "training_power_draw_(w)": 262640.30207328824,
                "training_dataset_size_(gradients)": 46400000000.0,
                "training_time_(hours)": 327.0,
                "training_compute_cost_(2023_usd)": 117605.50304358534,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2019-09-17T00:00:00",
                "link": "https://arxiv.org/abs/1909.08053",
                "reference": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism",
                "organization": "NVIDIA",
                "parameters": 8300000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Megatron-LM (8.3B) - 9.10e+21 FLOPs"
            },
            {
                "model": "Hide and Seek",
                "training_compute_(flop)": 1.15e+18,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 120000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Games",
                "organization_categorization": "Industry",
                "publication_date": "2019-09-17T00:00:00",
                "link": "https://arxiv.org/abs/1909.07528",
                "reference": "Emergent Tool Use From Multi-Agent Autocurricula",
                "organization": "OpenAI",
                "parameters": 1600000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Hide and Seek - 1.15e+18 FLOPs"
            },
            {
                "model": "Megatron-LM (2.5B)",
                "training_compute_(flop)": 2.359e+21,
                "training_power_draw_(w)": 78792.09062198647,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2019-09-17T00:00:00",
                "link": "https://arxiv.org/abs/1909.08053",
                "reference": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism",
                "organization": "NVIDIA",
                "parameters": 2500000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Megatron-LM (2.5B) - 2.36e+21 FLOPs"
            },
            {
                "model": "Megatron-LM (355M)",
                "training_compute_(flop)": 3.35e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 46400000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2019-09-17T00:00:00",
                "link": "https://arxiv.org/abs/1909.08053",
                "reference": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism",
                "organization": "NVIDIA",
                "parameters": 355000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Megatron-LM (355M) - 3.35e+20 FLOPs"
            },
            {
                "model": "Megatron-LM (1.2B)",
                "training_compute_(flop)": 1.13e+22,
                "training_power_draw_(w)": 338.2215428484996,
                "training_dataset_size_(gradients)": 157000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2019-09-17T00:00:00",
                "link": "https://arxiv.org/abs/1909.08053",
                "reference": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism",
                "organization": "NVIDIA",
                "parameters": 1200000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Megatron-LM (1.2B) - 1.13e+22 FLOPs"
            },
            {
                "model": "ResNet-152 + ObjectNet",
                "training_compute_(flop)": 1.94e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 50000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2019-09-06T00:00:00",
                "link": "https://papers.nips.cc/paper/2019/file/97af07a14cacba681feacf3012730892-Paper.pdf",
                "reference": "Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models",
                "organization": "Massachusetts Institute of Technology (MIT)",
                "parameters": 38000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "ResNet-152 + ObjectNet - 1.94e+19 FLOPs"
            },
            {
                "model": "UDSMProt",
                "training_compute_(flop)": 6.37e+17,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 149700000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": null,
                "publication_date": "2019-09-04T00:00:00",
                "link": "https://www.biorxiv.org/content/10.1101/704874v2.full.pdf",
                "reference": "UDSMProt: Universal Deep Sequence Models for Protein Classification",
                "organization": "Fraunhofer Heinrich Hertz Institute",
                "parameters": 28303800.0,
                "notable_model": true,
                "country": "Germany",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "Germany",
                "domain_top4": "Biology",
                "org_top5": "Other",
                "access_group": "Open",
                "model_and_compute": "UDSMProt - 6.37e+17 FLOPs"
            },
            {
                "model": "DEQ-Transformer (Medium, Adaptive Embedding)",
                "training_compute_(flop)": 8.1576e+17,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2019-09-03T00:00:00",
                "link": "https://arxiv.org/abs/1909.01377",
                "reference": "Deep Equilibrium Models",
                "organization": "Carnegie Mellon University (CMU),Intel Labs",
                "parameters": 110000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "DEQ-Transformer (Medium, Adaptive Embedding) - 8.16e+17 FLOPs"
            },
            {
                "model": "trRosetta",
                "training_compute_(flop)": 3.8047968e+19,
                "training_power_draw_(w)": 315.8562689911153,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 1080.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2019-08-22T00:00:00",
                "link": "https://www.pnas.org/doi/10.1073/pnas.1914677117",
                "reference": "Improved protein structure prediction using predictedinterresidue orientations",
                "organization": "Nankai University,University of Washington,Tianjin University,Harvard University",
                "parameters": null,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "trRosetta - 3.80e+19 FLOPs"
            },
            {
                "model": "R-Transformer",
                "training_compute_(flop)": 8649021100000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 912344.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2019-07-12T00:00:00",
                "link": "https://arxiv.org/abs/1907.05572",
                "reference": "R-Transformer: Recurrent Neural Network Enhanced Transformer",
                "organization": "Michigan State University,TAL Education Group (Xueersi)",
                "parameters": 15800000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "R-Transformer - 8.65e+15 FLOPs"
            },
            {
                "model": "Pluribus",
                "training_compute_(flop)": 6.6e+16,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Games",
                "organization_categorization": "Industry",
                "publication_date": "2019-07-11T00:00:00",
                "link": "https://www.science.org/cms/asset/910714a7-ee2a-486e-9970-42fb893b08d9/pap.pdf",
                "reference": "Superhuman AI for multiplayer poker",
                "organization": "Facebook AI Research",
                "parameters": null,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Pluribus - 6.60e+16 FLOPs"
            },
            {
                "model": "All-attention network + adaptive span",
                "training_compute_(flop)": 1.1657733e+19,
                "training_power_draw_(w)": 39463.657300276864,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": 24.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2019-07-02T00:00:00",
                "link": "https://arxiv.org/abs/1907.01470",
                "reference": "Augmenting Self-attention with Persistent Memory",
                "organization": "Facebook AI Research",
                "parameters": 133000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "All-attention network + adaptive span - 1.17e+19 FLOPs"
            },
            {
                "model": "RoBERTa Large",
                "training_compute_(flop)": 8.5067e+21,
                "training_power_draw_(w)": 526193.8152119832,
                "training_dataset_size_(gradients)": 42666666666.0,
                "training_time_(hours)": 120.0,
                "training_compute_cost_(2023_usd)": 91897.1433944948,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2019-07-01T00:00:00",
                "link": "https://arxiv.org/abs/1907.11692",
                "reference": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
                "organization": "Facebook,University of Washington",
                "parameters": 355000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "RoBERTa Large - 8.51e+21 FLOPs"
            },
            {
                "model": "RoBERTa Base",
                "training_compute_(flop)": 1.536e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2019-07-01T00:00:00",
                "link": "https://arxiv.org/abs/1907.11692",
                "reference": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
                "organization": "Facebook,University of Washington",
                "parameters": 125000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "RoBERTa Base - 1.54e+21 FLOPs"
            },
            {
                "model": "Tensorized Transformer (257M)",
                "training_compute_(flop)": 4.76e+18,
                "training_power_draw_(w)": 1027.8825149290835,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2019-06-24T00:00:00",
                "link": "https://arxiv.org/abs/1906.09777",
                "reference": "A Tensorized Transformer for Language Modeling",
                "organization": "Tianjin University,Microsoft Research Asia,Beijing Institute of Technology",
                "parameters": 256999999.99999997,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Unreleased",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "Tensorized Transformer (257M) - 4.76e+18 FLOPs"
            },
            {
                "model": "Tensorized Transformer (OBW)",
                "training_compute_(flop)": 2.388e+19,
                "training_power_draw_(w)": 1027.8825149290835,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2019-06-24T00:00:00",
                "link": "https://arxiv.org/abs/1906.09777",
                "reference": "A Tensorized Transformer for Language Modeling",
                "organization": "Tianjin University,Microsoft Research Asia,Beijing Institute of Technology",
                "parameters": 160000000.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Unreleased",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "Tensorized Transformer (OBW) - 2.39e+19 FLOPs"
            },
            {
                "model": "Tensorized Transformer (PTB)",
                "training_compute_(flop)": 2000000000000000.0,
                "training_power_draw_(w)": 1027.8825149290835,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2019-06-24T00:00:00",
                "link": "https://arxiv.org/abs/1906.09777",
                "reference": "A Tensorized Transformer for Language Modeling",
                "organization": "Tianjin University,Microsoft Research Asia,Beijing Institute of Technology",
                "parameters": 12000000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Unreleased",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "Tensorized Transformer (PTB) - 2.00e+15 FLOPs"
            },
            {
                "model": "Tensorized Transformer (W103)",
                "training_compute_(flop)": 1.58e+18,
                "training_power_draw_(w)": 1027.8825149290835,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2019-06-24T00:00:00",
                "link": "https://arxiv.org/abs/1906.09777",
                "reference": "A Tensorized Transformer for Language Modeling",
                "organization": "Tianjin University,Microsoft Research Asia,Beijing Institute of Technology",
                "parameters": 85300000.0,
                "notable_model": false,
                "country": "China",
                "model_accessibility": "Unreleased",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "Tensorized Transformer (W103) - 1.58e+18 FLOPs"
            },
            {
                "model": "TAPE Transformer",
                "training_compute_(flop)": 3e+19,
                "training_power_draw_(w)": 2467.192734691173,
                "training_dataset_size_(gradients)": 5198462441.0,
                "training_time_(hours)": 168.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2019-06-19T00:00:00",
                "link": "https://arxiv.org/abs/1906.08230",
                "reference": "Evaluating Protein Transfer Learning with TAPE",
                "organization": "University of California (UC) Berkeley,Covariant,Google,Chan Zuckerberg Initiative",
                "parameters": 38000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "TAPE Transformer - 3.00e+19 FLOPs"
            },
            {
                "model": "SAGAN",
                "training_compute_(flop)": 2.2374144e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 336.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2019-06-14T00:00:00",
                "link": "https://arxiv.org/abs/1805.08318",
                "reference": "Self-Attention Generative Adversarial Networks",
                "organization": "Rutgers University,Google Research",
                "parameters": null,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "SAGAN - 2.24e+20 FLOPs"
            },
            {
                "model": "AWD-LSTM + MoS + Partial Shuffled",
                "training_compute_(flop)": 3.15e+17,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2019-06-10T00:00:00",
                "link": "https://arxiv.org/abs/1906.03805",
                "reference": "Improving Neural Language Modeling via Adversarial Training",
                "organization": "University of Texas at Austin",
                "parameters": 35000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "AWD-LSTM + MoS + Partial Shuffled - 3.15e+17 FLOPs"
            },
            {
                "model": "Transformer-XL Large + Phrase Induction",
                "training_compute_(flop)": 3.7848651e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2019-06-04T00:00:00",
                "link": "https://arxiv.org/abs/1906.01702",
                "reference": "Improving Neural Language Models by Segmenting, Attending, and Predicting the Future",
                "organization": "Massachusetts Institute of Technology (MIT),University of Illinois Urbana-Champaign (UIUC)",
                "parameters": 256999999.99999997,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Transformer-XL Large + Phrase Induction - 3.78e+20 FLOPs"
            },
            {
                "model": "XLNet",
                "training_compute_(flop)": 6.19e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 32890000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 13190.205795780732,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2019-06-01T00:00:00",
                "link": "https://arxiv.org/abs/1906.08237",
                "reference": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
                "organization": "Carnegie Mellon University (CMU),Google Brain",
                "parameters": 340000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "XLNet - 6.19e+21 FLOPs"
            },
            {
                "model": "DLRM-2020",
                "training_compute_(flop)": 4e+18,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 38571428.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Recommendation",
                "organization_categorization": "Industry",
                "publication_date": "2019-05-31T00:00:00",
                "link": "https://arxiv.org/abs/1906.00091",
                "reference": "Deep Learning Recommendation Model for Personalization and Recommendation Systems",
                "organization": "Facebook AI",
                "parameters": 100000000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "DLRM-2020 - 4.00e+18 FLOPs"
            },
            {
                "model": "Grover-Mega",
                "training_compute_(flop)": 4.6386183e+21,
                "training_power_draw_(w)": 57923.87166662893,
                "training_dataset_size_(gradients)": 32000000000.0,
                "training_time_(hours)": 336.0,
                "training_compute_cost_(2023_usd)": 15094.332848871038,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2019-05-29T00:00:00",
                "link": "https://arxiv.org/abs/1905.12616",
                "reference": "Defending Against Neural Fake News",
                "organization": "University of Washington",
                "parameters": 1500000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "Grover-Mega - 4.64e+21 FLOPs"
            },
            {
                "model": "MnasNet-A3",
                "training_compute_(flop)": 1.5e+21,
                "training_power_draw_(w)": 115847.74333325788,
                "training_dataset_size_(gradients)": 1230000.0,
                "training_time_(hours)": 108.0,
                "training_compute_cost_(2023_usd)": 9551.591619865148,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2019-05-29T00:00:00",
                "link": "https://arxiv.org/abs/1807.11626",
                "reference": "MnasNet: Platform-Aware Neural Architecture Search for Mobile",
                "organization": "Google",
                "parameters": 5200000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "MnasNet-A3 - 1.50e+21 FLOPs"
            },
            {
                "model": "MnasNet-A1 + SSDLite",
                "training_compute_(flop)": 1.5e+21,
                "training_power_draw_(w)": 115847.74333325788,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 108.0,
                "training_compute_cost_(2023_usd)": 9551.591619865148,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2019-05-29T00:00:00",
                "link": "https://arxiv.org/abs/1807.11626",
                "reference": "MnasNet: Platform-Aware Neural Architecture Search for Mobile",
                "organization": "Google",
                "parameters": 4900000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "MnasNet-A1 + SSDLite - 1.50e+21 FLOPs"
            },
            {
                "model": "AWD-LSTM-DRILL + dynamic evaluation\u2020 (WT2)",
                "training_compute_(flop)": 4.08e+17,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2000000.0,
                "training_time_(hours)": 29.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2019-05-14T00:00:00",
                "link": "https://arxiv.org/abs/1905.05513",
                "reference": "Deep Residual Output Layers for Neural Language Generation",
                "organization": "IDIAP",
                "parameters": 34000000.0,
                "notable_model": true,
                "country": "Switzerland",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "AWD-LSTM-DRILL + dynamic evaluation\u2020 (WT2) - 4.08e+17 FLOPs"
            },
            {
                "model": "MuseNet",
                "training_compute_(flop)": 2.208301056e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Audio",
                "organization_categorization": "Industry",
                "publication_date": "2019-04-25T00:00:00",
                "link": "https://openai.com/index/musenet/",
                "reference": "MuseNet",
                "organization": "OpenAI",
                "parameters": 2038431744.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "MuseNet - 2.21e+20 FLOPs"
            },
            {
                "model": "Sparse Transformer (Enwik8)",
                "training_compute_(flop)": 4.104e+18,
                "training_power_draw_(w)": 4940.652929089749,
                "training_dataset_size_(gradients)": 90000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2019-04-23T00:00:00",
                "link": "https://arxiv.org/abs/1904.10509",
                "reference": "Generating Long Sequences with Sparse Transformers",
                "organization": "OpenAI",
                "parameters": 95000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Sparse Transformer (Enwik8) - 4.10e+18 FLOPs"
            },
            {
                "model": "Sparse Transformer (ImageNet)",
                "training_compute_(flop)": 1.45152e+21,
                "training_power_draw_(w)": 39525.22343271799,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 168.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2019-04-23T00:00:00",
                "link": "https://arxiv.org/abs/1904.10509",
                "reference": "Generating Long Sequences with Sparse Transformers",
                "organization": "OpenAI",
                "parameters": 152000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Sparse Transformer (ImageNet) - 1.45e+21 FLOPs"
            },
            {
                "model": "BERT-Large-CAS (PTB+WT2+WT103)",
                "training_compute_(flop)": 1.5405e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1300000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2019-04-20T00:00:00",
                "link": "https://arxiv.org/abs/1904.09408",
                "reference": "Language Models with Transformers",
                "organization": "Amazon",
                "parameters": 395000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "BERT-Large-CAS (PTB+WT2+WT103) - 1.54e+20 FLOPs"
            },
            {
                "model": "MEGNet (molecule model)",
                "training_compute_(flop)": 4.536e+17,
                "training_power_draw_(w)": 282.85734112857654,
                "training_dataset_size_(gradients)": 117416.0,
                "training_time_(hours)": 28.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Materials science",
                "organization_categorization": "Academia",
                "publication_date": "2019-04-10T00:00:00",
                "link": "https://arxiv.org/abs/1812.05055",
                "reference": "Graph Networks as a Universal Machine Learning Framework for Molecules and Crystals",
                "organization": "University of California San Diego",
                "parameters": 8720.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "MEGNet (molecule model) - 4.54e+17 FLOPs"
            },
            {
                "model": "MEGNet (crystal formation energy model)",
                "training_compute_(flop)": 4.536e+17,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 60000.0,
                "training_time_(hours)": 28.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Materials science",
                "organization_categorization": "Academia",
                "publication_date": "2019-04-10T00:00:00",
                "link": "https://arxiv.org/abs/1812.05055",
                "reference": "Graph Networks as a Universal Machine Learning Framework for Molecules and Crystals",
                "organization": "University of California San Diego",
                "parameters": 26128.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "MEGNet (crystal formation energy model) - 4.54e+17 FLOPs"
            },
            {
                "model": "MEGNet (crystal band gap model)",
                "training_compute_(flop)": 4.536e+17,
                "training_power_draw_(w)": 282.85734112857654,
                "training_dataset_size_(gradients)": 36720.0,
                "training_time_(hours)": 28.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Materials science",
                "organization_categorization": "Academia",
                "publication_date": "2019-04-10T00:00:00",
                "link": "https://arxiv.org/abs/1812.05055",
                "reference": "Graph Networks as a Universal Machine Learning Framework for Molecules and Crystals",
                "organization": "University of California San Diego",
                "parameters": 26128.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "MEGNet (crystal band gap model) - 4.54e+17 FLOPs"
            },
            {
                "model": "MEGNet (crystal elasticity model)",
                "training_compute_(flop)": 4.536e+17,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 4664.0,
                "training_time_(hours)": 28.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Materials science",
                "organization_categorization": "Academia",
                "publication_date": "2019-04-10T00:00:00",
                "link": "https://arxiv.org/abs/1812.05055",
                "reference": "Graph Networks as a Universal Machine Learning Framework for Molecules and Crystals",
                "organization": "University of California San Diego",
                "parameters": 26128.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "MEGNet (crystal elasticity model) - 4.54e+17 FLOPs"
            },
            {
                "model": "WeNet (PTB)",
                "training_compute_(flop)": 3.530047365121324e+18,
                "training_power_draw_(w)": 339.4439274233134,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 120.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2019-04-08T00:00:00",
                "link": "https://arxiv.org/pdf/1904.03819",
                "reference": "WeNet: Weighted Networks for Recurrent Network Architecture Search",
                "organization": "Amazon",
                "parameters": 23000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "WeNet (PTB) - 3.53e+18 FLOPs"
            },
            {
                "model": "WeNet (Penn Treebank)",
                "training_compute_(flop)": 7.30000001e+17,
                "training_power_draw_(w)": 339.4439274233134,
                "training_dataset_size_(gradients)": 912344.0,
                "training_time_(hours)": 24.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2019-04-08T00:00:00",
                "link": "https://arxiv.org/abs/1904.03819",
                "reference": "WeNet: Weighted Networks for Recurrent Network Architecture Search",
                "organization": "Amazon",
                "parameters": 23000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "WeNet (Penn Treebank) - 7.30e+17 FLOPs"
            },
            {
                "model": "Cross-lingual alignment",
                "training_compute_(flop)": 2.56e+18,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2019-04-04T00:00:00",
                "link": "https://arxiv.org/abs/1902.09492",
                "reference": "Cross-lingual alignment of contextual word embeddings, with applications to zero- shot dependency parsing.",
                "organization": "Tel Aviv University,Massachusetts Institute of Technology (MIT)",
                "parameters": null,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "Cross-lingual alignment - 2.56e+18 FLOPs"
            },
            {
                "model": "FAIRSEQ Adaptive Inputs",
                "training_compute_(flop)": 3.1804274e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2019-04-01T00:00:00",
                "link": "https://arxiv.org/abs/1904.01038",
                "reference": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling",
                "organization": "Facebook AI Research,Google Brain",
                "parameters": 247000000.00000003,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "FAIRSEQ Adaptive Inputs - 3.18e+19 FLOPs"
            },
            {
                "model": "UniRep",
                "training_compute_(flop)": 2.2e+19,
                "training_power_draw_(w)": 2471.867298164383,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 588.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2019-03-26T00:00:00",
                "link": "https://www.nature.com/articles/s41592-019-0598-1",
                "reference": "Unified rational protein engineering with sequence-based deep representation learning",
                "organization": "Harvard University",
                "parameters": 18200000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "UniRep - 2.20e+19 FLOPs"
            },
            {
                "model": "SciBERT",
                "training_compute_(flop)": 8.926848e+19,
                "training_power_draw_(w)": 1812.7026853205475,
                "training_dataset_size_(gradients)": 3170000000.0,
                "training_time_(hours)": 168.0,
                "training_compute_cost_(2023_usd)": 247.26289010271603,
                "domain_group": "Language",
                "organization_categorization": null,
                "publication_date": "2019-03-26T00:00:00",
                "link": "https://arxiv.org/abs/1903.10676",
                "reference": "SciBERT: A Pretrained Language Model for Scientific Text",
                "organization": "Allen Institute for AI",
                "parameters": 110000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Other",
                "access_group": "Open",
                "model_and_compute": "SciBERT - 8.93e+19 FLOPs"
            },
            {
                "model": "KataGo",
                "training_compute_(flop)": 2.32e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 241000000.0,
                "training_time_(hours)": 456.0,
                "training_compute_cost_(2023_usd)": 104.91425851608678,
                "domain_group": "Games",
                "organization_categorization": "Industry",
                "publication_date": "2019-02-27T00:00:00",
                "link": "https://arxiv.org/abs/1902.10565",
                "reference": "Accelerating Self-Play Learning in Go",
                "organization": "Jane Street",
                "parameters": 2500000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "KataGo - 2.32e+19 FLOPs"
            },
            {
                "model": "ProxylessNAS",
                "training_compute_(flop)": 3.723192e+18,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1230000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 122.74110657965193,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2019-02-23T00:00:00",
                "link": "https://arxiv.org/abs/1812.00332",
                "reference": "ProxylessNAS: Direct neural architecture search on target task and hardware",
                "organization": "Massachusetts Institute of Technology (MIT)",
                "parameters": null,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "ProxylessNAS - 3.72e+18 FLOPs"
            },
            {
                "model": "SSA",
                "training_compute_(flop)": 1.296e+19,
                "training_power_draw_(w)": 339.78426205130415,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 72.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2019-02-22T00:00:00",
                "link": "https://arxiv.org/abs/1902.08661",
                "reference": "Learning protein sequence embeddings using information from structure",
                "organization": "Massachusetts Institute of Technology (MIT)",
                "parameters": null,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "SSA - 1.30e+19 FLOPs"
            },
            {
                "model": "code2seq",
                "training_compute_(flop)": 1.1513908e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 46033536.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2019-02-21T00:00:00",
                "link": "https://arxiv.org/abs/1808.01400",
                "reference": "code2seq: Generating Sequences from Structured Representations of Code",
                "organization": "Technion - Israel Institute of Technology,Facebook AI Research",
                "parameters": 37000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "code2seq - 1.15e+19 FLOPs"
            },
            {
                "model": "GPT-2 (1.5B)",
                "training_compute_(flop)": 1.920000000001e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 10666666666.666666,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 4348.443645,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2019-02-14T00:00:00",
                "link": "https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf",
                "reference": "Language Models are Unsupervised Multitask Learners",
                "organization": "OpenAI",
                "parameters": 1500000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "GPT-2 (1.5B) - 1.92e+21 FLOPs"
            },
            {
                "model": "GPT-2 (124M)",
                "training_compute_(flop)": 7.936e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 10666666666.666666,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2019-02-14T00:00:00",
                "link": "https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf",
                "reference": "Language Models are Unsupervised Multitask Learners",
                "organization": "OpenAI",
                "parameters": 124000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "GPT-2 (124M) - 7.94e+20 FLOPs"
            },
            {
                "model": "GPT-2 (774M)",
                "training_compute_(flop)": 4.9536e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 10666666666.666666,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 20276.74430797595,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2019-02-14T00:00:00",
                "link": "https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf",
                "reference": "Language Models are Unsupervised Multitask Learners",
                "organization": "OpenAI",
                "parameters": 774000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "GPT-2 (774M) - 4.95e+21 FLOPs"
            },
            {
                "model": "GPT-2 (355M)",
                "training_compute_(flop)": 2.272000000071e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 10666666666.666666,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 9300.057144129723,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2019-02-14T00:00:00",
                "link": "https://openai.com/blog/better-language-models/",
                "reference": "Language Models are Unsupervised Multitask Learners",
                "organization": "OpenAI",
                "parameters": 355000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "GPT-2 (355M) - 2.27e+21 FLOPs"
            },
            {
                "model": "Compress-LSTM (66M)",
                "training_compute_(flop)": 3.31e+16,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 929000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2019-02-06T00:00:00",
                "link": "https://arxiv.org/abs/1902.02380",
                "reference": "Compression of Recurrent Neural Networks for Efficient Language Modeling",
                "organization": "Samsung R&D Institute Russia,National Research University Higher School of Economics",
                "parameters": 66000000.0,
                "notable_model": false,
                "country": "Russia",
                "model_accessibility": "Unreleased",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "Compress-LSTM (66M) - 3.31e+16 FLOPs"
            },
            {
                "model": "Hanabi 4 player",
                "training_compute_(flop)": 4.3e+18,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 20000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Games",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2019-02-01T00:00:00",
                "link": "https://arxiv.org/abs/1902.00506",
                "reference": "The Hanabi Challenge: A New Frontier for AI Research",
                "organization": "DeepMind,University of Oxford,Carnegie Mellon University (CMU),Google Brain",
                "parameters": 764000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "Hanabi 4 player - 4.30e+18 FLOPs"
            },
            {
                "model": "Mono3D++",
                "training_compute_(flop)": 4.85606016e+18,
                "training_power_draw_(w)": 2063.2867743998872,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 168.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2019-01-11T00:00:00",
                "link": "https://arxiv.org/abs/1901.03446",
                "reference": "Mono3D++: Monocular 3D Vehicle Detection with Two-Scale 3D Hypotheses and Task Priors",
                "organization": "University of California Los Angeles (UCLA),Megvii Inc",
                "parameters": null,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "Mono3D++ - 4.86e+18 FLOPs"
            },
            {
                "model": "Transformer-XL (257M)",
                "training_compute_(flop)": 3.7832771e+20,
                "training_power_draw_(w)": 14526.185855419688,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2019-01-09T00:00:00",
                "link": "https://arxiv.org/abs/1901.02860",
                "reference": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context",
                "organization": "Carnegie Mellon University (CMU),Google Brain",
                "parameters": 256999999.99999997,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "Transformer-XL (257M) - 3.78e+20 FLOPs"
            },
            {
                "model": "Decoupled weight decay regularization",
                "training_compute_(flop)": 4.716e+17,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 50000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2019-01-04T00:00:00",
                "link": "https://arxiv.org/abs/1711.05101",
                "reference": "Decoupled weight decay regularization.",
                "organization": "University of Freiburg",
                "parameters": 36500000.0,
                "notable_model": false,
                "country": "Germany",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2019,
                "era": "Deep learning era",
                "country_top8": "Germany",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "Decoupled weight decay regularization - 4.72e+17 FLOPs"
            },
            {
                "model": "StyleGAN",
                "training_compute_(flop)": 3.93e+16,
                "training_power_draw_(w)": 4955.197627044293,
                "training_dataset_size_(gradients)": 50000000.0,
                "training_time_(hours)": 168.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2018-12-12T00:00:00",
                "link": "https://arxiv.org/abs/1812.04948",
                "reference": "A Style-Based Generator Architecture for Generative Adversarial Networks",
                "organization": "NVIDIA",
                "parameters": 26200000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "StyleGAN - 3.93e+16 FLOPs"
            },
            {
                "model": "Vine copula (breast cancer)",
                "training_compute_(flop)": 8459100000000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 0.25,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2018-12-04T00:00:00",
                "link": "https://arxiv.org/abs/1812.01226",
                "reference": "Learning Vine Copula Models For Synthetic Data Generation",
                "organization": "Massachusetts Institute of Technology (MIT),Rey Juan Carlos University",
                "parameters": null,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Vine copula (breast cancer) - 8.46e+15 FLOPs"
            },
            {
                "model": "Vine copula (wine quality)",
                "training_compute_(flop)": 8459100000000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 0.25,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": null,
                "organization_categorization": "Academia",
                "publication_date": "2018-12-04T00:00:00",
                "link": "https://arxiv.org/abs/1812.01226",
                "reference": "Learning Vine Copula Models For Synthetic Data Generation",
                "organization": "Massachusetts Institute of Technology (MIT),Rey Juan Carlos University",
                "parameters": null,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Vine copula (wine quality) - 8.46e+15 FLOPs"
            },
            {
                "model": "Vine copula (crime)",
                "training_compute_(flop)": 8459100000000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 0.25,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": null,
                "organization_categorization": "Academia",
                "publication_date": "2018-12-04T00:00:00",
                "link": "https://arxiv.org/abs/1812.01226",
                "reference": "Learning Vine Copula Models For Synthetic Data Generation",
                "organization": "Massachusetts Institute of Technology (MIT),Rey Juan Carlos University",
                "parameters": null,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Vine copula (crime) - 8.46e+15 FLOPs"
            },
            {
                "model": "Multi-cell LSTM",
                "training_compute_(flop)": 2006640000000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 929000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2018-11-15T00:00:00",
                "link": "https://arxiv.org/abs/1811.06477",
                "reference": "Multi-cell LSTM Based Neural Language Model",
                "organization": "University of Hyderabad",
                "parameters": 7200000.0,
                "notable_model": true,
                "country": "India",
                "model_accessibility": "Unreleased",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Multi-cell LSTM - 2.01e+15 FLOPs"
            },
            {
                "model": "Fine-tuned-AWD-LSTM-DOC (fin)",
                "training_compute_(flop)": 5.188e+16,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2018-11-12T00:00:00",
                "link": "https://arxiv.org/abs/1811.04623",
                "reference": "Fine-tuning of Language Models with Discriminator",
                "organization": "Samsung R&D Institute Russia",
                "parameters": 46000000.0,
                "notable_model": true,
                "country": "Russia",
                "model_accessibility": "Unreleased",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Fine-tuned-AWD-LSTM-DOC (fin) - 5.19e+16 FLOPs"
            },
            {
                "model": "Mesh-TensorFlow Transformer 4.9B (language)",
                "training_compute_(flop)": 1.617408e+20,
                "training_power_draw_(w)": 148117.2291982981,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 13.0,
                "training_compute_cost_(2023_usd)": 935.3300509163912,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2018-11-05T00:00:00",
                "link": "https://arxiv.org/abs/1811.02084",
                "reference": "Mesh-TensorFlow: Deep Learning for Supercomputers",
                "organization": "Google Brain",
                "parameters": 4900000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Mesh-TensorFlow Transformer 4.9B (language) - 1.62e+20 FLOPs"
            },
            {
                "model": "Mesh-TensorFlow Transformer 2.9B (translation)",
                "training_compute_(flop)": 6.84288e+19,
                "training_power_draw_(w)": 37029.30729957453,
                "training_dataset_size_(gradients)": 1550000000.0,
                "training_time_(hours)": 22.0,
                "training_compute_cost_(2023_usd)": 395.71656000308855,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2018-11-05T00:00:00",
                "link": "https://arxiv.org/abs/1811.02084",
                "reference": "Mesh-TensorFlow: Deep Learning for Supercomputers",
                "organization": "Google Brain",
                "parameters": 2900000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Mesh-TensorFlow Transformer 2.9B (translation) - 6.84e+19 FLOPs"
            },
            {
                "model": "code2vec",
                "training_compute_(flop)": 3.1593888e+17,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 14162842.0,
                "training_time_(hours)": 36.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2018-10-30T00:00:00",
                "link": "https://arxiv.org/abs/1803.09473",
                "reference": "code2vec: Learning Distributed Representations of Code",
                "organization": "Technion - Israel Institute of Technology,Facebook AI Research",
                "parameters": null,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "code2vec - 3.16e+17 FLOPs"
            },
            {
                "model": "TrellisNet",
                "training_compute_(flop)": 2.78e+18,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2018-10-15T00:00:00",
                "link": "https://arxiv.org/abs/1810.06682",
                "reference": "Trellis Networks for Sequence Modeling",
                "organization": "Carnegie Mellon University (CMU),Bosch Center for Artificial Intelligence,Intel Labs",
                "parameters": 180000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "TrellisNet - 2.78e+18 FLOPs"
            },
            {
                "model": "BERT-Large",
                "training_compute_(flop)": 2.85e+20,
                "training_power_draw_(w)": 37049.928524121315,
                "training_dataset_size_(gradients)": 2649900000.0,
                "training_time_(hours)": 96.0,
                "training_compute_cost_(2023_usd)": 1751.4770087736404,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2018-10-11T00:00:00",
                "link": "https://arxiv.org/abs/1810.04805",
                "reference": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                "organization": "Google",
                "parameters": 340000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "BERT-Large - 2.85e+20 FLOPs"
            },
            {
                "model": "BigGAN-deep 512x512",
                "training_compute_(flop)": 1.8e+21,
                "training_power_draw_(w)": 116476.34773925862,
                "training_dataset_size_(gradients)": 584000000.0,
                "training_time_(hours)": 48.0,
                "training_compute_cost_(2023_usd)": 5219.566577065446,
                "domain_group": "Vision",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2018-09-28T00:00:00",
                "link": "https://arxiv.org/abs/1809.11096",
                "reference": "Large Scale GAN Training for High Fidelity Natural Image Synthesis",
                "organization": "Heriot-Watt University,DeepMind",
                "parameters": 112694781.0,
                "notable_model": false,
                "country": "United Kingdom",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Vision",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "BigGAN-deep 512x512 - 1.80e+21 FLOPs"
            },
            {
                "model": "Transformer (Adaptive Input Embeddings) WT103",
                "training_compute_(flop)": 4.47e+19,
                "training_power_draw_(w)": 4963.480727525225,
                "training_dataset_size_(gradients)": 100000000.0,
                "training_time_(hours)": 67.0,
                "training_compute_cost_(2023_usd)": 2880.917278699733,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2018-09-28T00:00:00",
                "link": "https://arxiv.org/abs/1809.10853",
                "reference": "Adaptive Input Representations for Neural Language Modeling",
                "organization": "Facebook AI Research",
                "parameters": 247000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Transformer (Adaptive Input Embeddings) WT103 - 4.47e+19 FLOPs"
            },
            {
                "model": "ADP-FAIRSEQ + NGRAMRES",
                "training_compute_(flop)": 1.49682e+17,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2018-09-28T00:00:00",
                "link": "https://arxiv.org/abs/2210.14431",
                "reference": "N-gram Is Back: Residual Learning of Neural Text Generation with n-gram Language Model",
                "organization": "Nara Institute of Science and Technology,Chinese University of Hong Kong (CUHK),Tsinghua University",
                "parameters": 247000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "ADP-FAIRSEQ + NGRAMRES - 1.50e+17 FLOPs"
            },
            {
                "model": "LSTM+NeuralCache",
                "training_compute_(flop)": 982800000000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2018-09-24T00:00:00",
                "link": "https://arxiv.org/abs/1809.08826",
                "reference": "Information-Weighted Neural Cache Language Models for ASR",
                "organization": "KU Leuven,ESAT - PSI,Apple",
                "parameters": 2100000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "LSTM+NeuralCache - 9.83e+14 FLOPs"
            },
            {
                "model": "Transformer + Simple Recurrent Unit",
                "training_compute_(flop)": 1.1e+19,
                "training_power_draw_(w)": 4964.696746005496,
                "training_dataset_size_(gradients)": 112500000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 45.373219244858774,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2018-09-17T00:00:00",
                "link": "https://arxiv.org/abs/1709.02755v5",
                "reference": "Simple Recurrent Units for Highly Parallelizable Recurrence",
                "organization": "ASAPP,Cornell University,Google,Princeton University",
                "parameters": 90000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "Transformer + Simple Recurrent Unit - 1.10e+19 FLOPs"
            },
            {
                "model": "(ensemble): AWD-LSTM-DOC (fin) \u00d7 5 (WT2)",
                "training_compute_(flop)": 6.66e+17,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2018-08-30T00:00:00",
                "link": "https://arxiv.org/abs/1808.10143",
                "reference": "Direct Output Connection for a High-Rank Language Model",
                "organization": "NTT Communication Science Laboratories,Tohoku University",
                "parameters": 185000000.0,
                "notable_model": true,
                "country": "Japan",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "Japan",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "(ensemble): AWD-LSTM-DOC (fin) \u00d7 5 (WT2) - 6.66e+17 FLOPs"
            },
            {
                "model": "AWD-LSTM-DOC (fin) (23M)",
                "training_compute_(flop)": 4.323e+16,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2018-08-30T00:00:00",
                "link": "https://arxiv.org/abs/1808.10143",
                "reference": "Direct Output Connection for a High-Rank Language Model",
                "organization": "NTT Communication Science Laboratories,Tohoku University",
                "parameters": 23000000.0,
                "notable_model": false,
                "country": "Japan",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "Japan",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "AWD-LSTM-DOC (fin) (23M) - 4.32e+16 FLOPs"
            },
            {
                "model": "Big Transformer for Back-Translation",
                "training_compute_(flop)": 4.7808e+20,
                "training_power_draw_(w)": 66225.44602672113,
                "training_dataset_size_(gradients)": 4520000000.0,
                "training_time_(hours)": 27.666,
                "training_compute_cost_(2023_usd)": 2442.1618775733145,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2018-08-28T00:00:00",
                "link": "https://arxiv.org/abs/1808.09381",
                "reference": "Understanding Back-Translation at Scale",
                "organization": "Facebook AI Research,Google Brain",
                "parameters": null,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Big Transformer for Back-Translation - 4.78e+20 FLOPs"
            },
            {
                "model": "AWD-LSTM-MoS+PDR + dynamic evaluation (PTB)",
                "training_compute_(flop)": 4.07e+17,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 929000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2018-08-14T00:00:00",
                "link": "https://arxiv.org/abs/1808.05908",
                "reference": "Improved Language Modeling by Decoding the Past",
                "organization": "IBM",
                "parameters": 60852800.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "AWD-LSTM-MoS+PDR + dynamic evaluation (PTB) - 4.07e+17 FLOPs"
            },
            {
                "model": "RGC+ASQ (PTB)",
                "training_compute_(flop)": 1.19e+16,
                "training_power_draw_(w)": 4140.473231727164,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2018-08-13T00:00:00",
                "link": "https://arxiv.org/abs/1808.04357",
                "reference": "RedSync : Reducing Synchronization Traffic for Distributed Deep Learning",
                "organization": "Tsinghua University,University of California Los Angeles (UCLA)",
                "parameters": 53477376.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "RGC+ASQ (PTB) - 1.19e+16 FLOPs"
            },
            {
                "model": "Dexterous In-Hand Manipulation [control policy]",
                "training_compute_(flop)": 2.16e+20,
                "training_power_draw_(w)": 4969.895818092433,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 50.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Robotics",
                "organization_categorization": "Industry",
                "publication_date": "2018-08-01T00:00:00",
                "link": "http://arxiv.org/abs/1808.00177\nhttps://journals.sagepub.com/doi/full/10.1177/0278364919887447\nhttps://openai.com/index/learning-dexterity/",
                "reference": "Learning Dexterous In-Hand Manipulation",
                "organization": "OpenAI",
                "parameters": 3181588.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Dexterous In-Hand Manipulation [control policy] - 2.16e+20 FLOPs"
            },
            {
                "model": "Big-Little Net",
                "training_compute_(flop)": 2.46048e+17,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1280000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2018-07-10T00:00:00",
                "link": "https://arxiv.org/abs/1807.03848",
                "reference": "Big-Little Net: An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition",
                "organization": "IBM",
                "parameters": 77360000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Big-Little Net - 2.46e+17 FLOPs"
            },
            {
                "model": "Big-Little Net (vision)",
                "training_compute_(flop)": 6.2988288e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1280000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2018-07-10T00:00:00",
                "link": "https://arxiv.org/abs/1807.03848",
                "reference": "BIG-LITTLE NET: AN EFFICIENT MULTI-SCALE FEATURE REPRESENTATION FOR VISUAL AND SPEECH RECOGNITION",
                "organization": "IBM",
                "parameters": 77360000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Big-Little Net (vision) - 6.30e+19 FLOPs"
            },
            {
                "model": "Big-Little Net (speech)",
                "training_compute_(flop)": 4.290048e+17,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 720000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Audio",
                "organization_categorization": "Industry",
                "publication_date": "2018-07-10T00:00:00",
                "link": "https://arxiv.org/abs/1807.03848",
                "reference": "Big-Little Net: An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition",
                "organization": "IBM",
                "parameters": 3320000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "Big-Little Net (speech) - 4.29e+17 FLOPs"
            },
            {
                "model": "FTW (For The Win)",
                "training_compute_(flop)": 3.49e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Games",
                "organization_categorization": "Industry",
                "publication_date": "2018-07-03T00:00:00",
                "link": "https://arxiv.org/abs/1807.01281",
                "reference": "Human-level performance in first-person multiplayer games with population-based deep reinforcement learning",
                "organization": "DeepMind",
                "parameters": 126001330.0,
                "notable_model": true,
                "country": "United Kingdom",
                "model_accessibility": "Unreleased",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "FTW (For The Win) - 3.49e+19 FLOPs"
            },
            {
                "model": "QT-Opt",
                "training_compute_(flop)": 1.395e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 11600000.0,
                "training_time_(hours)": 104.2,
                "training_compute_cost_(2023_usd)": 1317.8035861002786,
                "domain_group": "Multimodal",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2018-06-27T00:00:00",
                "link": "https://arxiv.org/abs/1806.10293",
                "reference": "QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation",
                "organization": "Google Brain,University of California (UC) Berkeley",
                "parameters": 1200000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Multimodal",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "QT-Opt - 1.40e+19 FLOPs"
            },
            {
                "model": "DARTS",
                "training_compute_(flop)": 3.2366286e+17,
                "training_power_draw_(w)": 284.6899784455654,
                "training_dataset_size_(gradients)": 2000000.0,
                "training_time_(hours)": 72.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2018-06-24T00:00:00",
                "link": "https://arxiv.org/abs/1806.09055",
                "reference": "DARTS: Differentiable Architecture Search",
                "organization": "DeepMind,Carnegie Mellon University (CMU)",
                "parameters": 33000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "DARTS - 3.24e+17 FLOPs"
            },
            {
                "model": "GPT-1",
                "training_compute_(flop)": 1.7578125e+19,
                "training_power_draw_(w)": 663.5535559333689,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 720.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2018-06-01T00:00:00",
                "link": "https://openai.com/blog/language-unsupervised/",
                "reference": "Improving Language Understanding by Generative Pre-Training",
                "organization": "OpenAI",
                "parameters": 117000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "GPT-1 - 1.76e+19 FLOPs"
            },
            {
                "model": "aLSTM(depth-2)+RecurrentPolicy (WT2)",
                "training_compute_(flop)": 7.296e+16,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Government",
                "publication_date": "2018-05-22T00:00:00",
                "link": "https://arxiv.org/abs/1805.08574",
                "reference": "Breaking the Activation Function Bottleneck through Adaptive Parameterization",
                "organization": "University of Manchester,Alan Turing Institute",
                "parameters": 32000000.0,
                "notable_model": true,
                "country": "United Kingdom",
                "model_accessibility": "Unreleased",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Language",
                "org_top5": "Other",
                "access_group": "Closed",
                "model_and_compute": "aLSTM(depth-2)+RecurrentPolicy (WT2) - 7.30e+16 FLOPs"
            },
            {
                "model": "aLSTM(depth-2)+RecurrentPolicy (PTB)",
                "training_compute_(flop)": 2.4e+16,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2018-05-22T00:00:00",
                "link": "https://arxiv.org/abs/1805.08574",
                "reference": "Breaking the Activation Function Bottleneck through Adaptive Parameterization",
                "organization": "University of Manchester",
                "parameters": 24000000.0,
                "notable_model": false,
                "country": "United Kingdom",
                "model_accessibility": "Unreleased",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "aLSTM(depth-2)+RecurrentPolicy (PTB) - 2.40e+16 FLOPs"
            },
            {
                "model": "Dropout-LSTM+Noise(Bernoulli) (WT2)",
                "training_compute_(flop)": 1.27e+17,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2018-05-03T00:00:00",
                "link": "https://arxiv.org/abs/1805.01500",
                "reference": "Noisin: Unbiased Regularization for Recurrent Neural Networks",
                "organization": "Columbia University,New York University (NYU),Princeton University",
                "parameters": 51000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Dropout-LSTM+Noise(Bernoulli) (WT2) - 1.27e+17 FLOPs"
            },
            {
                "model": "LSTM+Noise(Beta)",
                "training_compute_(flop)": 1.27e+17,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2018-05-03T00:00:00",
                "link": "https://arxiv.org/abs/1805.01500",
                "reference": "Noisin: Unbiased Regularization for Recurrent Neural Networks",
                "organization": "Columbia University,New York University (NYU),Princeton University",
                "parameters": 51000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "LSTM+Noise(Beta) - 1.27e+17 FLOPs"
            },
            {
                "model": "AWD-LSTM-MoS+Noisin+dynamic evaluation (PTB)",
                "training_compute_(flop)": 4.9e+16,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2018-05-03T00:00:00",
                "link": "https://arxiv.org/abs/1805.01500",
                "reference": "Noisin: Unbiased Regularization for Recurrent Neural Networks",
                "organization": "Columbia University,New York University (NYU),Princeton University",
                "parameters": 22000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "AWD-LSTM-MoS+Noisin+dynamic evaluation (PTB) - 4.90e+16 FLOPs"
            },
            {
                "model": "Dropout-LSTM+Noise(Laplace) - medium (WT2)",
                "training_compute_(flop)": 3.12e+16,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2018-05-03T00:00:00",
                "link": "https://arxiv.org/abs/1805.01500",
                "reference": "Noisin: Unbiased Regularization for Recurrent Neural Networks",
                "organization": "Columbia University,New York University (NYU),Princeton University",
                "parameters": 13000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Dropout-LSTM+Noise(Laplace) - medium (WT2) - 3.12e+16 FLOPs"
            },
            {
                "model": "Dropout-LSTM+Noise(Bernoulli) - large(PTB)",
                "training_compute_(flop)": 5.68e+16,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2018-05-03T00:00:00",
                "link": "https://arxiv.org/abs/1805.01500",
                "reference": "Noisin: Unbiased Regularization for Recurrent Neural Networks",
                "organization": "Columbia University,New York University (NYU),Princeton University",
                "parameters": 51000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Dropout-LSTM+Noise(Bernoulli) - large(PTB) - 5.68e+16 FLOPs"
            },
            {
                "model": "ResNeXt-101 32x48d",
                "training_compute_(flop)": 8.74395e+21,
                "training_power_draw_(w)": 209159.05867416784,
                "training_dataset_size_(gradients)": 940000000.0,
                "training_time_(hours)": 496.0,
                "training_compute_cost_(2023_usd)": 144340.00613911543,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2018-05-02T00:00:00",
                "link": "https://arxiv.org/abs/1805.00932",
                "reference": "Exploring the Limits of Weakly Supervised Pretraining",
                "organization": "Facebook",
                "parameters": 829000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "ResNeXt-101 32x48d - 8.74e+21 FLOPs"
            },
            {
                "model": "DNCON2",
                "training_compute_(flop)": 9.5e+16,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 444030000.0,
                "training_time_(hours)": 12.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2018-05-01T00:00:00",
                "link": "https://academic.oup.com/bioinformatics/article/34/9/1466/4708303?login=false",
                "reference": "DNCON2: improved protein contact prediction using two-level deep convolutional neural networks",
                "organization": "University of Missouri",
                "parameters": null,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "DNCON2 - 9.50e+16 FLOPs"
            },
            {
                "model": "RNNLM + Dynamic KL Regularization (WT2)",
                "training_compute_(flop)": 2.1024e+16,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2018-04-27T00:00:00",
                "link": "https://ojs.aaai.org/index.php/AAAI/article/view/11993",
                "reference": "Controlling Global Statistics in Recurrent Neural Network Text Generation",
                "organization": "Northwestern University",
                "parameters": 87600000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "RNNLM + Dynamic KL Regularization (WT2) - 2.10e+16 FLOPs"
            },
            {
                "model": "RNMT+",
                "training_compute_(flop)": 1.83e+19,
                "training_power_draw_(w)": 16602.143459185685,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 120.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2018-04-26T00:00:00",
                "link": "https://arxiv.org/abs/1804.09849",
                "reference": "The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation",
                "organization": "Google AI",
                "parameters": 378900000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "RNMT+ - 1.83e+19 FLOPs"
            },
            {
                "model": "YOLOv3",
                "training_compute_(flop)": 1.3416380824e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 5430000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2018-04-08T00:00:00",
                "link": "https://arxiv.org/abs/1804.02767",
                "reference": "YOLOv3: An Incremental Improvement",
                "organization": "University of Washington",
                "parameters": 56933216.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "YOLOv3 - 1.34e+19 FLOPs"
            },
            {
                "model": "LSTM (Hebbian, Cache, MbPA)",
                "training_compute_(flop)": 3.33e+19,
                "training_power_draw_(w)": 4153.309685987008,
                "training_dataset_size_(gradients)": 175181505.0,
                "training_time_(hours)": 144.0,
                "training_compute_cost_(2023_usd)": 590.5320553042133,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2018-03-27T00:00:00",
                "link": "https://arxiv.org/abs/1803.10049",
                "reference": "Fast Parametric Learning with Activation Memorization",
                "organization": "DeepMind,University College London (UCL)",
                "parameters": 530442240.0,
                "notable_model": false,
                "country": "United Kingdom",
                "model_accessibility": "Unreleased",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "LSTM (Hebbian, Cache, MbPA) - 3.33e+19 FLOPs"
            },
            {
                "model": "4 layer QRNN (h=2500)",
                "training_compute_(flop)": 5.9158815e+17,
                "training_power_draw_(w)": 268.1693571105067,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": 12.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2018-03-22T00:00:00",
                "link": "https://arxiv.org/abs/1803.08240",
                "reference": "An Analysis of Neural Language Modeling at Multiple Scales",
                "organization": "Salesforce Research",
                "parameters": 151000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "4 layer QRNN (h=2500) - 5.92e+17 FLOPs"
            },
            {
                "model": "ENAS",
                "training_compute_(flop)": 2.00664e+16,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 929000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2018-02-09T00:00:00",
                "link": "https://arxiv.org/abs/1802.03268",
                "reference": "Efficient Neural Architecture Search via Parameter Sharing",
                "organization": "Google Brain,Carnegie Mellon University (CMU),Stanford University",
                "parameters": 24000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "ENAS - 2.01e+16 FLOPs"
            },
            {
                "model": "AmoebaNet-A (F=448)",
                "training_compute_(flop)": 3.85296912e+20,
                "training_power_draw_(w)": 229206.26838084744,
                "training_dataset_size_(gradients)": 1150000.0,
                "training_time_(hours)": 168.0,
                "training_compute_cost_(2023_usd)": 11766.339677271537,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2018-02-05T00:00:00",
                "link": "https://arxiv.org/abs/1802.01548",
                "reference": "Regularized Evolution for Image Classifier Architecture Search",
                "organization": "Google Brain",
                "parameters": 469000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "AmoebaNet-A (F=448) - 3.85e+20 FLOPs"
            },
            {
                "model": "IMPALA",
                "training_compute_(flop)": 1.68e+20,
                "training_power_draw_(w)": 285.5725852593349,
                "training_dataset_size_(gradients)": 11400000000.0,
                "training_time_(hours)": 100.0,
                "training_compute_cost_(2023_usd)": 53.42846804494412,
                "domain_group": "Games",
                "organization_categorization": "Industry",
                "publication_date": "2018-02-05T00:00:00",
                "link": "https://arxiv.org/abs/1802.01561",
                "reference": "IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures",
                "organization": "DeepMind",
                "parameters": 1600000.0,
                "notable_model": true,
                "country": "United Kingdom",
                "model_accessibility": "Unreleased",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "IMPALA - 1.68e+20 FLOPs"
            },
            {
                "model": "ELMo",
                "training_compute_(flop)": 3300100000000010.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2000000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2018-02-01T00:00:00",
                "link": "https://arxiv.org/abs/1802.05365",
                "reference": "Deep contextualized word representations",
                "organization": "University of Washington,Allen Institute for AI",
                "parameters": 94000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "ELMo - 3.30e+15 FLOPs"
            },
            {
                "model": "QRNN",
                "training_compute_(flop)": 6.8866472e+17,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": 12.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2018-02-01T00:00:00",
                "link": "https://mlsys.org/Conferences/doc/2018/50.pdf",
                "reference": "Scalable Language Modeling: WikiText-103 on a Single GPU in 12 hours",
                "organization": "Salesforce Research",
                "parameters": 135000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "QRNN - 6.89e+17 FLOPs"
            },
            {
                "model": "ULM-FiT",
                "training_compute_(flop)": 2.72538e+17,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 103000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2018-01-18T00:00:00",
                "link": "https://arxiv.org/abs/1801.06146",
                "reference": "Universal Language Model Fine-tuning for Text Classification",
                "organization": "University of San Francisco,Insight Centre NUI Galway,Fast.ai",
                "parameters": 441000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "ULM-FiT - 2.73e+17 FLOPs"
            },
            {
                "model": "Refined Part Pooling",
                "training_compute_(flop)": 2.6244e+16,
                "training_power_draw_(w)": 1040.1094120986245,
                "training_dataset_size_(gradients)": 77616.0,
                "training_time_(hours)": 1.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2018-01-09T00:00:00",
                "link": "https://arxiv.org/abs/1711.09349",
                "reference": "Beyond Part Models: Person Retrieval with Refined Part Pooling (and a Strong Convolutional Baseline)",
                "organization": "Tsinghua University,University of Technology Sydney,University of Texas at San Antonio",
                "parameters": null,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Refined Part Pooling - 2.62e+16 FLOPs"
            },
            {
                "model": "RNNLM + Dynamic KL Regularization",
                "training_compute_(flop)": 502900000000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2018-01-01T00:00:00",
                "link": "https://ojs.aaai.org/index.php/AAAI/article/view/11993",
                "reference": "Controlling Global Statistics in Recurrent Neural Network Text Generation",
                "organization": "Northwestern University",
                "parameters": 13275200.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2018,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "RNNLM + Dynamic KL Regularization - 5.03e+14 FLOPs"
            },
            {
                "model": "AlphaZero",
                "training_compute_(flop)": 1.0600000001e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 3520000000.0,
                "training_time_(hours)": 24.0,
                "training_compute_cost_(2023_usd)": 229918.6146969874,
                "domain_group": "Games",
                "organization_categorization": "Industry",
                "publication_date": "2017-12-05T00:00:00",
                "link": "https://arxiv.org/abs/1712.01815",
                "reference": "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm",
                "organization": "DeepMind",
                "parameters": null,
                "notable_model": true,
                "country": "United Kingdom",
                "model_accessibility": "Unreleased",
                "year": 2017,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "AlphaZero - 1.06e+20 FLOPs"
            },
            {
                "model": "2-layer-LSTM+Deep-Gradient-Compression",
                "training_compute_(flop)": 1340000000000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 929000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2017-12-05T00:00:00",
                "link": "https://arxiv.org/abs/1712.01887",
                "reference": "Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training",
                "organization": "Tsinghua University,Stanford University,NVIDIA",
                "parameters": 6020000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2017,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "2-layer-LSTM+Deep-Gradient-Compression - 1.34e+15 FLOPs"
            },
            {
                "model": "PNASNet-5",
                "training_compute_(flop)": 6.62904e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1280000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2017-12-02T00:00:00",
                "link": "https://arxiv.org/abs/1712.00559",
                "reference": "Progressive Neural Architecture Search",
                "organization": "Johns Hopkins University,Google AI,Stanford University",
                "parameters": 86100000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2017,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "PNASNet-5 - 6.63e+19 FLOPs"
            },
            {
                "model": "AWD-LSTM-MoS + dynamic evaluation (WT2, 2017)",
                "training_compute_(flop)": 3.36e+18,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2017-11-10T00:00:00",
                "link": "https://arxiv.org/abs/1711.03953",
                "reference": "Breaking the Softmax Bottleneck: A High-Rank RNN Language Model",
                "organization": "Carnegie Mellon University (CMU)",
                "parameters": 35000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2017,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "AWD-LSTM-MoS + dynamic evaluation (WT2, 2017) - 3.36e+18 FLOPs"
            },
            {
                "model": "Fraternal dropout + AWD-LSTM 3-layer (WT2)",
                "training_compute_(flop)": 3.06e+17,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2017-10-31T00:00:00",
                "link": "https://arxiv.org/abs/1711.00066",
                "reference": "Fraternal Dropout",
                "organization": "Jagiellonian University,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms),University of Montreal / Universit\u00e9 de Montr\u00e9al",
                "parameters": 34000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2017,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Fraternal dropout + AWD-LSTM 3-layer (WT2) - 3.06e+17 FLOPs"
            },
            {
                "model": "Fraternal dropout + AWD-LSTM 3-layer (PTB)",
                "training_compute_(flop)": 6.95e+16,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 929000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2017-10-31T00:00:00",
                "link": "https://arxiv.org/abs/1711.00066",
                "reference": "Fraternal Dropout",
                "organization": "University of Montreal / Universit\u00e9 de Montr\u00e9al,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms)",
                "parameters": 24000000.0,
                "notable_model": false,
                "country": "Canada",
                "model_accessibility": "Unreleased",
                "year": 2017,
                "era": "Deep learning era",
                "country_top8": "Canada",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Fraternal dropout + AWD-LSTM 3-layer (PTB) - 6.95e+16 FLOPs"
            },
            {
                "model": "AlphaGo Master",
                "training_compute_(flop)": 3.4100000001e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 72.0,
                "training_compute_cost_(2023_usd)": 471445.3247973037,
                "domain_group": "Games",
                "organization_categorization": "Industry",
                "publication_date": "2017-10-19T00:00:00",
                "link": "https://www.nature.com/articles/nature24270",
                "reference": "Mastering the game of Go without human knowledge",
                "organization": "DeepMind",
                "parameters": null,
                "notable_model": true,
                "country": "United Kingdom",
                "model_accessibility": "Unreleased",
                "year": 2017,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "AlphaGo Master - 3.41e+20 FLOPs"
            },
            {
                "model": "AlphaGo Zero",
                "training_compute_(flop)": 6.49439910290227e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 6348800000.0,
                "training_time_(hours)": 480.0,
                "training_compute_cost_(2023_usd)": 1213.8707315707268,
                "domain_group": "Games",
                "organization_categorization": "Industry",
                "publication_date": "2017-10-18T00:00:00",
                "link": "https://www.nature.com/articles/nature24270",
                "reference": "Mastering the game of Go without human knowledge",
                "organization": "DeepMind",
                "parameters": 46400244.0,
                "notable_model": true,
                "country": "United Kingdom",
                "model_accessibility": "Unreleased",
                "year": 2017,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "AlphaGo Zero - 6.49e+20 FLOPs"
            },
            {
                "model": "AWD-LSTM+WT+Cache+IOG (WT2)",
                "training_compute_(flop)": 3180000000000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2017-09-26T00:00:00",
                "link": "https://arxiv.org/abs/1709.08907",
                "reference": "Input-to-Output Gate to Improve RNN Language Models",
                "organization": "NTT Communication Science Laboratories",
                "parameters": 53000000.0,
                "notable_model": true,
                "country": "Japan",
                "model_accessibility": "Unreleased",
                "year": 2017,
                "era": "Deep learning era",
                "country_top8": "Japan",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "AWD-LSTM+WT+Cache+IOG (WT2) - 3.18e+15 FLOPs"
            },
            {
                "model": "ISS",
                "training_compute_(flop)": 3400000000000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 929000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2017-09-15T00:00:00",
                "link": "https://arxiv.org/abs/1709.05027",
                "reference": "Learning Intrinsic Sparse Structures within Long Short-Term Memory",
                "organization": "Duke University,Microsoft",
                "parameters": 11100000.000000002,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2017,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "ISS - 3.40e+15 FLOPs"
            },
            {
                "model": "PyramidNet",
                "training_compute_(flop)": 2340000000000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1280000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2017-09-06T00:00:00",
                "link": "https://arxiv.org/abs/1610.02915v4",
                "reference": "Deep Pyramidal Residual Networks",
                "organization": "Korea Advanced Institute of Science and Technology (KAIST)",
                "parameters": 26000000.0,
                "notable_model": true,
                "country": "South Korea",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2017,
                "era": "Deep learning era",
                "country_top8": "South Korea",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "PyramidNet - 2.34e+15 FLOPs"
            },
            {
                "model": "GL-LWGC-AWD-MoS-LSTM + dynamic evaluation (WT2)",
                "training_compute_(flop)": 4.56e+17,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2017-08-29T00:00:00",
                "link": "https://arxiv.org/abs/1708.08863",
                "reference": "Gradual Learning of Recurrent Neural Networks",
                "organization": "Ben-Gurion University of the Negev",
                "parameters": 38000000.0,
                "notable_model": true,
                "country": "Israel",
                "model_accessibility": "Unreleased",
                "year": 2017,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "GL-LWGC-AWD-MoS-LSTM + dynamic evaluation (WT2) - 4.56e+17 FLOPs"
            },
            {
                "model": "GL-LWGC-AWD-MoS-LSTM + dynamic evaluation (PTB)",
                "training_compute_(flop)": 1.45e+17,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 929000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2017-08-29T00:00:00",
                "link": "https://arxiv.org/abs/1708.08863",
                "reference": "Gradual Learning of Recurrent Neural Networks",
                "organization": "Ben-Gurion University",
                "parameters": 26000000.0,
                "notable_model": false,
                "country": "Israel",
                "model_accessibility": "Unreleased",
                "year": 2017,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "GL-LWGC-AWD-MoS-LSTM + dynamic evaluation (PTB) - 1.45e+17 FLOPs"
            },
            {
                "model": "Libratus",
                "training_compute_(flop)": 5.51e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Games",
                "organization_categorization": "Academia",
                "publication_date": "2017-08-19T00:00:00",
                "link": "https://www.ijcai.org/proceedings/2017/0772.pdf",
                "reference": "Libratus: The Superhuman AI for No-Limit Poker",
                "organization": "Carnegie Mellon University (CMU)",
                "parameters": null,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2017,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Libratus - 5.51e+20 FLOPs"
            },
            {
                "model": "EI-REHN-1000D",
                "training_compute_(flop)": 1.06e+16,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 929000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2017-08-14T00:00:00",
                "link": "https://arxiv.org/abs/1708.04116",
                "reference": "Early Improving Recurrent Elastic Highway Network",
                "organization": "Korea Advanced Institute of Science and Technology (KAIST)",
                "parameters": 19000000.0,
                "notable_model": true,
                "country": "South Korea",
                "model_accessibility": "Unreleased",
                "year": 2017,
                "era": "Deep learning era",
                "country_top8": "South Korea",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "EI-REHN-1000D - 1.06e+16 FLOPs"
            },
            {
                "model": "OpenAI TI7 DOTA 1v1",
                "training_compute_(flop)": 6.046095222592002e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Games",
                "organization_categorization": "Industry",
                "publication_date": "2017-08-11T00:00:00",
                "link": "https://openai.com/research/dota-2",
                "reference": "Dota 2",
                "organization": "OpenAI",
                "parameters": null,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 2017,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "OpenAI TI7 DOTA 1v1 - 6.05e+20 FLOPs"
            },
            {
                "model": "RetinaNet-R101",
                "training_compute_(flop)": 2.065392e+18,
                "training_power_draw_(w)": 4174.823263154792,
                "training_dataset_size_(gradients)": 115000.0,
                "training_time_(hours)": 35.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2017-08-07T00:00:00",
                "link": "https://arxiv.org/abs/1708.02002",
                "reference": "Focal loss for dense object detection",
                "organization": "Facebook AI Research",
                "parameters": 53000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2017,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "RetinaNet-R101 - 2.07e+18 FLOPs"
            },
            {
                "model": "AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (WT2)",
                "training_compute_(flop)": 2.97e+17,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2017-08-07T00:00:00",
                "link": "https://arxiv.org/abs/1708.02182",
                "reference": "Regularizing and Optimizing LSTM Language Models",
                "organization": "Salesforce Research",
                "parameters": 33000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2017,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (WT2) - 2.97e+17 FLOPs"
            },
            {
                "model": "AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (PTB)",
                "training_compute_(flop)": 1e+17,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2017-08-07T00:00:00",
                "link": "https://arxiv.org/abs/1708.02182",
                "reference": "Regularizing and Optimizing LSTM Language Models",
                "organization": "Salesforce Research",
                "parameters": 24000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2017,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (PTB) - 1.00e+17 FLOPs"
            },
            {
                "model": "ConvS2S (ensemble of 8 models)",
                "training_compute_(flop)": 5.64e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 887500000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2017-07-25T00:00:00",
                "link": "https://arxiv.org/abs/1705.03122",
                "reference": "Convolutional Sequence to Sequence Learning",
                "organization": "Meta AI",
                "parameters": null,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 2017,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "ConvS2S (ensemble of 8 models) - 5.64e+19 FLOPs"
            },
            {
                "model": "Densely Connected LSTM + Var. Dropout",
                "training_compute_(flop)": 1.28e+16,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 929000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2017-07-19T00:00:00",
                "link": "https://arxiv.org/abs/1707.06130",
                "reference": "Improving Language Modeling using Densely Connected Recurrent Neural Networks",
                "organization": "Ghent University",
                "parameters": 23000000.0,
                "notable_model": false,
                "country": "Belgium",
                "model_accessibility": "Unreleased",
                "year": 2017,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Densely Connected LSTM + Var. Dropout - 1.28e+16 FLOPs"
            },
            {
                "model": "4 layer Densely Connected LSTM 14M (PTB)",
                "training_compute_(flop)": 7800000000000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2017-07-19T00:00:00",
                "link": "https://arxiv.org/pdf/1707.06130",
                "reference": "Improving Language Modeling using Densely Connected Recurrent Neural Networks",
                "organization": "Ghent University",
                "parameters": 14000000.0,
                "notable_model": false,
                "country": "Belgium",
                "model_accessibility": "Unreleased",
                "year": 2017,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "4 layer Densely Connected LSTM 14M (PTB) - 7.80e+15 FLOPs"
            },
            {
                "model": "JFT",
                "training_compute_(flop)": 8.43e+20,
                "training_power_draw_(w)": 31330.704406642573,
                "training_dataset_size_(gradients)": 5487300000000.0,
                "training_time_(hours)": 1440.0,
                "training_compute_cost_(2023_usd)": 17910.759507843868,
                "domain_group": "Vision",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2017-07-10T00:00:00",
                "link": "https://arxiv.org/abs/1707.02968",
                "reference": "Revisiting Unreasonable Effectiveness of Data in Deep Learning Era.",
                "organization": "Google Research,Carnegie Mellon University (CMU)",
                "parameters": 44654504.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2017,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "JFT - 8.43e+20 FLOPs"
            },
            {
                "model": "DeepLoc",
                "training_compute_(flop)": 5.781024e+17,
                "training_power_draw_(w)": 286.9303811522887,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 80.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2017-07-07T00:00:00",
                "link": "https://academic.oup.com/bioinformatics/article/33/21/3387/3931857",
                "reference": "DeepLoc: prediction of protein subcellular localization using deep learning",
                "organization": "Technical University of Denmark,University of Copenhagen",
                "parameters": null,
                "notable_model": true,
                "country": "Denmark",
                "model_accessibility": "Hosted access (no API)",
                "year": 2017,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "DeepLoc - 5.78e+17 FLOPs"
            },
            {
                "model": "Transformer",
                "training_compute_(flop)": 7.4245248e+18,
                "training_power_draw_(w)": 4180.032869488383,
                "training_dataset_size_(gradients)": 832000000.0,
                "training_time_(hours)": 84.0,
                "training_compute_cost_(2023_usd)": 438.0356518424336,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2017-06-12T00:00:00",
                "link": "https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf",
                "reference": "Attention Is All You Need",
                "organization": "Google Research,Google Brain",
                "parameters": 213000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2017,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Transformer - 7.42e+18 FLOPs"
            },
            {
                "model": "AlexNet + coordinating filters",
                "training_compute_(flop)": 4.7e+17,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2017-03-28T00:00:00",
                "link": "https://arxiv.org/abs/1703.09746",
                "reference": "Coordinating Filters for Faster Deep Neural Networks",
                "organization": "University of Pittsburgh,Duke University",
                "parameters": 60000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2017,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "AlexNet + coordinating filters - 4.70e+17 FLOPs"
            },
            {
                "model": "VDCNN (on Amazon Review Full dataset)",
                "training_compute_(flop)": 5.7267e+17,
                "training_power_draw_(w)": 282.2017592376111,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2017-01-27T00:00:00",
                "link": "https://arxiv.org/abs/1606.01781",
                "reference": "Very Deep Convolutional Networks for Text Classification",
                "organization": "Facebook AI Research,University of Le Mans",
                "parameters": 7800000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2017,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "VDCNN (on Amazon Review Full dataset) - 5.73e+17 FLOPs"
            },
            {
                "model": "MoE-Multi",
                "training_compute_(flop)": 9.393905664e+19,
                "training_power_draw_(w)": 32873.78910003616,
                "training_dataset_size_(gradients)": 87000000000.0,
                "training_time_(hours)": 288.0,
                "training_compute_cost_(2023_usd)": 3874.12265,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2017-01-23T00:00:00",
                "link": "https://arxiv.org/abs/1701.06538",
                "reference": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer",
                "organization": "Jagiellonian University,Google Brain",
                "parameters": 8700000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2017,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "MoE-Multi - 9.39e+19 FLOPs"
            },
            {
                "model": "DeepStack",
                "training_compute_(flop)": 1.446336e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 25380000000.0,
                "training_time_(hours)": 218.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Games",
                "organization_categorization": "Academia",
                "publication_date": "2017-01-06T00:00:00",
                "link": "https://arxiv.org/abs/1701.01724",
                "reference": "DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker",
                "organization": "University of Alberta,Charles University,Czech Technical University",
                "parameters": 2500000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2017,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "DeepStack - 1.45e+19 FLOPs"
            },
            {
                "model": "EnhanceNet",
                "training_compute_(flop)": 1.3079231999999998e+17,
                "training_power_draw_(w)": 282.4218009310853,
                "training_dataset_size_(gradients)": 9830400000.0,
                "training_time_(hours)": 24.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2016-12-23T00:00:00",
                "link": "https://www.semanticscholar.org/paper/EnhanceNet%3A-Single-Image-Super-Resolution-Through-Sajjadi-Scholkopf/fddc32f3880688238847077fd927ab3025db7a6a",
                "reference": "EnhanceNet: Single Image Super-Resolution Through Automated Texture Synthesis",
                "organization": "Max Planck Institute for Intelligent Systems",
                "parameters": 814464.0,
                "notable_model": true,
                "country": "Germany",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2016,
                "era": "Deep learning era",
                "country_top8": "Germany",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "EnhanceNet - 1.31e+17 FLOPs"
            },
            {
                "model": "GCRN-M1, dropout",
                "training_compute_(flop)": 3040000000000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 929000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2016-12-22T00:00:00",
                "link": "https://arxiv.org/abs/1612.07659",
                "reference": "Structured Sequence Modeling with Graph Convolutional Recurrent Networks",
                "organization": "Ecole Polytechnique F\u00b4ed\u00b4erale de Lausanne (EPFL)",
                "parameters": 42000000.0,
                "notable_model": false,
                "country": "Switzerland",
                "model_accessibility": "Unreleased",
                "year": 2016,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "GCRN-M1, dropout - 3.04e+15 FLOPs"
            },
            {
                "model": "HR-ResNet101",
                "training_compute_(flop)": 7.077e+18,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 8243968.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2016-12-13T00:00:00",
                "link": "https://www.semanticscholar.org/paper/Finding-Tiny-Faces-Hu-Ramanan/71f51e1b6691343ed031c1ed42efc96a7f1a0619",
                "reference": "Finding Tiny Faces",
                "organization": "Carnegie Mellon University (CMU)",
                "parameters": 44500000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2016,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "HR-ResNet101 - 7.08e+18 FLOPs"
            },
            {
                "model": "PolyNet",
                "training_compute_(flop)": 6.4e+19,
                "training_power_draw_(w)": 16797.38519252759,
                "training_dataset_size_(gradients)": 1280000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 617.1106542,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2016-11-17T00:00:00",
                "link": "https://arxiv.org/abs/1611.05725",
                "reference": "PolyNet: A Pursuit of Structural Diversity in Very Deep Networks",
                "organization": "Chinese University of Hong Kong (CUHK)",
                "parameters": 92000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2016,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "PolyNet - 6.40e+19 FLOPs"
            },
            {
                "model": "ResNeXt-101 (64\u00d74d)",
                "training_compute_(flop)": 1.2e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1280000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2016-11-16T00:00:00",
                "link": "https://arxiv.org/abs/1611.05431",
                "reference": "Aggregated Residual Transformations for Deep Neural Networks",
                "organization": "University of California San Diego,Facebook",
                "parameters": 83000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2016,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Academia, Industry",
                "access_group": "Open",
                "model_and_compute": "ResNeXt-101 (64\u00d74d) - 1.20e+19 FLOPs"
            },
            {
                "model": "NASv3 (CIFAR-10)",
                "training_compute_(flop)": 2.2e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 45000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 21184.441090423978,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2016-11-05T00:00:00",
                "link": "https://arxiv.org/abs/1611.01578",
                "reference": "Neural Architecture Search with Reinforcement Learning",
                "organization": "Google Brain",
                "parameters": 37400000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2016,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "NASv3 (CIFAR-10) - 2.20e+21 FLOPs"
            },
            {
                "model": "NAS with base 8 and shared embeddings",
                "training_compute_(flop)": 1.05e+16,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2016-11-05T00:00:00",
                "link": "https://arxiv.org/abs/1611.01578",
                "reference": "Neural Architecture Search with Reinforcement Learning",
                "organization": "Google Brain",
                "parameters": 54000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2016,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "NAS with base 8 and shared embeddings - 1.05e+16 FLOPs"
            },
            {
                "model": "BIDAF",
                "training_compute_(flop)": 3.4686144e+18,
                "training_power_draw_(w)": 4200.468649711526,
                "training_dataset_size_(gradients)": 879000.0,
                "training_time_(hours)": 60.0,
                "training_compute_cost_(2023_usd)": 41.25014556779628,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2016-11-05T00:00:00",
                "link": "https://arxiv.org/abs/1611.01603v6",
                "reference": "Bidirectional Attention Flow for Machine Comprehension",
                "organization": "University of Washington,Allen Institute for AI",
                "parameters": 2600000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2016,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "BIDAF - 3.47e+18 FLOPs"
            },
            {
                "model": "VD-LSTM+REAL Large",
                "training_compute_(flop)": 2.13e+16,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 929000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2016-11-04T00:00:00",
                "link": "https://arxiv.org/abs/1611.01462",
                "reference": "Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling",
                "organization": "Salesforce Research,Stanford University",
                "parameters": 51000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2016,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "VD-LSTM+REAL Large - 2.13e+16 FLOPs"
            },
            {
                "model": "SPIDER2",
                "training_compute_(flop)": 1.822e+16,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 13893600.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Biology",
                "organization_categorization": "Academia",
                "publication_date": "2016-10-28T00:00:00",
                "link": "https://link.springer.com/protocol/10.1007/978-1-4939-6406-2_6",
                "reference": "SPIDER2: A Package to Predict Secondary Structure, Accessible Surface Area, and Main-Chain Torsional Angles by Deep Neural Networks",
                "organization": "Griffith University,University of Iowa,Dezhou University",
                "parameters": 409536.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (non-commercial)",
                "year": 2016,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Biology",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "SPIDER2 - 1.82e+16 FLOPs"
            },
            {
                "model": "Xception",
                "training_compute_(flop)": 4.36e+20,
                "training_power_draw_(w)": 37828.64014138488,
                "training_dataset_size_(gradients)": 350000000.0,
                "training_time_(hours)": 720.0,
                "training_compute_cost_(2023_usd)": 13108.852355728606,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2016-10-07T00:00:00",
                "link": "https://arxiv.org/abs/1610.02357",
                "reference": "Xception: Deep Learning with Depthwise Separable Convolutions",
                "organization": "Google",
                "parameters": 22855952.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2016,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Xception - 4.36e+20 FLOPs"
            },
            {
                "model": "GNMT",
                "training_compute_(flop)": 6.620000000001e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 540000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 201331.569937687,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2016-09-26T00:00:00",
                "link": "https://arxiv.org/abs/1609.08144",
                "reference": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
                "organization": "Google",
                "parameters": 278000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Hosted access (no API)",
                "year": 2016,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "GNMT - 6.62e+21 FLOPs"
            },
            {
                "model": "Pointer Sentinel-LSTM (medium)",
                "training_compute_(flop)": 7490000000000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 929000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2016-09-26T00:00:00",
                "link": "https://arxiv.org/abs/1609.07843",
                "reference": "Pointer Sentinel Mixture Models",
                "organization": "MetaMind Inc,Salesforce",
                "parameters": 21000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2016,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Pointer Sentinel-LSTM (medium) - 7.49e+15 FLOPs"
            },
            {
                "model": "Zoneout + Variational LSTM (WT2)",
                "training_compute_(flop)": 1.6128e+16,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2016-09-26T00:00:00",
                "link": "https://arxiv.org/abs/1609.07843",
                "reference": "Pointer Sentinel Mixture Models",
                "organization": "MetaMind Inc,Salesforce",
                "parameters": 21000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2016,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Zoneout + Variational LSTM (WT2) - 1.61e+16 FLOPs"
            },
            {
                "model": "Zoneout + Variational LSTM (PTB)",
                "training_compute_(flop)": 7100000000000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2016-09-26T00:00:00",
                "link": "https://arxiv.org/abs/1609.07843",
                "reference": "Pointer Sentinel Mixture Models",
                "organization": "MetaMind Inc",
                "parameters": 20000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2016,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Zoneout + Variational LSTM (PTB) - 7.10e+15 FLOPs"
            },
            {
                "model": "Pointer Sentinel-LSTM (WT2)",
                "training_compute_(flop)": 1600000000000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2016-09-26T00:00:00",
                "link": "https://arxiv.org/abs/1609.07843",
                "reference": "Pointer Sentinel Mixture Models",
                "organization": "MetaMind Inc",
                "parameters": 21000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2016,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Pointer Sentinel-LSTM (WT2) - 1.60e+15 FLOPs"
            },
            {
                "model": "Knowledge distillation student model",
                "training_compute_(flop)": 1.008e+17,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 100000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2016-09-22T00:00:00",
                "link": "https://arxiv.org/abs/1606.07947",
                "reference": "Sequence-Level Knowledge Distillation",
                "organization": "Harvard University",
                "parameters": 84000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 2016,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Knowledge distillation student model - 1.01e+17 FLOPs"
            },
            {
                "model": "ResNet-200",
                "training_compute_(flop)": 2.9741645e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1281167.0,
                "training_time_(hours)": 500.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2016-09-17T00:00:00",
                "link": "https://link.springer.com/chapter/10.1007/978-3-319-46493-0_38",
                "reference": "Identity Mappings in Deep Residual Networks",
                "organization": "Microsoft Research Asia",
                "parameters": null,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Unreleased",
                "year": 2016,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "ResNet-200 - 2.97e+19 FLOPs"
            },
            {
                "model": "Attend-Infer-Repeat",
                "training_compute_(flop)": 6.448896e+16,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 48.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2016-08-12T00:00:00",
                "link": "https://arxiv.org/abs/1603.08575",
                "reference": "Attend, Infer, Repeat: Fast Scene Understanding with Generative Models",
                "organization": "Google DeepMind",
                "parameters": 82130304.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2016,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Attend-Infer-Repeat - 6.45e+16 FLOPs"
            },
            {
                "model": "VD-RHN",
                "training_compute_(flop)": 3570000000000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 929000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2016-07-12T00:00:00",
                "link": "https://arxiv.org/abs/1607.03474",
                "reference": "Recurrent Highway Networks",
                "organization": "ETH Zurich,IDSIA",
                "parameters": 32000000.0,
                "notable_model": false,
                "country": "Switzerland",
                "model_accessibility": "Unreleased",
                "year": 2016,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "VD-RHN - 3.57e+15 FLOPs"
            },
            {
                "model": "Variational RHN + WT (PTB)",
                "training_compute_(flop)": 1.28e+17,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2016-07-12T00:00:00",
                "link": "https://arxiv.org/abs/1607.03474",
                "reference": "Recurrent Highway Networks",
                "organization": "ETH Zurich,IDSIA",
                "parameters": 23000000.0,
                "notable_model": false,
                "country": "Switzerland",
                "model_accessibility": "Unreleased",
                "year": 2016,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Variational RHN + WT (PTB) - 1.28e+17 FLOPs"
            },
            {
                "model": "R-FCN",
                "training_compute_(flop)": 7.1935776e+17,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 10624000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2016-06-21T00:00:00",
                "link": "https://arxiv.org/abs/1605.06409",
                "reference": "R-fcn: Object detection via region-based fully convolutional networks.",
                "organization": "Tsinghua University,Microsoft Research",
                "parameters": null,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2016,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "R-FCN - 7.19e+17 FLOPs"
            },
            {
                "model": "Part-of-sentence tagging model",
                "training_compute_(flop)": 1.454112e+17,
                "training_power_draw_(w)": 289.523491253324,
                "training_dataset_size_(gradients)": 912344.0,
                "training_time_(hours)": 12.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2016-05-29T00:00:00",
                "link": "https://arxiv.org/abs/1603.01354",
                "reference": "End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF",
                "organization": "Carnegie Mellon University (CMU)",
                "parameters": null,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 2016,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Part-of-sentence tagging model - 1.45e+17 FLOPs"
            },
            {
                "model": "Named Entity Recognition model",
                "training_compute_(flop)": 9.69408e+16,
                "training_power_draw_(w)": 290.0785080846343,
                "training_dataset_size_(gradients)": 204567.0,
                "training_time_(hours)": 8.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2016-03-04T00:00:00",
                "link": "https://arxiv.org/abs/1603.01354",
                "reference": "End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF",
                "organization": "Carnegie Mellon University (CMU)",
                "parameters": null,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 2016,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Named Entity Recognition model - 9.69e+16 FLOPs"
            },
            {
                "model": "BIG LSTM+CNN INPUTS ",
                "training_compute_(flop)": 1.0220883e+20,
                "training_power_draw_(w)": 16564.40221110092,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 71.21404931614963,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2016-02-11T00:00:00",
                "link": "https://arxiv.org/abs/1602.02410",
                "reference": "Exploring the Limits of Language Modeling",
                "organization": "Google Brain",
                "parameters": 1040000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2016,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "BIG LSTM+CNN INPUTS  - 1.02e+20 FLOPs"
            },
            {
                "model": "10 LSTMS + KN-5 (OPTIMAL WEIGHTS)",
                "training_compute_(flop)": 8.7892439e+19,
                "training_power_draw_(w)": 16564.40221110092,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2016-02-11T00:00:00",
                "link": "https://arxiv.org/abs/1602.02410",
                "reference": "Exploring the Limits of Language Modeling",
                "organization": "Google Brain",
                "parameters": 1040000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2016,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "10 LSTMS + KN-5 (OPTIMAL WEIGHTS) - 8.79e+19 FLOPs"
            },
            {
                "model": "AlphaGo Lee",
                "training_compute_(flop)": 1.9e+21,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 300000000.0,
                "training_time_(hours)": 696.0,
                "training_compute_cost_(2023_usd)": 22206.80954117433,
                "domain_group": "Games",
                "organization_categorization": "Industry",
                "publication_date": "2016-01-27T00:00:00",
                "link": "https://www.nature.com/articles/nature16961",
                "reference": "Mastering the game of Go with deep neural networks and tree search",
                "organization": "DeepMind",
                "parameters": null,
                "notable_model": true,
                "country": "United Kingdom",
                "model_accessibility": "Unreleased",
                "year": 2016,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "AlphaGo Lee - 1.90e+21 FLOPs"
            },
            {
                "model": "Variational (untied weights, MC) LSTM (Large)",
                "training_compute_(flop)": 5886144000000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 929000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2015-12-16T00:00:00",
                "link": "https://arxiv.org/abs/1512.05287",
                "reference": "A Theoretically Grounded Application of Dropout in Recurrent Neural Networks",
                "organization": "University of Cambridge",
                "parameters": 66000000.0,
                "notable_model": true,
                "country": "United Kingdom",
                "model_accessibility": "Unreleased",
                "year": 2015,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Variational (untied weights, MC) LSTM (Large) - 5.89e+15 FLOPs"
            },
            {
                "model": "ResNet-152 (ImageNet)",
                "training_compute_(flop)": 1.041408e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1280000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2015-12-10T00:00:00",
                "link": "https://arxiv.org/abs/1512.03385",
                "reference": "Deep Residual Learning for Image Recognition",
                "organization": "Microsoft",
                "parameters": 60200000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2015,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "ResNet-152 (ImageNet) - 1.04e+19 FLOPs"
            },
            {
                "model": "ResNet-101 (ImageNet)",
                "training_compute_(flop)": 7.004e+18,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1280000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2015-12-10T00:00:00",
                "link": "https://arxiv.org/abs/1512.03385",
                "reference": "Deep Residual Learning for Image Recognition",
                "organization": "Microsoft",
                "parameters": 44500000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2015,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Open",
                "model_and_compute": "ResNet-101 (ImageNet) - 7.00e+18 FLOPs"
            },
            {
                "model": "DeepSpeech2 (English)",
                "training_compute_(flop)": 2.6e+19,
                "training_power_draw_(w)": 8463.467702301677,
                "training_dataset_size_(gradients)": 716400000.0,
                "training_time_(hours)": 120.0,
                "training_compute_cost_(2023_usd)": 214.33761025036443,
                "domain_group": "Audio",
                "organization_categorization": "Industry",
                "publication_date": "2015-12-08T00:00:00",
                "link": "https://arxiv.org/abs/1512.02595",
                "reference": "Deep Speech 2: End-to-End Speech Recognition in English and Mandarin",
                "organization": "Baidu Research - Silicon Valley AI Lab",
                "parameters": 38000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 2015,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "DeepSpeech2 (English) - 2.60e+19 FLOPs"
            },
            {
                "model": "Inception v3",
                "training_compute_(flop)": 1e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1200000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 1218.1810104797394,
                "domain_group": "Vision",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2015-12-02T00:00:00",
                "link": "https://arxiv.org/abs/1512.00567",
                "reference": "Rethinking the inception architecture for computer vision.",
                "organization": "Google,University College London (UCL)",
                "parameters": 23626728.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2015,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "Inception v3 - 1.00e+20 FLOPs"
            },
            {
                "model": "SAF R-CNN",
                "training_compute_(flop)": 1.231108125e+19,
                "training_power_draw_(w)": 290.9065505304121,
                "training_dataset_size_(gradients)": 350000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2015-10-28T00:00:00",
                "link": "https://arxiv.org/abs/1510.08160",
                "reference": "Scale-aware Fast R-CNN for Pedestrian Detection",
                "organization": "Beijing Institute of Technology,Sun Yat-sen University,Panasonic R&D,National University of Singapore",
                "parameters": 138000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2015,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "SAF R-CNN - 1.23e+19 FLOPs"
            },
            {
                "model": "AlphaGo Fan",
                "training_compute_(flop)": 3.8e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 12697600000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 4828.322276803044,
                "domain_group": "Games",
                "organization_categorization": "Industry",
                "publication_date": "2015-10-01T00:00:00",
                "link": "https://www.nature.com/articles/nature16961",
                "reference": "Mastering the game of Go with deep neural networks and tree search",
                "organization": "DeepMind",
                "parameters": 8209984.0,
                "notable_model": true,
                "country": "United Kingdom",
                "model_accessibility": "Unreleased",
                "year": 2015,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "AlphaGo Fan - 3.80e+20 FLOPs"
            },
            {
                "model": "LSTM-Char-Large",
                "training_compute_(flop)": 2650000000000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 929000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2015-08-26T00:00:00",
                "link": "https://arxiv.org/abs/1508.06615",
                "reference": "Character-Aware Neural Language Models",
                "organization": "Harvard University,New York University (NYU)",
                "parameters": 19000000.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2015,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "LSTM-Char-Large - 2.65e+15 FLOPs"
            },
            {
                "model": "DCNN",
                "training_compute_(flop)": 4.8098691e+17,
                "training_power_draw_(w)": 285.60949180561556,
                "training_dataset_size_(gradients)": 490356.0,
                "training_time_(hours)": 216.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2015-08-07T00:00:00",
                "link": "https://arxiv.org/abs/1508.01722",
                "reference": "Unconstrained Face Verification using Deep CNN Features",
                "organization": "University of Maryland,Rutgers University",
                "parameters": 5006000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2015,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "DCNN - 4.81e+17 FLOPs"
            },
            {
                "model": "Search-Proven Best LSTM",
                "training_compute_(flop)": 3340000000000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 929000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2015-07-06T00:00:00",
                "link": "https://proceedings.mlr.press/v37/jozefowicz15.pdf",
                "reference": "An Empirical Exploration of Recurrent Network Architectures",
                "organization": "Google",
                "parameters": 20000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2015,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Search-Proven Best LSTM - 3.34e+15 FLOPs"
            },
            {
                "model": "U-Net",
                "training_compute_(flop)": 5.0832252e+16,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 7864320.0,
                "training_time_(hours)": 10.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2015-05-18T00:00:00",
                "link": "https://www.semanticscholar.org/paper/U-Net%3A-Convolutional-Networks-for-Biomedical-Image-Ronneberger-Fischer/6364fdaa0a0eccd823a779fcdd489173f938e91a",
                "reference": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
                "organization": "University of Freiburg",
                "parameters": 37676160.0,
                "notable_model": true,
                "country": "Germany",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2015,
                "era": "Deep learning era",
                "country_top8": "Germany",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "U-Net - 5.08e+16 FLOPs"
            },
            {
                "model": "TC-DNN-BLSTM-DNN",
                "training_compute_(flop)": 1.9410191999999997e+17,
                "training_power_draw_(w)": 263.01387537165505,
                "training_dataset_size_(gradients)": 29160000.0,
                "training_time_(hours)": 51.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Audio",
                "organization_categorization": "Academia",
                "publication_date": "2015-04-06T00:00:00",
                "link": "https://www.semanticscholar.org/paper/Deep-Recurrent-Neural-Networks-for-Acoustic-Chan-Lane/ae18f67c10a5d4da91f128ec7a6cf7c784122cd5",
                "reference": "Deep Recurrent Neural Networks for Acoustic Modelling",
                "organization": "Carnegie Mellon University (CMU)",
                "parameters": 18413568.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2015,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "TC-DNN-BLSTM-DNN - 1.94e+17 FLOPs"
            },
            {
                "model": "genCNN + dyn eval",
                "training_compute_(flop)": 3.4153451e+16,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 929000.0,
                "training_time_(hours)": 48.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2015-03-17T00:00:00",
                "link": "https://aclanthology.org/P15-1151/",
                "reference": "genCNN: A Convolutional Architecture for Word Sequence Prediction",
                "organization": "Chinese Academy of Sciences,Huawei Noah's Ark Lab,Dublin City University",
                "parameters": 8000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2015,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "genCNN + dyn eval - 3.42e+16 FLOPs"
            },
            {
                "model": "MSRA (C, PReLU)",
                "training_compute_(flop)": 2.397403008e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1280000.0,
                "training_time_(hours)": 588.0,
                "training_compute_cost_(2023_usd)": 1394.115708804708,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2015-02-06T00:00:00",
                "link": "https://arxiv.org/abs/1502.01852",
                "reference": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification",
                "organization": "Microsoft Research",
                "parameters": 87048800.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2015,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "MSRA (C, PReLU) - 2.40e+19 FLOPs"
            },
            {
                "model": "ADAM (CIFAR-10)",
                "training_compute_(flop)": 624979800000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 50000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2014-12-22T00:00:00",
                "link": "https://arxiv.org/abs/1412.6980",
                "reference": "Adam: A Method for Stochastic Optimization",
                "organization": "University of Amsterdam,OpenAI,University of Toronto",
                "parameters": 2370000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2014,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "ADAM (CIFAR-10) - 6.25e+14 FLOPs"
            },
            {
                "model": "Fractional Max-Pooling",
                "training_compute_(flop)": 1e+17,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 901200.0,
                "training_time_(hours)": 18.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2014-12-18T00:00:00",
                "link": "https://arxiv.org/abs/1412.6071v4",
                "reference": "Fractional Max-Pooling",
                "organization": "University of Warwick",
                "parameters": 27000000.0,
                "notable_model": true,
                "country": "United Kingdom",
                "model_accessibility": "Unknown",
                "year": 2014,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Fractional Max-Pooling - 1.00e+17 FLOPs"
            },
            {
                "model": "SNM-skip",
                "training_compute_(flop)": 2.97600000001e+20,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2014-12-03T00:00:00",
                "link": "https://arxiv.org/abs/1412.1454",
                "reference": "Skip-gram Language Modeling Using Sparse Non-negative Matrix Probability Estimation",
                "organization": "Google",
                "parameters": 62000000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2014,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "SNM-skip - 2.98e+20 FLOPs"
            },
            {
                "model": "TA-CNN",
                "training_compute_(flop)": 1.0854e+16,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 45000.0,
                "training_time_(hours)": 3.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2014-11-29T00:00:00",
                "link": "https://arxiv.org/abs/1412.0069",
                "reference": "Pedestrian Detection aided by Deep Learning Semantic Tasks",
                "organization": "Chinese University of Hong Kong (CUHK)",
                "parameters": 706048.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2014,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "TA-CNN - 1.09e+16 FLOPs"
            },
            {
                "model": "GoogLeNet / InceptionV1",
                "training_compute_(flop)": 1.51e+18,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 571392000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2014-09-17T00:00:00",
                "link": "https://arxiv.org/abs/1409.4842",
                "reference": "Going deeper with convolutions",
                "organization": "Google,University of Michigan,University of North Carolina",
                "parameters": 6797700.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2014,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "GoogLeNet / InceptionV1 - 1.51e+18 FLOPs"
            },
            {
                "model": "SPN-4+KN5",
                "training_compute_(flop)": 4.4e+16,
                "training_power_draw_(w)": 290.04546165358084,
                "training_dataset_size_(gradients)": 929000.0,
                "training_time_(hours)": 40.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Government",
                "publication_date": "2014-09-14T00:00:00",
                "link": "https://www.comp.nus.edu.sg/~skok/papers/is14.pdf",
                "reference": "Language modeling with sum-product networks",
                "organization": "Singapore University of Technology & Design,DSO National Laboratories",
                "parameters": 5000000.0,
                "notable_model": true,
                "country": "Singapore",
                "model_accessibility": "Unreleased",
                "year": 2014,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Language",
                "org_top5": "Other",
                "access_group": "Closed",
                "model_and_compute": "SPN-4+KN5 - 4.40e+16 FLOPs"
            },
            {
                "model": "Seq2Seq LSTM",
                "training_compute_(flop)": 5.6e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 870000000.0,
                "training_time_(hours)": 240.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2014-09-10T00:00:00",
                "link": "https://arxiv.org/abs/1409.3215",
                "reference": "Sequence to Sequence Learning with Neural Networks",
                "organization": "Google",
                "parameters": 1920000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2014,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Seq2Seq LSTM - 5.60e+19 FLOPs"
            },
            {
                "model": "Large regularized LSTM",
                "training_compute_(flop)": 4.2966069e+16,
                "training_power_draw_(w)": 264.24675821957226,
                "training_dataset_size_(gradients)": 929000.0,
                "training_time_(hours)": 24.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2014-09-08T00:00:00",
                "link": "https://arxiv.org/abs/1409.2329",
                "reference": "Recurrent Neural Network Regularization",
                "organization": "New York University (NYU),Google Brain",
                "parameters": 66000000.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2014,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "Large regularized LSTM - 4.30e+16 FLOPs"
            },
            {
                "model": "VGG16",
                "training_compute_(flop)": 1.2291e+19,
                "training_power_draw_(w)": 2137.653074766455,
                "training_dataset_size_(gradients)": 1300000.0,
                "training_time_(hours)": 504.0,
                "training_compute_cost_(2023_usd)": 239.2376655010467,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2014-09-04T00:00:00",
                "link": "https://arxiv.org/abs/1409.1556",
                "reference": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
                "organization": "University of Oxford",
                "parameters": 138000000.0,
                "notable_model": true,
                "country": "United Kingdom",
                "model_accessibility": "Unknown",
                "year": 2014,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "VGG16 - 1.23e+19 FLOPs"
            },
            {
                "model": "VGG19",
                "training_compute_(flop)": 1.1e+19,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1300000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2014-09-04T00:00:00",
                "link": "https://arxiv.org/abs/1409.1556",
                "reference": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
                "organization": "University of Oxford",
                "parameters": 144000000.0,
                "notable_model": true,
                "country": "United Kingdom",
                "model_accessibility": "Unknown",
                "year": 2014,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "VGG19 - 1.10e+19 FLOPs"
            },
            {
                "model": "RNNsearch-50*",
                "training_compute_(flop)": 1.5552e+18,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 174000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2014-09-01T00:00:00",
                "link": "https://arxiv.org/abs/1409.0473",
                "reference": "Neural Machine Translation by Jointly Learning to Align and Translate",
                "organization": "Jacobs University Bremen,University of Montreal / Universit\u00e9 de Montr\u00e9al",
                "parameters": null,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2014,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "RNNsearch-50* - 1.56e+18 FLOPs"
            },
            {
                "model": "ACF-WIDER",
                "training_compute_(flop)": 76380000000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 144448.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2014-07-15T00:00:00",
                "link": "https://arxiv.org/abs/1407.4023",
                "reference": "Aggregate channel features for multi-view face detection",
                "organization": "Chinese Academy of Sciences",
                "parameters": 6144.0,
                "notable_model": true,
                "country": "China",
                "model_accessibility": "Unreleased",
                "year": 2014,
                "era": "Deep learning era",
                "country_top8": "China",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "ACF-WIDER - 7.64e+13 FLOPs"
            },
            {
                "model": "SmooCT",
                "training_compute_(flop)": 6.9e+16,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 48.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Games",
                "organization_categorization": "Academia",
                "publication_date": "2014-07-01T00:00:00",
                "link": "https://www.semanticscholar.org/paper/Self-play-Monte-Carlo-tree-search-in-computer-poker-Heinrich-Silver/7b687599b4425aa959036071030e1212a3b359c7",
                "reference": "Self-Play Monte-Carlo Tree Search in Computer Poker",
                "organization": "University College London (UCL)",
                "parameters": null,
                "notable_model": true,
                "country": "United Kingdom",
                "model_accessibility": "Unknown",
                "year": 2014,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Other",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "SmooCT - 6.90e+16 FLOPs"
            },
            {
                "model": "SPPNet",
                "training_compute_(flop)": 3.411072e+18,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1280000.0,
                "training_time_(hours)": 672.0,
                "training_compute_cost_(2023_usd)": 51.65209223027738,
                "domain_group": "Vision",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2014-06-18T00:00:00",
                "link": "https://arxiv.org/abs/1406.4729",
                "reference": "Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition",
                "organization": "Microsoft,Xi\u2019an Jiaotong University,University of Science and Technology of China (USTC)",
                "parameters": null,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2014,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "SPPNet - 3.41e+18 FLOPs"
            },
            {
                "model": "GANs",
                "training_compute_(flop)": 5.184e+17,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 120000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2014-06-10T00:00:00",
                "link": "https://arxiv.org/abs/1406.2661",
                "reference": "Generative Adversarial Networks",
                "organization": "University of Montreal / Universit\u00e9 de Montr\u00e9al",
                "parameters": null,
                "notable_model": true,
                "country": "Canada",
                "model_accessibility": "Unknown",
                "year": 2014,
                "era": "Deep learning era",
                "country_top8": "Canada",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "GANs - 5.18e+17 FLOPs"
            },
            {
                "model": "Image generation",
                "training_compute_(flop)": 475200000000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 47040000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2013-12-20T00:00:00",
                "link": "https://arxiv.org/abs/1312.6114",
                "reference": "Auto-Encoding Variational Bayes",
                "organization": "University of Amsterdam",
                "parameters": 784000.0,
                "notable_model": true,
                "country": "Netherlands",
                "model_accessibility": "Unknown",
                "year": 2013,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Image generation - 4.75e+14 FLOPs"
            },
            {
                "model": "DQN",
                "training_compute_(flop)": 2846883840000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 160000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Games",
                "organization_categorization": "Industry",
                "publication_date": "2013-12-19T00:00:00",
                "link": "https://arxiv.org/abs/1312.5602",
                "reference": "Playing Atari with Deep Reinforcement Learning",
                "organization": "DeepMind",
                "parameters": 836096.0,
                "notable_model": true,
                "country": "United Kingdom",
                "model_accessibility": "Unknown",
                "year": 2013,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "DQN - 2.85e+15 FLOPs"
            },
            {
                "model": "TransE",
                "training_compute_(flop)": 1.340928e+18,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 17500000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 30.028097313317172,
                "domain_group": "Language",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2013-12-05T00:00:00",
                "link": "https://papers.nips.cc/paper/2013/hash/1cecc7a77928ca8133fa24680a88d2f9-Abstract.html",
                "reference": "Translating Embeddings for Modeling Multi- relational Data",
                "organization": "Universite de Technologie de Compi\u00e8gne \u2013 CNRS,Google",
                "parameters": 942000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2013,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "TransE - 1.34e+18 FLOPs"
            },
            {
                "model": "Visualizing CNNs",
                "training_compute_(flop)": 5.32e+17,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 7680000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": 12.994002368456531,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2013-11-12T00:00:00",
                "link": "https://arxiv.org/abs/1311.2901",
                "reference": "Visualizing and Understanding Convolutional Networks",
                "organization": "New York University (NYU)",
                "parameters": null,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 2013,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Visualizing CNNs - 5.32e+17 FLOPs"
            },
            {
                "model": "Word2Vec (large)",
                "training_compute_(flop)": 3.888e+16,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 330000000000.0,
                "training_time_(hours)": 24.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2013-10-16T00:00:00",
                "link": "https://arxiv.org/abs/1310.4546",
                "reference": "Distributed Representations of Words and Phrases and their Compositionality",
                "organization": "Google",
                "parameters": 692000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2013,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Word2Vec (large) - 3.89e+16 FLOPs"
            },
            {
                "model": "RNTN",
                "training_compute_(flop)": 1.422e+16,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 155063.0,
                "training_time_(hours)": 5.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2013-10-01T00:00:00",
                "link": "https://www.semanticscholar.org/paper/Recursive-Deep-Models-for-Semantic-Compositionality-Socher-Perelygin/687bac2d3320083eb4530bf18bb8f8f721477600",
                "reference": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
                "organization": "Stanford University",
                "parameters": null,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2013,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "RNTN - 1.42e+16 FLOPs"
            },
            {
                "model": "RCTM",
                "training_compute_(flop)": 9331200000000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 4500000.0,
                "training_time_(hours)": 15.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2013-10-01T00:00:00",
                "link": "https://www.semanticscholar.org/paper/Recurrent-Continuous-Translation-Models-Kalchbrenner-Blunsom/944a1cfd79dbfb6fef460360a0765ba790f4027a",
                "reference": "Recurrent Continuous Translation Models",
                "organization": "University of Oxford",
                "parameters": null,
                "notable_model": true,
                "country": "United Kingdom",
                "model_accessibility": "Unknown",
                "year": 2013,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "RCTM - 9.33e+15 FLOPs"
            },
            {
                "model": "Mitosis",
                "training_compute_(flop)": 1.37e+17,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1000000.0,
                "training_time_(hours)": 24.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Academia",
                "publication_date": "2013-09-22T00:00:00",
                "link": "https://link.springer.com/chapter/10.1007/978-3-642-40763-5_51",
                "reference": "Mitosis Detection in Breast Cancer Histology Images with Deep Neural Networks",
                "organization": "IDSIA",
                "parameters": 37230.0,
                "notable_model": false,
                "country": "Switzerland",
                "model_accessibility": "Unknown",
                "year": 2013,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Multimodal",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Mitosis - 1.37e+17 FLOPs"
            },
            {
                "model": "RNN+weight noise+dynamic eval",
                "training_compute_(flop)": 4210000000000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 929000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2013-08-04T00:00:00",
                "link": "https://arxiv.org/abs/1308.0850",
                "reference": "Generating Sequences With Recurrent Neural Networks",
                "organization": "University of Toronto",
                "parameters": 54000000.0,
                "notable_model": false,
                "country": "Canada",
                "model_accessibility": "Unreleased",
                "year": 2013,
                "era": "Deep learning era",
                "country_top8": "Canada",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "RNN+weight noise+dynamic eval - 4.21e+15 FLOPs"
            },
            {
                "model": "Hierarchical Scene Labeling (Stanford Background)",
                "training_compute_(flop)": 2.3774688e+17,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 71380800.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2013-08-01T00:00:00",
                "link": "https://ieeexplore.ieee.org/document/6338939",
                "reference": "Learning Hierarchical Features for Scene Labeling",
                "organization": "New York University (NYU)",
                "parameters": 51609600.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2013,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Hierarchical Scene Labeling (Stanford Background) - 2.38e+17 FLOPs"
            },
            {
                "model": "Fisher Vector image classifier",
                "training_compute_(flop)": 90842400000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 2.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2013-06-12T00:00:00",
                "link": "https://hal.inria.fr/hal-00830491v2/document",
                "reference": "Image Classification with the Fisher Vector: Theory and Practice",
                "organization": "Universidad Nacional de Cordoba,Inteligent Systems Lab Amsterdam,University of Amsterdam,LEAR Team,INRIA,Xerox Research Centre Europe (XRCE)",
                "parameters": null,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2013,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "Fisher Vector image classifier - 9.08e+13 FLOPs"
            },
            {
                "model": "ReLU-Speech",
                "training_compute_(flop)": 1.2773376e+17,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 168.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Audio",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2013-05-26T00:00:00",
                "link": "https://www.semanticscholar.org/paper/On-rectified-linear-units-for-speech-processing-Zeiler-Ranzato/64da1980714cfc130632c5b92b9d98c2f6763de6",
                "reference": "On rectified linear units for speech processing",
                "organization": "Google,University of Toronto,New York University (NYU)",
                "parameters": 101706240.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2013,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "ReLU-Speech - 1.28e+17 FLOPs"
            },
            {
                "model": "DistBelief NNLM",
                "training_compute_(flop)": 2.612736e+18,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 6000000000.0,
                "training_time_(hours)": 336.0,
                "training_compute_cost_(2023_usd)": 3255.18396377876,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2013-01-16T00:00:00",
                "link": "https://arxiv.org/abs/1301.3781",
                "reference": "Efficient Estimation of Word Representations in Vector Space",
                "organization": "Google",
                "parameters": null,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2013,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "DistBelief NNLM - 2.61e+18 FLOPs"
            },
            {
                "model": "DistBelief Speech",
                "training_compute_(flop)": 3.114e+17,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1100000000.0,
                "training_time_(hours)": 120.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Audio",
                "organization_categorization": "Industry",
                "publication_date": "2012-12-03T00:00:00",
                "link": "https://www.semanticscholar.org/paper/Large-Scale-Distributed-Deep-Networks-Dean-Corrado/3127190433230b3dc1abd0680bb58dced4bcd90e",
                "reference": "Large Scale Distributed Deep Networks",
                "organization": "Google",
                "parameters": 47185920.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2012,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "DistBelief Speech - 3.11e+17 FLOPs"
            },
            {
                "model": "DNN EM segmentation",
                "training_compute_(flop)": 4.78e+17,
                "training_power_draw_(w)": 2116.2977934480996,
                "training_dataset_size_(gradients)": 3000000.0,
                "training_time_(hours)": 17.0,
                "training_compute_cost_(2023_usd)": 3.9045853984413377,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2012-12-03T00:00:00",
                "link": "https://www.semanticscholar.org/paper/Deep-Neural-Networks-Segment-Neuronal-Membranes-in-Ciresan-Giusti/09193e19b59fc8f05bee9d6efbfb1607ca5b6501",
                "reference": "Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images",
                "organization": "IDSIA,SUPSI",
                "parameters": 218896.0,
                "notable_model": true,
                "country": "Switzerland",
                "model_accessibility": "Unreleased",
                "year": 2012,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "DNN EM segmentation - 4.78e+17 FLOPs"
            },
            {
                "model": "AlexNet",
                "training_compute_(flop)": 4.7e+17,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2457600000.0,
                "training_time_(hours)": 132.0,
                "training_compute_cost_(2023_usd)": 15.56975299843643,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2012-09-30T00:00:00",
                "link": "https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html",
                "reference": "ImageNet Classification with Deep Convolutional Neural Networks",
                "organization": "University of Toronto",
                "parameters": 60000000.0,
                "notable_model": true,
                "country": "Canada",
                "model_accessibility": "Unknown",
                "year": 2012,
                "era": "Deep learning era",
                "country_top8": "Canada",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "AlexNet - 4.70e+17 FLOPs"
            },
            {
                "model": "LSTM LM",
                "training_compute_(flop)": 1.66e+16,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 27000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2012-09-09T00:00:00",
                "link": "https://www.semanticscholar.org/paper/LSTM-Neural-Networks-for-Language-Modeling-Sundermeyer-Schl%C3%BCter/f9a1b3850dfd837793743565a8af95973d395a4e",
                "reference": "LSTM Neural Networks for Language Modeling",
                "organization": "RWTH Aachen University",
                "parameters": 102720000.0,
                "notable_model": true,
                "country": "Germany",
                "model_accessibility": "Unknown",
                "year": 2012,
                "era": "Deep learning era",
                "country_top8": "Germany",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "LSTM LM - 1.66e+16 FLOPs"
            },
            {
                "model": "Unsupervised High-level Feature Learner",
                "training_compute_(flop)": 6e+17,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1200000000000.0,
                "training_time_(hours)": 72.0,
                "training_compute_cost_(2023_usd)": 16.029257461780738,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2012-07-12T00:00:00",
                "link": "https://arxiv.org/abs/1112.6209",
                "reference": "Building High-level Features Using Large Scale Unsupervised Learning",
                "organization": "Google",
                "parameters": 1000000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2012,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Unsupervised High-level Feature Learner - 6.00e+17 FLOPs"
            },
            {
                "model": "LBL",
                "training_compute_(flop)": 501999999999999.94,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 929000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2012-06-27T00:00:00",
                "link": "https://arxiv.org/abs/1206.6426",
                "reference": "A Fast and Simple Algorithm for Training Neural Probabilistic Language Models",
                "organization": "University College London (UCL)",
                "parameters": 2000000.0,
                "notable_model": false,
                "country": "United Kingdom",
                "model_accessibility": "Unreleased",
                "year": 2012,
                "era": "Deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "LBL - 5.02e+14 FLOPs"
            },
            {
                "model": "Dropout (ImageNet)",
                "training_compute_(flop)": 2.731968e+17,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2600000.0,
                "training_time_(hours)": 96.0,
                "training_compute_cost_(2023_usd)": 7.952555832759744,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2012-06-03T00:00:00",
                "link": "https://arxiv.org/abs/1207.0580",
                "reference": "Improving neural networks by preventing co-adaptation of feature detectors",
                "organization": "University of Toronto",
                "parameters": null,
                "notable_model": true,
                "country": "Canada",
                "model_accessibility": "Unreleased",
                "year": 2012,
                "era": "Deep learning era",
                "country_top8": "Canada",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Dropout (ImageNet) - 2.73e+17 FLOPs"
            },
            {
                "model": "Dropout (CIFAR)",
                "training_compute_(flop)": 4268700000000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 60000.0,
                "training_time_(hours)": 1.5,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2012-06-03T00:00:00",
                "link": "https://arxiv.org/abs/1207.0580",
                "reference": "Improving neural networks by preventing co-adaptation of feature detectors",
                "organization": "University of Toronto",
                "parameters": null,
                "notable_model": true,
                "country": "Canada",
                "model_accessibility": "Unreleased",
                "year": 2012,
                "era": "Deep learning era",
                "country_top8": "Canada",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Dropout (CIFAR) - 4.27e+15 FLOPs"
            },
            {
                "model": "Dropout (MNIST)",
                "training_compute_(flop)": 6039370800000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 60000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2012-06-03T00:00:00",
                "link": "https://arxiv.org/abs/1207.0580",
                "reference": "Improving neural networks by preventing co-adaptation of feature detectors",
                "organization": "University of Toronto",
                "parameters": 5594010.0,
                "notable_model": true,
                "country": "Canada",
                "model_accessibility": "Unreleased",
                "year": 2012,
                "era": "Deep learning era",
                "country_top8": "Canada",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Dropout (MNIST) - 6.04e+15 FLOPs"
            },
            {
                "model": "MCDNN (MNIST)",
                "training_compute_(flop)": 1.57e+16,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2100000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2012-02-13T00:00:00",
                "link": "https://arxiv.org/abs/1202.2745v1",
                "reference": "Multi-column Deep Neural Networks for Image Classification",
                "organization": "IDSIA,SUPSI",
                "parameters": 2653700.0,
                "notable_model": false,
                "country": "Switzerland",
                "model_accessibility": "Unreleased",
                "year": 2012,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "MCDNN (MNIST) - 1.57e+16 FLOPs"
            },
            {
                "model": "CNN committee (traffic sign)",
                "training_compute_(flop)": 991981425600000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 53280.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2011-10-03T00:00:00",
                "link": "https://www.semanticscholar.org/paper/A-committee-of-neural-networks-for-traffic-sign-Ciresan-Meier/dd7f8b53e6802787179a961e766760cbbe2d5011",
                "reference": "A committee of neural networks for traffic sign classification",
                "organization": "IDSIA",
                "parameters": 1388800.0,
                "notable_model": true,
                "country": "Switzerland",
                "model_accessibility": "Unreleased",
                "year": 2011,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "CNN committee (traffic sign) - 9.92e+14 FLOPs"
            },
            {
                "model": "CNN Committee (NIST)",
                "training_compute_(flop)": 2.6e+16,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 3380475.0,
                "training_time_(hours)": 42.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2011-09-18T00:00:00",
                "link": "https://www.semanticscholar.org/paper/Convolutional-Neural-Network-Committees-for-Ciresan-Meier/260a7615bbffec052d67dffde5bcf9b4687b50ee",
                "reference": "Convolutional Neural Network Committees for Handwritten Character Classification",
                "organization": "IDSIA",
                "parameters": 128420.0,
                "notable_model": true,
                "country": "Switzerland",
                "model_accessibility": "Unreleased",
                "year": 2011,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "CNN Committee (NIST) - 2.60e+16 FLOPs"
            },
            {
                "model": "CNN Committee (MNIST)",
                "training_compute_(flop)": 5.2e+16,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 420000.0,
                "training_time_(hours)": 98.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2011-09-18T00:00:00",
                "link": "https://www.semanticscholar.org/paper/Convolutional-Neural-Network-Committees-for-Ciresan-Meier/260a7615bbffec052d67dffde5bcf9b4687b50ee",
                "reference": "Convolutional Neural Network Committees for Handwritten Character Classification",
                "organization": "IDSIA",
                "parameters": 120620.0,
                "notable_model": true,
                "country": "Switzerland",
                "model_accessibility": "Unreleased",
                "year": 2011,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "CNN Committee (MNIST) - 5.20e+16 FLOPs"
            },
            {
                "model": "High Performance CNN (NORB)",
                "training_compute_(flop)": 2.57985e+16,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 50000.0,
                "training_time_(hours)": 4.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2011-07-16T00:00:00",
                "link": "https://people.idsia.ch/~juergen/ijcai2011.pdf",
                "reference": "Flexible, High Performance Convolutional Neural Networks for Image Classification",
                "organization": "IDSIA,SUPSI",
                "parameters": 4878300.0,
                "notable_model": true,
                "country": "Switzerland",
                "model_accessibility": "Unreleased",
                "year": 2011,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "High Performance CNN (NORB) - 2.58e+16 FLOPs"
            },
            {
                "model": "Deep Autoencoders",
                "training_compute_(flop)": 3.672864e+16,
                "training_power_draw_(w)": 246.22598104391827,
                "training_dataset_size_(gradients)": 4915200000.0,
                "training_time_(hours)": 48.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2011-04-29T00:00:00",
                "link": "https://www.semanticscholar.org/paper/Using-very-deep-autoencoders-for-content-based-Krizhevsky-Hinton/88080d28536f36588740737f3b7a1f6c1a409654",
                "reference": "Using very deep autoencoders for content-based image retrieval",
                "organization": "University of Toronto",
                "parameters": 139808256.0,
                "notable_model": true,
                "country": "Canada",
                "model_accessibility": "Unknown",
                "year": 2011,
                "era": "Deep learning era",
                "country_top8": "Canada",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Deep Autoencoders - 3.67e+16 FLOPs"
            },
            {
                "model": "KN5 LM + RNN 400/10 (WSJ)",
                "training_compute_(flop)": 1.7e+16,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 6400000.0,
                "training_time_(hours)": 504.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Audio",
                "organization_categorization": "Academia",
                "publication_date": "2010-09-26T00:00:00",
                "link": "https://www.isca-archive.org/interspeech_2010/mikolov10_interspeech.html",
                "reference": "Recurrent neural network based language model",
                "organization": "Brno University of Technology,Johns Hopkins University",
                "parameters": 22160000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2010,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "KN5 LM + RNN 400/10 (WSJ) - 1.70e+16 FLOPs"
            },
            {
                "model": "RNN 500/10 + RT09 LM (NIST RT05)",
                "training_compute_(flop)": 1.25e+16,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 5400000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Audio",
                "organization_categorization": "Academia",
                "publication_date": "2010-09-26T00:00:00",
                "link": "https://www.isca-archive.org/interspeech_2010/mikolov10_interspeech.html",
                "reference": "Recurrent neural network based language model",
                "organization": "Brno University of Technology,Johns Hopkins University",
                "parameters": 19250000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2010,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "RNN 500/10 + RT09 LM (NIST RT05) - 1.25e+16 FLOPs"
            },
            {
                "model": "RNN LM",
                "training_compute_(flop)": 5.396e+16,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 6400000.0,
                "training_time_(hours)": 504.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2010-09-26T00:00:00",
                "link": "https://www.semanticscholar.org/paper/Recurrent-neural-network-based-language-model-Mikolov-Karafi%C3%A1t/9819b600a828a57e1cde047bbe710d3446b30da5",
                "reference": "Recurrent neural network based language model",
                "organization": "Johns Hopkins University",
                "parameters": 70265000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 2010,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "RNN LM - 5.40e+16 FLOPs"
            },
            {
                "model": "RNN 1000/5 + RT09 LM (NIST RT05)",
                "training_compute_(flop)": 5e+16,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 5400000.0,
                "training_time_(hours)": 1200.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Audio",
                "organization_categorization": "Academia",
                "publication_date": "2010-09-26T00:00:00",
                "link": "https://www.isca-archive.org/interspeech_2010/mikolov10_interspeech.html",
                "reference": "Recurrent neural network based language model",
                "organization": "Brno University of Technology,Johns Hopkins University",
                "parameters": 77039000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2010,
                "era": "Deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "RNN 1000/5 + RT09 LM (NIST RT05) - 5.00e+16 FLOPs"
            },
            {
                "model": "Pooling CNN (Caltech 101)",
                "training_compute_(flop)": 1221124128768000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 3060.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2010-09-15T00:00:00",
                "link": "https://www.semanticscholar.org/paper/Evaluation-of-Pooling-Operations-in-Convolutional-Scherer-M%C3%BCller/5d21006fa32ff69f6b0a646f26ce0db84f2f4d33",
                "reference": "Evaluation of Pooling Operations in Convolutional Architectures for Object Recognition",
                "organization": "University of Bonn",
                "parameters": 294912.0,
                "notable_model": true,
                "country": "Germany",
                "model_accessibility": "Unreleased",
                "year": 2010,
                "era": "Deep learning era",
                "country_top8": "Germany",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Pooling CNN (Caltech 101) - 1.22e+15 FLOPs"
            },
            {
                "model": "Pooling CNN (NORB)",
                "training_compute_(flop)": 1456277227200000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 24300.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2010-09-15T00:00:00",
                "link": "https://www.semanticscholar.org/paper/Evaluation-of-Pooling-Operations-in-Convolutional-Scherer-M%C3%BCller/5d21006fa32ff69f6b0a646f26ce0db84f2f4d33",
                "reference": "Evaluation of Pooling Operations in Convolutional Architectures for Object Recognition",
                "organization": "University of Bonn",
                "parameters": 268664.0,
                "notable_model": true,
                "country": "Germany",
                "model_accessibility": "Unreleased",
                "year": 2010,
                "era": "Deep learning era",
                "country_top8": "Germany",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Pooling CNN (NORB) - 1.46e+15 FLOPs"
            },
            {
                "model": "iCCCP",
                "training_compute_(flop)": 1080000000000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 10000.0,
                "training_time_(hours)": 25.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2010-06-13T00:00:00",
                "link": "https://ieeexplore.ieee.org/document/5540096",
                "reference": "Latent hierarchical structural learning for object detection",
                "organization": "Massachusetts Institute of Technology (MIT)",
                "parameters": null,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2010,
                "era": "Deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "iCCCP - 1.08e+15 FLOPs"
            },
            {
                "model": "Feedforward NN",
                "training_compute_(flop)": 350000000000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 90000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2010-05-13T00:00:00",
                "link": "https://proceedings.mlr.press/v9/glorot10a.html",
                "reference": "Understanding the difficulty of training deep feedforward neural networks",
                "organization": "University of Montreal / Universit\u00e9 de Montr\u00e9al",
                "parameters": 7082000.0,
                "notable_model": true,
                "country": "Canada",
                "model_accessibility": "Unknown",
                "year": 2010,
                "era": "Deep learning era",
                "country_top8": "Canada",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Feedforward NN - 3.50e+14 FLOPs"
            },
            {
                "model": "6-layer MLP (MNIST)",
                "training_compute_(flop)": 130788000000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 60000.0,
                "training_time_(hours)": 2.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2010-03-01T00:00:00",
                "link": "https://arxiv.org/abs/1003.0358",
                "reference": "Deep Big Simple Neural Nets Excel on Handwritten Digit Recognition",
                "organization": "IDSIA,University of Lugano,SUPSI",
                "parameters": 12110000.0,
                "notable_model": false,
                "country": "Switzerland",
                "model_accessibility": "Unknown",
                "year": 2010,
                "era": "Deep learning era",
                "country_top8": "Other",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "6-layer MLP (MNIST) - 1.31e+14 FLOPs"
            },
            {
                "model": "LCNP LabelMe",
                "training_compute_(flop)": 3295150080000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 40000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2009-11-22T00:00:00",
                "link": "https://ieeexplore.ieee.org/document/5357786",
                "reference": "Large-scale object recognition with CUDA-accelerated hierarchical neural networks\n",
                "organization": "University of Bonn",
                "parameters": 13729792.0,
                "notable_model": true,
                "country": "Germany",
                "model_accessibility": "Unknown",
                "year": 2009,
                "era": "Pre deep learning era",
                "country_top8": "Germany",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "LCNP LabelMe - 3.30e+15 FLOPs"
            },
            {
                "model": "Two Stage Feature Extraction (MNIST)",
                "training_compute_(flop)": 20754000000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 50000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2009-09-01T00:00:00",
                "link": "https://www.semanticscholar.org/paper/What-is-the-best-multi-stage-architecture-for-Jarrett-Kavukcuoglu/1f88427d7aa8225e47f946ac41a0667d7b69ac52",
                "reference": "What is the best multi-stage architecture for object recognition?",
                "organization": "New York University (NYU)",
                "parameters": 258800.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2009,
                "era": "Pre deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Two Stage Feature Extraction (MNIST) - 2.08e+13 FLOPs"
            },
            {
                "model": "ConvNet Processor",
                "training_compute_(flop)": 306000000000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 30000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2009-08-31T00:00:00",
                "link": "https://ieeexplore.ieee.org/abstract/document/5272559/",
                "reference": "CNP: An FPGA-based processor for convolutional networks",
                "organization": "Courant Institute of Mathematical Sciences",
                "parameters": 14423.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2009,
                "era": "Pre deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "ConvNet Processor - 3.06e+14 FLOPs"
            },
            {
                "model": "GPU DBNs",
                "training_compute_(flop)": 1000000000000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 121344000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": null,
                "organization_categorization": "Academia",
                "publication_date": "2009-06-15T00:00:00",
                "link": "https://dl.acm.org/doi/abs/10.1145/1553374.1553486?utm_campaign=The+Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-95Z7-X4Dl-RJK6gYKvjyDIrYaGhBeqWoc0ldqyPEKni0Ip6UE7452hr-ygY52z00LBpYgM",
                "reference": "Large-scale Deep Unsupervised Learning using Graphics Processors",
                "organization": "Stanford University",
                "parameters": 100000000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 2009,
                "era": "Pre deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "GPU DBNs - 1.00e+15 FLOPs"
            },
            {
                "model": "Long-Range Autonomous Off-Road Driving System",
                "training_compute_(flop)": 278721000000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 450000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Multimodal",
                "organization_categorization": "Academia",
                "publication_date": "2009-01-08T00:00:00",
                "link": "https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.20276",
                "reference": "Learning long\u2010range vision for autonomous off\u2010road driving",
                "organization": "Courant Institute of Mathematical Sciences",
                "parameters": 12410.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2009,
                "era": "Pre deep learning era",
                "country_top8": "United States",
                "domain_top4": "Multimodal",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Long-Range Autonomous Off-Road Driving System - 2.79e+14 FLOPs"
            },
            {
                "model": "GNN",
                "training_compute_(flop)": 1614600000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 207.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": null,
                "organization_categorization": "Academia",
                "publication_date": "2008-12-09T00:00:00",
                "link": "https://ieeexplore.ieee.org/document/4700287",
                "reference": "The Graph Neural Network Model",
                "organization": "University of Siena",
                "parameters": 30.0,
                "notable_model": true,
                "country": "Italy",
                "model_accessibility": "Open weights (unrestricted)",
                "year": 2008,
                "era": "Pre deep learning era",
                "country_top8": "Other",
                "domain_top4": "Other",
                "org_top5": "Academia",
                "access_group": "Open",
                "model_and_compute": "GNN - 1.61e+09 FLOPs"
            },
            {
                "model": "SB-LM",
                "training_compute_(flop)": 1.4494464e+18,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 1800000000000.0,
                "training_time_(hours)": 24.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2007-06-22T00:00:00",
                "link": "https://www.semanticscholar.org/paper/Large-Language-Models-in-Machine-Translation-Brants-Popat/ba786c46373892554b98df42df7af6f5da343c9d",
                "reference": "Large Language Models in Machine Translation",
                "organization": "Google",
                "parameters": 300000000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2007,
                "era": "Pre deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "SB-LM - 1.45e+18 FLOPs"
            },
            {
                "model": "KN-LM",
                "training_compute_(flop)": 7.7303808e+17,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 31000000000.0,
                "training_time_(hours)": 48.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Industry",
                "publication_date": "2007-06-22T00:00:00",
                "link": "https://www.semanticscholar.org/paper/Large-Language-Models-in-Machine-Translation-Brants-Popat/ba786c46373892554b98df42df7af6f5da343c9d",
                "reference": "Large Language Models in Machine Translation",
                "organization": "Google",
                "parameters": 21000000000.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2007,
                "era": "Pre deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Language",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "KN-LM - 7.73e+17 FLOPs"
            },
            {
                "model": "Sparse Vision Encoding",
                "training_compute_(flop)": 9604800000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 10.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2006-11-01T00:00:00",
                "link": "https://proceedings.neurips.cc/paper_files/paper/2006/file/2d71b2ae158c7c5912cc0bbde2bb9d95-Paper.pdf",
                "reference": "Efficient sparse coding algorithms",
                "organization": "Stanford University",
                "parameters": null,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2006,
                "era": "Pre deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Sparse Vision Encoding - 9.60e+12 FLOPs"
            },
            {
                "model": "Hybrid CNN/SVM Object Categorizer",
                "training_compute_(flop)": 97977600000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 291600.0,
                "training_time_(hours)": 51.75,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2006-06-17T00:00:00",
                "link": "https://ieeexplore.ieee.org/document/1640771/",
                "reference": "Large-scale learning with svm and convolutional nets for generic object categorization",
                "organization": "Courant Institute of Mathematical Sciences",
                "parameters": 3590057.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2006,
                "era": "Pre deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Hybrid CNN/SVM Object Categorizer - 9.80e+13 FLOPs"
            },
            {
                "model": "SVM-CNN",
                "training_compute_(flop)": 745200000000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 583200.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2006-06-17T00:00:00",
                "link": "https://www.semanticscholar.org/paper/Large-scale-Learning-with-SVM-and-Convolutional-for-Huang-LeCun/cf03fdf52dd6e4249cbbdbd0bffbbbe5ca389feb",
                "reference": "Large-scale Learning with SVM and Convolutional for Generic Object Categorization",
                "organization": "New York University (NYU)",
                "parameters": 90857.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2006,
                "era": "Pre deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "SVM-CNN - 7.45e+14 FLOPs"
            },
            {
                "model": "RankNet",
                "training_compute_(flop)": 3482081588304.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 3464289.0,
                "training_time_(hours)": 5.85,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Search",
                "organization_categorization": "Industry",
                "publication_date": "2005-08-07T00:00:00",
                "link": "https://dl.acm.org/doi/abs/10.1145/1102351.1102363",
                "reference": "Learning to rank using gradient descent",
                "organization": "Microsoft Research,Microsoft",
                "parameters": 5711.0,
                "notable_model": true,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 2005,
                "era": "Pre deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "RankNet - 3.48e+12 FLOPs"
            },
            {
                "model": "BiLSTM for Speech",
                "training_compute_(flop)": 24124575958774.88,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Audio",
                "organization_categorization": "Academia",
                "publication_date": "2005-08-01T00:00:00",
                "link": "https://www.sciencedirect.com/science/article/abs/pii/S0893608005001206",
                "reference": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures",
                "organization": "IDSIA,Technical University of Munich",
                "parameters": 152061.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unknown",
                "year": 2005,
                "era": "Pre deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Other",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "BiLSTM for Speech - 2.41e+13 FLOPs"
            },
            {
                "model": "Synergistic Face Detector",
                "training_compute_(flop)": 52603200000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 26.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia, Industry",
                "publication_date": "2004-12-01T00:00:00",
                "link": "https://proceedings.neurips.cc/paper/2004/hash/06c284d3f757b15c02f47f3ff06dc275-Abstract.html",
                "reference": "Synergistic Face Detection and Pose Estimation with Energy-Based Models",
                "organization": "NEC Laboratories,Courant Institute of Mathematical Sciences",
                "parameters": 16636.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unreleased",
                "year": 2004,
                "era": "Pre deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Academia, Industry",
                "access_group": "Closed",
                "model_and_compute": "Synergistic Face Detector - 5.26e+13 FLOPs"
            },
            {
                "model": "Invariant CNN",
                "training_compute_(flop)": 974230000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 24300.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2004-06-27T00:00:00",
                "link": "https://www.semanticscholar.org/paper/Learning-methods-for-generic-object-recognition-to-LeCun-Huang/f354310098e09c1e1dc88758fca36767fd9d084d",
                "reference": "Learning methods for generic object recognition with invariance to pose and lighting",
                "organization": "New York University (NYU)",
                "parameters": 90575.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 2004,
                "era": "Pre deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Invariant CNN - 9.74e+11 FLOPs"
            },
            {
                "model": "NPLM (Brown)",
                "training_compute_(flop)": 132076260000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 13994528.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2003-03-15T00:00:00",
                "link": "https://dl.acm.org/doi/10.5555/944919.944966",
                "reference": "A Neural Probabilistic Language Model",
                "organization": "University of Montreal / Universit\u00e9 de Montr\u00e9al",
                "parameters": 4124233.0,
                "notable_model": true,
                "country": "Canada",
                "model_accessibility": "Unknown",
                "year": 2003,
                "era": "Pre deep learning era",
                "country_top8": "Canada",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "NPLM (Brown) - 1.32e+14 FLOPs"
            },
            {
                "model": "NPLM (AP News)",
                "training_compute_(flop)": 1666869200000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 13994528.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2003-03-15T00:00:00",
                "link": "https://dl.acm.org/doi/10.5555/944919.944966",
                "reference": "A Neural Probabilistic Language Model",
                "organization": "University of Montreal / Universit\u00e9 de Montr\u00e9al",
                "parameters": 11904264.0,
                "notable_model": true,
                "country": "Canada",
                "model_accessibility": "Unknown",
                "year": 2003,
                "era": "Pre deep learning era",
                "country_top8": "Canada",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "NPLM (AP News) - 1.67e+15 FLOPs"
            },
            {
                "model": "Decision tree (classification)",
                "training_compute_(flop)": 63000000000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 753616.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "2001-12-08T00:00:00",
                "link": "https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/viola-cvpr-01.pdf",
                "reference": "Rapid object detection using a boosted cascade of simple features",
                "organization": "Mitsubishi Electric Research Labs,Compaq CRL",
                "parameters": 12000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 2001,
                "era": "Pre deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Decision tree (classification) - 6.30e+13 FLOPs"
            },
            {
                "model": "PoE MNIST",
                "training_compute_(flop)": 51810000000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 54000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "2000-11-28T00:00:00",
                "link": "https://proceedings.neurips.cc/paper_files/paper/2000/file/1f1baa5b8edac74eb4eaa329f14a0361-Paper.pdf",
                "reference": "Recognizing Hand-written Digits Using Hierarchical Products of Experts",
                "organization": "University College London (UCL)",
                "parameters": 3925310.0,
                "notable_model": true,
                "country": "United Kingdom",
                "model_accessibility": "Unknown",
                "year": 2000,
                "era": "Pre deep learning era",
                "country_top8": "United Kingdom",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "PoE MNIST - 5.18e+13 FLOPs"
            },
            {
                "model": "Neural LM",
                "training_compute_(flop)": 6339000000000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 32000000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "2000-11-28T00:00:00",
                "link": "https://papers.nips.cc/paper_files/paper/2000/file/728f206c2a01bf572b5940d7d9a8fa4c-Paper.pdf",
                "reference": "A Neural Probabilistic Language Model",
                "organization": "University of Montreal / Universit\u00e9 de Montr\u00e9al",
                "parameters": 6906980.0,
                "notable_model": true,
                "country": "Canada",
                "model_accessibility": "Unknown",
                "year": 2000,
                "era": "Pre deep learning era",
                "country_top8": "Canada",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Neural LM - 6.34e+15 FLOPs"
            },
            {
                "model": "Credibilty Network",
                "training_compute_(flop)": 5443200.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2800.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "1999-07-01T00:00:00",
                "link": "https://proceedings.neurips.cc/paper_files/paper/1999/file/5a142a55461d5fef016acfb927fee0bd-Paper.pdf",
                "reference": "Learning to Parse Images",
                "organization": "University College London (UCL),University of Toronto",
                "parameters": 324.0,
                "notable_model": false,
                "country": "Multinational",
                "model_accessibility": "Unreleased",
                "year": 1999,
                "era": "Pre deep learning era",
                "country_top8": "Multinational",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Credibilty Network - 5.44e+06 FLOPs"
            },
            {
                "model": "LeNet-5",
                "training_compute_(flop)": 2810937600000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 60000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "1998-11-01T00:00:00",
                "link": "http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf",
                "reference": "Gradient-based Learning Applied to Document Recognition",
                "organization": "AT&T",
                "parameters": 60000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 1998,
                "era": "Pre deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "LeNet-5 - 2.81e+12 FLOPs"
            },
            {
                "model": "LSTM",
                "training_compute_(flop)": 31512000000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 853000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "1997-11-15T00:00:00",
                "link": "https://direct.mit.edu/neco/article-abstract/9/8/1735/6109/Long-Short-Term-Memory?redirectedFrom=fulltext",
                "reference": "Long short-term memory",
                "organization": "Technical University of Munich",
                "parameters": 10504.0,
                "notable_model": true,
                "country": "Germany",
                "model_accessibility": "Unknown",
                "year": 1997,
                "era": "Pre deep learning era",
                "country_top8": "Germany",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "LSTM - 3.15e+13 FLOPs"
            },
            {
                "model": "System 11",
                "training_compute_(flop)": 25859616000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 23750.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "1996-06-18T00:00:00",
                "link": "https://ieeexplore.ieee.org/document/655647",
                "reference": "Neural Network-Based Face Detection",
                "organization": "Carnegie Mellon University (CMU)",
                "parameters": 6452.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 1996,
                "era": "Pre deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "System 11 - 2.59e+10 FLOPs"
            },
            {
                "model": "LISSOM",
                "training_compute_(flop)": 195544800000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 2000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "1995-11-27T00:00:00",
                "link": "https://www.semanticscholar.org/paper/Laterally-Interconnected-Self-Organizing-Maps-in-Choe-Sirosh/785f5facc76538ceba6f6b9e2d7b641d322e9854",
                "reference": "Laterally Interconnected Self-Organizing Maps in Hand-Written Digit Recognition",
                "organization": "University of Texas at Austin",
                "parameters": 432800.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 1995,
                "era": "Pre deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "LISSOM - 1.96e+11 FLOPs"
            },
            {
                "model": "Predictive Coding NN",
                "training_compute_(flop)": 18621900000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 600000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "1994-12-02T00:00:00",
                "link": "https://proceedings.neurips.cc/paper/1994/file/5705e1164a8394aace6018e27d20d237-Paper.pdf",
                "reference": "Predictive Coding with Neural Nets: Application to Text Compression",
                "organization": "Technical University of Munich",
                "parameters": 206910.0,
                "notable_model": true,
                "country": "Germany",
                "model_accessibility": "Unknown",
                "year": 1994,
                "era": "Pre deep learning era",
                "country_top8": "Germany",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Predictive Coding NN - 1.86e+13 FLOPs"
            },
            {
                "model": "Ceramic-MLP",
                "training_compute_(flop)": 4531200000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 80.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Materials science",
                "organization_categorization": "Academia",
                "publication_date": "1994-01-07T00:00:00",
                "link": "https://www.sciencedirect.com/science/article/abs/pii/S0921883108605506",
                "reference": "Ceramic powder characterization by multilayer perceptron (MLP) data compression and classification",
                "organization": "Sapienza Universit\u00e0 di Roma",
                "parameters": 1888.0,
                "notable_model": true,
                "country": "Italy",
                "model_accessibility": "Unknown",
                "year": 1994,
                "era": "Pre deep learning era",
                "country_top8": "Other",
                "domain_top4": "Other",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Ceramic-MLP - 4.53e+09 FLOPs"
            },
            {
                "model": "Siamese-TDNN",
                "training_compute_(flop)": 12869570138112.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 7701.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "1993-08-01T00:00:00",
                "link": "https://www.semanticscholar.org/paper/Signature-Verification-Using-A-%22Siamese%22-Time-Delay-Bromley-Bentz/997dc5d9a058753f034422afe7bd0cc0b8ad808b",
                "reference": "Signature Verification using a \"Siamese\" Time Delay Neural Network",
                "organization": "Bell Laboratories",
                "parameters": 744.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 1993,
                "era": "Pre deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Siamese-TDNN - 1.29e+13 FLOPs"
            },
            {
                "model": "Cancer drug mechanism prediction",
                "training_compute_(flop)": 53460000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 141.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Medicine",
                "organization_categorization": "Government",
                "publication_date": "1992-10-16T00:00:00",
                "link": "https://pubmed.ncbi.nlm.nih.gov/1411538/",
                "reference": "Neural computing in cancer drug development: predicting mechanism of action",
                "organization": "National Cancer Institute",
                "parameters": 594.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 1992,
                "era": "Pre deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Government",
                "access_group": "Closed",
                "model_and_compute": "Cancer drug mechanism prediction - 5.35e+07 FLOPs"
            },
            {
                "model": "Fuzzy NN",
                "training_compute_(flop)": 1403117760.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 436.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Audio",
                "organization_categorization": "Academia",
                "publication_date": "1992-09-01T00:00:00",
                "link": "https://ieeexplore.ieee.org/document/159058",
                "reference": "Multilayer perceptron, fuzzy sets, and classification",
                "organization": "Indian Statistical Institute",
                "parameters": 1166.0,
                "notable_model": false,
                "country": "India",
                "model_accessibility": "Unknown",
                "year": 1992,
                "era": "Pre deep learning era",
                "country_top8": "Other",
                "domain_top4": "Other",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Fuzzy NN - 1.40e+09 FLOPs"
            },
            {
                "model": "TD-Gammon",
                "training_compute_(flop)": 18232157622832.703,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 6300000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Games",
                "organization_categorization": "Industry",
                "publication_date": "1992-05-01T00:00:00",
                "link": "https://papers.nips.cc/paper/1991/file/68ce199ec2c5517597ce0a4d89620f55-Paper.pdf",
                "reference": "Practical Issues in Temporal Difference Learning",
                "organization": "IBM",
                "parameters": 25000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 1992,
                "era": "Pre deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "TD-Gammon - 1.82e+13 FLOPs"
            },
            {
                "model": "NETtalk reimplementation",
                "training_compute_(flop)": 35811936000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 7242.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Audio",
                "organization_categorization": "Academia",
                "publication_date": "1990-06-01T00:00:00",
                "link": "https://www.sciencedirect.com/science/article/abs/pii/B9781558601413500079",
                "reference": "A Comparative Study of ID3 and Backpropagation for English Text-to-speech Mapping",
                "organization": "Oregon State University",
                "parameters": 27480.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 1990,
                "era": "Pre deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "NETtalk reimplementation - 3.58e+10 FLOPs"
            },
            {
                "model": "Zip CNN",
                "training_compute_(flop)": 1496338054440.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 7291.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "1989-12-01T00:00:00",
                "link": "https://ieeexplore.ieee.org/document/6795724",
                "reference": "Backpropagation applied to handwritten zip code recognition",
                "organization": "AT&T,Bell Laboratories",
                "parameters": 9760.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 1989,
                "era": "Pre deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Zip CNN - 1.50e+12 FLOPs"
            },
            {
                "model": "Innervator",
                "training_compute_(flop)": 120000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 10240.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Mathematics",
                "organization_categorization": "Academia",
                "publication_date": "1989-12-01T00:00:00",
                "link": "https://www.researchgate.net/publication/220885651_Designing_Neural_Networks_using_Genetic_Algorithms",
                "reference": "Designing neural networks using genetic algorithms",
                "organization": "Stanford University,California Institute of Technology",
                "parameters": 10.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 1989,
                "era": "Pre deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Innervator - 1.20e+08 FLOPs"
            },
            {
                "model": "ALVINN",
                "training_compute_(flop)": 10548576000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 55200.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Driving",
                "organization_categorization": "Academia",
                "publication_date": "1989-12-01T00:00:00",
                "link": "https://proceedings.neurips.cc/paper/1988/hash/812b4ba287f5ee0bc9d43bbf5bbe87fb-Abstract.html",
                "reference": "ALVINN: an autonomous land vehicle in a neural network",
                "organization": "Carnegie Mellon University (CMU)",
                "parameters": 36627.0,
                "notable_model": false,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 1989,
                "era": "Pre deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "ALVINN - 1.05e+10 FLOPs"
            },
            {
                "model": "Speaker-independent vowel classification",
                "training_compute_(flop)": 7485696000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 4104.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Audio",
                "organization_categorization": "Academia",
                "publication_date": "1989-11-27T00:00:00",
                "link": "https://www.semanticscholar.org/paper/Performance-Comparisons-Between-Backpropagation-and-Atlas-Cole/e42d2b89fcb4a1a3dfa63408f424f76975ed1e1b",
                "reference": "Performance Comparisons Between Backpropagation Networks and Classification Trees on Three Real-World Applications",
                "organization": "University of Washington",
                "parameters": 3040.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 1989,
                "era": "Pre deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Speaker-independent vowel classification - 7.49e+09 FLOPs"
            },
            {
                "model": "Handwritten Digit Recognition System",
                "training_compute_(flop)": 181440000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 9840.0,
                "training_time_(hours)": 72.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "1989-11-27T00:00:00",
                "link": "https://www.semanticscholar.org/paper/Handwritten-Digit-Recognition-with-a-Network-LeCun-Boser/86ab4cae682fbd49c5a5bedb630e5a40fa7529f6",
                "reference": "Handwritten Digit Recognition with a Back-Propagation Network",
                "organization": "AT&T",
                "parameters": 2578.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 1989,
                "era": "Pre deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Handwritten Digit Recognition System - 1.81e+11 FLOPs"
            },
            {
                "model": "Invariant image recognition",
                "training_compute_(flop)": 27000000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 6.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "1989-06-18T00:00:00",
                "link": "https://ieeexplore.ieee.org/document/118669",
                "reference": "Invariant image recognition using a multi-network neural model",
                "organization": "Complutense University of Madrid",
                "parameters": null,
                "notable_model": true,
                "country": "Spain",
                "model_accessibility": "Unknown",
                "year": 1989,
                "era": "Pre deep learning era",
                "country_top8": "Other",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Invariant image recognition - 2.70e+10 FLOPs"
            },
            {
                "model": "MLN-ASR",
                "training_compute_(flop)": 296425000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 12600.0,
                "training_time_(hours)": 0.1,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Audio",
                "organization_categorization": "Academia",
                "publication_date": "1988-08-01T00:00:00",
                "link": "https://aaai.org/papers/00734-aaai88-130-data-driven-execution-of-multi-layered-networks-for-automatic-speech-recognition/",
                "reference": "Data-Driven Execution of Multi-Layered Networks for Automatic Speech Recognition",
                "organization": "McGill University",
                "parameters": 10000.0,
                "notable_model": true,
                "country": "Canada",
                "model_accessibility": "Unknown",
                "year": 1988,
                "era": "Pre deep learning era",
                "country_top8": "Canada",
                "domain_top4": "Other",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "MLN-ASR - 2.96e+08 FLOPs"
            },
            {
                "model": "Translation-invariant MLP",
                "training_compute_(flop)": 18032947200.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 160.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": null,
                "organization_categorization": "Academia",
                "publication_date": "1987-06-15T00:00:00",
                "link": "https://www.cs.toronto.edu/~hinton/absps/parle.pdf",
                "reference": "Learning Translation Invariant Recognition in a Massively Parallel Network",
                "organization": "Carnegie Mellon University (CMU)",
                "parameters": 816.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 1987,
                "era": "Pre deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Translation-invariant MLP - 1.80e+10 FLOPs"
            },
            {
                "model": "NetTalk (transcription)",
                "training_compute_(flop)": 28328002560.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 5120.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Audio",
                "organization_categorization": "Academia",
                "publication_date": "1987-06-06T00:00:00",
                "link": "http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=03A3D3EDF0BAF35405ABCF083411B55E?doi=10.1.1.154.7012&rep=rep1&type=pdf",
                "reference": "Parallel Networks that Learn to Pronounce English Text",
                "organization": "Princeton University",
                "parameters": 18629.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 1987,
                "era": "Pre deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "NetTalk (transcription) - 2.83e+10 FLOPs"
            },
            {
                "model": "NetTalk (dictionary)",
                "training_compute_(flop)": 27664065000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 5000.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Audio",
                "organization_categorization": "Academia",
                "publication_date": "1987-06-06T00:00:00",
                "link": "http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=03A3D3EDF0BAF35405ABCF083411B55E?doi=10.1.1.154.7012&rep=rep1&type=pdf",
                "reference": "Parallel Networks that Learn to Pronounce English Text",
                "organization": "Princeton University",
                "parameters": 18629.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 1987,
                "era": "Pre deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "NetTalk (dictionary) - 2.77e+10 FLOPs"
            },
            {
                "model": "Back-propagation",
                "training_compute_(flop)": 673920000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 104.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Mathematics",
                "organization_categorization": "Academia",
                "publication_date": "1986-10-01T00:00:00",
                "link": "https://www.semanticscholar.org/paper/Learning-representations-by-back-propagating-errors-Rumelhart-Hinton/052b1d8ce63b07fec3de9dbb583772d860b7c769",
                "reference": "Learning representations by back-propagating errors",
                "organization": "University of California San Diego,Carnegie Mellon University (CMU)",
                "parameters": 720.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 1986,
                "era": "Pre deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Back-propagation - 6.74e+08 FLOPs"
            },
            {
                "model": "Distributed representation NN",
                "training_compute_(flop)": 388800000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 100.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": null,
                "organization_categorization": "Academia",
                "publication_date": "1986-08-15T00:00:00",
                "link": "https://www.cs.toronto.edu/~hinton/absps/families.pdf",
                "reference": "Learning distributed representations of concepts.",
                "organization": "Carnegie Mellon University (CMU)",
                "parameters": 432.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 1986,
                "era": "Pre deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Distributed representation NN - 3.89e+08 FLOPs"
            },
            {
                "model": "ASE+ACE",
                "training_compute_(flop)": 324000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 500000.0,
                "training_time_(hours)": 2.8,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Robotics",
                "organization_categorization": "Academia",
                "publication_date": "1983-09-01T00:00:00",
                "link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6313077",
                "reference": "Neuronlike adaptive elements that can solve difficult learning control problems",
                "organization": "University of Massachusetts Amherst",
                "parameters": 324.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 1983,
                "era": "Pre deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "ASE+ACE - 3.24e+08 FLOPs"
            },
            {
                "model": "Neocognitron",
                "training_compute_(flop)": 273738240.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 5.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "1980-04-01T00:00:00",
                "link": "https://link.springer.com/article/10.1007/BF00344251",
                "reference": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position",
                "organization": "NHK Broadcasting Science Research Laboratories",
                "parameters": 1140576.0,
                "notable_model": true,
                "country": "Japan",
                "model_accessibility": "Unknown",
                "year": 1980,
                "era": "Pre deep learning era",
                "country_top8": "Japan",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Neocognitron - 2.74e+08 FLOPs"
            },
            {
                "model": "Cognitron",
                "training_compute_(flop)": 5184000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 5.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": null,
                "organization_categorization": "Industry",
                "publication_date": "1975-09-01T00:00:00",
                "link": "https://link.springer.com/article/10.1007%2FBF00342633",
                "reference": "Cognitron: a self-organizing multilayered neural network",
                "organization": "Biological Cybernetics",
                "parameters": 21600.0,
                "notable_model": true,
                "country": "Japan",
                "model_accessibility": "Unknown",
                "year": 1975,
                "era": "Pre deep learning era",
                "country_top8": "Japan",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Cognitron - 5.18e+06 FLOPs"
            },
            {
                "model": "LTE speaker verification system",
                "training_compute_(flop)": 105917060.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 417.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Audio",
                "organization_categorization": "Industry",
                "publication_date": "1966-11-01T00:00:00",
                "link": "https://pubs.aip.org/asa/jasa/article-abstract/40/5/966/754180/Experimental-Studies-in-Speaker-Verification-Using?redirectedFrom=fulltext",
                "reference": "Experimental Studies in Speaker Verification, Using an Adaptive System",
                "organization": "IBM",
                "parameters": 2061.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 1966,
                "era": "Pre deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "LTE speaker verification system - 1.06e+08 FLOPs"
            },
            {
                "model": "Heuristic Reinforcement Learning",
                "training_compute_(flop)": 1080000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 3.0,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Robotics",
                "organization_categorization": "Academia",
                "publication_date": "1965-10-01T00:00:00",
                "link": "https://ieeexplore.ieee.org/document/1098193",
                "reference": "A heuristic approach to reinforcement learning control systems",
                "organization": "Purdue University",
                "parameters": null,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 1965,
                "era": "Pre deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Heuristic Reinforcement Learning - 1.08e+06 FLOPs"
            },
            {
                "model": "Print Recognition Logic",
                "training_compute_(flop)": 22500000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": 2.5,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Industry",
                "publication_date": "1963-01-01T00:00:00",
                "link": "https://ieeexplore.ieee.org/document/5392331",
                "reference": "Computer-Automated Design of Multifont Print Recognition Logic",
                "organization": "IBM",
                "parameters": null,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 1963,
                "era": "Pre deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Print Recognition Logic - 2.25e+07 FLOPs"
            },
            {
                "model": "Linear Decision Functions",
                "training_compute_(flop)": 1559250.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 500.0,
                "training_time_(hours)": 0.4375,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Mathematics",
                "organization_categorization": "Industry",
                "publication_date": "1962-06-01T00:00:00",
                "link": "https://ieeexplore.ieee.org/document/4066882?denied=",
                "reference": "Linear Decision Functions, with Application to Pattern Recognition",
                "organization": "Bell Laboratories",
                "parameters": null,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 1962,
                "era": "Pre deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Linear Decision Functions - 1.56e+06 FLOPs"
            },
            {
                "model": "ADALINE",
                "training_compute_(flop)": 6600.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 100.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "1960-06-30T00:00:00",
                "link": "https://isl.stanford.edu/~widrow/papers/c1960adaptiveswitching.pdf",
                "reference": "Adaptive switching circuits",
                "organization": "Stanford University",
                "parameters": 17.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 1960,
                "era": "Pre deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "ADALINE - 6.60e+03 FLOPs"
            },
            {
                "model": "Perceptron (1960)",
                "training_compute_(flop)": 720000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 100.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Vision",
                "organization_categorization": "Academia",
                "publication_date": "1960-03-30T00:00:00",
                "link": "https://www.semanticscholar.org/paper/Perceptron-Simulation-Experiments-Rosenblatt/ae76ce1ba27ac29addce4aab93b927e9bc7f7c67",
                "reference": "Perceptron Simulation Experiments",
                "organization": "Cornell Aeronautical Laboratory",
                "parameters": 1000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 1960,
                "era": "Pre deep learning era",
                "country_top8": "United States",
                "domain_top4": "Vision",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Perceptron (1960) - 7.20e+08 FLOPs"
            },
            {
                "model": "Samuel Neural Checkers",
                "training_compute_(flop)": 428400000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Games",
                "organization_categorization": "Industry",
                "publication_date": "1959-07-01T00:00:00",
                "link": "https://ieeexplore.ieee.org/abstract/document/5392560",
                "reference": "Some studies in machine learning using the game of checkers",
                "organization": "IBM",
                "parameters": 16.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 1959,
                "era": "Pre deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Samuel Neural Checkers - 4.28e+08 FLOPs"
            },
            {
                "model": "Pandemonium (morse)",
                "training_compute_(flop)": 600000000.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": null,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Language",
                "organization_categorization": "Academia",
                "publication_date": "1959-02-01T00:00:00",
                "link": "https://aitopics.org/doc/classics:504E1BAC/",
                "reference": "Pandemonium: A Paradigm for Learning",
                "organization": "Massachusetts Institute of Technology (MIT)",
                "parameters": null,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 1959,
                "era": "Pre deep learning era",
                "country_top8": "United States",
                "domain_top4": "Language",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Pandemonium (morse) - 6.00e+08 FLOPs"
            },
            {
                "model": "Perceptron Mark I",
                "training_compute_(flop)": 694894.9377361819,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 100.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": null,
                "organization_categorization": "Academia",
                "publication_date": "1957-01-01T00:00:00",
                "link": "https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf",
                "reference": "The Perceptron\u2014a perceiving and recognizing automaton",
                "organization": "Cornell Aeronautical Laboratory,Cornell University",
                "parameters": 1000.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 1957,
                "era": "Pre deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Academia",
                "access_group": "Closed",
                "model_and_compute": "Perceptron Mark I - 6.95e+05 FLOPs"
            },
            {
                "model": "Theseus",
                "training_compute_(flop)": 40.0,
                "training_power_draw_(w)": null,
                "training_dataset_size_(gradients)": 40.0,
                "training_time_(hours)": null,
                "training_compute_cost_(2023_usd)": null,
                "domain_group": "Robotics",
                "organization_categorization": "Industry",
                "publication_date": "1950-07-02T00:00:00",
                "link": "https://www.technologyreview.com/2018/12/19/138508/mighty-mouse/",
                "reference": "Mighty Mouse",
                "organization": "Bell Laboratories",
                "parameters": 40.0,
                "notable_model": true,
                "country": "United States",
                "model_accessibility": "Unknown",
                "year": 1950,
                "era": "Pre deep learning era",
                "country_top8": "United States",
                "domain_top4": "Other",
                "org_top5": "Industry",
                "access_group": "Closed",
                "model_and_compute": "Theseus - 4.00e+01 FLOPs"
            }
        ]
    }
}